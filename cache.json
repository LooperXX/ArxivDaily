{"2022-07-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2207.07087v1","updated":"2022-07-14T17:40:00Z","published":"2022-07-14T17:40:00Z","title":"Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated\n  Neural Text Retrievers","summary":"  Prompt tuning attempts to update few task-specific parameters in pre-trained\nmodels. It has achieved comparable performance to fine-tuning of the full\nparameter set on both language understanding and generation tasks. In this\nwork, we study the problem of prompt tuning for neural text retrievers. We\nintroduce parameter-efficient prompt tuning for text retrieval across\nin-domain, cross-domain, and cross-topic settings. Through an extensive\nanalysis, we show that the strategy can mitigate the two issues --\nparameter-inefficiency and weak generalizability -- faced by fine-tuning based\nretrieval methods. Notably, it can significantly improve the out-of-domain\nzero-shot generalization of the retrieval models. By updating only 0.1% of the\nmodel parameters, the prompt tuning strategy can help retrieval models achieve\nbetter generalization performance than traditional methods in which all\nparameters are updated. Finally, to facilitate research on retrievers'\ncross-topic generalizability, we curate and release an academic retrieval\ndataset with 18K query-results pairs in 87 topics, making it the largest\ntopic-specific one to date.\n","authors":["Weng Lam Tam","Xiao Liu","Kaixuan Ji","Lilong Xue","Xingjian Zhang","Yuxiao Dong","Jiahua Liu","Maodi Hu","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2207.07087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07061v1","updated":"2022-07-14T17:00:19Z","published":"2022-07-14T17:00:19Z","title":"Confident Adaptive Language Modeling","summary":"  Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.\n","authors":["Tal Schuster","Adam Fisch","Jai Gupta","Mostafa Dehghani","Dara Bahri","Vinh Q. Tran","Yi Tay","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2207.07061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07051v1","updated":"2022-07-14T16:51:09Z","published":"2022-07-14T16:51:09Z","title":"Language models show human-like content effects on reasoning","summary":"  Abstract reasoning is a key ability for an intelligent system. Large language\nmodels achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect, and depends on our knowledge and beliefs about the content of the\nreasoning problem. For example, humans reason much more reliably about logical\nrules that are grounded in everyday situations than arbitrary rules about\nabstract attributes. The training experiences of language models similarly\nendow them with prior expectations that reflect human knowledge and beliefs. We\ntherefore hypothesized that language models would show human-like content\neffects on abstract reasoning problems. We explored this hypothesis across\nthree logical reasoning tasks: natural language inference, judging the logical\nvalidity of syllogisms, and the Wason selection task (Wason, 1968). We find\nthat state of the art large language models (with 7 or 70 billion parameters;\nHoffman et al., 2022) reflect many of the same patterns observed in humans\nacross these tasks -- like humans, models reason more effectively about\nbelievable situations than unrealistic or abstract ones. Our findings have\nimplications for understanding both these cognitive effects, and the factors\nthat contribute to language model performance.\n","authors":["Ishita Dasgupta","Andrew K. Lampinen","Stephanie C. Y. Chan","Antonia Creswell","Dharshan Kumaran","James L. McClelland","Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2207.07051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07036v1","updated":"2022-07-14T16:21:33Z","published":"2022-07-14T16:21:33Z","title":"A Single Self-Supervised Model for Many Speech Modalities Enables\n  Zero-Shot Modality Transfer","summary":"  While audio-visual speech models can yield superior performance and\nrobustness compared to audio-only models, their development and adoption are\nhindered by the lack of labeled and unlabeled audio-visual data and the cost to\ndeploy one model per modality. In this paper, we present u-HuBERT, a\nself-supervised pre-training framework that can leverage both multimodal and\nunimodal speech with a unified masked cluster prediction objective. By\nutilizing modality dropout during pre-training, we demonstrate that a single\nfine-tuned model can achieve performance on par or better than the\nstate-of-the-art modality-specific models. Moreover, our model fine-tuned only\non audio can perform well with audio-visual and visual speech input, achieving\nzero-shot modality generalization for speech recognition and speaker\nverification. In particular, our single model yields 1.2%/1.4%/27.2% speech\nrecognition word error rate on LRS3 with audio-visual/audio/visual input.\n","authors":["Wei-Ning Hsu","Bowen Shi"],"pdf_url":"https://arxiv.org/pdf/2207.07036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07025v1","updated":"2022-07-14T15:58:06Z","published":"2022-07-14T15:58:06Z","title":"Learning to translate by learning to communicate","summary":"  We formulate and test a technique to use Emergent Communication (EC) with a\npretrained multilingual model to improve on modern Unsupervised NMT systems,\nespecially for low-resource languages. It has been argued that the currently\ndominant paradigm in NLP of pretraining on text-only corpora will not yield\nrobust natural language understanding systems, and the need for grounded,\ngoal-oriented, and interactive language learning has been highlighted. In our\napproach, we embed a modern multilingual model (mBART, Liu et. al. 2020) into\nan EC image-reference game, in which the model is incentivized to use\nmultilingual generations to accomplish a vision-grounded task, with the\nhypothesis that this will align multiple languages to a shared task space. We\npresent two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022), one\nof which outperforms a backtranslation-based baseline in 6/8 translation\nsettings, and proves especially beneficial for the very low-resource languages\nof Nepali and Sinhala.\n","authors":["C. M. Downey","Leo Z. Liu","Xuhui Zhou","Shane Steinert-Threlkeld"],"pdf_url":"https://arxiv.org/pdf/2207.07025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06991v1","updated":"2022-07-14T15:20:36Z","published":"2022-07-14T15:20:36Z","title":"Language Modelling with Pixels","summary":"  Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches, instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust to noisy text inputs than BERT,\nfurther confirming the benefits of modelling language with pixels.\n","authors":["Phillip Rust","Jonas F. Lotz","Emanuele Bugliarello","Elizabeth Salesky","Miryam de Lhoneux","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2207.06991v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2203.11670v2","updated":"2022-07-14T15:03:07Z","published":"2022-03-22T12:41:55Z","title":"Improving Meta-learning for Low-resource Text Classification and\n  Generation via Memory Imitation","summary":"  Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.\n","authors":["Yingxiu Zhao","Zhiliang Tian","Huaxiu Yao","Yinhe Zheng","Dongkyu Lee","Yiping Song","Jian Sun","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11670v2.pdf","comment":"ACL 2022 Camera Ready; modified emails"},{"id":"http://arxiv.org/abs/2207.06966v1","updated":"2022-07-14T14:51:50Z","published":"2022-07-14T14:51:50Z","title":"Scene Text Recognition with Permuted Autoregressive Sequence Models","summary":"  Context-aware STR methods typically use internal autoregressive (AR) language\nmodels (LM). Inherent limitations of AR models motivated two-stage methods\nwhich employ an external LM. The conditional independence of the external LM on\nthe input image may cause it to erroneously rectify correct predictions,\nleading to significant inefficiencies. Our method, PARSeq, learns an ensemble\nof internal AR LMs with shared weights using Permutation Language Modeling. It\nunifies context-free non-AR and context-aware AR inference, and iterative\nrefinement using bidirectional context. Using synthetic training data, PARSeq\nachieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and\nmore challenging datasets. It establishes new SOTA results (96.0% accuracy)\nwhen trained on real data. PARSeq is optimal on accuracy vs parameter count,\nFLOPS, and latency because of its simple, unified structure and parallel token\nprocessing. Due to its extensive use of attention, it is robust on\narbitrarily-oriented text which is common in real-world images. Code,\npretrained weights, and data are available at: https://github.com/baudm/parseq.\n","authors":["Darwin Bautista","Rowel Atienza"],"pdf_url":"https://arxiv.org/pdf/2207.06966v1.pdf","comment":"Accepted at the 17th European Conference on Computer Vision (ECCV\n  2022)"},{"id":"http://arxiv.org/abs/2207.06960v1","updated":"2022-07-14T14:39:30Z","published":"2022-07-14T14:39:30Z","title":"Forming Trees with Treeformers","summary":"  Popular models such as Transformers and LSTMs use tokens as its unit of\ninformation. That is, each token is encoded into a vector representation, and\nthose vectors are used directly in a computation. However, humans frequently\nconsider spans of tokens (i.e., phrases) instead of their constituent tokens.\nIn this paper we introduce Treeformer, an architecture inspired by the CKY\nalgorithm and Transformer which learns a composition operator and pooling\nfunction in order to construct hierarchical encodings for phrases and\nsentences. Our extensive experiments demonstrate the benefits of incorporating\na hierarchical structure into the Transformer, and show significant\nimprovements compared to a baseline Transformer in machine translation,\nabstractive summarization, and various natural language understanding tasks.\n","authors":["Nilay Patel","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2207.06960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06897v1","updated":"2022-07-14T13:26:03Z","published":"2022-07-14T13:26:03Z","title":"Beware the Rationalization Trap! When Language Model Explainability\n  Diverges from our Mental Models of Language","summary":"  Language models learn and represent language differently than humans; they\nlearn the form and not the meaning. Thus, to assess the success of language\nmodel explainability, we need to consider the impact of its divergence from a\nuser's mental model of language. In this position paper, we argue that in order\nto avoid harmful rationalization and achieve truthful understanding of language\nmodels, explanation processes must satisfy three main conditions: (1)\nexplanations have to truthfully represent the model behavior, i.e., have a high\nfidelity; (2) explanations must be complete, as missing information distorts\nthe truth; and (3) explanations have to take the user's mental model into\naccount, progressively verifying a person's knowledge and adapting their\nunderstanding. We introduce a decision tree model to showcase potential reasons\nwhy current explanations fail to reach their objectives. We further emphasize\nthe need for human-centered design to explain the model from multiple\nperspectives, progressively adapting explanations to changing user\nexpectations.\n","authors":["Rita Sevastjanova","Mennatallah El-Assady"],"pdf_url":"https://arxiv.org/pdf/2207.06897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09509v4","updated":"2022-07-14T13:04:29Z","published":"2022-03-17T17:57:56Z","title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and\n  Implicit Hate Speech Detection","summary":"  Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset. Our code and data can be found at\nhttps://github.com/microsoft/ToxiGen.\n","authors":["Thomas Hartvigsen","Saadia Gabriel","Hamid Palangi","Maarten Sap","Dipankar Ray","Ece Kamar"],"pdf_url":"https://arxiv.org/pdf/2203.09509v4.pdf","comment":"Published as a long paper at ACL 2022. Code:\n  https://github.com/microsoft/TOXIGEN"},{"id":"http://arxiv.org/abs/2207.06882v1","updated":"2022-07-14T13:00:41Z","published":"2022-07-14T13:00:41Z","title":"Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically\n  Ambiguous Settings for Low Resource Languages","summary":"  We leverage pre-trained language models to solve the task of complex NER for\ntwo low-resource languages: Chinese and Spanish. We use the technique of Whole\nWord Masking(WWM) to boost the performance of masked language modeling\nobjective on large and unsupervised corpora. We experiment with multiple neural\nnetwork architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on\ntop of a fine-tuned BERT layer. All our models outperform the baseline by a\nsignificant margin and our best performing model obtains a competitive position\non the evaluation leaderboard for the blind test set.\n","authors":["Amit Pandey","Swayatta Daw","Narendra Babu Unnam","Vikram Pudi"],"pdf_url":"https://arxiv.org/pdf/2207.06882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06881v1","updated":"2022-07-14T13:00:22Z","published":"2022-07-14T13:00:22Z","title":"Recurrent Memory Transformer","summary":"  Transformer-based models show their effectiveness across multiple domains and\ntasks. The self-attention allows to combine information from all sequence\nelements into context-aware representations. However, global and local\ninformation has to be stored mostly in the same element-wise representations.\nMoreover, the length of an input sequence is limited by quadratic computational\ncomplexity of self-attention.\n  In this work, we propose and study a memory-augmented segment-level recurrent\nTransformer (Recurrent Memory Transformer). Memory allows to store and process\nlocal and global information as well as to pass information between segments of\nthe long sequence with the help of recurrence. We implement a memory mechanism\nwith no changes to Transformer model by adding special memory tokens to the\ninput or output sequence. Then Transformer is trained to control both memory\noperations and sequence representations processing.\n  Results of experiments show that our model performs on par with the\nTransformer-XL on language modeling for smaller memory sizes and outperforms it\nfor tasks that require longer sequence processing. We show that adding memory\ntokens to Tr-XL is able to improve it performance. This makes Recurrent Memory\nTransformer a promising architecture for applications that require learning of\nlong-term dependencies and general purpose in memory processing, such as\nalgorithmic tasks and reasoning.\n","authors":["Aydar Bulatov","Yuri Kuratov","Mikhail S. Burtsev"],"pdf_url":"https://arxiv.org/pdf/2207.06881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.06197v3","updated":"2022-07-14T12:52:50Z","published":"2021-07-25T09:10:10Z","title":"A comparison of latent semantic analysis and correspondence analysis of\n  document-term matrices","summary":"  Latent semantic analysis (LSA) and correspondence analysis (CA) are two\ntechniques that use a singular value decomposition (SVD) for dimensionality\nreduction. LSA has been extensively used to obtain low-dimensional\nrepresentations that capture relationships among documents and terms. In this\narticle, we present a theoretical analysis and comparison of the two techniques\nin the context of document-term matrices. We show that CA has some attractive\nproperties as compared to LSA, for instance that effects of margins arising\nfrom differing document-lengths and term-frequencies are effectively\neliminated, so that the CA solution is optimally suited to focus on\nrelationships among documents and terms. A unifying framework is proposed that\nincludes both CA and LSA as special cases. We empirically compare CA to various\nLSA based methods on text categorization in English and authorship attribution\non historical Dutch texts, and find that CA performs significantly better. We\nalso apply CA to a long-standing question regarding the authorship of the Dutch\nnational anthem Wilhelmus and provide further support that it can be attributed\nto the author Datheen, amongst several contenders.\n","authors":["Qianqian Qi","David J. Hessen","Tejaswini Deoskar","Peter G. M. van der Heijden"],"pdf_url":"https://arxiv.org/pdf/2108.06197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06872v1","updated":"2022-07-14T12:49:15Z","published":"2022-07-14T12:49:15Z","title":"Data Augmentation for Low-Resource Quechua ASR Improvement","summary":"  Automatic Speech Recognition (ASR) is a key element in new services that\nhelps users to interact with an automated system. Deep learning methods have\nmade it possible to deploy systems with word error rates below 5% for ASR of\nEnglish. However, the use of these methods is only available for languages with\nhundreds or thousands of hours of audio and their corresponding transcriptions.\nFor the so-called low-resource languages to speed up the availability of\nresources that can improve the performance of their ASR systems, methods of\ncreating new resources on the basis of existing ones are being investigated. In\nthis paper we describe our data augmentation approach to improve the results of\nASR models for low-resource and agglutinative languages. We carry out\nexperiments developing an ASR for Quechua using the wav2letter++ model. We\nreduced WER by 8.73% through our approach to the base model. The resulting ASR\nmodel obtained 22.75% WER and was trained with 99 hours of original resources\nand 99 hours of synthetic data obtained with a combination of text augmentation\nand synthetic speech generati\n","authors":["Rodolfo Zevallos","Nuria Bel","Guillermo Cámbara","Mireia Farrús","Jordi Luque"],"pdf_url":"https://arxiv.org/pdf/2207.06872v1.pdf","comment":"Accepted to INTERSPEECH 2022. arXiv admin note: substantial text\n  overlap with arXiv:2204.00291"},{"id":"http://arxiv.org/abs/2207.06867v1","updated":"2022-07-14T12:43:36Z","published":"2022-07-14T12:43:36Z","title":"Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic\n  Knowledge Distillation of Self-Supervised Speech Models","summary":"  Self-supervised learning (SSL) is seen as a very promising approach with high\nperformance for several speech downstream tasks. Since the parameters of SSL\nmodels are generally so large that training and inference require a lot of\nmemory and computational cost, it is desirable to produce compact SSL models\nwithout a significant performance degradation by applying compression methods\nsuch as knowledge distillation (KD). Although the KD approach is able to shrink\nthe depth and/or width of SSL model structures, there has been little research\non how varying the depth and width impacts the internal representation of the\nsmall-footprint model. This paper provides an empirical study that addresses\nthe question. We investigate the performance on SUPERB while varying the\nstructure and KD methods so as to keep the number of parameters constant; this\nallows us to analyze the contribution of the representation introduced by\nvarying the model architecture. Experiments demonstrate that a certain depth is\nessential for solving content-oriented tasks (e.g. automatic speech\nrecognition) accurately, whereas a certain width is necessary for achieving\nhigh performance on several speaker-oriented tasks (e.g. speaker\nidentification). Based on these observations, we identify, for SUPERB, a more\ncompressed model with better performance than previous studies.\n","authors":["Takanori Ashihara","Takafumi Moriya","Kohei Matsuura","Tomohiro Tanaka"],"pdf_url":"https://arxiv.org/pdf/2207.06867v1.pdf","comment":"Accepted at Interspeech 2022"},{"id":"http://arxiv.org/abs/2206.08823v2","updated":"2022-07-14T12:42:15Z","published":"2022-06-17T15:04:05Z","title":"Language with Vision: a Study on Grounded Word and Sentence Embeddings","summary":"  Language grounding to vision is an active field of research aiming to enrich\ntext-based representations of word meanings by leveraging perceptual knowledge\nfrom vision. Despite many attempts at language grounding, it is still unclear\nhow to effectively inject visual knowledge into the word embeddings of a\nlanguage in such a way that a proper balance of textual and visual knowledge is\nmaintained. Some common concerns are the following. Is visual grounding\nbeneficial for abstract words or is its contribution only limited to concrete\nwords? What is the optimal way of bridging the gap between text and vision? How\nmuch do we gain by visually grounding textual embeddings? The present study\naddresses these questions by proposing a simple yet very effective grounding\napproach for pre-trained word embeddings. Our model aligns textual embeddings\nwith vision while largely preserving the distributional statistics that\ncharacterize word use in text corpora. By applying a learned alignment, we are\nable to generate visually grounded embeddings for unseen words, including\nabstract words. A series of evaluations on word similarity benchmarks shows\nthat visual grounding is beneficial not only for concrete words, but also for\nabstract words. We also show that our method for visual grounding offers\nadvantages for contextualized embeddings, but only when these are trained on\ncorpora of relatively modest size. Code and grounded embeddings for English are\navailable at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.\n","authors":["Hassan Shahmohammadi","Maria Heitmeier","Elnaz Shafaei-Bajestan","Hendrik P. A. Lensch","Harald Baayen"],"pdf_url":"https://arxiv.org/pdf/2206.08823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.12835v4","updated":"2022-07-14T12:11:46Z","published":"2022-04-27T10:39:52Z","title":"Learning to Parallelize in a Shared-Memory Environment with Transformers","summary":"  In past years, the world has switched to many-core and multi-core shared\nmemory architectures. As a result, there is a growing need to utilize these\narchitectures by introducing shared memory parallelization schemes to software\napplications. OpenMP is the most comprehensive API that implements such\nschemes, characterized by a readable interface. Nevertheless, introducing\nOpenMP into code is challenging due to pervasive pitfalls in management of\nparallel shared memory. To facilitate the performance of this task, many\nsource-to-source (S2S) compilers have been created over the years, tasked with\ninserting OpenMP directives into code automatically. In addition to having\nlimited robustness to their input format, these compilers still do not achieve\nsatisfactory coverage and precision in locating parallelizable code and\ngenerating appropriate directives. In this work, we propose leveraging recent\nadvances in ML techniques, specifically in natural language processing (NLP),\nto replace S2S compilers altogether. We create a database (corpus), Open-OMP,\nspecifically for this goal. Open-OMP contains over 28,000 code snippets, half\nof which contain OpenMP directives while the other half do not need\nparallelization at all with high probability. We use the corpus to train\nsystems to automatically classify code segments in need of parallelization, as\nwell as suggest individual OpenMP clauses. We train several transformer models,\nnamed PragFormer, for these tasks, and show that they outperform\nstatistically-trained baselines and automatic S2S parallelization compilers in\nboth classifying the overall need for an OpenMP directive and the introduction\nof private and reduction clauses.\n  Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.\n","authors":["Re'em Harel","Yuval Pinter","Gal Oren"],"pdf_url":"https://arxiv.org/pdf/2204.12835v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06839v1","updated":"2022-07-14T11:53:04Z","published":"2022-07-14T11:53:04Z","title":"Neural Data-to-Text Generation Based on Small Datasets: Comparing the\n  Added Value of Two Semi-Supervised Learning Approaches on Top of a Large\n  Language Model","summary":"  This study discusses the effect of semi-supervised learning in combination\nwith pretrained language models for data-to-text generation. It is not known\nwhether semi-supervised learning is still helpful when a large-scale language\nmodel is also supplemented. This study aims to answer this question by\ncomparing a data-to-text system only supplemented with a language model, to two\ndata-to-text systems that are additionally enriched by a data augmentation or a\npseudo-labeling semi-supervised learning approach.\n  Results show that semi-supervised learning results in higher scores on\ndiversity metrics. In terms of output quality, extending the training set of a\ndata-to-text system with a language model using the pseudo-labeling approach\ndid increase text quality scores, but the data augmentation approach yielded\nsimilar scores to the system without training set extension. These results\nindicate that semi-supervised learning approaches can bolster output quality\nand diversity, even when a language model is also present.\n","authors":["Chris van der Lee","Thiago Castro Ferreira","Chris Emmery","Travis Wiltshire","Emiel Krahmer"],"pdf_url":"https://arxiv.org/pdf/2207.06839v1.pdf","comment":"22 pages (excluding bibliography and appendix)"},{"id":"http://arxiv.org/abs/2207.06814v1","updated":"2022-07-14T10:48:42Z","published":"2022-07-14T10:48:42Z","title":"BERTIN: Efficient Pre-Training of a Spanish Language Model using\n  Perplexity Sampling","summary":"  The pre-training of large language models usually requires massive amounts of\nresources, both in terms of computation and data. Frequently used web sources\nsuch as Common Crawl might contain enough noise to make this pre-training\nsub-optimal. In this work, we experiment with different sampling methods from\nthe Spanish version of mC4, and present a novel data-centric technique which we\nname $\\textit{perplexity sampling}$ that enables the pre-training of language\nmodels in roughly half the amount of steps and using one fifth of the data. The\nresulting models are comparable to the current state-of-the-art, and even\nachieve better results for certain tasks. Our work is proof of the versatility\nof Transformers, and paves the way for small teams to train their models on a\nlimited budget. Our models are available at this\n$\\href{https://huggingface.co/bertin-project}{URL}$.\n","authors":["Javier de la Rosa","Eduardo G. Ponferrada","Paulo Villegas","Pablo Gonzalez de Prado Salas","Manu Romero","Marıa Grandury"],"pdf_url":"https://arxiv.org/pdf/2207.06814v1.pdf","comment":"Published at Procesamiento del Lenguaje Natural"},{"id":"http://arxiv.org/abs/2207.06803v1","updated":"2022-07-14T10:31:21Z","published":"2022-07-14T10:31:21Z","title":"FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform\n  Libraries","summary":"  Discrete Fourier Transform (DFT) libraries are one of the most critical\nsoftware components for scientific computing. Inspired by FFTW, a widely used\nlibrary for DFT HPC calculations, we apply compiler technologies for the\ndevelopment of HPC Fourier transform libraries. In this work, we introduce\nFFTc, a domain-specific language, based on Multi-Level Intermediate\nRepresentation (MLIR), for expressing Fourier Transform algorithms. We present\nthe initial design, implementation, and preliminary results of FFTc.\n","authors":["Yifei He","Artur Podobas","Måns I. Andersson","Stefano Markidis"],"pdf_url":"https://arxiv.org/pdf/2207.06803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.14192v2","updated":"2022-07-14T09:44:03Z","published":"2021-12-28T15:46:28Z","title":"Robust Security Analysis Based on Random Geometry Theory for\n  Satellite-Terrestrial-Vehicle Network","summary":"  Driven by B5G and 6G technologies, multi-network fusion is an indispensable\ntendency for future communications. In this paper, we focus on and analyze the\n\\emph{security performance} (SP) of the \\emph{satellite-terrestrial downlink\ntransmission} (STDT). Here, the STDT is composed of a satellite network and a\nvehicular network with a legitimate mobile receiver and an mobile eavesdropper\ndistributing. To theoretically analyze the SP of this system from the\nperspective of mobile terminals better, the random geometry theory is adopted,\nwhich assumes that both terrestrial vehicles are distributed stochastically in\none beam of the satellite. Furthermore, based on this theory, the closed-form\nanalytical expressions for two crucial and specific indicators in the STDT are\nderived, respectively, the secrecy outage probability and the ergodic secrecy\ncapacity. Additionally, several related variables restricting the SP of the\nSTDT are discussed, and specific schemes are presented to enhance the SP. Then,\nthe asymptotic property is investigated in the high signal-to-noise ratio\nscenario, and accurate and asymptotic closed-form expressions are given.\nFinally, simulation results show that, under the precondition of guaranteeing\nthe reliability of the STDT, the asymptotic solutions outperform the\ncorresponding accurate results significantly in the effectiveness.\n","authors":["Xudong Li","Ye Fan","Rugui Yao","Peng Wang","Nan Qi","Xiaoya Zuo"],"pdf_url":"https://arxiv.org/pdf/2112.14192v2.pdf","comment":"The theoretical analysis in the original manuscript is insufficient,\n  and the system model is not convincing. With the consideration of these\n  flaws, we decide to withdraw our work for further improvement"},{"id":"http://arxiv.org/abs/2207.06729v1","updated":"2022-07-14T08:27:17Z","published":"2022-07-14T08:27:17Z","title":"Open Terminology Management and Sharing Toolkit for Federation of\n  Terminology Databases","summary":"  Consolidated access to current and reliable terms from different subject\nfields and languages is necessary for content creators and translators.\nTerminology is also needed in AI applications such as machine translation,\nspeech recognition, information extraction, and other natural language\nprocessing tools. In this work, we facilitate standards-based sharing and\nmanagement of terminology resources by providing an open terminology management\nsolution - the EuroTermBank Toolkit. It allows organisations to manage and\nsearch their terms, create term collections, and share them within and outside\nthe organisation by participating in the network of federated databases. The\ndata curated in the federated databases are automatically shared with\nEuroTermBank, the largest multilingual terminology resource in Europe, allowing\ntranslators and language service providers as well as researchers and students\nto access terminology resources in their most current version.\n","authors":["Andis Lagzdiņš","Uldis Siliņš","Mārcis Pinnis","Toms Bergmanis","Artūrs Vasiļevskis","Andrejs Vasiļjevs"],"pdf_url":"https://arxiv.org/pdf/2207.06729v1.pdf","comment":"LREC 2022"},{"id":"http://arxiv.org/abs/2207.06717v1","updated":"2022-07-14T07:59:45Z","published":"2022-07-14T07:59:45Z","title":"Layout-Aware Information Extraction for Document-Grounded Dialogue:\n  Dataset, Method and Demonstration","summary":"  Building document-grounded dialogue systems have received growing interest as\ndocuments convey a wealth of human knowledge and commonly exist in enterprises.\nWherein, how to comprehend and retrieve information from documents is a\nchallenging research problem. Previous work ignores the visual property of\ndocuments and treats them as plain text, resulting in incomplete modality. In\nthis paper, we propose a Layout-aware document-level Information Extraction\ndataset, LIE, to facilitate the study of extracting both structural and\nsemantic knowledge from visually rich documents (VRDs), so as to generate\naccurate responses in dialogue systems. LIE contains 62k annotations of three\nextraction tasks from 4,061 pages in product and official documents, becoming\nthe largest VRD-based information extraction dataset to the best of our\nknowledge. We also develop benchmark methods that extend the token-based\nlanguage model to consider layout features like humans. Empirical results show\nthat layout is critical for VRD-based extraction, and system demonstration also\nverifies that the extracted knowledge can help locate the answers that users\ncare about.\n","authors":["Zhenyu Zhang","Bowen Yu","Haiyang Yu","Tingwen Liu","Cheng Fu","Jingyang Li","Chengguang Tang","Jian Sun","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2207.06717v1.pdf","comment":"Accepted to ACM Multimedia (MM) Industry Track 2022"},{"id":"http://arxiv.org/abs/2207.06710v1","updated":"2022-07-14T07:38:13Z","published":"2022-07-14T07:38:13Z","title":"Overview of Abusive and Threatening Language Detection in Urdu at FIRE\n  2021","summary":"  With the growth of social media platform influence, the effect of their\nmisuse becomes more and more impactful. The importance of automatic detection\nof threatening and abusive language can not be overestimated. However, most of\nthe existing studies and state-of-the-art methods focus on English as the\ntarget language, with limited work on low- and medium-resource languages. In\nthis paper, we present two shared tasks of abusive and threatening language\ndetection for the Urdu language which has more than 170 million speakers\nworldwide. Both are posed as binary classification tasks where participating\nsystems are required to classify tweets in Urdu into two classes, namely: (i)\nAbusive and Non-Abusive for the first task, and (ii) Threatening and\nNon-Threatening for the second. We present two manually annotated datasets\ncontaining tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening\nand Non-Threatening. The abusive dataset contains 2400 annotated tweets in the\ntrain part and 1100 annotated tweets in the test part. The threatening dataset\ncontains 6000 annotated tweets in the train part and 3950 annotated tweets in\nthe test part. We also provide logistic regression and BERT-based baseline\nclassifiers for both tasks. In this shared task, 21 teams from six countries\nregistered for participation (India, Pakistan, China, Malaysia, United Arab\nEmirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is\nAbusive Language Detection and 9 teams submitted their runs for Subtask B,\nwhich is Threatening Language detection, and seven teams submitted their\ntechnical reports. The best performing system achieved an F1-score value of\n0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based\ntransformer model showed the best performance.\n","authors":["Maaz Amjad","Alisa Zhila","Grigori Sidorov","Andrey Labunets","Sabur Butta","Hamza Imam Amjad","Oxana Vitman","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2207.06710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06670v1","updated":"2022-07-14T05:50:16Z","published":"2022-07-14T05:50:16Z","title":"Two-Pass Low Latency End-to-End Spoken Language Understanding","summary":"  End-to-end (E2E) models are becoming increasingly popular for spoken language\nunderstanding (SLU) systems and are beginning to achieve competitive\nperformance to pipeline-based approaches. However, recent work has shown that\nthese models struggle to generalize to new phrasings for the same intent\nindicating that models cannot understand the semantic content of the given\nutterance. In this work, we incorporated language models pre-trained on\nunlabeled text data inside E2E-SLU frameworks to build strong semantic\nrepresentations. Incorporating both semantic and acoustic information can\nincrease the inference time, leading to high latency when deployed for\napplications like voice assistants. We developed a 2-pass SLU system that makes\nlow latency prediction using acoustic information from the few seconds of the\naudio in the first pass and makes higher quality prediction in the second pass\nby combining semantic and acoustic representations. We take inspiration from\nprior work on 2-pass end-to-end speech recognition systems that attends on both\naudio and first-pass hypothesis using a deliberation network. The proposed\n2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech\nCommands Challenge Set and SLURP dataset and reduces latency, thus improving\nuser experience. Our code and models are publicly available as part of the\nESPnet-SLU toolkit.\n","authors":["Siddhant Arora","Siddharth Dalmia","Xuankai Chang","Brian Yan","Alan Black","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2207.06670v1.pdf","comment":"INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2108.09416v2","updated":"2022-07-14T03:28:40Z","published":"2021-08-21T01:31:03Z","title":"2020 U.S. presidential election in swing states: Gender differences in\n  Twitter conversations","summary":"  Social media is commonly used by the public during election campaigns to\nexpress their opinions regarding different issues. Among various social media\nchannels, Twitter provides an efficient platform for researchers and\npoliticians to explore public opinion regarding a wide range of topics such as\nthe economy and foreign policy. Current literature mainly focuses on analyzing\nthe content of tweets without considering the gender of users. This research\ncollects and analyzes a large number of tweets and uses computational, human\ncoding, and statistical analyses to identify topics in more than 300,000 tweets\nposted during the 2020 U.S. presidential election and to compare female and\nmale users regarding the average weight of the discussed topics. Our findings\nare based upon a wide range of topics, such as tax, climate change, and the\nCOVID-19 pandemic. Out of the topics, there exists a significant difference\nbetween female and male users for more than 70% of topics.\n","authors":["Amir Karami","Spring B. Clark","Anderson Mackenzie","Dorathea Lee","Michael Zhu","Hannah R. Boyajieff","Bailey Goldschmidt"],"pdf_url":"https://arxiv.org/pdf/2108.09416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11211v4","updated":"2022-07-14T02:45:27Z","published":"2022-05-23T11:41:02Z","title":"Non-Parametric Domain Adaptation for End-to-End Speech Translation","summary":"  End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.\n","authors":["Yichao Du","Weizhi Wang","Zhirui Zhang","Boxing Chen","Tong Xu","Jun Xie","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2205.11211v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13696v2","updated":"2022-07-14T01:30:07Z","published":"2022-03-25T15:04:51Z","title":"Speech-enhanced and Noise-aware Networks for Robust Speech Recognition","summary":"  Compensation for channel mismatch and noise interference is essential for\nrobust automatic speech recognition. Enhanced speech has been introduced into\nthe multi-condition training of acoustic models to improve their generalization\nability. In this paper, a noise-aware training framework based on two cascaded\nneural structures is proposed to jointly optimize speech enhancement and speech\nrecognition. The feature enhancement module is composed of a multi-task\nautoencoder, where noisy speech is decomposed into clean speech and noise. By\nconcatenating its enhanced, noise-aware, and noisy features for each frame, the\nacoustic-modeling module maps each feature-augmented frame into a triphone\nstate by optimizing the lattice-free maximum mutual information and cross\nentropy between the predicted and actual state sequences. On top of the\nfactorized time delay neural network (TDNN-F) and its convolutional variant\n(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error\nrate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared\nwith the best existing systems that use bigram and trigram language models for\ndecoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction\nof 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based\nsystem also outperforms the baseline CNN-TDNNF system on the AMI task.\n","authors":["Hung-Shin Lee","Pin-Yuan Chen","Yao-Fei Cheng","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2203.13696v2.pdf","comment":"submitted to ISCSLP 2022"},{"id":"http://arxiv.org/abs/2207.06591v1","updated":"2022-07-14T01:07:55Z","published":"2022-07-14T01:07:55Z","title":"A tool to overcome technical barriers for bias assessment in human\n  language technologies","summary":"  Automatic processing of language is becoming pervasive in our lives, often\ntaking central roles in our decision making, like choosing the wording for our\nmessages and mails, translating our readings, or even having full conversations\nwith us. Word embeddings are a key component of modern natural language\nprocessing systems. They provide a representation of words that has boosted the\nperformance of many applications, working as a semblance of meaning. Word\nembeddings seem to capture a semblance of the meaning of words from raw text,\nbut, at the same time, they also distill stereotypes and societal biases which\nare subsequently relayed to the final applications. Such biases can be\ndiscriminatory. It is very important to detect and mitigate those biases, to\nprevent discriminatory behaviors of automated processes, which can be much more\nharmful than in the case of humans because their of their scale. There are\ncurrently many tools and techniques to detect and mitigate biases in word\nembeddings, but they present many barriers for the engagement of people without\ntechnical skills. As it happens, most of the experts in bias, either social\nscientists or people with deep knowledge of the context where bias is harmful,\ndo not have such skills, and they cannot engage in the processes of bias\ndetection because of the technical barriers. We have studied the barriers in\nexisting tools and have explored their possibilities and limitations with\ndifferent kinds of users. With this exploration, we propose to develop a tool\nthat is specially aimed to lower the technical barriers and provide the\nexploration power to address the requirements of experts, scientists and people\nin general who are willing to audit these technologies.\n","authors":["Laura Alonso Alemany","Luciana Benotti","Lucía González","Jorge Sánchez","Beatriz Busaniche","Alexia Halvorsen","Matías Bordone"],"pdf_url":"https://arxiv.org/pdf/2207.06591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07238v1","updated":"2022-07-14T23:59:06Z","published":"2022-07-14T23:59:06Z","title":"Emotion Recognition in Conversation using Probabilistic Soft Logic","summary":"  Creating agents that can both appropriately respond to conversations and\nunderstand complex human linguistic tendencies and social cues has been a long\nstanding challenge in the NLP community. A recent pillar of research revolves\naround emotion recognition in conversation (ERC); a sub-field of emotion\nrecognition that focuses on conversations or dialogues that contain two or more\nutterances. In this work, we explore an approach to ERC that exploits the use\nof neural embeddings along with complex structures in dialogues. We implement\nour approach in a framework called Probabilistic Soft Logic (PSL), a\ndeclarative templating language that uses first-order like logical rules, that\nwhen combined with data, define a particular class of graphical model.\nAdditionally, PSL provides functionality for the incorporation of results from\nneural models into PSL models. This allows our model to take advantage of\nadvanced neural methods, such as sentence embeddings, and logical reasoning\nover the structure of a dialogue. We compare our method with state-of-the-art\npurely neural ERC systems, and see almost a 20% improvement. With these\nresults, we provide an extensive qualitative and quantitative analysis over the\nDailyDialog conversation dataset.\n","authors":["Eriq Augustine","Pegah Jandaghi","Alon Albalak","Connor Pryor","Charles Dickens","William Wang","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2207.07238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12184v2","updated":"2022-07-14T23:27:43Z","published":"2022-03-23T04:06:01Z","title":"A Theoretically Grounded Benchmark for Evaluating Machine Commonsense","summary":"  Programming machines with commonsense reasoning (CSR) abilities is a\nlongstanding challenge in the Artificial Intelligence community. Current CSR\nbenchmarks use multiple-choice (and in relatively fewer cases, generative)\nquestion-answering instances to evaluate machine commonsense. Recent progress\nin transformer-based language representation models suggest that considerable\nprogress has been made on existing benchmarks. However, although tens of CSR\nbenchmarks currently exist, and are growing, it is not evident that the full\nsuite of commonsense capabilities have been systematically evaluated.\nFurthermore, there are doubts about whether language models are 'fitting' to a\nbenchmark dataset's training partition by picking up on subtle, but normatively\nirrelevant (at least for CSR), statistical features to achieve good performance\non the testing partition. To address these challenges, we propose a benchmark\ncalled Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based\non discriminative question answering, but with questions designed to evaluate\ndiverse aspects of commonsense, such as space, time, and world states. TG-CSR\nis based on a subset of commonsense categories first proposed as a viable\ntheory of commonsense by Gordon and Hobbs. The benchmark is also designed to be\nfew-shot (and in the future, zero-shot), with only a few training and\nvalidation examples provided. This report discusses the structure and\nconstruction of the benchmark. Preliminary results suggest that the benchmark\nis challenging even for advanced language representation models designed for\ndiscriminative CSR question answering tasks.\n  Benchmark access and leaderboard:\nhttps://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:\nhttps://usc-isi-i2.github.io/TGCSR/\n","authors":["Henrique Santos","Ke Shen","Alice M. Mulvehill","Yasaman Razeghi","Deborah L. McGuinness","Mayank Kejriwal"],"pdf_url":"https://arxiv.org/pdf/2203.12184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06500v2","updated":"2022-07-14T22:14:17Z","published":"2021-10-13T05:15:00Z","title":"Differentially Private Fine-tuning of Language Models","summary":"  We give simpler, sparser, and faster algorithms for differentially private\nfine-tuning of large-scale pre-trained language models, which achieve the\nstate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.\nWe propose a meta-framework for this problem, inspired by the recent success of\nhighly parameter-efficient methods for fine-tuning. Our experiments show that\ndifferentially private adaptations of these approaches outperform previous\nprivate algorithms in three important dimensions: utility, privacy, and the\ncomputational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models.\nFor example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using\nRoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of\n$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large\nachieves an accuracy of $90.2\\%$. Our findings are similar for natural language\ngeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,\nGPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8\nrespectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the\nnon-private baseline is $48.1$. All our experiments suggest that larger models\nare better suited for private fine-tuning: while they are well known to achieve\nsuperior accuracy non-privately, we find that they also better maintain their\naccuracy when privacy is introduced.\n","authors":["Da Yu","Saurabh Naik","Arturs Backurs","Sivakanth Gopi","Huseyin A. Inan","Gautam Kamath","Janardhan Kulkarni","Yin Tat Lee","Andre Manoel","Lukas Wutschitz","Sergey Yekhanin","Huishuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2110.06500v2.pdf","comment":"ICLR 2022. Code available at\n  https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models"},{"id":"http://arxiv.org/abs/2204.09817v3","updated":"2022-07-14T20:45:13Z","published":"2022-04-21T00:04:35Z","title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language\n  Processing","summary":"  Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n","authors":["Benedikt Boecking","Naoto Usuyama","Shruthi Bannur","Daniel C. Castro","Anton Schwaighofer","Stephanie Hyland","Maria Wetscherek","Tristan Naumann","Aditya Nori","Javier Alvarez-Valle","Hoifung Poon","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2204.09817v3.pdf","comment":"To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:\n  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook"},{"id":"http://arxiv.org/abs/2202.00842v5","updated":"2022-07-14T20:40:04Z","published":"2022-02-02T01:27:21Z","title":"Streaming Multi-Talker ASR with Token-Level Serialized Output Training","summary":"  This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output branches, the\nt-SOT model has only a single output branch that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of ``virtual''\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.\n","authors":["Naoyuki Kanda","Jian Wu","Yu Wu","Xiong Xiao","Zhong Meng","Xiaofei Wang","Yashesh Gaur","Zhuo Chen","Jinyu Li","Takuya Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2202.00842v5.pdf","comment":"6 pages, 1 figure, 7 tables, v2: minor fixes, v3: Appendix D has been\n  added, v4: citation to [27] has been added, v5: citations to [28][29][30]\n  have been added with minor fixes, short version accepted for presentation at\n  Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.16685v2","updated":"2022-07-14T20:38:18Z","published":"2022-03-30T21:42:00Z","title":"Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings","summary":"  This paper presents a streaming speaker-attributed automatic speech\nrecognition (SA-ASR) model that can recognize ``who spoke what'' with low\nlatency even when multiple people are speaking simultaneously. Our model is\nbased on token-level serialized output training (t-SOT) which was recently\nproposed to transcribe multi-talker speech in a streaming fashion. To further\nrecognize speaker identities, we propose an encoder-decoder based speaker\nembedding extractor that can estimate a speaker representation for each\nrecognized token not only from non-overlapping speech but also from overlapping\nspeech. The proposed speaker embedding, named t-vector, is extracted\nsynchronously with the t-SOT ASR model, enabling joint execution of speaker\nidentification (SID) or speaker diarization (SD) with the multi-talker\ntranscription with low latency. We evaluate the proposed model for a joint task\nof ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed\nmodel achieves substantially better accuracy than a prior streaming model and\nshows comparable or sometimes even superior results to the state-of-the-art\noffline SA-ASR model.\n","authors":["Naoyuki Kanda","Jian Wu","Yu Wu","Xiong Xiao","Zhong Meng","Xiaofei Wang","Yashesh Gaur","Zhuo Chen","Jinyu Li","Takuya Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2203.16685v2.pdf","comment":"Accepted for presentation at Interspeech 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2207.07116v1","updated":"2022-07-14T17:59:58Z","published":"2022-07-14T17:59:58Z","title":"Bootstrapped Masked Autoencoders for Vision BERT Pretraining","summary":"  We propose bootstrapped masked autoencoders (BootMAE), a new approach for\nvision BERT pretraining. BootMAE improves the original masked autoencoders\n(MAE) with two core designs: 1) momentum encoder that provides online feature\nas extra BERT prediction targets; 2) target-aware decoder that tries to reduce\nthe pressure on the encoder to memorize target-specific information in BERT\npretraining. The first design is motivated by the observation that using a\npretrained MAE to extract the features as the BERT prediction target for masked\ntokens can achieve better pretraining performance. Therefore, we add a momentum\nencoder in parallel with the original MAE encoder, which bootstraps the\npretraining performance by using its own representation as the BERT prediction\ntarget. In the second design, we introduce target-specific information (e.g.,\npixel values of unmasked patches) from the encoder directly to the decoder to\nreduce the pressure on the encoder of memorizing the target-specific\ninformation. Thus, the encoder focuses on semantic modeling, which is the goal\nof BERT pretraining, and does not need to waste its capacity in memorizing the\ninformation of unmasked tokens related to the prediction target. Through\nextensive experiments, our BootMAE achieves $84.2\\%$ Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\\%$ under the same\npre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic\nsegmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object\ndetection and segmentation on COCO dataset. Code is released at\nhttps://github.com/LightDXY/BootMAE.\n","authors":["Xiaoyi Dong","Jianmin Bao","Ting Zhang","Dongdong Chen","Weiming Zhang","Lu Yuan","Dong Chen","Fang Wen","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2207.07116v1.pdf","comment":"ECCV 2022, code is available at https://github.com/LightDXY/BootMAE"},{"id":"http://arxiv.org/abs/2207.07115v1","updated":"2022-07-14T17:59:37Z","published":"2022-07-14T17:59:37Z","title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin\n  Memory Model","summary":"  We present XMem, a video object segmentation architecture for long videos\nwith unified feature memory stores inspired by the Atkinson-Shiffrin memory\nmodel. Prior work on video object segmentation typically only uses one type of\nfeature memory. For videos longer than a minute, a single feature memory model\ntightly links memory consumption and accuracy. In contrast, following the\nAtkinson-Shiffrin model, we develop an architecture that incorporates multiple\nindependent yet deeply-connected feature memory stores: a rapidly updated\nsensory memory, a high-resolution working memory, and a compact thus sustained\nlong-term memory. Crucially, we develop a memory potentiation algorithm that\nroutinely consolidates actively used working memory elements into the long-term\nmemory, which avoids memory explosion and minimizes performance decay for\nlong-term prediction. Combined with a new memory reading mechanism, XMem\ngreatly exceeds state-of-the-art performance on long-video datasets while being\non par with state-of-the-art methods (that do not work on long videos) on\nshort-video datasets. Code is available at https://hkchengrex.github.io/XMem\n","authors":["Ho Kei Cheng","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2207.07115v1.pdf","comment":"Accepted to ECCV 2022. Project page:\n  https://hkchengrex.github.io/XMem"},{"id":"http://arxiv.org/abs/2207.07110v1","updated":"2022-07-14T17:59:05Z","published":"2022-07-14T17:59:05Z","title":"Fine-grained Few-shot Recognition by Deep Object Parsing","summary":"  In our framework, an object is made up of K distinct parts or units, and we\nparse a test instance by inferring the K parts, where each part occupies a\ndistinct location in the feature space, and the instance features at this\nlocation, manifest as an active subset of part templates shared across all\ninstances. We recognize test instances by comparing its active templates and\nthe relative geometry of its part locations against those of the presented\nfew-shot instances. We propose an end-to-end training method to learn part\ntemplates on-top of a convolutional backbone. To combat visual distortions such\nas orientation, pose and size, we learn multi-scale templates, and at test-time\nparse and match instances across these scales. We show that our method is\ncompetitive with the state-of-the-art, and by virtue of parsing enjoys\ninterpretability as well.\n","authors":["Pengkai Zhu","Ruizhao Zhu","Samarth Mishra","Venkatesh Saligrama"],"pdf_url":"https://arxiv.org/pdf/2207.07110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07106v1","updated":"2022-07-14T17:58:02Z","published":"2022-07-14T17:58:02Z","title":"Benchmarking Omni-Vision Representation through the Lens of Visual\n  Realms","summary":"  Though impressive performance has been achieved in specific visual realms\n(e.g. faces, dogs, and places), an omni-vision representation generalizing to\nmany natural visual domains is highly desirable. But, existing benchmarks are\nbiased and inefficient to evaluate the omni-vision representation -- these\nbenchmarks either only include several specific realms, or cover most realms at\nthe expense of subsuming numerous datasets that have extensive realm\noverlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It\nincludes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.\nWithout semantic overlapping, these datasets cover most visual realms\ncomprehensively and meanwhile efficiently. In addition, we propose a new\nsupervised contrastive learning framework, namely Relational Contrastive\nlearning (ReCo), for a better omni-vision representation. Beyond pulling two\ninstances from the same concept closer -- the typical supervised contrastive\nlearning framework -- ReCo also pulls two instances from the same semantic\nrealm closer, encoding the semantic relation between concepts, and facilitating\nomni-vision representation learning. We benchmark ReCo and other advances in\nomni-vision representation studies that are different in architectures (from\nCNNs to transformers) and in learning paradigms (from supervised learning to\nself-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo\nto other supervised contrastive learning methods and reveal multiple practical\nobservations to facilitate future research.\n","authors":["Yuanhan Zhang","Zhenfei Yin","Jing Shao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2207.07106v1.pdf","comment":"The project page at https://zhangyuanhan-ai.github.io/OmniBenchmark"},{"id":"http://arxiv.org/abs/2207.07104v1","updated":"2022-07-14T17:57:13Z","published":"2022-07-14T17:57:13Z","title":"Relighting4D: Neural Relightable Human from Videos","summary":"  Human relighting is a highly desirable yet challenging task. Existing works\neither require expensive one-light-at-a-time (OLAT) captured data using light\nstage or cannot freely change the viewpoints of the rendered body. In this\nwork, we propose a principled framework, Relighting4D, that enables\nfree-viewpoints relighting from only human videos under unknown illuminations.\nOur key insight is that the space-time varying geometry and reflectance of the\nhuman body can be decomposed as a set of neural fields of normal, occlusion,\ndiffuse, and specular maps. These neural fields are further integrated into\nreflectance-aware physically based rendering, where each vertex in the neural\nfield absorbs and reflects the light from the environment. The whole framework\ncan be learned from videos in a self-supervised manner, with physically\ninformed priors designed for regularization. Extensive experiments on both real\nand synthetic datasets demonstrate that our framework is capable of relighting\ndynamic human actors with free-viewpoints.\n","authors":["Zhaoxi Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2207.07104v1.pdf","comment":"ECCV 2022; Project Page\n  https://frozenburning.github.io/projects/relighting4d Codes are available at\n  https://github.com/FrozenBurning/Relighting4D"},{"id":"http://arxiv.org/abs/2207.07097v1","updated":"2022-07-14T17:46:37Z","published":"2022-07-14T17:46:37Z","title":"ReAct: Temporal Action Detection with Relational Queries","summary":"  This work aims at advancing temporal action detection (TAD) using an\nencoder-decoder framework with action queries, similar to DETR, which has shown\ngreat success in object detection. However, the framework suffers from several\nproblems if directly applied to TAD: the insufficient exploration of\ninter-query relation in the decoder, the inadequate classification training due\nto a limited number of training samples, and the unreliable classification\nscores at inference. To this end, we first propose a relational attention\nmechanism in the decoder, which guides the attention among queries based on\ntheir relations. Moreover, we propose two losses to facilitate and stabilize\nthe training of action classification. Lastly, we propose to predict the\nlocalization quality of each action query at inference in order to distinguish\nhigh-quality queries. The proposed method, named ReAct, achieves the\nstate-of-the-art performance on THUMOS14, with much lower computational costs\nthan previous methods. Besides, extensive ablation studies are conducted to\nverify the effectiveness of each proposed component. The code is available at\nhttps://github.com/sssste/React.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Jing Zhang","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2207.07097v1.pdf","comment":"ECCV2022"},{"id":"http://arxiv.org/abs/2207.06262v2","updated":"2022-07-14T17:45:34Z","published":"2022-07-13T15:07:50Z","title":"Organic Priors in Non-Rigid Structure from Motion","summary":"  This paper advocates the use of organic priors in classical non-rigid\nstructure from motion (NRSfM). By organic priors, we mean invaluable\nintermediate prior information intrinsic to the NRSfM matrix factorization\ntheory. It is shown that such priors reside in the factorized matrices, and\nquite surprisingly, existing methods generally disregard them. The paper's main\ncontribution is to put forward a simple, methodical, and practical method that\ncan effectively exploit such organic priors to solve NRSfM. The proposed method\ndoes not make assumptions other than the popular one on the low-rank shape and\noffers a reliable solution to NRSfM under orthographic projection. Our work\nreveals that the accessibility of organic priors is independent of the camera\nmotion and shape deformation type. Besides that, the paper provides insights\ninto the NRSfM factorization -- both in terms of shape and motion -- and is the\nfirst approach to show the benefit of single rotation averaging for NRSfM.\nFurthermore, we outline how to effectively recover motion and non-rigid 3D\nshape using the proposed organic prior based approach and demonstrate results\nthat outperform prior-free NRSfM performance by a significant margin. Finally,\nwe present the benefits of our method via extensive experiments and evaluations\non several benchmark datasets.\n","authors":["Suryansh Kumar","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2207.06262v2.pdf","comment":"To appear in ECCV 2022 Conference (Oral Presentation). Draft info: 18\n  Pages, 4 Figures, and 6 Tables"},{"id":"http://arxiv.org/abs/2207.07092v1","updated":"2022-07-14T17:44:49Z","published":"2022-07-14T17:44:49Z","title":"Explaining Image Enhancement Black-Box Methods through a Path Planning\n  Based Algorithm","summary":"  Nowadays, image-to-image translation methods, are the state of the art for\nthe enhancement of natural images. Even if they usually show high performance\nin terms of accuracy, they often suffer from several limitations such as the\ngeneration of artifacts and the scalability to high resolutions. Moreover,\ntheir main drawback is the completely black-box approach that does not allow to\nprovide the final user with any insight about the enhancement processes\napplied. In this paper we present a path planning algorithm which provides a\nstep-by-step explanation of the output produced by state of the art enhancement\nmethods, overcoming black-box limitation. This algorithm, called eXIE, uses a\nvariant of the A* algorithm to emulate the enhancement process of another\nmethod through the application of an equivalent sequence of enhancing\noperators. We applied eXIE to explain the output of several state-of-the-art\nmodels trained on the Five-K dataset, obtaining sequences of enhancing\noperators able to produce very similar results in terms of performance and\novercoming the huge limitation of poor interpretability of the best performing\nalgorithms.\n","authors":["Marco Cotogni","Claudio Cusano"],"pdf_url":"https://arxiv.org/pdf/2207.07092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2007.09264v2","updated":"2022-07-14T17:43:20Z","published":"2020-07-17T22:22:34Z","title":"Surface Normal Estimation of Tilted Images via Spatial Rectifier","summary":"  In this paper, we present a spatial rectifier to estimate surface normals of\ntilted images. Tilted images are of particular interest as more visual data are\ncaptured by arbitrarily oriented sensors such as body-/robot-mounted cameras.\nExisting approaches exhibit bounded performance on predicting surface normals\nbecause they were trained using gravity-aligned images. Our two main hypotheses\nare: (1) visual scene layout is indicative of the gravity direction; and (2)\nnot all surfaces are equally represented by a learned estimator due to the\nstructured distribution of the training data, thus, there exists a\ntransformation for each tilted image that is more responsive to the learned\nestimator than others. We design a spatial rectifier that is learned to\ntransform the surface normal distribution of a tilted image to the rectified\none that matches the gravity-aligned training data distribution. Along with the\nspatial rectifier, we propose a novel truncated angular loss that offers a\nstronger gradient at smaller angular errors and robustness to outliers. The\nresulting estimator outperforms the state-of-the-art methods including data\naugmentation baselines not only on ScanNet and NYUv2 but also on a new dataset\ncalled Tilt-RGBD that includes considerable roll and pitch camera motion.\n","authors":["Tien Do","Khiem Vuong","Stergios I. Roumeliotis","Hyun Soo Park"],"pdf_url":"https://arxiv.org/pdf/2007.09264v2.pdf","comment":"Appearing in the European Conference on Computer Vision 2020. This\n  version fixes a typo on the L2 loss function"},{"id":"http://arxiv.org/abs/2207.07089v1","updated":"2022-07-14T17:40:05Z","published":"2022-07-14T17:40:05Z","title":"A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse\n  Representation Based Domain Adaption to Energy Efficient Abnormal Beat\n  Detection for Practical ECG Surveillance","summary":"  This paper proposes a low-cost and highly accurate ECG-monitoring system\nintended for personalized early arrhythmia detection for wearable mobile\nsensors. Earlier supervised approaches for personalized ECG monitoring require\nboth abnormal and normal heartbeats for the training of the dedicated\nclassifier. However, in a real-world scenario where the personalized algorithm\nis embedded in a wearable device, such training data is not available for\nhealthy people with no cardiac disorder history. In this study, (i) we propose\na null space analysis on the healthy signal space obtained via sparse\ndictionary learning, and investigate how a simple null space projection or\nalternatively regularized least squares-based classification methods can reduce\nthe computational complexity, without sacrificing the detection accuracy, when\ncompared to sparse representation-based classification. (ii) Then we introduce\na sparse representation-based domain adaptation technique in order to project\nother existing users' abnormal and normal signals onto the new user's signal\nspace, enabling us to train the dedicated classifier without having any\nabnormal heartbeat of the new user. Therefore, zero-shot learning can be\nachieved without the need for synthetic abnormal heartbeat generation. An\nextensive set of experiments performed on the benchmark MIT-BIH ECG dataset\nshows that when this domain adaptation-based training data generator is used\nwith a simple 1-D CNN classifier, the method outperforms the prior work by a\nsignificant margin. (iii) Then, by combining (i) and (ii), we propose an\nensemble classifier that further improves the performance. This approach for\nzero-shot arrhythmia detection achieves an average accuracy level of 98.2% and\nan F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring\nscheme is proposed using the above-mentioned innovations.\n","authors":["Mehmet Yamaç","Mert Duman","İlke Adalıoğlu","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2207.07089v1.pdf","comment":"Software implementation: https://github.com/MertDuman/Zero-Shot-ECG"},{"id":"http://arxiv.org/abs/2207.07080v1","updated":"2022-07-14T17:30:13Z","published":"2022-07-14T17:30:13Z","title":"An Asymmetric Contrastive Loss for Handling Imbalanced Datasets","summary":"  Contrastive learning is a representation learning method performed by\ncontrasting a sample to other similar samples so that they are brought closely\ntogether, forming clusters in the feature space. The learning process is\ntypically conducted using a two-stage training architecture, and it utilizes\nthe contrastive loss (CL) for its feature learning. Contrastive learning has\nbeen shown to be quite successful in handling imbalanced datasets, in which\nsome classes are overrepresented while some others are underrepresented.\nHowever, previous studies have not specifically modified CL for imbalanced\ndatasets. In this work, we introduce an asymmetric version of CL, referred to\nas ACL, in order to directly address the problem of class imbalance. In\naddition, we propose the asymmetric focal contrastive loss (AFCL) as a further\ngeneralization of both ACL and focal contrastive loss (FCL). Results on the\nFMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of\noutperforming CL and FCL in terms of both weighted and unweighted\nclassification accuracies. In the appendix, we provide a full axiomatic\ntreatment on entropy, along with complete proofs.\n","authors":["Valentino Vito","Lim Yohanes Stefanus"],"pdf_url":"https://arxiv.org/pdf/2207.07080v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.07078v1","updated":"2022-07-14T17:27:19Z","published":"2022-07-14T17:27:19Z","title":"Towards Grand Unification of Object Tracking","summary":"  We present a unified method, termed Unicorn, that can simultaneously solve\nfour tracking problems (SOT, MOT, VOS, MOTS) with a single network using the\nsame model parameters. Due to the fragmented definitions of the object tracking\nproblem itself, most existing trackers are developed to address a single or\npart of tasks and overspecialize on the characteristics of specific tasks. By\ncontrast, Unicorn provides a unified solution, adopting the same input,\nbackbone, embedding, and head across all tracking tasks. For the first time, we\naccomplish the great unification of the tracking network architecture and\nlearning paradigm. Unicorn performs on-par or better than its task-specific\ncounterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,\nBDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will\nserve as a solid step towards the general vision model. Code is available at\nhttps://github.com/MasterBin-IIAU/Unicorn.\n","authors":["Bin Yan","Yi Jiang","Peize Sun","Dong Wang","Zehuan Yuan","Ping Luo","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2207.07078v1.pdf","comment":"ECCV2022 Oral"},{"id":"http://arxiv.org/abs/2207.07077v1","updated":"2022-07-14T17:26:00Z","published":"2022-07-14T17:26:00Z","title":"Egocentric Scene Understanding via Multimodal Spatial Rectifier","summary":"  In this paper, we study a problem of egocentric scene understanding, i.e.,\npredicting depths and surface normals from an egocentric image. Egocentric\nscene understanding poses unprecedented challenges: (1) due to large head\nmovements, the images are taken from non-canonical viewpoints (i.e., tilted\nimages) where existing models of geometry prediction do not apply; (2) dynamic\nforeground objects including hands constitute a large proportion of visual\nscenes. These challenges limit the performance of the existing models learned\nfrom large indoor datasets, such as ScanNet and NYUv2, which comprise\npredominantly upright images of static scenes. We present a multimodal spatial\nrectifier that stabilizes the egocentric images to a set of reference\ndirections, which allows learning a coherent visual representation. Unlike\nunimodal spatial rectifier that often produces excessive perspective warp for\negocentric images, the multimodal spatial rectifier learns from multiple\ndirections that can minimize the impact of the perspective warp. To learn\nvisual representations of the dynamic foreground objects, we present a new\ndataset called EDINA (Egocentric Depth on everyday INdoor Activities) that\ncomprises more than 500K synchronized RGBD frames and gravity directions.\nEquipped with the multimodal spatial rectifier and the EDINA dataset, our\nproposed method on single-view depth and surface normal estimation\nsignificantly outperforms the baselines not only on our EDINA dataset, but also\non other popular egocentric datasets, such as First Person Hand Action (FPHA)\nand EPIC-KITCHENS.\n","authors":["Tien Do","Khiem Vuong","Hyun Soo Park"],"pdf_url":"https://arxiv.org/pdf/2207.07077v1.pdf","comment":"Appearing in the Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR), 2022"},{"id":"http://arxiv.org/abs/2203.07160v2","updated":"2022-07-14T17:21:47Z","published":"2022-03-14T15:02:48Z","title":"CAR: Class-aware Regularizations for Semantic Segmentation","summary":"  Recent segmentation methods, such as OCR and CPNet, utilizing \"class level\"\ninformation in addition to pixel features, have achieved notable success for\nboosting the accuracy of existing network modules. However, the extracted\nclass-level information was simply concatenated to pixel features, without\nexplicitly being exploited for better pixel representation learning. Moreover,\nthese approaches learn soft class centers based on coarse mask prediction,\nwhich is prone to error accumulation. In this paper, aiming to use class level\ninformation more effectively, we propose a universal Class-Aware Regularization\n(CAR) approach to optimize the intra-class variance and inter-class distance\nduring feature learning, motivated by the fact that humans can recognize an\nobject by itself no matter which other objects it appears with. Three novel\nloss functions are proposed. The first loss function encourages more compact\nclass representations within each class, the second directly maximizes the\ndistance between different class centers, and the third further pushes the\ndistance between inter-class centers and pixels. Furthermore, the class center\nin our approach is directly generated from ground truth instead of from the\nerror-prone coarse prediction. Our method can be easily applied to most\nexisting segmentation models during training, including OCR and CPNet, and can\nlargely improve their accuracy at no additional inference overhead. Extensive\nexperiments and ablation studies conducted on multiple benchmark datasets\ndemonstrate that the proposed CAR can boost the accuracy of all baseline models\nby up to 2.23% mIOU with superior generalization ability. The complete code is\navailable at https://github.com/edwardyehuang/CAR.\n","authors":["Ye Huang","Di Kang","Liang Chen","Xuefei Zhe","Wenjing Jia","Xiangjian He","Linchao Bao"],"pdf_url":"https://arxiv.org/pdf/2203.07160v2.pdf","comment":"ECCV 2022 camera ready. Codes and models are available at\n  https://github.com/edwardyehuang/CAR"},{"id":"http://arxiv.org/abs/2207.07059v1","updated":"2022-07-14T16:58:47Z","published":"2022-07-14T16:58:47Z","title":"Semi-Supervised Temporal Action Detection with Proposal-Free Masking","summary":"  Existing temporal action detection (TAD) methods rely on a large number of\ntraining data with segment-level annotations. Collecting and annotating such a\ntraining set is thus highly expensive and unscalable. Semi-supervised TAD\n(SS-TAD) alleviates this problem by leveraging unlabeled videos freely\navailable at scale. However, SS-TAD is also a much more challenging problem\nthan supervised TAD, and consequently much under-studied. Prior SS-TAD methods\ndirectly combine an existing proposal-based TAD method and a SSL method. Due to\ntheir sequential localization (e.g, proposal generation) and classification\ndesign, they are prone to proposal error propagation. To overcome this\nlimitation, in this work we propose a novel Semi-supervised Temporal action\ndetection model based on PropOsal-free Temporal mask (SPOT) with a parallel\nlocalization (mask generation) and classification architecture. Such a novel\ndesign effectively eliminates the dependence between localization and\nclassification by cutting off the route for error propagation in-between. We\nfurther introduce an interaction mechanism between classification and\nlocalization for prediction refinement, and a new pretext task for\nself-supervised model pre-training. Extensive experiments on two standard\nbenchmarks show that our SPOT outperforms state-of-the-art alternatives, often\nby a large margin. The PyTorch implementation of SPOT is available at\nhttps://github.com/sauradip/SPOT\n","authors":["Sauradip Nag","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2207.07059v1.pdf","comment":"ECCV 2022; Code available at https://github.com/sauradip/SPOT"},{"id":"http://arxiv.org/abs/2201.07219v3","updated":"2022-07-14T16:57:37Z","published":"2022-01-16T13:09:47Z","title":"Contrastive Pretraining for Echocardiography Segmentation with Limited\n  Data","summary":"  Contrastive learning has proven useful in many applications where access to\nlabelled data is limited. The lack of annotated data is particularly\nproblematic in medical image segmentation as it is difficult to have clinical\nexperts manually annotate large volumes of data such as cardiac structures in\nultrasound images of the heart. In this paper, We propose a self supervised\ncontrastive learning method to segment the left ventricle from echocardiography\nwhen limited annotated images exist. Furthermore, we study the effect of\ncontrastive pretraining on two well-known segmentation networks, UNet and\nDeepLabV3. Our results show that contrastive pretraining helps improve the\nperformance on left ventricle segmentation, particularly when annotated data is\nscarce. We show how to achieve comparable results to state-of-the-art fully\nsupervised algorithms when we train our models in a self-supervised fashion\nfollowed by fine-tuning on just 5\\% of the data. We show that our solution\noutperforms what is currently published on a large public dataset\n(EchoNet-Dynamic) achieving a Dice score of 0.9252. We also compare the\nperformance of our solution on another smaller dataset (CAMUS) to demonstrate\nthe generalizability of our proposed solution. The code is available at\n(https://github.com/BioMedIA-MBZUAI/contrastive-echo).\n","authors":["Mohamed Saeed","Rand Muhtaseb","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2201.07219v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.03804v2","updated":"2022-07-14T16:44:11Z","published":"2021-10-07T21:39:33Z","title":"FOCUS: Familiar Objects in Common and Uncommon Settings","summary":"  Standard training datasets for deep learning often contain objects in common\nsettings (e.g., \"a horse on grass\" or \"a ship in water\") since they are usually\ncollected by randomly scraping the web. Uncommon and rare settings (e.g., \"a\nplane on water\", \"a car in snowy weather\") are thus severely under-represented\nin the training data. This can lead to an undesirable bias in model predictions\ntowards common settings and create a false sense of accuracy. In this paper, we\nintroduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset\nfor stress-testing the generalization power of deep image classifiers. By\nleveraging the power of modern search engines, we deliberately gather data\ncontaining objects in common and uncommon settings in a wide range of\nlocations, weather conditions, and time of day. We present a detailed analysis\nof the performance of various popular image classifiers on our dataset and\ndemonstrate a clear drop in performance when classifying images in uncommon\nsettings. By analyzing deep features of these models, we show that such errors\ncan be due to the use of spurious features in model predictions. We believe\nthat our dataset will aid researchers in understanding the inability of deep\nmodels to generalize well to uncommon settings and drive future work on\nimproving their distributional robustness.\n","authors":["Priyatham Kattakinda","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2110.03804v2.pdf","comment":"23 pages, 14 figures, 4 tables. Accepted to ICML 2022"},{"id":"http://arxiv.org/abs/2204.01737v3","updated":"2022-07-14T16:35:45Z","published":"2022-04-04T17:37:54Z","title":"Feature robustness and sex differences in medical imaging: a case study\n  in MRI-based Alzheimer's disease detection","summary":"  Convolutional neural networks have enabled significant improvements in\nmedical image-based diagnosis. It is, however, increasingly clear that these\nmodels are susceptible to performance degradation when facing spurious\ncorrelations and dataset shift, leading, e.g., to underperformance on\nunderrepresented patient groups. In this paper, we compare two classification\nschemes on the ADNI MRI dataset: a simple logistic regression model using\nmanually selected volumetric features, and a convolutional neural network\ntrained on 3D MRI data. We assess the robustness of the trained models in the\nface of varying dataset splits, training set sex composition, and stage of\ndisease. In contrast to earlier work in other imaging modalities, we do not\nobserve a clear pattern of improved model performance for the majority group in\nthe training dataset. Instead, while logistic regression is fully robust to\ndataset composition, we find that CNN performance is generally improved for\nboth male and female subjects when including more female subjects in the\ntraining dataset. We hypothesize that this might be due to inherent differences\nin the pathology of the two sexes. Moreover, in our analysis, the logistic\nregression model outperforms the 3D CNN, emphasizing the utility of manual\nfeature specification based on prior knowledge, and the need for more robust\nautomatic feature selection.\n","authors":["Eike Petersen","Aasa Feragen","Maria Luise da Costa Zemsch","Anders Henriksen","Oskar Eiler Wiese Christensen","Melanie Ganz"],"pdf_url":"https://arxiv.org/pdf/2204.01737v3.pdf","comment":"Accepted for presentation at MICCAI 2022"},{"id":"http://arxiv.org/abs/2207.07039v1","updated":"2022-07-14T16:32:28Z","published":"2022-07-14T16:32:28Z","title":"Convolutional Bypasses Are Better Vision Transformer Adapters","summary":"  The pretrain-then-finetune paradigm has been widely adopted in computer\nvision. But as the size of Vision Transformer (ViT) grows exponentially, the\nfull finetuning becomes prohibitive in view of the heavier storage overhead.\nMotivated by parameter-efficient transfer learning (PETL) on language\ntransformers, recent studies attempt to insert lightweight adaptation modules\n(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune\nthese modules while the pretrained weights are frozen. However, these modules\nwere originally proposed to finetune language models. Although ported well to\nViT, their design lacks prior knowledge for visual tasks. In this paper, we\npropose to construct Convolutional Bypasses (Convpass) in ViT as adaptation\nmodules, introducing only a small amount (less than 0.5% of model parameters)\nof trainable parameters to adapt the large ViT. Different from other PETL\nmethods, Convpass benefits from the hard-coded inductive bias of convolutional\nlayers and thus is more suitable for visual tasks, especially in the low-data\nregime. Experimental results on VTAB-1k benchmark and few-shot learning\ndatasets demonstrate that Convpass outperforms current language-oriented\nadaptation modules, demonstrating the necessity to tailor vision-oriented\nadaptation modules for vision models.\n","authors":["Shibo Jie","Zhi-Hong Deng"],"pdf_url":"https://arxiv.org/pdf/2207.07039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07036v1","updated":"2022-07-14T16:21:33Z","published":"2022-07-14T16:21:33Z","title":"A Single Self-Supervised Model for Many Speech Modalities Enables\n  Zero-Shot Modality Transfer","summary":"  While audio-visual speech models can yield superior performance and\nrobustness compared to audio-only models, their development and adoption are\nhindered by the lack of labeled and unlabeled audio-visual data and the cost to\ndeploy one model per modality. In this paper, we present u-HuBERT, a\nself-supervised pre-training framework that can leverage both multimodal and\nunimodal speech with a unified masked cluster prediction objective. By\nutilizing modality dropout during pre-training, we demonstrate that a single\nfine-tuned model can achieve performance on par or better than the\nstate-of-the-art modality-specific models. Moreover, our model fine-tuned only\non audio can perform well with audio-visual and visual speech input, achieving\nzero-shot modality generalization for speech recognition and speaker\nverification. In particular, our single model yields 1.2%/1.4%/27.2% speech\nrecognition word error rate on LRS3 with audio-visual/audio/visual input.\n","authors":["Wei-Ning Hsu","Bowen Shi"],"pdf_url":"https://arxiv.org/pdf/2207.07036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03030v2","updated":"2022-07-14T16:20:50Z","published":"2021-12-01T20:54:36Z","title":"Pose2Room: Understanding 3D Scenes from Human Activities","summary":"  With wearable IMU sensors, one can estimate human poses from wearable devices\nwithout requiring visual input~\\cite{von2017sparse}. In this work, we pose the\nquestion: Can we reason about object structure in real-world environments\nsolely from human trajectory information? Crucially, we observe that human\nmotion and interactions tend to give strong information about the objects in a\nscene -- for instance a person sitting indicates the likely presence of a chair\nor sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of\nthe objects in a scene characterized by their class categories and oriented 3D\nbounding boxes, based on an input observed human trajectory in the environment.\nP2R-Net models the probability distribution of object class as well as a deep\nGaussian mixture model for object boxes, enabling sampling of multiple,\ndiverse, likely modes of object configurations from an observed human\ntrajectory. In our experiments we show that P2R-Net can effectively learn\nmulti-modal distributions of likely objects for human motions, and produce a\nvariety of plausible object structures of the environment, even without any\nvisual information. The results demonstrate that P2R-Net consistently\noutperforms the baselines on the PROX dataset and the VirtualHome platform.\n","authors":["Yinyu Nie","Angela Dai","Xiaoguang Han","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2112.03030v2.pdf","comment":"Accepted by ECCV'2022; Project page:\n  https://yinyunie.github.io/pose2room-page/ Video:\n  https://www.youtube.com/watch?v=MFfKTcvbM5o"},{"id":"http://arxiv.org/abs/2207.07032v1","updated":"2022-07-14T16:12:31Z","published":"2022-07-14T16:12:31Z","title":"Adversarial Attacks on Monocular Pose Estimation","summary":"  Advances in deep learning have resulted in steady progress in computer vision\nwith improved accuracy on tasks such as object detection and semantic\nsegmentation. Nevertheless, deep neural networks are vulnerable to adversarial\nattacks, thus presenting a challenge in reliable deployment. Two of the\nprominent tasks in 3D scene-understanding for robotics and advanced drive\nassistance systems are monocular depth and pose estimation, often learned\ntogether in an unsupervised manner. While studies evaluating the impact of\nadversarial attacks on monocular depth estimation exist, a systematic\ndemonstration and analysis of adversarial perturbations against pose estimation\nare lacking. We show how additive imperceptible perturbations can not only\nchange predictions to increase the trajectory drift but also catastrophically\nalter its geometry. We also study the relation between adversarial\nperturbations targeting monocular depth and pose estimation networks, as well\nas the transferability of perturbations to other networks with different\narchitectures and losses. Our experiments show how the generated perturbations\nlead to notable errors in relative rotation and translation predictions and\nelucidate vulnerabilities of the networks.\n","authors":["Hemang Chawla","Arnav Varma","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2207.07032v1.pdf","comment":"Accepted at the 2022 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2022)"},{"id":"http://arxiv.org/abs/2207.07027v1","updated":"2022-07-14T15:59:03Z","published":"2022-07-14T15:59:03Z","title":"MedFuse: Multi-modal fusion with clinical time-series data and chest\n  X-ray images","summary":"  Multi-modal fusion approaches aim to integrate information from different\ndata sources. Unlike natural datasets, such as in audio-visual applications,\nwhere samples consist of \"paired\" modalities, data in healthcare is often\ncollected asynchronously. Hence, requiring the presence of all modalities for a\ngiven sample is not realistic for clinical tasks and significantly limits the\nsize of the dataset during training. In this paper, we propose MedFuse, a\nconceptually simple yet promising LSTM-based fusion module that can accommodate\nuni-modal as well as multi-modal input. We evaluate the fusion method and\nintroduce new benchmark results for in-hospital mortality prediction and\nphenotype classification, using clinical time-series data in the MIMIC-IV\ndataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more\ncomplex multi-modal fusion strategies, MedFuse provides a performance\nimprovement by a large margin on the fully paired test set. It also remains\nrobust across the partially paired test set containing samples with missing\nchest X-ray images. We release our code for reproducibility and to enable the\nevaluation of competing models in the future.\n","authors":["Nasir Hayat","Krzysztof J. Geras","Farah E. Shamout"],"pdf_url":"https://arxiv.org/pdf/2207.07027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07016v1","updated":"2022-07-14T15:50:44Z","published":"2022-07-14T15:50:44Z","title":"Accurate Ground-Truth Depth Image Generation via Overfit Training of\n  Point Cloud Registration using Local Frame Sets","summary":"  Accurate three-dimensional perception is a fundamental task in several\ncomputer vision applications. Recently, commercial RGB-depth (RGB-D) cameras\nhave been widely adopted as single-view depth-sensing devices owing to their\nefficient depth-sensing abilities. However, the depth quality of most RGB-D\nsensors remains insufficient owing to the inherent noise from a single-view\nenvironment. Recently, several studies have focused on the single-view depth\nenhancement of RGB-D cameras. Recent research has proposed deep-learning-based\napproaches that typically train networks using high-quality supervised depth\ndatasets, which indicates that the quality of the ground-truth (GT) depth\ndataset is a top-most important factor for accurate system; however, such\nhigh-quality GT datasets are difficult to obtain. In this study, we developed a\nnovel method for high-quality GT depth generation based on an RGB-D stream\ndataset. First, we defined consecutive depth frames in a local spatial region\nas a local frame set. Then, the depth frames were aligned to a certain frame in\nthe local frame set using an unsupervised point cloud registration scheme. The\nregistration parameters were trained based on an overfit-training scheme, which\nwas primarily used to construct a single GT depth image for each frame set. The\nfinal GT depth dataset was constructed using several local frame sets, and each\nlocal frame set was trained independently. The primary advantage of this study\nis that a high-quality GT depth dataset can be constructed under various\nscanning environments using only the RGB-D stream dataset. Moreover, our\nproposed method can be used as a new benchmark GT dataset for accurate\nperformance evaluations. We evaluated our GT dataset on previously benchmarked\nGT depth datasets and demonstrated that our method is superior to\nstate-of-the-art depth enhancement frameworks.\n","authors":["Jiwan Kim","Minchang Kim","Yeong-Gil Shin","Minyoung Chung"],"pdf_url":"https://arxiv.org/pdf/2207.07016v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2203.08133v2","updated":"2022-07-14T15:39:07Z","published":"2022-03-15T17:56:59Z","title":"Animatable Implicit Neural Representations for Creating Realistic\n  Avatars from Videos","summary":"  This paper addresses the challenge of reconstructing an animatable human\nmodel from a multi-view video. Some recent works have proposed to decompose a\nnon-rigidly deforming scene into a canonical neural radiance field and a set of\ndeformation fields that map observation-space points to the canonical space,\nthereby enabling them to learn the dynamic scene from images. However, they\nrepresent the deformation field as translational vector field or SE(3) field,\nwhich makes the optimization highly under-constrained. Moreover, these\nrepresentations cannot be explicitly controlled by input motions. Instead, we\nintroduce a pose-driven deformation field based on the linear blend skinning\nalgorithm, which combines the blend weight field and the 3D human skeleton to\nproduce observation-to-canonical correspondences. Since 3D human skeletons are\nmore observable, they can regularize the learning of the deformation field.\nMoreover, the pose-driven deformation field can be controlled by input skeletal\nmotions to generate new deformation fields to animate the canonical human\nmodel. Experiments show that our approach significantly outperforms recent\nhuman modeling methods. The code is available at\nhttps://zju3dv.github.io/animatable_nerf/.\n","authors":["Sida Peng","Zhen Xu","Junting Dong","Qianqian Wang","Shangzhan Zhang","Qing Shuai","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.08133v2.pdf","comment":"Project page: https://zju3dv.github.io/animatable_nerf/ and\n  https://zju3dv.github.io/animatable_sdf/. arXiv admin note: substantial text\n  overlap with arXiv:2105.02872"},{"id":"http://arxiv.org/abs/2207.05225v3","updated":"2022-07-14T15:28:31Z","published":"2022-07-11T23:45:12Z","title":"Susceptibility of Continual Learning Against Adversarial Attacks","summary":"  The recent advances in continual (incremental or lifelong) learning have\nconcentrated on the prevention of forgetting that can lead to catastrophic\nconsequences, but there are two outstanding challenges that must be addressed.\nThe first is the evaluation of the robustness of the proposed methods. The\nsecond is ensuring the security of learned tasks remains largely unexplored.\nThis paper presents a comprehensive study of the susceptibility of the\ncontinually learned tasks (including both current and previously learned tasks)\nthat are vulnerable to forgetting. Such vulnerability of tasks against\nadversarial attacks raises profound issues in data integrity and privacy. We\nconsider all three scenarios (i.e, task-incremental leaning, domain-incremental\nlearning and class-incremental learning) of continual learning and explore\nthree regularization-based experiments, three replay-based experiments, and one\nhybrid technique based on the reply and exemplar approach. We examine the\nrobustness of these methods. In particular, we consider cases where we\ndemonstrate that any class belonging to the current or previously learned tasks\nis prone to misclassification. Our observations, we identify potential\nlimitations in continual learning approaches against adversarial attacks. Our\nempirical study recommends that the research community consider the robustness\nof the proposed continual learning approaches and invest extensive efforts in\nmitigating catastrophic forgetting.\n","authors":["Hikmat Khan","Pir Masoom Shah","Syed Farhan Alam Zaidi","Saif ul Islam"],"pdf_url":"https://arxiv.org/pdf/2207.05225v3.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2105.12639v4","updated":"2022-07-14T15:27:38Z","published":"2021-05-26T15:58:11Z","title":"Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy,\n  Uncertainty, and Robustness","summary":"  Neural network ensembles, such as Bayesian neural networks (BNNs), have shown\nsuccess in the areas of uncertainty estimation and robustness. However, a\ncrucial challenge prohibits their use in practice. BNNs require a large number\nof predictions to produce reliable results, leading to a significant increase\nin computational cost. To alleviate this issue, we propose spatial smoothing, a\nmethod that spatially ensembles neighboring feature map points of convolutional\nneural networks. By simply adding a few blur layers to the models, we\nempirically show that spatial smoothing improves accuracy, uncertainty\nestimation, and robustness of BNNs across a whole range of ensemble sizes. In\nparticular, BNNs incorporating spatial smoothing achieve high predictive\nperformance merely with a handful of ensembles. Moreover, this method also can\nbe applied to canonical deterministic neural networks to improve the\nperformances. A number of evidences suggest that the improvements can be\nattributed to the stabilized feature maps and the smoothing of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing them\nas special cases of spatial smoothing. These not only enhance accuracy, but\nalso improve uncertainty estimation and robustness by making the loss landscape\nsmoother in the same manner as spatial smoothing. The code is available at\nhttps://github.com/xxxnell/spatial-smoothing.\n","authors":["Namuk Park","Songkuk Kim"],"pdf_url":"https://arxiv.org/pdf/2105.12639v4.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2207.06991v1","updated":"2022-07-14T15:20:36Z","published":"2022-07-14T15:20:36Z","title":"Language Modelling with Pixels","summary":"  Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches, instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust to noisy text inputs than BERT,\nfurther confirming the benefits of modelling language with pixels.\n","authors":["Phillip Rust","Jonas F. Lotz","Emanuele Bugliarello","Elizabeth Salesky","Miryam de Lhoneux","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2207.06991v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2207.06989v1","updated":"2022-07-14T15:17:19Z","published":"2022-07-14T15:17:19Z","title":"Tree Structure-Aware Few-Shot Image Classification via Hierarchical\n  Aggregation","summary":"  In this paper, we mainly focus on the problem of how to learn additional\nfeature representations for few-shot image classification through pretext tasks\n(e.g., rotation or color permutation and so on). This additional knowledge\ngenerated by pretext tasks can further improve the performance of few-shot\nlearning (FSL) as it differs from human-annotated supervision (i.e., class\nlabels of FSL tasks). To solve this problem, we present a plug-in Hierarchical\nTree Structure-aware (HTS) method, which not only learns the relationship of\nFSL and pretext tasks, but more importantly, can adaptively select and\naggregate feature representations generated by pretext tasks to maximize the\nperformance of FSL tasks. A hierarchical tree constructing component and a\ngated selection aggregating component is introduced to construct the tree\nstructure and find richer transferable knowledge that can rapidly adapt to\nnovel classes with a few labeled images. Extensive experiments show that our\nHTS can significantly enhance multiple few-shot methods to achieve new\nstate-of-the-art performance on four benchmark datasets. The code is available\nat: https://github.com/remiMZ/HTS-ECCV22.\n","authors":["Min Zhang","Siteng Huang","Wenbin Li","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2207.06989v1.pdf","comment":"22 pages, 9 figures and 4 tables Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06985v1","updated":"2022-07-14T15:10:29Z","published":"2022-07-14T15:10:29Z","title":"ObjectBox: From Centers to Boxes for Anchor-Free Object Detection","summary":"  We present ObjectBox, a novel single-stage anchor-free and highly\ngeneralizable object detection approach. As opposed to both existing\nanchor-based and anchor-free detectors, which are more biased toward specific\nobject scales in their label assignments, we use only object center locations\nas positive samples and treat all objects equally in different feature levels\nregardless of the objects' sizes or shapes. Specifically, our label assignment\nstrategy considers the object center locations as shape- and size-agnostic\nanchors in an anchor-free fashion, and allows learning to occur at all scales\nfor every object. To support this, we define new regression targets as the\ndistances from two corners of the center cell location to the four sides of the\nbounding box. Moreover, to handle scale-variant objects, we propose a tailored\nIoU loss to deal with boxes with different sizes. As a result, our proposed\nobject detector does not need any dataset-dependent hyperparameters to be tuned\nacross datasets. We evaluate our method on MS-COCO 2017 and PASCAL VOC 2012\ndatasets, and compare our results to state-of-the-art methods. We observe that\nObjectBox performs favorably in comparison to prior works. Furthermore, we\nperform rigorous ablation experiments to evaluate different components of our\nmethod. Our code is available at: https://github.com/MohsenZand/ObjectBox.\n","authors":["Mohsen Zand","Ali Etemad","Michael Greenspan"],"pdf_url":"https://arxiv.org/pdf/2207.06985v1.pdf","comment":"ECCV 2022 Oral"},{"id":"http://arxiv.org/abs/2207.06975v1","updated":"2022-07-14T14:57:01Z","published":"2022-07-14T14:57:01Z","title":"Learning Discriminative Representation via Metric Learning for\n  Imbalanced Medical Image Classification","summary":"  Data imbalance between common and rare diseases during model training often\ncauses intelligent diagnosis systems to have biased predictions towards common\ndiseases. The state-of-the-art approaches apply a two-stage learning framework\nto alleviate the class-imbalance issue, where the first stage focuses on\ntraining of a general feature extractor and the second stage focuses on\nfine-tuning the classifier head for class rebalancing. However, existing\ntwo-stage approaches do not consider the fine-grained property between\ndifferent diseases, often causing the first stage less effective for medical\nimage classification than for natural image classification tasks. In this\nstudy, we propose embedding metric learning into the first stage of the\ntwo-stage framework specially to help the feature extractor learn to extract\nmore discriminative feature representations. Extensive experiments mainly on\nthree medical image datasets show that the proposed approach consistently\noutperforms existing onestage and two-stage approaches, suggesting that metric\nlearning can be used as an effective plug-in component in the two-stage\nframework for fine-grained class-imbalanced image classification tasks.\n","authors":["Chenghua Zeng","Huijuan Lu","Kanghao Chen","Ruixuan Wang","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2207.06975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06968v1","updated":"2022-07-14T14:53:50Z","published":"2022-07-14T14:53:50Z","title":"PR-DARTS: Pruning-Based Differentiable Architecture Search","summary":"  The deployment of Convolutional Neural Networks (CNNs) on edge devices is\nhindered by the substantial gap between performance requirements and available\nprocessing power. While recent research has made large strides in developing\nnetwork pruning methods for reducing the computing overhead of CNNs, there\nremains considerable accuracy loss, especially at high pruning ratios.\nQuestioning that the architectures designed for non-pruned networks might not\nbe effective for pruned networks, we propose to search architectures for\npruning methods by defining a new search space and a novel search objective. To\nimprove the generalization of the pruned networks, we propose two novel\nPrunedConv and PrunedLinear operations. Specifically, these operations mitigate\nthe problem of unstable gradients by regularizing the objective function of the\npruned networks. The proposed search objective enables us to train architecture\nparameters regarding the pruned weight elements. Quantitative analyses\ndemonstrate that our searched architectures outperform those used in the\nstate-of-the-art pruning networks on CIFAR-10 and ImageNet. In terms of\nhardware effectiveness, PR-DARTS increases MobileNet-v2's accuracy from 73.44%\nto 81.35% (+7.91% improvement) and runs 3.87$\\times$ faster.\n","authors":["Hamid Mousavi","Mohammad Loni","Mina Alibeigi","Masoud Daneshtalab"],"pdf_url":"https://arxiv.org/pdf/2207.06968v1.pdf","comment":"18 pages with 11 figures"},{"id":"http://arxiv.org/abs/2207.06966v1","updated":"2022-07-14T14:51:50Z","published":"2022-07-14T14:51:50Z","title":"Scene Text Recognition with Permuted Autoregressive Sequence Models","summary":"  Context-aware STR methods typically use internal autoregressive (AR) language\nmodels (LM). Inherent limitations of AR models motivated two-stage methods\nwhich employ an external LM. The conditional independence of the external LM on\nthe input image may cause it to erroneously rectify correct predictions,\nleading to significant inefficiencies. Our method, PARSeq, learns an ensemble\nof internal AR LMs with shared weights using Permutation Language Modeling. It\nunifies context-free non-AR and context-aware AR inference, and iterative\nrefinement using bidirectional context. Using synthetic training data, PARSeq\nachieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and\nmore challenging datasets. It establishes new SOTA results (96.0% accuracy)\nwhen trained on real data. PARSeq is optimal on accuracy vs parameter count,\nFLOPS, and latency because of its simple, unified structure and parallel token\nprocessing. Due to its extensive use of attention, it is robust on\narbitrarily-oriented text which is common in real-world images. Code,\npretrained weights, and data are available at: https://github.com/baudm/parseq.\n","authors":["Darwin Bautista","Rowel Atienza"],"pdf_url":"https://arxiv.org/pdf/2207.06966v1.pdf","comment":"Accepted at the 17th European Conference on Computer Vision (ECCV\n  2022)"},{"id":"http://arxiv.org/abs/2207.06965v1","updated":"2022-07-14T14:49:32Z","published":"2022-07-14T14:49:32Z","title":"AutoMerge: A Framework for Map Assembling and Smoothing in City-scale\n  Environments","summary":"  We present AutoMerge, a LiDAR data processing framework for assembling a\nlarge number of map segments into a complete map. Traditional large-scale map\nmerging methods are fragile to incorrect data associations, and are primarily\nlimited to working only offline. AutoMerge utilizes multi-perspective fusion\nand adaptive loop closure detection for accurate data associations, and it uses\nincremental merging to assemble large maps from individual trajectory segments\ngiven in random order and with no initial estimations. Furthermore, after\nassembling the segments, AutoMerge performs fine matching and pose-graph\noptimization to globally smooth the merged map. We demonstrate AutoMerge on\nboth city-scale merging (120km) and campus-scale repeated merging (4.5km x 8).\nThe experiments show that AutoMerge (i) surpasses the second- and third- best\nmethods by 14% and 24% recall in segment retrieval, (ii) achieves comparable 3D\nmapping accuracy for 120 km large-scale map assembly, (iii) and it is robust to\ntemporally-spaced revisits. To the best of our knowledge, AutoMerge is the\nfirst mapping approach that can merge hundreds of kilometers of individual\nsegments without the aid of GPS.\n","authors":["Peng Yin","Haowen Lai","Shiqi Zhao","Ruijie Fu","Ivan Cisneros","Ruohai Ge","Ji Zhang","Howie Choset","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2207.06965v1.pdf","comment":"18 pages, 18 figure"},{"id":"http://arxiv.org/abs/2207.06955v1","updated":"2022-07-14T14:25:36Z","published":"2022-07-14T14:25:36Z","title":"Learning Implicit Templates for Point-Based Clothed Human Modeling","summary":"  We present FITE, a First-Implicit-Then-Explicit framework for modeling human\navatars in clothing. Our framework first learns implicit surface templates\nrepresenting the coarse clothing topology, and then employs the templates to\nguide the generation of point sets which further capture pose-dependent\nclothing deformations such as wrinkles. Our pipeline incorporates the merits of\nboth implicit and explicit representations, namely, the ability to handle\nvarying topology and the ability to efficiently capture fine details. We also\npropose diffused skinning to facilitate template training especially for loose\nclothing, and projection-based pose-encoding to extract pose information from\nmesh templates without predefined UV map or connectivity. Our code is publicly\navailable at https://github.com/jsnln/fite.\n","authors":["Siyou Lin","Hongwen Zhang","Zerong Zheng","Ruizhi Shao","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2207.06955v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06953v1","updated":"2022-07-14T14:25:19Z","published":"2022-07-14T14:25:19Z","title":"Tackling Background Distraction in Video Object Segmentation","summary":"  Semi-supervised video object segmentation (VOS) aims to densely track certain\ndesignated objects in videos. One of the main challenges in this task is the\nexistence of background distractors that appear similar to the target objects.\nWe propose three novel strategies to suppress such distractors: 1) a\nspatio-temporally diversified template construction scheme to obtain\ngeneralized properties of the target objects; 2) a learnable distance-scoring\nfunction to exclude spatially-distant distractors by exploiting the temporal\nconsistency between two consecutive frames; 3) swap-and-attach augmentation to\nforce each object to have unique features by providing training samples\ncontaining entangled objects. On all public benchmark datasets, our model\nachieves a comparable performance to contemporary state-of-the-art approaches,\neven with real-time performance. Qualitative results also demonstrate the\nsuperiority of our approach over existing methods. We believe our approach will\nbe widely used for future VOS research.\n","authors":["Suhwan Cho","Heansung Lee","Minhyeok Lee","Chaewon Park","Sungjun Jang","Minjung Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2207.06953v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.05727v2","updated":"2022-07-14T14:14:25Z","published":"2022-07-07T15:02:04Z","title":"Enhancing Fairness of Visual Attribute Predictors","summary":"  The performance of deep neural networks for image recognition tasks such as\npredicting a smiling face is known to degrade with under-represented classes of\nsensitive attributes. We address this problem by introducing fairness-aware\nregularization losses based on batch estimates of Demographic Parity, Equalized\nOdds, and a novel Intersection-over-Union measure. The experiments performed on\nfacial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma\nclassification challenge show the effectiveness of our proposed fairness losses\nfor bias mitigation as they improve model fairness while maintaining high\nclassification performance. To the best of our knowledge, our work is the first\nattempt to incorporate these types of losses in an end-to-end training scheme\nfor mitigating biases of visual attribute predictors. Our code is available at\nhttps://github.com/nish03/FVAP.\n","authors":["Tobias Hänel","Nishant Kumar","Dmitrij Schlesinger","Mengze Li","Erdem Ünal","Abouzar Eslami","Stefan Gumhold"],"pdf_url":"https://arxiv.org/pdf/2207.05727v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2207.06946v1","updated":"2022-07-14T14:11:34Z","published":"2022-07-14T14:11:34Z","title":"Insurgency as Complex Network: Image Co-Appearance and Hierarchy in the\n  PKK","summary":"  Despite a growing recognition of the importance of insurgent group structure\non conflict outcomes, there is very little empirical research thereon. Though\nthis problem is rooted in the inaccessibility of data on militant group\nstructure, insurgents frequently publish large volumes of image data on the\ninternet. In this paper, I develop a new methodology that leverages this\nabundant but underutilized source of data by automating the creation of a\nsocial network graph based on co-appearance in photographs using deep learning.\nUsing a trove of 19,115 obituary images published online by the PKK, a Kurdish\nmilitant group in Turkey, I demonstrate that an individual's centrality in the\nresulting co-appearance network is closely correlated with their rank in the\ninsurgent group.\n","authors":["Ollie Ballinger"],"pdf_url":"https://arxiv.org/pdf/2207.06946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06937v1","updated":"2022-07-14T14:01:03Z","published":"2022-07-14T14:01:03Z","title":"Real-time Streaming Video Denoising with Bidirectional Buffers","summary":"  Video streams are delivered continuously to save the cost of storage and\ndevice memory. Real-time denoising algorithms are typically adopted on the user\ndevice to remove the noise involved during the shooting and transmission of\nvideo streams. However, sliding-window-based methods feed multiple input frames\nfor a single output and lack computation efficiency. Recent multi-output\ninference works propagate the bidirectional temporal feature with a parallel or\nrecurrent framework, which either suffers from performance drops on the\ntemporal edges of clips or can not achieve online inference. In this paper, we\npropose a Bidirectional Streaming Video Denoising (BSVD) framework, to achieve\nhigh-fidelity real-time denoising for streaming videos with both past and\nfuture temporal receptive fields. The bidirectional temporal fusion for online\ninference is considered not applicable in the MoViNet. However, we introduce a\nnovel Bidirectional Buffer Block as the core module of our BSVD, which makes it\npossible during our pipeline-style inference. In addition, our method is\nconcise and flexible to be utilized in both non-blind and blind video\ndenoising. We compare our model with various state-of-the-art video denoising\nmodels qualitatively and quantitatively on synthetic and real noise. Our method\noutperforms previous methods in terms of restoration fidelity and runtime. Our\nsource code is publicly available at https://github.com/ChenyangQiQi/BSVD\n","authors":["Chenyang Qi","Junming Chen","Xin Yang","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2207.06937v1.pdf","comment":"Accepted to ACM MM 2022; Github link:\n  https://github.com/ChenyangQiQi/BSVD ;"},{"id":"http://arxiv.org/abs/2207.03078v3","updated":"2022-07-14T13:51:02Z","published":"2022-07-07T04:24:17Z","title":"What Makes for Automatic Reconstruction of Pulmonary Segments","summary":"  3D reconstruction of pulmonary segments plays an important role in surgical\ntreatment planning of lung cancer, which facilitates preservation of pulmonary\nfunction and helps ensure low recurrence rates. However, automatic\nreconstruction of pulmonary segments remains unexplored in the era of deep\nlearning. In this paper, we investigate what makes for automatic reconstruction\nof pulmonary segments. First and foremost, we formulate, clinically and\ngeometrically, the anatomical definitions of pulmonary segments, and propose\nevaluation metrics adhering to these definitions. Second, we propose ImPulSe\n(Implicit Pulmonary Segment), a deep implicit surface model designed for\npulmonary segment reconstruction. The automatic reconstruction of pulmonary\nsegments by ImPulSe is accurate in metrics and visually appealing. Compared\nwith canonical segmentation methods, ImPulSe outputs continuous predictions of\narbitrary resolutions with higher training efficiency and fewer parameters.\nLastly, we experiment with different network inputs to analyze what matters in\nthe task of pulmonary segment reconstruction. Our code is available at\nhttps://github.com/M3DV/ImPulSe.\n","authors":["Kaiming Kuang","Li Zhang","Jingyu Li","Hongwei Li","Jiajun Chen","Bo Du","Jiancheng Yang"],"pdf_url":"https://arxiv.org/pdf/2207.03078v3.pdf","comment":"MICCAI 2022"},{"id":"http://arxiv.org/abs/2207.06899v1","updated":"2022-07-14T13:28:08Z","published":"2022-07-14T13:28:08Z","title":"Factorized and Controllable Neural Re-Rendering of Outdoor Scene for\n  Photo Extrapolation","summary":"  Expanding an existing tourist photo from a partially captured scene to a full\nscene is one of the desired experiences for photography applications. Although\nphoto extrapolation has been well studied, it is much more challenging to\nextrapolate a photo (i.e., selfie) from a narrow field of view to a wider one\nwhile maintaining a similar visual style. In this paper, we propose a\nfactorized neural re-rendering model to produce photorealistic novel views from\ncluttered outdoor Internet photo collections, which enables the applications\nincluding controllable scene re-rendering, photo extrapolation and even\nextrapolated 3D photo generation. Specifically, we first develop a novel\nfactorized re-rendering pipeline to handle the ambiguity in the decomposition\nof geometry, appearance and illumination. We also propose a composited training\nstrategy to tackle the unexpected occlusion in Internet images. Moreover, to\nenhance photo-realism when extrapolating tourist photographs, we propose a\nnovel realism augmentation process to complement appearance details, which\nautomatically propagates the texture details from a narrow captured photo to\nthe extrapolated neural rendered image. The experiments and photo editing\nexamples on outdoor scenes demonstrate the superior performance of our proposed\nmethod in both photo-realism and downstream applications.\n","authors":["Boming Zhao","Bangbang Yang","Zhenyang Li","Zuoyue Li","Guofeng Zhang","Jiashu Zhao","Dawei Yin","Zhaopeng Cui","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2207.06899v1.pdf","comment":"Accepted to ACM Multimedia 2022. Project Page:\n  https://zju3dv.github.io/neural_outdoor_rerender/"},{"id":"http://arxiv.org/abs/2207.06893v1","updated":"2022-07-14T13:24:27Z","published":"2022-07-14T13:24:27Z","title":"E2FIF: Push the limit of Binarized Deep Imagery Super-resolution using\n  End-to-end Full-precision Information Flow","summary":"  Binary neural network (BNN) provides a promising solution to deploy\nparameter-intensive deep single image super-resolution (SISR) models onto real\ndevices with limited storage and computational resources. To achieve comparable\nperformance with the full-precision counterpart, most existing BNNs for SISR\nmainly focus on compensating the information loss incurred by binarizing\nweights and activations in the network through better approximations to the\nbinarized convolution. In this study, we revisit the difference between BNNs\nand their full-precision counterparts and argue that the key for good\ngeneralization performance of BNNs lies on preserving a complete full-precision\ninformation flow as well as an accurate gradient flow passing through each\nbinarized convolution layer. Inspired by this, we propose to introduce a\nfull-precision skip connection or its variant over each binarized convolution\nlayer across the entire network, which can increase the forward expressive\ncapability and the accuracy of back-propagated gradient, thus enhancing the\ngeneralization performance. More importantly, such a scheme is applicable to\nany existing BNN backbones for SISR without introducing any additional\ncomputation cost. To testify its efficacy, we evaluate it using four different\nbackbones for SISR on four benchmark datasets and report obviously superior\nperformance over existing BNNs and even some 4-bit competitors.\n","authors":["Zhiqiang Lang","Lei Zhang","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2207.06893v1.pdf","comment":"Simple but Stronge baseline for binarized SR networks. Code is\n  available at https://github.com/pppLang/E2FIF"},{"id":"http://arxiv.org/abs/2206.13393v2","updated":"2022-07-14T12:58:05Z","published":"2022-06-20T11:38:55Z","title":"Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing\n  Framework for Alzheimer's Disease","summary":"  Cross-modal fusion of different types of neuroimaging data has shown great\npromise for predicting the progression of Alzheimer's Disease(AD). However,\nmost existing methods applied in neuroimaging can not efficiently fuse the\nfunctional and structural information from multi-modal neuroimages. In this\nwork, a novel cross-modal transformer generative adversarial network(CT-GAN) is\nproposed to fuse functional information contained in resting-state functional\nmagnetic resonance imaging (rs-fMRI) and structural information contained in\nDiffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match\nfunctional information to structural information efficiently and maximize the\ncapability of extracting complementary information from rs-fMRI and DTI. By\ncapturing the deep complementary information between structural features and\nfunctional features, the proposed CT-GAN can detect the AD-related brain\nconnectivity, which could be used as a bio-marker of AD. Experimental results\nshow that the proposed model can not only improve classification performance\nbut also detect the AD-related brain connectivity effectively.\n","authors":["Junren Pan","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2206.13393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06873v1","updated":"2022-07-14T12:50:09Z","published":"2022-07-14T12:50:09Z","title":"BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen\n  Neural Networks","summary":"  High-quality calibrated uncertainty estimates are crucial for numerous\nreal-world applications, especially for deep learning-based deployed ML\nsystems. While Bayesian deep learning techniques allow uncertainty estimation,\ntraining them with large-scale datasets is an expensive process that does not\nalways yield models competitive with non-Bayesian counterparts. Moreover, many\nof the high-performing deep learning models that are already trained and\ndeployed are non-Bayesian in nature and do not provide uncertainty estimates.\nTo address these issues, we propose BayesCap that learns a Bayesian identity\nmapping for the frozen model, allowing uncertainty estimation. BayesCap is a\nmemory-efficient method that can be trained on a small fraction of the original\ndataset, enhancing pretrained non-Bayesian computer vision models by providing\ncalibrated uncertainty estimates for the predictions without (i) hampering the\nperformance of the model and (ii) the need for expensive retraining the model\nfrom scratch. The proposed method is agnostic to various architectures and\ntasks. We show the efficacy of our method on a wide variety of tasks with a\ndiverse set of architectures, including image super-resolution, deblurring,\ninpainting, and crucial application such as medical image translation.\nMoreover, we apply the derived uncertainty estimates to detect\nout-of-distribution samples in critical scenarios like depth estimation in\nautonomous driving. Code is available at\nhttps://github.com/ExplainableML/BayesCap.\n","authors":["Uddeshya Upadhyay","Shyamgopal Karthik","Yanbei Chen","Massimiliano Mancini","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2207.06873v1.pdf","comment":"Accepted at ECCV 2022. Code is available at\n  https://github.com/ExplainableML/BayesCap"},{"id":"http://arxiv.org/abs/2207.06235v2","updated":"2022-07-14T12:35:04Z","published":"2022-07-13T14:31:09Z","title":"Entry-Flipped Transformer for Inference and Prediction of Participant\n  Behavior","summary":"  Some group activities, such as team sports and choreographed dances, involve\nclosely coupled interaction between participants. Here we investigate the tasks\nof inferring and predicting participant behavior, in terms of motion paths and\nactions, under such conditions. We narrow the problem to that of estimating how\na set target participants react to the behavior of other observed participants.\nOur key idea is to model the spatio-temporal relations among participants in a\nmanner that is robust to error accumulation during frame-wise inference and\nprediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),\nwhich models the relations of participants by attention mechanisms on both\nspatial and temporal domains. Unlike typical transformers, we tackle the\nproblem of error accumulation by flipping the order of query, key, and value\nentries, to increase the importance and fidelity of observed features in the\ncurrent frame. Comparative experiments show that our EF-Transformer achieves\nthe best performance on a newly-collected tennis doubles dataset, a Ceilidh\ndance dataset, and two pedestrian datasets. Furthermore, it is also\ndemonstrated that our EF-Transformer is better at limiting accumulated errors\nand recovering from wrong estimations.\n","authors":["Bo Hu","Tat-Jen Cham"],"pdf_url":"https://arxiv.org/pdf/2207.06235v2.pdf","comment":"Accepted in ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06861v1","updated":"2022-07-14T12:29:52Z","published":"2022-07-14T12:29:52Z","title":"Immunofluorescence Capillary Imaging Segmentation: Cases Study","summary":"  Nonunion is one of the challenges faced by orthopedics clinics for the\ntechnical difficulties and high costs in photographing interosseous\ncapillaries. Segmenting vessels and filling capillaries are critical in\nunderstanding the obstacles encountered in capillary growth. However, existing\ndatasets for blood vessel segmentation mainly focus on the large blood vessels\nof the body, and the lack of labeled capillary image datasets greatly limits\nthe methodological development and applications of vessel segmentation and\ncapillary filling. Here, we present a benchmark dataset, named IFCIS-155,\nconsisting of 155 2D capillary images with segmentation boundaries and vessel\nfillings annotated by biomedical experts, and 19 large-scale, high-resolution\n3D capillary images. To obtain better images of interosseous capillaries, we\nleverage state-of-the-art immunofluorescence imaging techniques to highlight\nthe rich vascular morphology of interosseous capillaries. We conduct\ncomprehensive experiments to verify the effectiveness of the dataset and the\nbenchmarking deep learning models (\\eg UNet/UNet++ and the modified\nUNet/UNet++). Our work offers a benchmark dataset for training deep learning\nmodels for capillary image segmentation and provides a potential tool for\nfuture capillary research. The IFCIS-155 dataset and code are all publicly\navailable at \\url{https://github.com/ncclabsustech/IFCIS-55}.\n","authors":["Runpeng Hou","Ziyuan Ye","Chengyu Yang","Linhao Fu","Chao Liu","Quanying Liu"],"pdf_url":"https://arxiv.org/pdf/2207.06861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1904.03848v2","updated":"2022-07-14T12:27:15Z","published":"2019-04-08T05:41:48Z","title":"Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes","summary":"  Unsupervised deep learning for optical flow computation has achieved\npromising results. Most existing deep-net based methods rely on image\nbrightness consistency and local smoothness constraint to train the networks.\nTheir performance degrades at regions where repetitive textures or occlusions\noccur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical\nflow method which incorporates global geometric constraints into network\nlearning. In particular, we investigate multiple ways of enforcing the epipolar\nconstraint in flow estimation. To alleviate a \"chicken-and-egg\" type of problem\nencountered in dynamic scenes where multiple motions may be present, we propose\na low-rank constraint as well as a union-of-subspaces constraint for training.\nExperimental results on various benchmarking datasets show that our method\nachieves competitive performance compared with supervised methods and\noutperforms state-of-the-art unsupervised deep-learning methods.\n","authors":["Yiran Zhong","Pan Ji","Jianyuan Wang","Yuchao Dai","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/1904.03848v2.pdf","comment":"CVPR 2019"},{"id":"http://arxiv.org/abs/2207.06854v1","updated":"2022-07-14T12:19:32Z","published":"2022-07-14T12:19:32Z","title":"AIParsing: Anchor-free Instance-level Human Parsing","summary":"  Most state-of-the-art instance-level human parsing models adopt two-stage\nanchor-based detectors and, therefore, cannot avoid the heuristic anchor box\ndesign and the lack of analysis on a pixel level. To address these two issues,\nwe have designed an instance-level human parsing network which is anchor-free\nand solvable on a pixel level. It consists of two simple sub-networks: an\nanchor-free detection head for bounding box predictions and an edge-guided\nparsing head for human segmentation. The anchor-free detector head inherits the\npixel-like merits and effectively avoids the sensitivity of hyper-parameters as\nproved in object detection applications. By introducing the part-aware boundary\nclue, the edge-guided parsing head is capable to distinguish adjacent human\nparts from among each other up to 58 parts in a single human instance, even\noverlapping instances. Meanwhile, a refinement head integrating box-level score\nand part-level parsing quality is exploited to improve the quality of the\nparsing results. Experiments on two multiple human parsing datasets (i.e., CIHP\nand LV-MHP-v2.0) and one video instance-level human parsing dataset (i.e., VIP)\nshow that our method achieves the best global-level and instance-level\nperformance over state-of-the-art one-stage top-down alternatives.\n","authors":["Sanyi Zhang","Xiaochun Cao","Guo-Jun Qi","Zhanjie Song","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2207.06854v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing (TIP)"},{"id":"http://arxiv.org/abs/2207.06841v1","updated":"2022-07-14T11:54:58Z","published":"2022-07-14T11:54:58Z","title":"Deep Dictionary Learning with An Intra-class Constraint","summary":"  In recent years, deep dictionary learning (DDL)has attracted a great amount\nof attention due to its effectiveness for representation learning and visual\nrecognition.~However, most existing methods focus on unsupervised deep\ndictionary learning, failing to further explore the category information.~To\nmake full use of the category information of different samples, we propose a\nnovel deep dictionary learning model with an intra-class constraint (DDLIC) for\nvisual classification. Specifically, we design the intra-class compactness\nconstraint on the intermediate representation at different levels to encourage\nthe intra-class representations to be closer to each other, and eventually the\nlearned representation becomes more discriminative.~Unlike the traditional DDL\nmethods, during the classification stage, our DDLIC performs a layer-wise\ngreedy optimization in a similar way to the training stage. Experimental\nresults on four image datasets show that our method is superior to the\nstate-of-the-art methods.\n","authors":["Xia Yuan","Jianping Gou","Baosheng Yu","Jiali Yu","Zhang Yi"],"pdf_url":"https://arxiv.org/pdf/2207.06841v1.pdf","comment":"6 pages, 3 figures, 2 tables. It has been accepted in ICME2022"},{"id":"http://arxiv.org/abs/2201.03560v2","updated":"2022-07-14T11:54:22Z","published":"2022-01-10T16:14:27Z","title":"Iterative training of robust k-space interpolation networks for improved\n  image reconstruction with limited scan specific training samples","summary":"  Purpose: To evaluate an iterative learning approach for enhanced performance\nof Robust Artificial-neural-networks for K-space Interpolation (RAKI), when\nonly a limited amount of training data (auto-calibration signals, ACS) are\navailable for accelerated standard 2D imaging. Methods: In a first step, the\nRAKI model was optimized for the case of strongly limited training data amount.\nIn the iterative learning approach (termed iterative RAKI), the optimized RAKI\nmodel is initially trained using original and augmented ACS obtained from a\nlinear parallel imaging reconstruction. Subsequently, the RAKI convolution\nfilters are refined iteratively using original and augmented ACS extracted from\nthe previous RAKI reconstruction. Evaluation was carried out on 200\nretrospectively undersampled in-vivo datasets from the fastMRI neuro database\nwith different contrast settings. Results: For limited training data (18 and 22\nACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard\nRAKI by reducing residual artefacts and yields strong noise suppression when\ncompared to standard parallel imaging, underlined by quantitative\nreconstruction quality metrics. In combination with a phase constraint, further\nreconstruction improvements can be achieved. Additionally, iterative RAKI shows\nbetter performance than both GRAPPA and RAKI in case of pre-scan calibration\nwith varying contrast between training- and undersampled data. Conclusion: The\niterative learning approach with RAKI benefits from standard RAKIs well known\nnoise suppression feature but requires less original training data for the\naccurate reconstruction of standard 2D images thereby improving net\nacceleration.\n","authors":["Peter Dawood","Felix Breuer","Paul R. Burd","István Homolya","Johannes Oberberger","Peter M. Jakob","Martin Blaimer"],"pdf_url":"https://arxiv.org/pdf/2201.03560v2.pdf","comment":"Submitted to Magnetic Resonance in Medicine"},{"id":"http://arxiv.org/abs/2008.00230v4","updated":"2022-07-14T11:47:17Z","published":"2020-08-01T10:01:32Z","title":"RGB-D Salient Object Detection: A Survey","summary":"  Salient object detection (SOD), which simulates the human visual perception\nsystem to locate the most attractive object(s) in a scene, has been widely\napplied to various computer vision tasks. Now, with the advent of depth\nsensors, depth maps with affluent spatial information that can be beneficial in\nboosting the performance of SOD, can easily be captured. Although various RGB-D\nbased SOD models with promising performance have been proposed over the past\nseveral years, an in-depth understanding of these models and challenges in this\ntopic remains lacking. In this paper, we provide a comprehensive survey of\nRGB-D based SOD models from various perspectives, and review related benchmark\ndatasets in detail. Further, considering that the light field can also provide\ndepth maps, we review SOD models and popular benchmark datasets from this\ndomain as well. Moreover, to investigate the SOD ability of existing models, we\ncarry out a comprehensive evaluation, as well as attribute-based evaluation of\nseveral representative RGB-D based SOD models. Finally, we discuss several\nchallenges and open directions of RGB-D based SOD for future research. All\ncollected models, benchmark datasets, source code links, datasets constructed\nfor attribute-based evaluation, and codes for evaluation will be made publicly\navailable at https://github.com/taozh2017/RGBDSODsurvey\n","authors":["Tao Zhou","Deng-Ping Fan","Ming-Ming Cheng","Jianbing Shen","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2008.00230v4.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2207.06832v1","updated":"2022-07-14T11:42:18Z","published":"2022-07-14T11:42:18Z","title":"Enforcing connectivity of 3D linear structures using their 2D\n  projections","summary":"  Many biological and medical tasks require the delineation of 3D curvilinear\nstructures such as blood vessels and neurites from image volumes. This is\ntypically done using neural networks trained by minimizing voxel-wise loss\nfunctions that do not capture the topological properties of these structures.\nAs a result, the connectivity of the recovered structures is often wrong, which\nlessens their usefulness. In this paper, we propose to improve the 3D\nconnectivity of our results by minimizing a sum of topology-aware losses on\ntheir 2D projections. This suffices to increase the accuracy and to reduce the\nannotation effort required to provide the required annotated training data.\n","authors":["Doruk Oner","Hussein Osman","Mateusz Kozinski","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2207.06832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06831v1","updated":"2022-07-14T11:40:32Z","published":"2022-07-14T11:40:32Z","title":"iColoriT: Towards Propagating Local Hint to the Right Region in\n  Interactive Colorization by Leveraging Vision Transformer","summary":"  Point-interactive image colorization aims to colorize grayscale images when a\nuser provides the colors for specific locations. It is essential for\npoint-interactive colorization methods to appropriately propagate user-provided\ncolors (i.e., user hints) in the entire image to obtain a reasonably colorized\nimage with minimal user effort. However, existing approaches often produce\npartially colorized results due to the inefficient design of stacking\nconvolutional layers to propagate hints to distant relevant regions. To address\nthis problem, we present iColoriT, a novel point-interactive colorization\nVision Transformer capable of propagating user hints to relevant regions,\nleveraging the global receptive field of Transformers. The self-attention\nmechanism of Transformers enables iColoriT to selectively colorize relevant\nregions with only a few local hints. Our approach colorizes images in real-time\nby utilizing pixel shuffling, an efficient upsampling technique that replaces\nthe decoder architecture. Also, in order to mitigate the artifacts caused by\npixel shuffling with large upsampling ratios, we present the local stabilizing\nlayer. Extensive quantitative and qualitative results demonstrate that our\napproach highly outperforms existing methods for point-interactive\ncolorization, producing accurately colorized images with a user's minimal\neffort.\n","authors":["Sanghyeon Lee","Jooyeol Yun","Minho Park"],"pdf_url":"https://arxiv.org/pdf/2207.06831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06828v1","updated":"2022-07-14T11:32:42Z","published":"2022-07-14T11:32:42Z","title":"Pose-based Tremor Classification for Parkinson's Disease Diagnosis from\n  Video","summary":"  Parkinson's disease (PD) is a progressive neurodegenerative disorder that\nresults in a variety of motor dysfunction symptoms, including tremors,\nbradykinesia, rigidity and postural instability. The diagnosis of PD mainly\nrelies on clinical experience rather than a definite medical test, and the\ndiagnostic accuracy is only about 73-84% since it is challenged by the\nsubjective opinions or experiences of different medical experts. Therefore, an\nefficient and interpretable automatic PD diagnosis system is valuable for\nsupporting clinicians with more robust diagnostic decision-making. To this end,\nwe propose to classify Parkinson's tremor since it is one of the most\npredominant symptoms of PD with strong generalizability. Different from other\ncomputer-aided time and resource-consuming Parkinson's Tremor (PT)\nclassification systems that rely on wearable sensors, we propose SPAPNet, which\nonly requires consumer-grade non-intrusive video recording of camera-facing\nhuman movements as input to provide undiagnosed patients with low-cost PT\nclassification results as a PD warning sign. For the first time, we propose to\nuse a novel attention module with a lightweight pyramidal\nchannel-squeezing-fusion architecture to extract relevant PT information and\nfilter the noise efficiently. This design aids in improving both classification\nperformance and system interpretability. Experimental results show that our\nsystem outperforms state-of-the-arts by achieving a balanced accuracy of 90.9%\nand an F1-score of 90.6% in classifying PT with the non-PT class.\n","authors":["Haozheng Zhang","Edmond S. L. Ho","Xiatian Zhang","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2207.06828v1.pdf","comment":"MICCAI 2022"},{"id":"http://arxiv.org/abs/2207.06827v1","updated":"2022-07-14T11:32:00Z","published":"2022-07-14T11:32:00Z","title":"Point-to-Box Network for Accurate Object Detection via Single Point\n  Supervision","summary":"  Object detection using single point supervision has received increasing\nattention over the years. In this paper, we attribute such a large performance\ngap to the failure of generating high-quality proposal bags which are crucial\nfor multiple instance learning (MIL). To address this problem, we introduce a\nlightweight alternative to the off-the-shelf proposal (OTSP) method and thereby\ncreate the Point-to-Box Network (P2BNet), which can construct an inter-objects\nbalanced proposal bag by generating proposals in an anchor-like way. By fully\ninvestigating the accurate position information, P2BNet further constructs an\ninstance-level bag, avoiding the mixture of multiple objects. Finally, a\ncoarse-to-fine policy in a cascade fashion is utilized to improve the IoU\nbetween proposals and ground-truth (GT). Benefiting from these strategies,\nP2BNet is able to produce high-quality instance-level bags for object\ndetection. P2BNet improves the mean average precision (AP) by more than 50%\nrelative to the previous best PSOD method on the MS COCO dataset. It also\ndemonstrates the great potential to bridge the performance gap between point\nsupervised and bounding-box supervised detectors. The code will be released at\ngithub.com/ucas-vg/P2BNet.\n","authors":["Pengfei Chen","Xuehui Yu","Xumeng Han","Najmul Hassan","Kai Wang","Jiachen Li","Jian Zhao","Humphrey Shi","Zhenjun Han","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2207.06827v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.06825v1","updated":"2022-07-14T11:30:38Z","published":"2022-07-14T11:30:38Z","title":"Refign: Align and Refine for Adaptation of Semantic Segmentation to\n  Adverse Conditions","summary":"  Due to the scarcity of dense pixel-level semantic annotations for images\nrecorded in adverse visual conditions, there has been a keen interest in\nunsupervised domain adaptation (UDA) for the semantic segmentation of such\nimages. UDA adapts models trained on normal conditions to the target\nadverse-condition domains. Meanwhile, multiple datasets with driving scenes\nprovide corresponding images of the same scenes across multiple conditions,\nwhich can serve as a form of weak supervision for domain adaptation. We propose\nRefign, a generic extension to self-training-based UDA methods which leverages\nthese cross-domain correspondences. Refign consists of two steps: (1) aligning\nthe normal-condition image to the corresponding adverse-condition image using\nan uncertainty-aware dense matching network, and (2) refining the adverse\nprediction with the normal prediction using an adaptive label correction\nmechanism. We design custom modules to streamline both steps and set the new\nstate of the art for domain-adaptive semantic segmentation on several\nadverse-condition benchmarks, including ACDC and Dark Zurich. The approach\nintroduces no extra training parameters, minimal computational overhead --\nduring training only -- and can be used as a drop-in extension to improve any\ngiven self-training-based UDA method. Code is available at\nhttps://github.com/brdav/refign.\n","authors":["David Bruggemann","Christos Sakaridis","Prune Truong","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2207.06825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00380v2","updated":"2022-07-14T11:28:50Z","published":"2022-06-01T10:30:59Z","title":"Strongly Augmented Contrastive Clustering","summary":"  Deep clustering has attracted increasing attention in recent years due to its\ncapability of joint representation learning and clustering via deep neural\nnetworks. In its latest developments, the contrastive learning has emerged as\nan effective technique to substantially enhance the deep clustering\nperformance. However, the existing contrastive learning based deep clustering\nalgorithms mostly focus on some carefully-designed augmentations (often with\nlimited transformations to preserve the structure), referred to as weak\naugmentations, but cannot go beyond the weak augmentations to explore the more\nopportunities in stronger augmentations (with more aggressive transformations\nor even severe distortions). In this paper, we present an end-to-end deep\nclustering approach termed Strongly Augmented Contrastive Clustering (SACC),\nwhich extends the conventional two-augmentation-view paradigm to multiple views\nand jointly leverages strong and weak augmentations for strengthened deep\nclustering. Particularly, we utilize a backbone network with triply-shared\nweights, where a strongly augmented view and two weakly augmented views are\nincorporated. Based on the representations produced by the backbone, the\nweak-weak view pair and the strong-weak view pairs are simultaneously exploited\nfor the instance-level contrastive learning (via an instance projector) and the\ncluster-level contrastive learning (via a cluster projector), which, together\nwith the backbone, can be jointly optimized in a purely unsupervised manner.\nExperimental results on five challenging image datasets have shown the\nsuperiority of our SACC approach over the state-of-the-art. The code is\navailable at https://github.com/dengxiaozhi/SACC.\n","authors":["Xiaozhi Deng","Dong Huang","Ding-Hua Chen","Chang-Dong Wang","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2206.00380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06823v1","updated":"2022-07-14T11:27:02Z","published":"2022-07-14T11:27:02Z","title":"DEXTER: An end-to-end system to extract table contents from electronic\n  medical health documents","summary":"  In this paper, we propose DEXTER, an end to end system to extract information\nfrom tables present in medical health documents, such as electronic health\nrecords (EHR) and explanation of benefits (EOB). DEXTER consists of four\nsub-system stages: i) table detection ii) table type classification iii) cell\ndetection; and iv) cell content extraction. We propose a two-stage transfer\nlearning-based approach using CDeC-Net architecture along with Non-Maximal\nsuppression for table detection. We design a conventional computer vision-based\napproach for table type classification and cell detection using parameterized\nkernels based on image size for detecting rows and columns. Finally, we extract\nthe text from the detected cells using pre-existing OCR engine Tessaract. To\nevaluate our system, we manually annotated a sample of the real-world medical\ndataset (referred to as Meddata) consisting of wide variations of documents (in\nterms of appearance) covering different table structures, such as bordered,\npartially bordered, borderless, or coloured tables. We experimentally show that\nDEXTER outperforms the commercially available Amazon Textract and Microsoft\nAzure Form Recognizer systems on the annotated real-world medical dataset\n","authors":["Nandhinee PR","Harinath Krishnamoorthy","Anil Goyal","Sudarsun Santhiappan"],"pdf_url":"https://arxiv.org/pdf/2207.06823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02738v2","updated":"2022-07-14T10:57:30Z","published":"2022-02-06T08:49:21Z","title":"Enhancing variational generation through self-decomposition","summary":"  In this article we introduce the notion of Split Variational Autoencoder\n(SVAE), whose output $\\hat{x}$ is obtained as a weighted sum $\\sigma \\odot\n\\hat{x_1} + (1-\\sigma) \\odot \\hat{x_2}$ of two generated images\n$\\hat{x_1},\\hat{x_2}$, and $\\sigma$ is a {\\em learned} compositional map. The\ncomposing images $\\hat{x_1},\\hat{x_2}$, as well as the $\\sigma$-map are\nautomatically synthesized by the model. The network is trained as a usual\nVariational Autoencoder with a negative loglikelihood loss between training and\nreconstructed images. No additional loss is required for $\\hat{x_1},\\hat{x_2}$\nor $\\sigma$, neither any form of human tuning. The decomposition is\nnondeterministic, but follows two main schemes, that we may roughly categorize\nas either \\say{syntactic} or \\say{semantic}. In the first case, the map tends\nto exploit the strong correlation between adjacent pixels, splitting the image\nin two complementary high frequency sub-images. In the second case, the map\ntypically focuses on the contours of objects, splitting the image in\ninteresting variations of its content, with more marked and distinctive\nfeatures. In this case, according to empirical observations, the Fr\\'echet\nInception Distance (FID) of $\\hat{x_1}$ and $\\hat{x_2}$ is usually lower (hence\nbetter) than that of $\\hat{x}$, that clearly suffers from being the average of\nthe former. In a sense, a SVAE forces the Variational Autoencoder to make\nchoices, in contrast with its intrinsic tendency to {\\em average} between\nalternatives with the aim to minimize the reconstruction loss towards a\nspecific sample. According to the FID metric, our technique, tested on typical\ndatasets such as Mnist, Cifar10 and CelebA, allows us to outperform all\nprevious purely variational architectures (not relying on normalization flows).\n","authors":["Andrea Asperti","Laura Bugo","Daniele Filippini"],"pdf_url":"https://arxiv.org/pdf/2202.02738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06817v1","updated":"2022-07-14T10:53:53Z","published":"2022-07-14T10:53:53Z","title":"Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for\n  Few-Shot Learning","summary":"  Most existing few-shot learning (FSL) methods require a large amount of\nlabeled data in meta-training, which is a major limit. To reduce the\nrequirement of labels, a semi-supervised meta-training setting has been\nproposed for FSL, which includes only a few labeled samples and numbers of\nunlabeled samples in base classes. However, existing methods under this setting\nrequire class-aware sample selection from the unlabeled set, which violates the\nassumption of unlabeled set. In this paper, we propose a practical\nsemi-supervised meta-training setting with truly unlabeled data. Under the new\nsetting, the performance of existing methods drops notably. To better utilize\nboth the labeled and truly unlabeled data, we propose a simple and effective\nmeta-training framework, called pseudo-labeling based on meta-learning (PLML).\nFirstly, we train a classifier via common semi-supervised learning (SSL) and\nuse it to obtain the pseudo-labels of unlabeled data. Then we build few-shot\ntasks from labeled and pseudo-labeled data and run meta-learning over the\nconstructed tasks to learn the FSL model. Surprisingly, through extensive\nexperiments across two FSL datasets, we find that this simple meta-training\nframework effectively prevents the performance degradation of FSL under limited\nlabeled data. Besides, benefiting from meta-training, the proposed method\nimproves the classifiers learned by two representative SSL algorithms as well.\n","authors":["Xingping Dong","Ling Shao","Shengcai Liao"],"pdf_url":"https://arxiv.org/pdf/2207.06817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06799v1","updated":"2022-07-14T10:23:17Z","published":"2022-07-14T10:23:17Z","title":"A Multi-Modality Ovarian Tumor Ultrasound Image Dataset for Unsupervised\n  Cross-Domain Semantic Segmentation","summary":"  Ovarian cancer is one of the most harmful gynecological diseases. Detecting\novarian tumors in early stage with computer-aided techniques can efficiently\ndecrease the mortality rate. With the improvement of medical treatment\nstandard, ultrasound images are widely applied in clinical treatment. However,\nrecent notable methods mainly focus on single-modality ultrasound ovarian tumor\nsegmentation or recognition, which means there still lacks of researches on\nexploring the representation capability of multi-modality ultrasound ovarian\ntumor images. To solve this problem, we propose a Multi-Modality Ovarian Tumor\nUltrasound (MMOTU) image dataset containing 1469 2d ultrasound images and 170\ncontrast enhanced ultrasonography (CEUS) images with pixel-wise and global-wise\nannotations. Based on MMOTU, we mainly focus on unsupervised cross-domain\nsemantic segmentation task. To solve the domain shift problem, we propose a\nfeature alignment based architecture named Dual-Scheme Domain-Selected Network\n(DS$^2$Net). Specifically, we first design source-encoder and target-encoder to\nextract two-style features of source and target images. Then, we propose\nDomain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module\n(DUSM) to extract the distinct and universal features in two styles\n(source-style or target-style). Finally, we fuse these two kinds of features\nand feed them into the source-decoder and target-decoder to generate final\npredictions. Extensive comparison experiments and analysis on MMOTU image\ndataset show that DS$^2$Net can boost the segmentation performance for\nbidirectional cross-domain adaptation of 2d ultrasound images and CEUS images.\n","authors":["Qi Zhao","Shuchang Lyu","Wenpei Bai","Linghan Cai","Binghao Liu","Meijing Wu","Xiubo Sang","Min Yang","Lijiang Chen"],"pdf_url":"https://arxiv.org/pdf/2207.06799v1.pdf","comment":"code: https://github.com/cv516Buaa/MMOTU_DS2Net; paper:10 pages, 8\n  figures, 9 tables, 15 formulas"},{"id":"http://arxiv.org/abs/2112.15111v3","updated":"2022-07-14T10:22:24Z","published":"2021-12-30T16:07:59Z","title":"Improving the Behaviour of Vision Transformers with Token-consistent\n  Stochastic Layers","summary":"  We introduce token-consistent stochastic layers in vision transformers,\nwithout causing any severe drop in performance. The added stochasticity\nimproves network calibration, robustness and strengthens privacy. We use linear\nlayers with token-consistent stochastic parameters inside the multilayer\nperceptron blocks, without altering the architecture of the transformer. The\nstochastic parameters are sampled from the uniform distribution, both during\ntraining and inference. The applied linear operations preserve the topological\nstructure, formed by the set of tokens passing through the shared multilayer\nperceptron. This operation encourages the learning of the recognition task to\nrely on the topological structures of the tokens, instead of their values,\nwhich in turn offers the desired robustness and privacy of the visual features.\nThe effectiveness of the token-consistent stochasticity is demonstrated on\nthree different applications, namely, network calibration, adversarial\nrobustness, and feature privacy, by boosting the performance of the respective\nestablished baselines.\n","authors":["Nikola Popovic","Danda Pani Paudel","Thomas Probst","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2112.15111v3.pdf","comment":"This article is under consideration at the Computer Vision and Image\n  Understanding journal"},{"id":"http://arxiv.org/abs/2207.06793v1","updated":"2022-07-14T10:16:25Z","published":"2022-07-14T10:16:25Z","title":"Neural apparent BRDF fields for multiview photometric stereo","summary":"  We propose to tackle the multiview photometric stereo problem using an\nextension of Neural Radiance Fields (NeRFs), conditioned on light source\ndirection. The geometric part of our neural representation predicts surface\nnormal direction, allowing us to reason about local surface reflectance. The\nappearance part of our neural representation is decomposed into a neural\nbidirectional reflectance function (BRDF), learnt as part of the fitting\nprocess, and a shadow prediction network (conditioned on light source\ndirection) allowing us to model the apparent BRDF. This balance of learnt\ncomponents with inductive biases based on physical image formation models\nallows us to extrapolate far from the light source and viewer directions\nobserved during training. We demonstrate our approach on a multiview\nphotometric stereo benchmark and show that competitive performance can be\nobtained with the neural density representation of a NeRF.\n","authors":["Meghna Asthana","William A. P. Smith","Patrik Huber"],"pdf_url":"https://arxiv.org/pdf/2207.06793v1.pdf","comment":"9 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2207.06789v1","updated":"2022-07-14T10:04:18Z","published":"2022-07-14T10:04:18Z","title":"Inertial Hallucinations -- When Wearable Inertial Devices Start Seeing\n  Things","summary":"  We propose a novel approach to multimodal sensor fusion for Ambient Assisted\nLiving (AAL) which takes advantage of learning using privileged information\n(LUPI). We address two major shortcomings of standard multimodal approaches,\nlimited area coverage and reduced reliability. Our new framework fuses the\nconcept of modality hallucination with triplet learning to train a model with\ndifferent modalities to handle missing sensors at inference time. We evaluate\nthe proposed model on inertial data from a wearable accelerometer device, using\nRGB videos and skeletons as privileged modalities, and show an improvement of\naccuracy of an average 6.6% on the UTD-MHAD dataset and an average 5.5% on the\nBerkeley MHAD dataset, reaching a new state-of-the-art for inertial-only\nclassification accuracy on these datasets. We validate our framework through\nseveral ablation studies.\n","authors":["Alessandro Masullo","Toby Perrett","Tilo Burghardt","Ian Craddock","Dima Damen","Majid Mirmehdi"],"pdf_url":"https://arxiv.org/pdf/2207.06789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13812v2","updated":"2022-07-14T09:54:52Z","published":"2022-03-25T17:57:13Z","title":"Spatially Multi-conditional Image Generation","summary":"  In most scenarios, conditional image generation can be thought of as an\ninversion of the image understanding process. Since generic image understanding\ninvolves solving multiple tasks, it is natural to aim at generating images via\nmulti-conditioning. However, multi-conditional image generation is a very\nchallenging problem due to the heterogeneity and the sparsity of the (in\npractice) available conditioning labels. In this work, we propose a novel\nneural architecture to address the problem of heterogeneity and sparsity of the\nspatially multi-conditional labels. Our choice of spatial conditioning, such as\nby semantics and depth, is driven by the promise it holds for better control of\nthe image generation process. The proposed method uses a transformer-like\narchitecture operating pixel-wise, which receives the available labels as input\ntokens to merge them in a learned homogeneous space of labels. The merged\nlabels are then used for image generation via conditional generative\nadversarial training. In this process, the sparsity of the labels is handled by\nsimply dropping the input tokens corresponding to the missing labels at the\ndesired locations, thanks to the proposed pixel-wise operating architecture.\nOur experiments on three benchmark datasets demonstrate the clear superiority\nof our method over the state-of-the-art and compared baselines. The source code\nwill be made publicly available.\n","authors":["Ritika Chakraborty","Nikola Popovic","Danda Pani Paudel","Thomas Probst","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.13812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06780v1","updated":"2022-07-14T09:40:34Z","published":"2022-07-14T09:40:34Z","title":"An Empirical Evaluation of Four Off-the-Shelf Proprietary\n  Visual-Inertial Odometry Systems","summary":"  Commercial visual-inertial odometry (VIO) systems have been gaining attention\nas cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion\ntracking methods for estimating accurate and consistent camera pose data, in\naddition to their ability to operate without external localization from motion\ncapture or global positioning systems. It is unclear from existing results,\nhowever, which commercial VIO platforms are the most stable, consistent, and\naccurate in terms of state estimation for indoor and outdoor robotic\napplications. We assess four popular proprietary VIO systems (Apple ARKit,\nGoogle ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of\nboth indoor and outdoor experiments where we show their positioning stability,\nconsistency, and accuracy. We present our complete results as a benchmark\ncomparison for the research community.\n","authors":["Jungha Kim","Minkyeong Song","Yeoeun Lee","Moonkyeong Jung","Pyojin Kim"],"pdf_url":"https://arxiv.org/pdf/2207.06780v1.pdf","comment":"submitted, under review paper"},{"id":"http://arxiv.org/abs/2202.06675v2","updated":"2022-07-14T09:31:22Z","published":"2022-02-14T13:00:31Z","title":"Can Machines Help Us Answering Question 16 in Datasheets, and In Turn\n  Reflecting on Inappropriate Content?","summary":"  Large datasets underlying much of current machine learning raise serious\nissues concerning inappropriate content such as offensive, insulting,\nthreatening, or might otherwise cause anxiety. This calls for increased dataset\ndocumentation, e.g., using datasheets. They, among other topics, encourage to\nreflect on the composition of the datasets. So far, this documentation,\nhowever, is done manually and therefore can be tedious and error-prone,\nespecially for large image datasets. Here we ask the arguably \"circular\"\nquestion of whether a machine can help us reflect on inappropriate content,\nanswering Question 16 in Datasheets. To this end, we propose to use the\ninformation stored in pre-trained transformer models to assist us in the\ndocumentation process. Specifically, prompt-tuning based on a dataset of\nsocio-moral values steers CLIP to identify potentially inappropriate content,\ntherefore reducing human labor. We then document the inappropriate images found\nusing word clouds, based on captions generated using a vision-language model.\nThe documentations of two popular, large-scale computer vision datasets --\nImageNet and OpenImages -- produced this way suggest that machines can indeed\nhelp dataset creators to answer Question 16 on inappropriate image content.\n","authors":["Patrick Schramowski","Christopher Tauchmann","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2202.06675v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2110.04222"},{"id":"http://arxiv.org/abs/2207.06766v1","updated":"2022-07-14T09:24:05Z","published":"2022-07-14T09:24:05Z","title":"GeoSegNet: Point Cloud Semantic Segmentation via Geometric\n  Encoder-Decoder Modeling","summary":"  Semantic segmentation of point clouds, aiming to assign each point a semantic\ncategory, is critical to 3D scene understanding.Despite of significant advances\nin recent years, most of existing methods still suffer from either the\nobject-level misclassification or the boundary-level ambiguity. In this paper,\nwe present a robust semantic segmentation network by deeply exploring the\ngeometry of point clouds, dubbed GeoSegNet. Our GeoSegNet consists of a\nmulti-geometry based encoder and a boundary-guided decoder. In the encoder, we\ndevelop a new residual geometry module from multi-geometry perspectives to\nextract object-level features. In the decoder, we introduce a contrastive\nboundary learning module to enhance the geometric representation of boundary\npoints. Benefiting from the geometric encoder-decoder modeling, our GeoSegNet\ncan infer the segmentation of objects effectively while making the\nintersections (boundaries) of two or more objects clear. Experiments show\nobvious improvements of our method over its competitors in terms of the overall\nsegmentation accuracy and object boundary clearness. Code is available at\nhttps://github.com/Chen-yuiyui/GeoSegNet.\n","authors":["Chen Chen","Yisen Wang","Honghua Chen","Xuefeng Yan","Dayong Ren","Yanwen Guo","Haoran Xie","Fu Lee Wang","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2207.06766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10120v4","updated":"2022-07-14T09:22:50Z","published":"2022-05-17T14:00:58Z","title":"Privacy Preserving Image Registration","summary":"  Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\nliterature on image registration is generally based on the assumption that\nimages are usually accessible to the researcher, from which the spatial\ntransformation is subsequently estimated. This common assumption may not be met\nin current practical applications, since the sensitive nature of medical images\nmay ultimately require their analysis under privacy constraints, preventing to\nshare the image content in clear form. In this work, we formulate the problem\nof image registration under a privacy preserving regime, where images are\nassumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we first propose to optimize the underlying image registration\noperations using gradient approximations. We further revisit the use of\nhomomorphic encryption and use a packing method to allow the encryption and\nmultiplication of large matrices more efficiently. We demonstrate our privacy\npreserving framework in linear and non-linear registration problems, evaluating\nits accuracy and scalability with respect to standard image registration. Our\nresults show that privacy preserving image registration is feasible and can be\nadopted in sensitive medical imaging applications.\n","authors":["Riccardo Taiello","Melek Önen","Olivier Humbert","Marco Lorenzi"],"pdf_url":"https://arxiv.org/pdf/2205.10120v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06763v1","updated":"2022-07-14T09:17:00Z","published":"2022-07-14T09:17:00Z","title":"Neighbor Correspondence Matching for Flow-based Video Frame Synthesis","summary":"  Video frame synthesis, which consists of interpolation and extrapolation, is\nan essential video processing technique that can be applied to various\nscenarios. However, most existing methods cannot handle small objects or large\nmotion well, especially in high-resolution videos such as 4K videos. To\neliminate such limitations, we introduce a neighbor correspondence matching\n(NCM) algorithm for flow-based frame synthesis. Since the current frame is not\navailable in video frame synthesis, NCM is performed in a\ncurrent-frame-agnostic fashion to establish multi-scale correspondences in the\nspatial-temporal neighborhoods of each pixel. Based on the powerful motion\nrepresentation capability of NCM, we further propose to estimate intermediate\nflows for frame synthesis in a heterogeneous coarse-to-fine scheme.\nSpecifically, the coarse-scale module is designed to leverage neighbor\ncorrespondences to capture large motion, while the fine-scale module is more\ncomputationally efficient to speed up the estimation process. Both modules are\ntrained progressively to eliminate the resolution gap between training dataset\nand real-world videos. Experimental results show that NCM achieves\nstate-of-the-art performance on several benchmarks. In addition, NCM can be\napplied to various practical scenarios such as video compression to achieve\nbetter performance.\n","authors":["Zhaoyang Jia","Yan Lu","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2207.06763v1.pdf","comment":"Accepted to ACM MM 2022"},{"id":"http://arxiv.org/abs/2207.04394v3","updated":"2022-07-14T09:16:33Z","published":"2022-07-10T06:32:56Z","title":"Radiomics-Guided Global-Local Transformer for Weakly Supervised\n  Pathology Localization in Chest X-Rays","summary":"  Before the recent success of deep learning methods for automated medical\nimage analysis, practitioners used handcrafted radiomic features to\nquantitatively describe local patches of medical images. However, extracting\ndiscriminative radiomic features relies on accurate pathology localization,\nwhich is difficult to acquire in real-world settings. Despite advances in\ndisease classification and localization from chest X-rays, many approaches fail\nto incorporate clinically-informed domain knowledge. For these reasons, we\npropose a Radiomics-Guided Transformer (RGT) that fuses \\textit{global} image\ninformation with \\textit{local} knowledge-guided radiomics information to\nprovide accurate cardiopulmonary pathology localization and classification\n\\textit{without any bounding box annotations}. RGT consists of an image\nTransformer branch, a radiomics Transformer branch, and fusion layers that\naggregate image and radiomic information. Using the learned self-attention of\nits image branch, RGT extracts a bounding box for which to compute radiomic\nfeatures, which are further processed by the radiomics branch; learned image\nand radiomic features are then fused and mutually interact via cross-attention\nlayers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap\naccurate pathology localization only using image-level disease labels.\nExperiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior\nworks in weakly supervised disease localization (by an average margin of 3.6\\%\nover various intersection-over-union thresholds) and classification (by 1.1\\%\nin average area under the receiver operating characteristic curve). We publicly\nrelease our codes and pre-trained models at\n\\url{https://github.com/VITA-Group/chext}.\n","authors":["Yan Han","Gregory Holste","Ying Ding","Ahmed Tewfik","Yifan Peng","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.04394v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06754v1","updated":"2022-07-14T09:04:51Z","published":"2022-07-14T09:04:51Z","title":"E2-AEN: End-to-End Incremental Learning with Adaptively Expandable\n  Network","summary":"  Expandable networks have demonstrated their advantages in dealing with\ncatastrophic forgetting problem in incremental learning. Considering that\ndifferent tasks may need different structures, recent methods design dynamic\nstructures adapted to different tasks via sophisticated skills. Their routine\nis to search expandable structures first and then train on the new tasks,\nwhich, however, breaks tasks into multiple training stages, leading to\nsuboptimal or overmuch computational cost. In this paper, we propose an\nend-to-end trainable adaptively expandable network named E2-AEN, which\ndynamically generates lightweight structures for new tasks without any accuracy\ndrop in previous tasks. Specifically, the network contains a serial of powerful\nfeature adapters for augmenting the previously learned representations to new\ntasks, and avoiding task interference. These adapters are controlled via an\nadaptive gate-based pruning strategy which decides whether the expanded\nstructures can be pruned, making the network structure dynamically changeable\naccording to the complexity of the new tasks. Moreover, we introduce a novel\nsparsity-activation regularization to encourage the model to learn\ndiscriminative features with limited parameters. E2-AEN reduces cost and can be\nbuilt upon any feed-forward architectures in an end-to-end manner. Extensive\nexperiments on both classification (i.e., CIFAR and VDD) and detection (i.e.,\nCOCO, VOC and ICCV2021 SSLAD challenge) benchmarks demonstrate the\neffectiveness of the proposed method, which achieves the new remarkable\nresults.\n","authors":["Guimei Cao","Zhanzhan Cheng","Yunlu Xu","Duo Li","Shiliang Pu","Yi Niu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2207.06754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01832v2","updated":"2022-07-14T08:59:41Z","published":"2021-12-03T10:41:12Z","title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video\n  Retrieval","summary":"  In this paper we revisit \\emph{feature fusion}, an old-fashioned topic, in\nthe new context of text-to-video retrieval. Different from previous research\nthat considers feature fusion only at one end, let it be video or text, we aim\nfor feature fusion for both ends within a unified framework. We hypothesize\nthat optimizing the convex combination of the features is preferred to modeling\ntheir correlations by computationally heavy multi-head self attention. We\npropose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature\nfusion at both early and late stages and at both video and text ends, making it\na powerful method for exploiting diverse (off-the-shelf) features. The\ninterpretability of LAFF can be used for feature selection. Extensive\nexperiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and\nTRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video\nretrieval.\n","authors":["Fan Hu","Aozhu Chen","Ziyue Wang","Fangming Zhou","Jianfeng Dong","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2112.01832v2.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2101.07518v3","updated":"2022-07-14T08:57:44Z","published":"2021-01-19T09:03:40Z","title":"BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring","summary":"  Image motion blur usually results from moving objects or camera shakes. Such\nblur is generally directional and non-uniform. Previous research efforts\nattempt to solve non-uniform blur by using self-recurrent multi-scale or\nmulti-patch architectures accompanying with self-attention. However, using\nself-recurrent frameworks typically leads to a longer inference time, while\ninter-pixel or inter-channel self-attention may cause excessive memory usage.\nThis paper proposes blur-aware attention networks (BANet) that accomplish\naccurate and efficient deblurring via a single forward pass. Our BANet utilizes\nregion-based self-attention with multi-kernel strip pooling to disentangle blur\npatterns of different degrees and with cascaded parallel dilated convolution to\naggregate multi-scale content features. Extensive experimental results on the\nGoPro and HIDE benchmarks demonstrate that the proposed BANet performs\nfavorably against the state-of-the-art in blurred image restoration and can\nprovide deblurred results in real-time.\n","authors":["Fu-Jen Tsai","Yan-Tsung Peng","Yen-Yu Lin","Chung-Chi Tsai","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2101.07518v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06746v1","updated":"2022-07-14T08:55:41Z","published":"2022-07-14T08:55:41Z","title":"Single-Pixel Image Reconstruction Based on Block Compressive Sensing and\n  Deep Learning","summary":"  Single-pixel imaging (SPI) is a novel imaging technique whose working\nprinciple is based on the compressive sensing (CS) theory. In SPI, data is\nobtained through a series of compressive measurements and the corresponding\nimage is reconstructed. Typically, the reconstruction algorithm such as basis\npursuit relies on the sparsity assumption in images. However, recent advances\nin deep learning have found its uses in reconstructing CS images. Despite\nshowing a promising result in simulations, it is often unclear how such an\nalgorithm can be implemented in an actual SPI setup. In this paper, we\ndemonstrate the use of deep learning on the reconstruction of SPI images in\nconjunction with block compressive sensing (BCS). We also proposed a novel\nreconstruction model based on convolutional neural networks that outperforms\nother competitive CS reconstruction algorithms. Besides, by incorporating BCS\nin our deep learning model, we were able to reconstruct images of any size\nabove a certain smallest image size. In addition, we show that our model is\ncapable of reconstructing images obtained from an SPI setup while being priorly\ntrained on natural images, which can be vastly different from the SPI images.\nThis opens up opportunity for the feasibility of pretrained deep learning\nmodels for CS reconstructions of images from various domain areas.\n","authors":["Stephen L. H. Lau","Edwin K. P. Chong"],"pdf_url":"https://arxiv.org/pdf/2207.06746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06744v1","updated":"2022-07-14T08:52:07Z","published":"2022-07-14T08:52:07Z","title":"TRIE++: Towards End-to-End Information Extraction from Visually Rich\n  Documents","summary":"  Recently, automatically extracting information from visually rich documents\n(e.g., tickets and resumes) has become a hot and vital research topic due to\nits widespread commercial value. Most existing methods divide this task into\ntwo subparts: the text reading part for obtaining the plain text from the\noriginal document images and the information extraction part for extracting key\ncontents. These methods mainly focus on improving the second, while neglecting\nthat the two parts are highly correlated. This paper proposes a unified\nend-to-end information extraction framework from visually rich documents, where\ntext reading and information extraction can reinforce each other via a\nwell-designed multi-modal context block. Specifically, the text reading part\nprovides multi-modal features like visual, textual and layout features. The\nmulti-modal context block is developed to fuse the generated multi-modal\nfeatures and even the prior knowledge from the pre-trained language model for\nbetter semantic representation. The information extraction part is responsible\nfor generating key contents with the fused context features. The framework can\nbe trained in an end-to-end trainable manner, achieving global optimization.\nWhat is more, we define and group visually rich documents into four categories\nacross two dimensions, the layout and text type. For each document category, we\nprovide or recommend the corresponding benchmarks, experimental settings and\nstrong baselines for remedying the problem that this research area lacks the\nuniform evaluation standard. Extensive experiments on four kinds of benchmarks\n(from fixed layout to variable layout, from full-structured text to\nsemi-unstructured text) are reported, demonstrating the proposed method's\neffectiveness. Data, source code and models are available.\n","authors":["Zhanzhan Cheng","Peng Zhang","Can Li","Qiao Liang","Yunlu Xu","Pengfei Li","Shiliang Pu","Yi Niu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2207.06744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06738v1","updated":"2022-07-14T08:44:01Z","published":"2022-07-14T08:44:01Z","title":"Semi-supervised Vector-Quantization in Visual SLAM using HGCN","summary":"  In this paper, two semi-supervised appearance based loop closure detection\ntechnique, HGCN-FABMAP and HGCN-BoW are introduced. Furthermore an extension to\nthe current state of the art localization SLAM algorithm, ORB-SLAM, is\npresented. The proposed HGCN-FABMAP method is implemented in an off-line manner\nincorporating Bayesian probabilistic schema for loop detection decision making.\nSpecifically, we let a Hyperbolic Graph Convolutional Neural Network (HGCN) to\noperate over the SURF features graph space, and perform vector quantization\npart of the SLAM procedure. This part previously was performed in an\nunsupervised manner using algorithms like HKmeans, kmeans++,..etc. The main\nAdvantage of using HGCN, is that it scales linearly in number of graph edges.\nExperimental results shows that HGCN-FABMAP algorithm needs far more cluster\ncentroids than HGCN-ORB, otherwise it fails to detect loop closures. Therefore\nwe consider HGCN-ORB to be more efficient in terms of memory consumption, also\nwe conclude the superiority of HGCN-BoW and HGCN-FABMAP with respect to other\nalgorithms.\n","authors":["Amir Zarringhalam","Saeed Shiry Ghidary","Ali Mohades Khorasani"],"pdf_url":"https://arxiv.org/pdf/2207.06738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06733v1","updated":"2022-07-14T08:38:17Z","published":"2022-07-14T08:38:17Z","title":"ConCL: Concept Contrastive Learning for Dense Prediction Pre-training in\n  Pathology Images","summary":"  Detectingandsegmentingobjectswithinwholeslideimagesis essential in\ncomputational pathology workflow. Self-supervised learning (SSL) is appealing\nto such annotation-heavy tasks. Despite the extensive benchmarks in natural\nimages for dense tasks, such studies are, unfortunately, absent in current\nworks for pathology. Our paper intends to narrow this gap. We first benchmark\nrepresentative SSL methods for dense prediction tasks in pathology images.\nThen, we propose concept contrastive learning (ConCL), an SSL framework for\ndense pre-training. We explore how ConCL performs with concepts provided by\ndifferent sources and end up with proposing a simple dependency-free concept\ngenerating method that does not rely on external segmentation algorithms or\nsaliency detection models. Extensive experiments demonstrate the superiority of\nConCL over previous state-of-the-art SSL methods across different settings.\nAlong our exploration, we distll several important and intriguing components\ncontributing to the success of dense pre-training for pathology images. We hope\nthis work could provide useful data points and encourage the community to\nconduct ConCL pre-training for problems of interest. Code is available.\n","authors":["Jiawei Yang","Hanbo Chen","Yuan Liang","Junzhou Huang","Lei He","Jianhua Yao"],"pdf_url":"https://arxiv.org/pdf/2207.06733v1.pdf","comment":"Accepted as an ECCV 2022 paper. Code is available at\n  https://github.com/Jiawei-Yang/ConCL or\n  https://github.com/TencentAILabHealthcare/ConCL"},{"id":"http://arxiv.org/abs/2205.14629v2","updated":"2022-07-14T08:28:36Z","published":"2022-05-29T11:23:01Z","title":"Superclass Adversarial Attack","summary":"  Adversarial attacks have only focused on changing the predictions of the\nclassifier, but their danger greatly depends on how the class is mistaken. For\nexample, when an automatic driving system mistakes a Persian cat for a Siamese\ncat, it is hardly a problem. However, if it mistakes a cat for a 120km/h\nminimum speed sign, serious problems can arise. As a stepping stone to more\nthreatening adversarial attacks, we consider the superclass adversarial attack,\nwhich causes misclassification of not only fine classes, but also superclasses.\nWe conducted the first comprehensive analysis of superclass adversarial attacks\n(an existing and 19 new methods) in terms of accuracy, speed, and stability,\nand identified several strategies to achieve better performance. Although this\nstudy is aimed at superclass misclassification, the findings can be applied to\nother problem settings involving multiple classes, such as top-k and\nmulti-label classification attacks.\n","authors":["Soichiro Kumano","Hiroshi Kera","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2205.14629v2.pdf","comment":"ICML Workshop 2022 on Adversarial Machine Learning Frontiers"},{"id":"http://arxiv.org/abs/2207.01377v5","updated":"2022-07-14T08:26:58Z","published":"2022-07-04T12:56:04Z","title":"Detection of ADHD based on Eye Movements during Natural Viewing","summary":"  Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental\ndisorder that is highly prevalent and requires clinical specialists to\ndiagnose. It is known that an individual's viewing behavior, reflected in their\neye movements, is directly related to attentional mechanisms and higher-order\ncognitive processes. We therefore explore whether ADHD can be detected based on\nrecorded eye movements together with information about the video stimulus in a\nfree-viewing task. To this end, we develop an end-to-end deep learning-based\nsequence model which we pre-train on a related task for which more data are\navailable. We find that the method is in fact able to detect ADHD and\noutperforms relevant baselines. We investigate the relevance of the input\nfeatures in an ablation study. Interestingly, we find that the model's\nperformance is closely related to the content of the video, which provides\ninsights for future experimental designs.\n","authors":["Shuwen Deng","Paul Prasse","David R. Reich","Sabine Dziemian","Maja Stegenwallner-Schütz","Daniel Krakowczyk","Silvia Makowski","Nicolas Langer","Tobias Scheffer","Lena A. Jäger"],"pdf_url":"https://arxiv.org/pdf/2207.01377v5.pdf","comment":"Pre-print for Proceedings of the European Conference on Machine\n  Learning, 2022"},{"id":"http://arxiv.org/abs/2207.06726v1","updated":"2022-07-14T08:22:58Z","published":"2022-07-14T08:22:58Z","title":"Octuplet Loss: Make Face Recognition Robust to Image Resolution","summary":"  Image resolution, or in general, image quality, plays an essential role in\nthe performance of today's face recognition systems. To address this problem,\nwe propose a novel combination of the popular triplet loss to improve\nrobustness against image resolution via fine-tuning of existing face\nrecognition models. With octuplet loss, we leverage the relationship between\nhigh-resolution images and their synthetically down-sampled variants jointly\nwith their identity labels. Fine-tuning several state-of-the-art approaches\nwith our method proves that we can significantly boost performance for\ncross-resolution (high-to-low resolution) face verification on various datasets\nwithout meaningfully exacerbating the performance on high-to-high resolution\nimages. Our method applied on the FaceTransformer network achieves 95.12% face\nverification accuracy on the challenging XQLFW dataset while reaching 99.73% on\nthe LFW database. Moreover, the low-to-low face verification accuracy benefits\nfrom our method. We release our code to allow seamless integration of the\noctuplet loss into existing frameworks.\n","authors":["Martin Knoche","Mohamed Elkadeem","Stefan Hörmann","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2207.06726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06189v2","updated":"2022-07-14T07:41:55Z","published":"2022-07-13T13:32:18Z","title":"Collaborative Quantization Embeddings for Intra-Subject Prostate MR\n  Image Registration","summary":"  Image registration is useful for quantifying morphological changes in\nlongitudinal MR images from prostate cancer patients. This paper describes a\ndevelopment in improving the learning-based registration algorithms, for this\nchallenging clinical application often with highly variable yet limited\ntraining data. First, we report that the latent space can be clustered into a\nmuch lower dimensional space than that commonly found as bottleneck features at\nthe deep layer of a trained registration network. Based on this observation, we\npropose a hierarchical quantization method, discretizing the learned feature\nvectors using a jointly-trained dictionary with a constrained size, in order to\nimprove the generalisation of the registration networks. Furthermore, a novel\ncollaborative dictionary is independently optimised to incorporate additional\nprior information, such as the segmentation of the gland or other regions of\ninterest, in the latent quantized space. Based on 216 real clinical images from\n86 prostate cancer patients, we show the efficacy of both the designed\ncomponents. Improved registration accuracy was obtained with statistical\nsignificance, in terms of both Dice on gland and target registration error on\ncorresponding landmarks, the latter of which achieved 5.46 mm, an improvement\nof 28.7\\% from the baseline without quantization. Experimental results also\nshow that the difference in performance was indeed minimised between training\nand testing data.\n","authors":["Ziyi Shen","Qianye Yang","Yuming Shen","Francesco Giganti","Vasilis Stavrinides","Richard Fan","Caroline Moore","Mirabela Rusu","Geoffrey Sonn","Philip Torr","Dean Barratt","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2207.06189v2.pdf","comment":"preprint version, accepted for MICCAI 2022 (25th International\n  Conference on Medical Image Computing and Computer Assisted Intervention)"},{"id":"http://arxiv.org/abs/2112.11833v4","updated":"2022-07-14T07:40:46Z","published":"2021-12-22T12:18:43Z","title":"Deep learning for brain metastasis detection and segmentation in\n  longitudinal MRI data","summary":"  Brain metastases occur frequently in patients with metastatic cancer. Early\nand accurate detection of brain metastases is very essential for treatment\nplanning and prognosis in radiation therapy. To improve brain metastasis\ndetection performance with deep learning, a custom detection loss called\nvolume-level sensitivity-specificity (VSS) is proposed, which rates individual\nmetastasis detection sensitivity and specificity in (sub-)volume levels. As\nsensitivity and precision are always a trade-off in a metastasis level, either\na high sensitivity or a high precision can be achieved by adjusting the weights\nin the VSS loss without decline in dice score coefficient for segmented\nmetastases. To reduce metastasis-like structures being detected as false\npositive metastases, a temporal prior volume is proposed as an additional input\nof DeepMedic. The modified network is called DeepMedic+ for distinction. Our\nproposed VSS loss improves the sensitivity of brain metastasis detection for\nDeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it\nimproves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic\nwith the same VSS loss, 44.4% of the false positive metastases are reduced in\nthe high sensitivity model and the precision reaches 99.6% for the high\nspecificity model. The mean dice coefficient for all metastases is about 0.81.\nWith the ensemble of the high sensitivity and high specificity models, on\naverage only 1.5 false positive metastases per patient needs further check,\nwhile the majority of true positive metastases are confirmed. The ensemble\nlearning is able to distinguish high confidence true positive metastases from\nmetastases candidates that require special expert review or further follow-up,\nbeing particularly well-fit to the requirements of expert support in real\nclinical practice.\n","authors":["Yixing Huang","Christoph Bert","Philipp Sommer","Benjamin Frey","Udo Gaipl","Luitpold V. Distel","Thomas Weissmann","Michael Uder","Manuel A. Schmidt","Arnd Dörfler","Andreas Maier","Rainer Fietkau","Florian Putz"],"pdf_url":"https://arxiv.org/pdf/2112.11833v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.11526v3","updated":"2022-07-14T07:33:29Z","published":"2021-10-21T23:49:23Z","title":"Wide Neural Networks Forget Less Catastrophically","summary":"  A primary focus area in continual learning research is alleviating the\n\"catastrophic forgetting\" problem in neural networks by designing new\nalgorithms that are more robust to the distribution shifts. While the recent\nprogress in continual learning literature is encouraging, our understanding of\nwhat properties of neural networks contribute to catastrophic forgetting is\nstill limited. To address this, instead of focusing on continual learning\nalgorithms, in this work, we focus on the model itself and study the impact of\n\"width\" of the neural network architecture on catastrophic forgetting, and show\nthat width has a surprisingly significant effect on forgetting. To explain this\neffect, we study the learning dynamics of the network from various perspectives\nsuch as gradient orthogonality, sparsity, and lazy training regime. We provide\npotential explanations that are consistent with the empirical results across\ndifferent architectures and continual learning benchmarks.\n","authors":["Seyed Iman Mirzadeh","Arslan Chaudhry","Dong Yin","Huiyi Hu","Razvan Pascanu","Dilan Gorur","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2110.11526v3.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2203.10539v2","updated":"2022-07-14T07:30:05Z","published":"2022-03-20T12:14:58Z","title":"End-to-End Video Text Spotting with Transformer","summary":"  Recent video text spotting methods usually require the three-staged pipeline,\ni.e., detecting text in individual images, recognizing localized text, tracking\ntext streams with post-processing to generate final results. These methods\ntypically follow the tracking-by-match paradigm and develop sophisticated\npipelines. In this paper, rooted in Transformer sequence modeling, we propose a\nsimple, but effective end-to-end video text DEtection, Tracking, and\nRecognition framework (TransDETR). TransDETR mainly includes two advantages: 1)\nDifferent from the explicit match paradigm in the adjacent frame, TransDETR\ntracks and recognizes each text implicitly by the different query termed text\nquery over long-range temporal sequence (more than 7 frames). 2) TransDETR is\nthe first end-to-end trainable video text spotting framework, which\nsimultaneously addresses the three sub-tasks (e.g., text detection, tracking,\nrecognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013\nVideo, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to\ndemonstrate that TransDETR achieves state-of-the-art performance with up to\naround 8.0% improvements on video text spotting tasks. The code of TransDETR\ncan be found at https://github.com/weijiawu/TransDETR.\n","authors":["Weijia Wu","Yuanqiang Cai","Chunhua Shen","Debing Zhang","Ying Fu","Hong Zhou","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2203.10539v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.06706v1","updated":"2022-07-14T07:24:02Z","published":"2022-07-14T07:24:02Z","title":"SHREC 2022 Track on Online Detection of Heterogeneous Gestures","summary":"  This paper presents the outcomes of a contest organized to evaluate methods\nfor the online recognition of heterogeneous gestures from sequences of 3D hand\nposes. The task is the detection of gestures belonging to a dictionary of 16\nclasses characterized by different pose and motion features. The dataset\nfeatures continuous sequences of hand tracking data where the gestures are\ninterleaved with non-significant motions. The data have been captured using the\nHololens 2 finger tracking system in a realistic use-case of mixed reality\ninteraction. The evaluation is based not only on the detection performances but\nalso on the latency and the false positives, making it possible to understand\nthe feasibility of practical interaction tools based on the algorithms\nproposed. The outcomes of the contest's evaluation demonstrate the necessity of\nfurther research to reduce recognition errors, while the computational cost of\nthe algorithms proposed is sufficiently low.\n","authors":["Ariel Caputo","Marco Emporio","Andrea Giachetti","Marco Cristani","Guido Borghi","Andrea D'Eusanio","Minh-Quan Le","Hai-Dang Nguyen","Minh-Triet Tran","F. Ambellan","M. Hanik","E. Nava-Yazdani","C. von Tycowicz"],"pdf_url":"https://arxiv.org/pdf/2207.06706v1.pdf","comment":"Accepted on Computer & Graphics journal"},{"id":"http://arxiv.org/abs/2207.06695v1","updated":"2022-07-14T06:54:47Z","published":"2022-07-14T06:54:47Z","title":"DavarOCR: A Toolbox for OCR and Multi-Modal Document Understanding","summary":"  This paper presents DavarOCR, an open-source toolbox for OCR and document\nunderstanding tasks. DavarOCR currently implements 19 advanced algorithms,\ncovering 9 different task forms. DavarOCR provides detailed usage instructions\nand the trained models for each algorithm. Compared with the previous\nopensource OCR toolbox, DavarOCR has relatively more complete support for the\nsub-tasks of the cutting-edge technology of document understanding. In order to\npromote the development and application of OCR technology in academia and\nindustry, we pay more attention to the use of modules that different\nsub-domains of technology can share. DavarOCR is publicly released at\nhttps://github.com/hikopensource/Davar-Lab-OCR.\n","authors":["Liang Qiao","Hui Jiang","Ying Chen","Can Li","Pengfei Li","Zaisheng Li","Baorui Zou","Dashan Guo","Yingda Xu","Yunlu Xu","Zhanzhan Cheng","Yi Niu"],"pdf_url":"https://arxiv.org/pdf/2207.06695v1.pdf","comment":"Short paper, Accept by ACM MM2022"},{"id":"http://arxiv.org/abs/2207.06694v1","updated":"2022-07-14T06:49:59Z","published":"2022-07-14T06:49:59Z","title":"Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text\n  Spotting","summary":"  End-to-end text spotting has attached great attention recently due to its\nbenefits on global optimization and high maintainability for real applications.\nHowever, the input scale has always been a tough trade-off since recognizing a\nsmall text instance usually requires enlarging the whole image, which brings\nhigh computational costs. In this paper, to address this problem, we propose a\nnovel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting\nframework, which aims to infer images in different small but recognizable\nresolutions and achieve a better balance between accuracy and efficiency.\nConcretely, we adopt a resolution selector to dynamically decide the input\nresolutions for different images, which is constraint by both inference\naccuracy and computational cost. Another sequential knowledge distillation\nstrategy is conducted on the text recognition branch, making the low-res input\nobtains comparable performance to a high-res image. The proposed method can be\noptimized end-to-end and adopted in any current text spotting framework to\nimprove the practicability. Extensive experiments on several text spotting\nbenchmarks show that the proposed method vastly improves the usability of\nlow-res models. The code is available at\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/.\n","authors":["Ying Chen","Liang Qiao1","Zhanzhan Cheng","Shiliang Pu","Yi Niu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2207.06694v1.pdf","comment":"Accept by ECCV2022"},{"id":"http://arxiv.org/abs/2203.08344v5","updated":"2022-07-14T06:43:48Z","published":"2022-03-16T01:32:21Z","title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild","summary":"  We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n","authors":["Takehiko Ohkawa","Yu-Jhe Li","Qichen Fu","Ryosuke Furuta","Kris M. Kitani","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2203.08344v5.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06202v2","updated":"2022-07-14T06:38:54Z","published":"2022-07-13T13:59:59Z","title":"Adversarially-Aware Robust Object Detector","summary":"  Object detection, as a fundamental computer vision task, has achieved a\nremarkable progress with the emergence of deep neural networks. Nevertheless,\nfew works explore the adversarial robustness of object detectors to resist\nadversarial attacks for practical applications in various real-world scenarios.\nDetectors have been greatly challenged by unnoticeable perturbation, with sharp\nperformance drop on clean images and extremely poor performance on adversarial\nimages. In this work, we empirically explore the model training for adversarial\nrobustness in object detection, which greatly attributes to the conflict\nbetween learning clean images and adversarial images. To mitigate this issue,\nwe propose a Robust Detector (RobustDet) based on adversarially-aware\nconvolution to disentangle gradients for model learning on clean and\nadversarial images. RobustDet also employs the Adversarial Image Discriminator\n(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable\nrobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that\nour model effectively disentangles gradients and significantly enhances the\ndetection robustness with maintaining the detection ability on clean images.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2207.06202v2.pdf","comment":"ECCV2022 oral paper"},{"id":"http://arxiv.org/abs/2207.06684v1","updated":"2022-07-14T06:23:38Z","published":"2022-07-14T06:23:38Z","title":"Subgraph Frequency Distribution Estimation using Graph Neural Networks","summary":"  Small subgraphs (graphlets) are important features to describe fundamental\nunits of a large network. The calculation of the subgraph frequency\ndistributions has a wide application in multiple domains including biology and\nengineering. Unfortunately due to the inherent complexity of this task, most of\nthe existing methods are computationally intensive and inefficient. In this\nwork, we propose GNNS, a novel representational learning framework that\nutilizes graph neural networks to sample subgraphs efficiently for estimating\ntheir frequency distribution. Our framework includes an inference model and a\ngenerative model that learns hierarchical embeddings of nodes, subgraphs, and\ngraph types. With the learned model and embeddings, subgraphs are sampled in a\nhighly scalable and parallel way and the frequency distribution estimation is\nthen performed based on these sampled subgraphs. Eventually, our methods\nachieve comparable accuracy and a significant speedup by three orders of\nmagnitude compared to existing methods.\n","authors":["Zhongren Chen","Xinyue Xu","Shengyi Jiang","Hao Wang","Lu Mi"],"pdf_url":"https://arxiv.org/pdf/2207.06684v1.pdf","comment":"accepted by KDD 2022 Workshop on Deep Learning on Graphs"},{"id":"http://arxiv.org/abs/2207.06673v1","updated":"2022-07-14T05:59:54Z","published":"2022-07-14T05:59:54Z","title":"Detecting Volunteer Cotton Plants in a Corn Field with Deep Learning on\n  UAV Remote-Sensing Imagery","summary":"  The cotton boll weevil, Anthonomus grandis Boheman is a serious pest to the\nU.S. cotton industry that has cost more than 16 billion USD in damages since it\nentered the United States from Mexico in the late 1800s. This pest has been\nnearly eradicated; however, southern part of Texas still faces this issue and\nis always prone to the pest reinfestation each year due to its sub-tropical\nclimate where cotton plants can grow year-round. Volunteer cotton (VC) plants\ngrowing in the fields of inter-seasonal crops, like corn, can serve as hosts to\nthese pests once they reach pin-head square stage (5-6 leaf stage) and\ntherefore need to be detected, located, and destroyed or sprayed . In this\npaper, we present a study to detect VC plants in a corn field using YOLOv3 on\nthree band aerial images collected by unmanned aircraft system (UAS). The\ntwo-fold objectives of this paper were : (i) to determine whether YOLOv3 can be\nused for VC detection in a corn field using RGB (red, green, and blue) aerial\nimages collected by UAS and (ii) to investigate the behavior of YOLOv3 on\nimages at three different scales (320 x 320, S1; 416 x 416, S2; and 512 x 512,\nS3 pixels) based on average precision (AP), mean average precision (mAP) and\nF1-score at 95% confidence level. No significant differences existed for mAP\namong the three scales, while a significant difference was found for AP between\nS1 and S3 (p = 0.04) and S2 and S3 (p = 0.02). A significant difference was\nalso found for F1-score between S2 and S3 (p = 0.02). The lack of significant\ndifferences of mAP at all the three scales indicated that the trained YOLOv3\nmodel can be used on a computer vision-based remotely piloted aerial\napplication system (RPAAS) for VC detection and spray application in near\nreal-time.\n","authors":["Pappu Kumar Yadav","J. Alex Thomasson","Robert Hardin","Stephen W. Searcy","Ulisses Braga-Neto","Sorin C. Popescu","Daniel E. Martin","Roberto Rodriguez","Karem Meza","Juan Enciso","Jorge Solorzano Diaz","Tianyi Wang"],"pdf_url":"https://arxiv.org/pdf/2207.06673v1.pdf","comment":"38 Pages"},{"id":"http://arxiv.org/abs/2111.12485v2","updated":"2022-07-14T05:39:09Z","published":"2021-11-24T13:29:17Z","title":"Graph Modularity: Towards Understanding the Cross-Layer Transition of\n  Feature Representations in Deep Neural Networks","summary":"  There are good arguments to support the claim that deep neural networks\n(DNNs) capture better feature representations than the previous hand-crafted\nfeature engineering, which leads to a significant performance improvement. In\nthis paper, we move a tiny step towards understanding the dynamics of feature\nrepresentations over layers. Specifically, we model the process of class\nseparation of intermediate representations in pre-trained DNNs as the evolution\nof communities in dynamic graphs. Then, we introduce modularity, a generic\nmetric in graph theory, to quantify the evolution of communities. In the\npreliminary experiment, we find that modularity roughly tends to increase as\nthe layer goes deeper and the degradation and plateau arise when the model\ncomplexity is great relative to the dataset. Through an asymptotic analysis, we\nprove that modularity can be broadly used for different applications. For\nexample, modularity provides new insights to quantify the difference between\nfeature representations. More crucially, we demonstrate that the degradation\nand plateau in modularity curves represent redundant layers in DNNs and can be\npruned with minimal impact on performance, which provides theoretical guidance\nfor layer pruning. Our code is available at\nhttps://github.com/yaolu-zjut/Dynamic-Graphs-Construction.\n","authors":["Yao Lu","Wen Yang","Yunzhe Zhang","Zuohui Chen","Jinyin Chen","Qi Xuan","Zhen Wang","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2111.12485v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2112.02450v3","updated":"2022-07-14T05:23:52Z","published":"2021-12-04T23:55:46Z","title":"Adaptive Feature Interpolation for Low-Shot Image Generation","summary":"  Training of generative models especially Generative Adversarial Networks can\neasily diverge in low-data setting. To mitigate this issue, we propose a novel\nimplicit data augmentation approach which facilitates stable training and\nsynthesize high-quality samples without need of label information.\nSpecifically, we view the discriminator as a metric embedding of the real data\nmanifold, which offers proper distances between real data points. We then\nutilize information in the feature space to develop a fully unsupervised and\ndata-driven augmentation method. Experiments on few-shot generation tasks show\nthe proposed method significantly improve results from strong baselines with\nhundreds of training samples.\n","authors":["Mengyu Dai","Haibin Hang","Xiaoyang Guo"],"pdf_url":"https://arxiv.org/pdf/2112.02450v3.pdf","comment":"ECCV'22. Code available at\n  https://github.com/dzld00/Adaptive-Feature-Interpolation-for-Low-Shot-Image-Generation"},{"id":"http://arxiv.org/abs/2207.06661v1","updated":"2022-07-14T05:18:20Z","published":"2022-07-14T05:18:20Z","title":"Deep Point-to-Plane Registration by Efficient Backpropagation for Error\n  Minimizing Function","summary":"  Traditional algorithms of point set registration minimizing point-to-plane\ndistances often achieve a better estimation of rigid transformation than those\nminimizing point-to-point distances. Nevertheless, recent deep-learning-based\nmethods minimize the point-to-point distances. In contrast to these methods,\nthis paper proposes the first deep-learning-based approach to point-to-plane\nregistration. A challenging part of this problem is that a typical solution for\npoint-to-plane registration requires an iterative process of accumulating small\ntransformations obtained by minimizing a linearized energy function. The\niteration significantly increases the size of the computation graph needed for\nbackpropagation and can slow down both forward and backward network\nevaluations. To solve this problem, we consider the estimated rigid\ntransformation as a function of input point clouds and derive its analytic\ngradients using the implicit function theorem. The analytic gradient that we\nintroduce is independent of how the error minimizing function (i.e., the rigid\ntransformation) is obtained, thus allowing us to calculate both the rigid\ntransformation and its gradient efficiently. We implement the proposed\npoint-to-plane registration module over several previous methods that minimize\npoint-to-point distances and demonstrate that the extensions outperform the\nbase methods even with point clouds with noise and low-quality point normals\nestimated with local point distributions.\n","authors":["Tatsuya Yatagawa","Yutaka Ohtake","Hiromasa Suzuki"],"pdf_url":"https://arxiv.org/pdf/2207.06661v1.pdf","comment":"25 pages, 10 figures"},{"id":"http://arxiv.org/abs/2207.06659v1","updated":"2022-07-14T05:13:50Z","published":"2022-07-14T05:13:50Z","title":"Forcing the Whole Video as Background: An Adversarial Learning Strategy\n  for Weakly Temporal Action Localization","summary":"  With video-level labels, weakly supervised temporal action localization\n(WTAL) applies a localization-by-classification paradigm to detect and classify\nthe action in untrimmed videos. Due to the characteristic of classification,\nclass-specific background snippets are inevitably mis-activated to improve the\ndiscriminability of the classifier in WTAL. To alleviate the disturbance of\nbackground, existing methods try to enlarge the discrepancy between action and\nbackground through modeling background snippets with pseudo-snippet-level\nannotations, which largely rely on artificial hypotheticals. Distinct from the\nprevious works, we present an adversarial learning strategy to break the\nlimitation of mining pseudo background snippets. Concretely, the background\nclassification loss forces the whole video to be regarded as the background by\na background gradient reinforcement strategy, confusing the recognition model.\nReversely, the foreground(action) loss guides the model to focus on action\nsnippets under such conditions. As a result, competition between the two\nclassification losses drives the model to boost its ability for action\nmodeling. Simultaneously, a novel temporal enhancement network is designed to\nfacilitate the model to construct temporal relation of affinity snippets based\non the proposed strategy, for further improving the performance of action\nlocalization. Finally, extensive experiments conducted on THUMOS14 and\nActivityNet1.2 demonstrate the effectiveness of the proposed method.\n","authors":["Ziqiang Li","Yongxin Ge","Jiaruo Yu","Zhongming Chen"],"pdf_url":"https://arxiv.org/pdf/2207.06659v1.pdf","comment":"9 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2207.06658v1","updated":"2022-07-14T05:05:43Z","published":"2022-07-14T05:05:43Z","title":"Universal Adaptive Data Augmentation","summary":"  Existing automatic data augmentation (DA) methods either ignore updating DA's\nparameters according to the target model's state during training or adopt\nupdate strategies that are not effective enough. In this work, we design a\nnovel data augmentation strategy called \"Universal Adaptive Data Augmentation\"\n(UADA). Different from existing methods, UADA would adaptively update DA's\nparameters according to the target model's gradient information during\ntraining: given a pre-defined set of DA operations, we randomly decide types\nand magnitudes of DA operations for every data batch during training, and\nadaptively update DA's parameters along the gradient direction of the loss\nconcerning DA's parameters. In this way, UADA can increase the training loss of\nthe target networks, and the target networks would learn features from harder\nsamples to improve the generalization. Moreover, UADA is very general and can\nbe utilized in numerous tasks, e.g., image classification, semantic\nsegmentation and object detection. Extensive experiments with various models\nare conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and\nVOC07+12 to prove the significant performance improvements brought by our\nproposed adaptive augmentation.\n","authors":["Xiaogang Xu","Hengshuang Zhao","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2207.06658v1.pdf","comment":"under submission"},{"id":"http://arxiv.org/abs/2207.06657v1","updated":"2022-07-14T05:05:18Z","published":"2022-07-14T05:05:18Z","title":"Exploration of an End-to-End Automatic Number-plate Recognition neural\n  network for Indian datasets","summary":"  Indian vehicle number plates have wide variety in terms of size, font, script\nand shape. Development of Automatic Number Plate Recognition (ANPR) solutions\nis therefore challenging, necessitating a diverse dataset to serve as a\ncollection of examples. However, a comprehensive dataset of Indian scenario is\nmissing, thereby, hampering the progress towards publicly available and\nreproducible ANPR solutions. Many countries have invested efforts to develop\ncomprehensive ANPR datasets like Chinese City Parking Dataset (CCPD) for China\nand Application-oriented License Plate (AOLP) dataset for US. In this work, we\nrelease an expanding dataset presently consisting of 1.5k images and a scalable\nand reproducible procedure of enhancing this dataset towards development of\nANPR solution for Indian conditions. We have leveraged this dataset to explore\nan End-to-End (E2E) ANPR architecture for Indian scenario which was originally\nproposed for Chinese Vehicle number-plate recognition based on the CCPD\ndataset. As we customized the architecture for our dataset, we came across\ninsights, which we have discussed in this paper. We report the hindrances in\ndirect reusability of the model provided by the authors of CCPD because of the\nextreme diversity in Indian number plates and differences in distribution with\nrespect to the CCPD dataset. An improvement of 42.86% was observed in LP\ndetection after aligning the characteristics of Indian dataset with Chinese\ndataset. In this work, we have also compared the performance of the E2E\nnumber-plate detection model with YOLOv5 model, pre-trained on COCO dataset and\nfine-tuned on Indian vehicle images. Given that the number Indian vehicle\nimages used for fine-tuning the detection module and yolov5 were same, we\nconcluded that it is more sample efficient to develop an ANPR solution for\nIndian conditions based on COCO dataset rather than CCPD dataset.\n","authors":["Sai Sirisha Nadiminti","Pranav Kant Gaur","Abhilash Bhardwaj"],"pdf_url":"https://arxiv.org/pdf/2207.06657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06654v1","updated":"2022-07-14T04:54:26Z","published":"2022-07-14T04:54:26Z","title":"Prototypical Contrast Adaptation for Domain Adaptive Semantic\n  Segmentation","summary":"  Unsupervised Domain Adaptation (UDA) aims to adapt the model trained on the\nlabeled source domain to an unlabeled target domain. In this paper, we present\nPrototypical Contrast Adaptation (ProCA), a simple and efficient contrastive\nlearning method for unsupervised domain adaptive semantic segmentation.\nPrevious domain adaptation methods merely consider the alignment of the\nintra-class representational distributions across various domains, while the\ninter-class structural relationship is insufficiently explored, resulting in\nthe aligned representations on the target domain might not be as easily\ndiscriminated as done on the source domain anymore. Instead, ProCA incorporates\ninter-class information into class-wise prototypes, and adopts the\nclass-centered distribution alignment for adaptation. By considering the same\nclass prototypes as positives and other class prototypes as negatives to\nachieve class-centered distribution alignment, ProCA achieves state-of-the-art\nperformance on classical domain adaptation tasks, {\\em i.e., GTA5 $\\to$\nCityscapes \\text{and} SYNTHIA $\\to$ Cityscapes}. Code is available at\n\\href{https://github.com/jiangzhengkai/ProCA}{ProCA}\n","authors":["Zhengkai Jiang","Yuxi Li","Ceyuan Yang","Peng Gao","Yabiao Wang","Ying Tai","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2207.06654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.02086v2","updated":"2022-07-14T04:42:44Z","published":"2021-12-03T18:53:16Z","title":"Data-Free Neural Architecture Search via Recursive Label Calibration","summary":"  This paper aims to explore the feasibility of neural architecture search\n(NAS) given only a pre-trained model without using any original training data.\nThis is an important circumstance for privacy protection, bias avoidance, etc.,\nin real-world scenarios. To achieve this, we start by synthesizing usable data\nthrough recovering the knowledge from a pre-trained deep neural network. Then\nwe use the synthesized data and their predicted soft-labels to guide neural\narchitecture search. We identify that the NAS task requires the synthesized\ndata (we target at image domain here) with enough semantics, diversity, and a\nminimal domain gap from the natural images. For semantics, we propose recursive\nlabel calibration to produce more informative outputs. For diversity, we\npropose a regional update strategy to generate more diverse and\nsemantically-enriched synthetic data. For minimal domain gap, we use input and\nfeature-level regularization to mimic the original data distribution in latent\nspace. We instantiate our proposed framework with three popular NAS algorithms:\nDARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the\narchitectures discovered by searching with our synthetic data achieve accuracy\nthat is comparable to, or even higher than, architectures discovered by\nsearching from the original ones, for the first time, deriving the conclusion\nthat NAS can be done effectively with no need of access to the original or\ncalled natural data if the synthesis method is well designed.\n","authors":["Zechun Liu","Zhiqiang Shen","Yun Long","Eric Xing","Kwang-Ting Cheng","Chas Leichner"],"pdf_url":"https://arxiv.org/pdf/2112.02086v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.07235v1","updated":"2022-07-14T23:54:54Z","published":"2022-07-14T23:54:54Z","title":"Single Model Uncertainty Estimation via Stochastic Data Centering","summary":"  We are interested in estimating the uncertainties of deep neural networks,\nwhich play an important role in many scientific and engineering problems. In\nthis paper, we present a striking new finding that an ensemble of neural\nnetworks with the same weight initialization, trained on datasets that are\nshifted by a constant bias gives rise to slightly inconsistent trained models,\nwhere the differences in predictions are a strong indicator of epistemic\nuncertainties. Using the neural tangent kernel (NTK), we demonstrate that this\nphenomena occurs in part because the NTK is not shift-invariant. Since this is\nachieved via a trivial input transformation, we show that it can therefore be\napproximated using just a single neural network -- using a technique that we\ncall $\\Delta-$UQ -- that estimates uncertainty around prediction by\nmarginalizing out the effect of the biases. We show that $\\Delta-$UQ's\nuncertainty estimates are superior to many of the current methods on a variety\nof benchmarks -- outlier rejection, calibration under distribution shift, and\nsequential design optimization of black box functions.\n","authors":["Jayaraman J. Thiagarajan","Rushil Anirudh","Vivek Narayanaswamy","Peer-Timo Bremer"],"pdf_url":"https://arxiv.org/pdf/2207.07235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07232v1","updated":"2022-07-14T23:40:22Z","published":"2022-07-14T23:40:22Z","title":"Lipschitz Bound Analysis of Neural Networks","summary":"  Lipschitz Bound Estimation is an effective method of regularizing deep neural\nnetworks to make them robust against adversarial attacks. This is useful in a\nvariety of applications ranging from reinforcement learning to autonomous\nsystems. In this paper, we highlight the significant gap in obtaining a\nnon-trivial Lipschitz bound certificate for Convolutional Neural Networks\n(CNNs) and empirically support it with extensive graphical analysis. We also\nshow that unrolling Convolutional layers or Toeplitz matrices can be employed\nto convert Convolutional Neural Networks (CNNs) to a Fully Connected Network.\nFurther, we propose a simple algorithm to show the existing 20x-50x gap in a\nparticular data distribution between the actual lipschitz constant and the\nobtained tight bound. We also ran sets of thorough experiments on various\nnetwork architectures and benchmark them on datasets like MNIST and CIFAR-10.\nAll these proposals are supported by extensive testing, graphs, histograms and\ncomparative analysis.\n","authors":["Sarosij Bose"],"pdf_url":"https://arxiv.org/pdf/2207.07232v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2201.01763v3","updated":"2022-07-14T23:05:52Z","published":"2022-01-05T18:50:50Z","title":"Robust Self-Supervised Audio-Visual Speech Recognition","summary":"  Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.\n","authors":["Bowen Shi","Wei-Ning Hsu","Abdelrahman Mohamed"],"pdf_url":"https://arxiv.org/pdf/2201.01763v3.pdf","comment":"Interspeech 2022"},{"id":"http://arxiv.org/abs/2205.07180v2","updated":"2022-07-14T23:02:59Z","published":"2022-05-15T04:48:41Z","title":"Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT","summary":"  This paper investigates self-supervised pre-training for audio-visual speaker\nrepresentation learning where a visual stream showing the speaker's mouth area\nis used alongside speech as inputs. Our study focuses on the Audio-Visual\nHidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose\naudio-visual speech pre-training framework. We conducted extensive experiments\nprobing the effectiveness of pre-training and visual modality. Experimental\nresults suggest that AV-HuBERT generalizes decently to speaker related\ndownstream tasks, improving label efficiency by roughly ten fold for both\naudio-only and audio-visual speaker verification. We also show that\nincorporating visual information, even just the lip area, greatly improves the\nperformance and noise robustness, reducing EER by 38% in the clean condition\nand 75% in noisy conditions.\n","authors":["Bowen Shi","Abdelrahman Mohamed","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2205.07180v2.pdf","comment":"Interspeech 2022"},{"id":"http://arxiv.org/abs/2207.05249v3","updated":"2022-07-14T22:49:24Z","published":"2022-07-12T01:18:58Z","title":"Efficient Human Vision Inspired Action Recognition using Adaptive\n  Spatiotemporal Sampling","summary":"  Adaptive sampling that exploits the spatiotemporal redundancy in videos is\ncritical for always-on action recognition on wearable devices with limited\ncomputing and battery resources. The commonly used fixed sampling strategy is\nnot context-aware and may under-sample the visual content, and thus adversely\nimpacts both computation efficiency and accuracy. Inspired by the concepts of\nfoveal vision and pre-attentive processing from the human visual perception\nmechanism, we introduce a novel adaptive spatiotemporal sampling scheme for\nefficient action recognition. Our system pre-scans the global scene context at\nlow-resolution and decides to skip or request high-resolution features at\nsalient regions for further processing. We validate the system on EPIC-KITCHENS\nand UCF-101 datasets for action recognition, and show that our proposed\napproach can greatly speed up inference with a tolerable loss of accuracy\ncompared with those from state-of-the-art baselines. Source code is available\nin https://github.com/knmac/adaptive_spatiotemporal.\n","authors":["Khoi-Nguyen C. Mac","Minh N. Do","Minh P. Vo"],"pdf_url":"https://arxiv.org/pdf/2207.05249v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08220v2","updated":"2022-07-14T22:43:44Z","published":"2021-10-15T17:31:10Z","title":"Combining Diverse Feature Priors","summary":"  To improve model generalization, model designers often restrict the features\nthat their models use, either implicitly or explicitly. In this work, we\nexplore the design space of leveraging such feature priors by viewing them as\ndistinct perspectives on the data. Specifically, we find that models trained\nwith diverse sets of feature priors have less overlapping failure modes, and\ncan thus be combined more effectively. Moreover, we demonstrate that jointly\ntraining such models on additional (unlabeled) data allows them to correct each\nother's mistakes, which, in turn, leads to better generalization and resilience\nto spurious correlations. Code available at\nhttps://github.com/MadryLab/copriors\n","authors":["Saachi Jain","Dimitris Tsipras","Aleksander Madry"],"pdf_url":"https://arxiv.org/pdf/2110.08220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2001.02658v4","updated":"2022-07-14T22:03:25Z","published":"2020-01-08T18:02:56Z","title":"Distributionally Robust Deep Learning using Hardness Weighted Sampling","summary":"  Limiting failures of machine learning systems is of paramount importance for\nsafety-critical applications. In order to improve the robustness of machine\nlearning systems, Distributionally Robust Optimization (DRO) has been proposed\nas a generalization of Empirical Risk Minimization (ERM). However, its use in\ndeep learning has been severely restricted due to the relative inefficiency of\nthe optimizers available for DRO in comparison to the wide-spread variants of\nStochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with\nhardness weighted sampling, a principled and efficient optimization method for\nDRO in machine learning that is particularly suited in the context of deep\nlearning. Similar to a hard example mining strategy in practice, the proposed\nalgorithm is straightforward to implement and computationally as efficient as\nSGD-based optimizers used for deep learning, requiring minimal overhead\ncomputation. In contrast to typical ad hoc hard mining approaches, we prove the\nconvergence of our DRO algorithm for over-parameterized deep learning networks\nwith ReLU activation and a finite number of layers and parameters. Our\nexperiments on fetal brain 3D MRI segmentation and brain tumor segmentation in\nMRI demonstrate the feasibility and the usefulness of our approach. Using our\nhardness weighted sampling for training a state-of-the-art deep learning\npipeline leads to improved robustness to anatomical variabilities in automatic\nfetal brain 3D MRI segmentation using deep learning and to improved robustness\nto the image protocol variations in brain tumor segmentation. Our code is\navailable at https://github.com/LucasFidon/HardnessWeightedSampler.\n","authors":["Lucas Fidon","Michael Aertsen","Thomas Deprest","Doaa Emam","Frédéric Guffens","Nada Mufti","Esther Van Elslander","Ernst Schwartz","Michael Ebner","Daniela Prayer","Gregor Kasprian","Anna L. David","Andrew Melbourne","Sébastien Ourselin","Jan Deprest","Georg Langs","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2001.02658v4.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:019.html"},{"id":"http://arxiv.org/abs/2110.13989v2","updated":"2022-07-14T21:35:24Z","published":"2021-10-26T19:48:19Z","title":"Revisiting Batch Norm Initialization","summary":"  Batch normalization (BN) is comprised of a normalization component followed\nby an affine transformation and has become essential for training deep neural\nnetworks. Standard initialization of each BN in a network sets the affine\ntransformation scale and shift to 1 and 0, respectively. However, after\ntraining we have observed that these parameters do not alter much from their\ninitialization. Furthermore, we have noticed that the normalization process can\nstill yield overly large values, which is undesirable for training. We revisit\nthe BN formulation and present a new initialization method and update approach\nfor BN to address the aforementioned issues. Experiments are designed to\nemphasize and demonstrate the positive influence of proper BN scale\ninitialization on performance, and use rigorous statistical significance tests\nfor evaluation. The approach can be used with existing implementations at no\nadditional computational cost. Source code is available at\nhttps://github.com/osu-cvl/revisiting-bn-init.\n","authors":["Jim Davis","Logan Frank"],"pdf_url":"https://arxiv.org/pdf/2110.13989v2.pdf","comment":"European Conference on Computer Vision, October 2022"},{"id":"http://arxiv.org/abs/2204.09817v3","updated":"2022-07-14T20:45:13Z","published":"2022-04-21T00:04:35Z","title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language\n  Processing","summary":"  Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n","authors":["Benedikt Boecking","Naoto Usuyama","Shruthi Bannur","Daniel C. Castro","Anton Schwaighofer","Stephanie Hyland","Maria Wetscherek","Tristan Naumann","Aditya Nori","Javier Alvarez-Valle","Hoifung Poon","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2204.09817v3.pdf","comment":"To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:\n  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook"},{"id":"http://arxiv.org/abs/2205.08209v2","updated":"2022-07-14T20:44:56Z","published":"2022-05-17T10:13:27Z","title":"blob loss: instance imbalance aware loss functions for semantic\n  segmentation","summary":"  Deep convolutional neural networks have proven to be remarkably effective in\nsemantic segmentation tasks. Most popular loss functions were introduced\ntargeting improved volumetric scores, such as the Sorensen Dice coefficient. By\ndesign, DSC can tackle class imbalance; however, it does not recognize instance\nimbalance within a class. As a result, a large foreground instance can dominate\nminor instances and still produce a satisfactory Sorensen Dice coefficient.\nNevertheless, missing out on instances will lead to poor detection performance.\nThis represents a critical issue in applications such as disease progression\nmonitoring. For example, it is imperative to locate and surveil small-scale\nlesions in the follow-up of multiple sclerosis patients. We propose a novel\nfamily of loss functions, nicknamed blob loss, primarily aimed at maximizing\ninstance-level detection metrics, such as F1 score and sensitivity. Blob loss\nis designed for semantic segmentation problems in which the instances are the\nconnected components within a class. We extensively evaluate a DSC-based blob\nloss in five complex 3D semantic segmentation tasks featuring pronounced\ninstance heterogeneity in terms of texture and morphology. Compared to soft\nDice loss, we achieve 5 percent improvement for MS lesions, 3 percent\nimprovement for liver tumor, and an average 2 percent improvement for\nMicroscopy segmentation tasks considering F1 score.\n","authors":["Florian Kofler","Suprosanna Shit","Ivan Ezhov","Lucas Fidon","Izabela Horvath","Rami Al-Maskari","Hongwei Li","Harsharan Bhatia","Timo Loehr","Marie Piraud","Ali Erturk","Jan Kirschke","Jan Peeken","Tom Vercauteren","Claus Zimmer","Benedikt Wiestler","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2205.08209v2.pdf","comment":"23 pages, 7 figures // corrected one mistake where it said beta\n  instead of alpha in the text"},{"id":"http://arxiv.org/abs/2207.07189v1","updated":"2022-07-14T20:18:58Z","published":"2022-07-14T20:18:58Z","title":"Current Trends in Deep Learning for Earth Observation: An Open-source\n  Benchmark Arena for Image Classification","summary":"  We present 'AiTLAS: Benchmark Arena' -- an open-source benchmark framework\nfor evaluating state-of-the-art deep learning approaches for image\nclassification in Earth Observation (EO). To this end, we present a\ncomprehensive comparative analysis of more than 400 models derived from nine\ndifferent state-of-the-art architectures, and compare them to a variety of\nmulti-class and multi-label classification tasks from 22 datasets with\ndifferent sizes and properties. In addition to models trained entirely on these\ndatasets, we also benchmark models trained in the context of transfer learning,\nleveraging pre-trained model variants, as it is typically performed in\npractice. All presented approaches are general and can be easily extended to\nmany other remote sensing image classification tasks not considered in this\nstudy. To ensure reproducibility and facilitate better usability and further\ndevelopments, all of the experimental resources including the trained models,\nmodel configurations and processing details of the datasets (with their\ncorresponding splits used for training and evaluating the models) are publicly\navailable on the repository: https://github.com/biasvariancelabs/aitlas-arena.\n","authors":["Ivica Dimitrovski","Ivan Kitanovski","Dragi Kocev","Nikola Simidjievski"],"pdf_url":"https://arxiv.org/pdf/2207.07189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.09106v2","updated":"2022-07-14T20:13:26Z","published":"2022-06-18T03:50:19Z","title":"Embodied Scene-aware Human Pose Estimation","summary":"  We propose embodied scene-aware human pose estimation where we estimate 3D\nposes based on a simulated agent's proprioception and scene awareness, along\nwith external third-person observations. Unlike prior methods that often resort\nto multistage optimization, non-causal inference, and complex contact modeling\nto estimate human pose and human scene interactions, our method is one stage,\ncausal, and recovers global 3D human poses in a simulated environment. Since 2D\nthird-person observations are coupled with the camera pose, we propose to\ndisentangle the camera pose and use a multi-step projection gradient defined in\nthe global coordinate frame as the movement cue for our embodied agent.\nLeveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we\nsimulate our agent in everyday environments (libraries, offices, bedrooms,\netc.) and equip our agent with environmental sensors to intelligently navigate\nand interact with scene geometries. Our method also relies only on 2D keypoints\nand can be trained on synthetic datasets derived from popular human motion\ndatabases. To evaluate, we use the popular H36M and PROX datasets and, for the\nfirst time, achieve a success rate of 96.7% on the challenging PROX dataset\nwithout ever using PROX motion sequences for training.\n","authors":["Zhengyi Luo","Shun Iwase","Ye Yuan","Kris Kitani"],"pdf_url":"https://arxiv.org/pdf/2206.09106v2.pdf","comment":"Project website: https://embodiedscene.github.io/embodiedpose/\n  Zhengyi Luo and Shun Iwase contributed equally"},{"id":"http://arxiv.org/abs/2107.14178v2","updated":"2022-07-14T20:11:17Z","published":"2021-07-29T17:03:36Z","title":"ReFormer: The Relational Transformer for Image Captioning","summary":"  Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation\n","authors":["Xuewen Yang","Yingru Liu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2107.14178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07173v1","updated":"2022-07-14T19:16:56Z","published":"2022-07-14T19:16:56Z","title":"Image Clustering with Contrastive Learning and Multi-scale Graph\n  Convolutional Networks","summary":"  Deep clustering has recently attracted significant attention. Despite the\nremarkable progress, most of the previous deep clustering works still suffer\nfrom two limitations. First, many of them focus on some distribution-based\nclustering loss, lacking the ability to exploit sample-wise (or\naugmentation-wise) relationships via contrastive learning. Second, they often\nneglect the indirect sample-wise structure information, overlooking the rich\npossibilities of multi-scale neighborhood structure learning. In view of this,\nthis paper presents a new deep clustering approach termed Image clustering with\ncontrastive learning and multi-scale Graph Convolutional Networks (IcicleGCN),\nwhich bridges the gap between convolutional neural network (CNN) and graph\nconvolutional network (GCN) as well as the gap between contrastive learning and\nmulti-scale neighborhood structure learning for the image clustering task. The\nproposed IcicleGCN framework consists of four main modules, namely, the\nCNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster\nStructure Learning and Instance reconstruction Module (JC-SLIM), and the\nMulti-scale GCN module (M-GCN). Specifically, with two random augmentations\nperformed on each image, the backbone network with two weight-sharing views is\nutilized to learn the representations for the augmented samples, which are then\nfed to ISM and JC-SLIM for instance-level and cluster-level contrastive\nlearning, respectively. Further, to enforce multi-scale neighborhood structure\nlearning, two streams of GCNs and an auto-encoder are simultaneously trained\nvia (i) the layer-wise interaction with representation fusion and (ii) the\njoint self-adaptive learning that ensures their last-layer output distributions\nto be consistent. Experiments on multiple image datasets demonstrate the\nsuperior clustering performance of IcicleGCN over the state-of-the-art.\n","authors":["Yuanku Xu","Dong Huang","Chang-Dong Wang","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2207.07173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.00559v3","updated":"2022-07-14T18:35:59Z","published":"2022-04-01T16:39:16Z","title":"DFNet: Enhance Absolute Pose Regression with Direct Feature Matching","summary":"  We introduce a camera relocalization pipeline that combines absolute pose\nregression (APR) and direct feature matching. By incorporating\nexposure-adaptive novel view synthesis, our method successfully addresses\nphotometric distortions in outdoor environments that existing photometric-based\nmethods fail to handle. With domain-invariant feature matching, our solution\nimproves pose regression accuracy using semi-supervised learning on unlabeled\ndata. In particular, the pipeline consists of two components: Novel View\nSynthesizer and DFNet. The former synthesizes novel views compensating for\nchanges in exposure and the latter regresses camera poses and extracts robust\nfeatures that close the domain gap between real images and synthetic ones.\nFurthermore, we introduce an online synthetic data generation scheme. We show\nthat these approaches effectively enhance camera pose estimation both in indoor\nand outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by\noutperforming existing single-image APR methods by as much as 56%, comparable\nto 3D structure-based methods.\n","authors":["Shuai Chen","Xinghui Li","Zirui Wang","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2204.00559v3.pdf","comment":"ECCV 2022"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2207.07116v1","updated":"2022-07-14T17:59:58Z","published":"2022-07-14T17:59:58Z","title":"Bootstrapped Masked Autoencoders for Vision BERT Pretraining","summary":"  We propose bootstrapped masked autoencoders (BootMAE), a new approach for\nvision BERT pretraining. BootMAE improves the original masked autoencoders\n(MAE) with two core designs: 1) momentum encoder that provides online feature\nas extra BERT prediction targets; 2) target-aware decoder that tries to reduce\nthe pressure on the encoder to memorize target-specific information in BERT\npretraining. The first design is motivated by the observation that using a\npretrained MAE to extract the features as the BERT prediction target for masked\ntokens can achieve better pretraining performance. Therefore, we add a momentum\nencoder in parallel with the original MAE encoder, which bootstraps the\npretraining performance by using its own representation as the BERT prediction\ntarget. In the second design, we introduce target-specific information (e.g.,\npixel values of unmasked patches) from the encoder directly to the decoder to\nreduce the pressure on the encoder of memorizing the target-specific\ninformation. Thus, the encoder focuses on semantic modeling, which is the goal\nof BERT pretraining, and does not need to waste its capacity in memorizing the\ninformation of unmasked tokens related to the prediction target. Through\nextensive experiments, our BootMAE achieves $84.2\\%$ Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\\%$ under the same\npre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic\nsegmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object\ndetection and segmentation on COCO dataset. Code is released at\nhttps://github.com/LightDXY/BootMAE.\n","authors":["Xiaoyi Dong","Jianmin Bao","Ting Zhang","Dongdong Chen","Weiming Zhang","Lu Yuan","Dong Chen","Fang Wen","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2207.07116v1.pdf","comment":"ECCV 2022, code is available at https://github.com/LightDXY/BootMAE"},{"id":"http://arxiv.org/abs/2207.07105v1","updated":"2022-07-14T17:58:02Z","published":"2022-07-14T17:58:02Z","title":"Continuous-time Analysis for Variational Inequalities: An Overview and\n  Desiderata","summary":"  Algorithms that solve zero-sum games, multi-objective agent objectives, or,\nmore generally, variational inequality (VI) problems are notoriously unstable\non general problems. Owing to the increasing need for solving such problems in\nmachine learning, this instability has been highlighted in recent years as a\nsignificant research challenge. In this paper, we provide an overview of recent\nprogress in the use of continuous-time perspectives in the analysis and design\nof methods targeting the broad VI problem class. Our presentation draws\nparallels between single-objective problems and multi-objective problems,\nhighlighting the challenges of the latter. We also formulate various desiderata\nfor algorithms that apply to general VIs and we argue that achieving these\ndesiderata may profit from an understanding of the associated continuous-time\ndynamics.\n","authors":["Tatjana Chavdarova","Ya-Ping Hsieh","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2207.07105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02829v2","updated":"2022-07-14T17:57:53Z","published":"2022-07-06T17:36:59Z","title":"Online Bilevel Optimization: Regret Analysis of Online Alternating\n  Gradient Methods","summary":"  Online optimization is a well-established optimization paradigm that aims to\nmake a sequence of correct decisions given knowledge of the correct answer to\nprevious decision tasks. Bilevel programming involves a hierarchical\noptimization problem where the feasible region of the so-called outer problem\nis restricted by the graph of the solution set mapping of the inner problem.\nThis paper brings these two ideas together and studies an online bilevel\noptimization setting in which a sequence of time-varying bilevel problems are\nrevealed one after the other. We extend the known regret bounds for\nsingle-level online algorithms to the bilevel setting. Specifically, we\nintroduce new notions of bilevel regret, develop an online alternating\ntime-averaged gradient method that is capable of leveraging smoothness, and\nprovide regret bounds in terms of the path-length of the inner and outer\nminimizer sequences.\n","authors":["Davoud Ataee Tarzanagh","Laura Balzano"],"pdf_url":"https://arxiv.org/pdf/2207.02829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.11291v4","updated":"2022-07-14T17:56:18Z","published":"2021-10-21T17:18:59Z","title":"Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs\n  Theory","summary":"  Schr\\\"odinger Bridge (SB) is an entropy-regularized optimal transport problem\nthat has received increasing attention in deep generative modeling for its\nmathematical flexibility compared to the Scored-based Generative Model (SGM).\nHowever, it remains unclear whether the optimization principle of SB relates to\nthe modern training of deep generative models, which often rely on constructing\nlog-likelihood objectives.This raises questions on the suitability of SB models\nas a principled alternative for generative applications. In this work, we\npresent a novel computational framework for likelihood training of SB models\ngrounded on Forward-Backward Stochastic Differential Equations Theory - a\nmathematical methodology appeared in stochastic optimal control that transforms\nthe optimality condition of SB into a set of SDEs. Crucially, these SDEs can be\nused to construct the likelihood objectives for SB that, surprisingly,\ngeneralizes the ones for SGM as special cases. This leads to a new optimization\nprinciple that inherits the same SB optimality yet without losing applications\nof modern generative training techniques, and we show that the resulting\ntraining algorithm achieves comparable results on generating realistic images\non MNIST, CelebA, and CIFAR10. Our code is available at\nhttps://github.com/ghliu/SB-FBSDE.\n","authors":["Tianrong Chen","Guan-Horng Liu","Evangelos A. Theodorou"],"pdf_url":"https://arxiv.org/pdf/2110.11291v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07089v1","updated":"2022-07-14T17:40:05Z","published":"2022-07-14T17:40:05Z","title":"A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse\n  Representation Based Domain Adaption to Energy Efficient Abnormal Beat\n  Detection for Practical ECG Surveillance","summary":"  This paper proposes a low-cost and highly accurate ECG-monitoring system\nintended for personalized early arrhythmia detection for wearable mobile\nsensors. Earlier supervised approaches for personalized ECG monitoring require\nboth abnormal and normal heartbeats for the training of the dedicated\nclassifier. However, in a real-world scenario where the personalized algorithm\nis embedded in a wearable device, such training data is not available for\nhealthy people with no cardiac disorder history. In this study, (i) we propose\na null space analysis on the healthy signal space obtained via sparse\ndictionary learning, and investigate how a simple null space projection or\nalternatively regularized least squares-based classification methods can reduce\nthe computational complexity, without sacrificing the detection accuracy, when\ncompared to sparse representation-based classification. (ii) Then we introduce\na sparse representation-based domain adaptation technique in order to project\nother existing users' abnormal and normal signals onto the new user's signal\nspace, enabling us to train the dedicated classifier without having any\nabnormal heartbeat of the new user. Therefore, zero-shot learning can be\nachieved without the need for synthetic abnormal heartbeat generation. An\nextensive set of experiments performed on the benchmark MIT-BIH ECG dataset\nshows that when this domain adaptation-based training data generator is used\nwith a simple 1-D CNN classifier, the method outperforms the prior work by a\nsignificant margin. (iii) Then, by combining (i) and (ii), we propose an\nensemble classifier that further improves the performance. This approach for\nzero-shot arrhythmia detection achieves an average accuracy level of 98.2% and\nan F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring\nscheme is proposed using the above-mentioned innovations.\n","authors":["Mehmet Yamaç","Mert Duman","İlke Adalıoğlu","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2207.07089v1.pdf","comment":"Software implementation: https://github.com/MertDuman/Zero-Shot-ECG"},{"id":"http://arxiv.org/abs/2207.07087v1","updated":"2022-07-14T17:40:00Z","published":"2022-07-14T17:40:00Z","title":"Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated\n  Neural Text Retrievers","summary":"  Prompt tuning attempts to update few task-specific parameters in pre-trained\nmodels. It has achieved comparable performance to fine-tuning of the full\nparameter set on both language understanding and generation tasks. In this\nwork, we study the problem of prompt tuning for neural text retrievers. We\nintroduce parameter-efficient prompt tuning for text retrieval across\nin-domain, cross-domain, and cross-topic settings. Through an extensive\nanalysis, we show that the strategy can mitigate the two issues --\nparameter-inefficiency and weak generalizability -- faced by fine-tuning based\nretrieval methods. Notably, it can significantly improve the out-of-domain\nzero-shot generalization of the retrieval models. By updating only 0.1% of the\nmodel parameters, the prompt tuning strategy can help retrieval models achieve\nbetter generalization performance than traditional methods in which all\nparameters are updated. Finally, to facilitate research on retrievers'\ncross-topic generalizability, we curate and release an academic retrieval\ndataset with 18K query-results pairs in 87 topics, making it the largest\ntopic-specific one to date.\n","authors":["Weng Lam Tam","Xiao Liu","Kaixuan Ji","Lilong Xue","Xingjian Zhang","Yuxiao Dong","Jiahua Liu","Maodi Hu","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2207.07087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07080v1","updated":"2022-07-14T17:30:13Z","published":"2022-07-14T17:30:13Z","title":"An Asymmetric Contrastive Loss for Handling Imbalanced Datasets","summary":"  Contrastive learning is a representation learning method performed by\ncontrasting a sample to other similar samples so that they are brought closely\ntogether, forming clusters in the feature space. The learning process is\ntypically conducted using a two-stage training architecture, and it utilizes\nthe contrastive loss (CL) for its feature learning. Contrastive learning has\nbeen shown to be quite successful in handling imbalanced datasets, in which\nsome classes are overrepresented while some others are underrepresented.\nHowever, previous studies have not specifically modified CL for imbalanced\ndatasets. In this work, we introduce an asymmetric version of CL, referred to\nas ACL, in order to directly address the problem of class imbalance. In\naddition, we propose the asymmetric focal contrastive loss (AFCL) as a further\ngeneralization of both ACL and focal contrastive loss (FCL). Results on the\nFMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of\noutperforming CL and FCL in terms of both weighted and unweighted\nclassification accuracies. In the appendix, we provide a full axiomatic\ntreatment on entropy, along with complete proofs.\n","authors":["Valentino Vito","Lim Yohanes Stefanus"],"pdf_url":"https://arxiv.org/pdf/2207.07080v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.07072v1","updated":"2022-07-14T17:21:13Z","published":"2022-07-14T17:21:13Z","title":"A Query-Optimal Algorithm for Finding Counterfactuals","summary":"  We design an algorithm for finding counterfactuals with strong theoretical\nguarantees on its performance. For any monotone model $f : X^d \\to \\{0,1\\}$ and\ninstance $x^\\star$, our algorithm makes \\[ {S(f)^{O(\\Delta_f(x^\\star))}\\cdot\n\\log d}\\] queries to $f$ and returns {an {\\sl optimal}} counterfactual for\n$x^\\star$: a nearest instance $x'$ to $x^\\star$ for which $f(x')\\ne\nf(x^\\star)$. Here $S(f)$ is the sensitivity of $f$, a discrete analogue of the\nLipschitz constant, and $\\Delta_f(x^\\star)$ is the distance from $x^\\star$ to\nits nearest counterfactuals. The previous best known query complexity was\n$d^{\\,O(\\Delta_f(x^\\star))}$, achievable by brute-force local search. We\nfurther prove a lower bound of $S(f)^{\\Omega(\\Delta_f(x^\\star))} + \\Omega(\\log\nd)$ on the query complexity of any algorithm, thereby showing that the\nguarantees of our algorithm are essentially optimal.\n","authors":["Guy Blanc","Caleb Koch","Jane Lange","Li-Yang Tan"],"pdf_url":"https://arxiv.org/pdf/2207.07072v1.pdf","comment":"22 pages, ICML 2022"},{"id":"http://arxiv.org/abs/2207.07068v1","updated":"2022-07-14T17:16:45Z","published":"2022-07-14T17:16:45Z","title":"Bia Mitigation for Machine Learning Classifiers: A Comprehensive Survey","summary":"  This paper provides a comprehensive survey of bias mitigation methods for\nachieving fairness in Machine Learning (ML) models. We collect a total of 234\npublications concerning bias mitigation for ML classifiers. These methods can\nbe distinguished based on their intervention procedure (i.e., pre-processing,\nin-processing, post-processing) and the technology they apply. We investigate\nhow existing bias mitigation methods are evaluated in the literature. In\nparticular, we consider datasets, metrics and benchmarking. Based on the\ngathered insights (e.g., what is the most popular fairness metric? How many\ndatasets are used for evaluating bias mitigation methods?). We hope to support\npractitioners in making informed choices when developing and evaluating new\nbias mitigation methods.\n","authors":["Max Hort","Zhenpeng Chen","Jie M. Zhang","Federica Sarro","Mark Harman"],"pdf_url":"https://arxiv.org/pdf/2207.07068v1.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.07065v1","updated":"2022-07-14T17:08:25Z","published":"2022-07-14T17:08:25Z","title":"On the Strong Correlation Between Model Invariance and Generalization","summary":"  Generalization and invariance are two essential properties of any machine\nlearning model. Generalization captures a model's ability to classify unseen\ndata while invariance measures consistency of model predictions on\ntransformations of the data. Existing research suggests a positive\nrelationship: a model generalizing well should be invariant to certain visual\nfactors. Building on this qualitative implication we make two contributions.\nFirst, we introduce effective invariance (EI), a simple and reasonable measure\nof model invariance which does not rely on image labels. Given predictions on a\ntest image and its transformed version, EI measures how well the predictions\nagree and with what level of confidence. Second, using invariance scores\ncomputed by EI, we perform large-scale quantitative correlation studies between\ngeneralization and invariance, focusing on rotation and grayscale\ntransformations. From a model-centric view, we observe generalization and\ninvariance of different models exhibit a strong linear relationship, on both\nin-distribution and out-of-distribution datasets. From a dataset-centric view,\nwe find a certain model's accuracy and invariance linearly correlated on\ndifferent test sets. Apart from these major findings, other minor but\ninteresting insights are also discussed.\n","authors":["Weijian Deng","Stephen Gould","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2207.07065v1.pdf","comment":"18 pages, 11 figures; this version is not fully edited and will be\n  updated soon"},{"id":"http://arxiv.org/abs/2207.07061v1","updated":"2022-07-14T17:00:19Z","published":"2022-07-14T17:00:19Z","title":"Confident Adaptive Language Modeling","summary":"  Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.\n","authors":["Tal Schuster","Adam Fisch","Jai Gupta","Mostafa Dehghani","Dara Bahri","Vinh Q. Tran","Yi Tay","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2207.07061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07505v2","updated":"2022-07-14T16:59:27Z","published":"2022-03-14T21:30:43Z","title":"Closing the Loop: A Framework for Trustworthy Machine Learning in Power\n  Systems","summary":"  Deep decarbonization of the energy sector will require massive penetration of\nstochastic renewable energy resources and an enormous amount of grid asset\ncoordination; this represents a challenging paradigm for the power system\noperators who are tasked with maintaining grid stability and security in the\nface of such changes. With its ability to learn from complex datasets and\nprovide predictive solutions on fast timescales, machine learning (ML) is\nwell-posed to help overcome these challenges as power systems transform in the\ncoming decades. In this work, we outline five key challenges (dataset\ngeneration, data pre-processing, model training, model assessment, and model\nembedding) associated with building trustworthy ML models which learn from\nphysics-based simulation data. We then demonstrate how linking together\nindividual modules, each of which overcomes a respective challenge, at\nsequential stages in the machine learning pipeline can help enhance the overall\nperformance of the training process. In particular, we implement methods that\nconnect different elements of the learning pipeline through feedback, thus\n\"closing the loop\" between model training, performance assessments, and\nre-training. We demonstrate the effectiveness of this framework, its\nconstituent modules, and its feedback connections by learning the N-1\nsmall-signal stability margin associated with a detailed model of a proposed\nNorth Sea Wind Power Hub system.\n","authors":["Jochen Stiasny","Samuel Chevalier","Rahul Nellikkath","Brynjar Sævarsson","Spyros Chatzivasileiadis"],"pdf_url":"https://arxiv.org/pdf/2203.07505v2.pdf","comment":"In proceedings of the 11th Bulk Power Systems Dynamics and Control\n  Symposium (IREP 2022), July 25-30, 2022, Banff, Canada. 21 pages, 12 figures,\n  5 tables"},{"id":"http://arxiv.org/abs/2207.07059v1","updated":"2022-07-14T16:58:47Z","published":"2022-07-14T16:58:47Z","title":"Semi-Supervised Temporal Action Detection with Proposal-Free Masking","summary":"  Existing temporal action detection (TAD) methods rely on a large number of\ntraining data with segment-level annotations. Collecting and annotating such a\ntraining set is thus highly expensive and unscalable. Semi-supervised TAD\n(SS-TAD) alleviates this problem by leveraging unlabeled videos freely\navailable at scale. However, SS-TAD is also a much more challenging problem\nthan supervised TAD, and consequently much under-studied. Prior SS-TAD methods\ndirectly combine an existing proposal-based TAD method and a SSL method. Due to\ntheir sequential localization (e.g, proposal generation) and classification\ndesign, they are prone to proposal error propagation. To overcome this\nlimitation, in this work we propose a novel Semi-supervised Temporal action\ndetection model based on PropOsal-free Temporal mask (SPOT) with a parallel\nlocalization (mask generation) and classification architecture. Such a novel\ndesign effectively eliminates the dependence between localization and\nclassification by cutting off the route for error propagation in-between. We\nfurther introduce an interaction mechanism between classification and\nlocalization for prediction refinement, and a new pretext task for\nself-supervised model pre-training. Extensive experiments on two standard\nbenchmarks show that our SPOT outperforms state-of-the-art alternatives, often\nby a large margin. The PyTorch implementation of SPOT is available at\nhttps://github.com/sauradip/SPOT\n","authors":["Sauradip Nag","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2207.07059v1.pdf","comment":"ECCV 2022; Code available at https://github.com/sauradip/SPOT"},{"id":"http://arxiv.org/abs/2201.07219v3","updated":"2022-07-14T16:57:37Z","published":"2022-01-16T13:09:47Z","title":"Contrastive Pretraining for Echocardiography Segmentation with Limited\n  Data","summary":"  Contrastive learning has proven useful in many applications where access to\nlabelled data is limited. The lack of annotated data is particularly\nproblematic in medical image segmentation as it is difficult to have clinical\nexperts manually annotate large volumes of data such as cardiac structures in\nultrasound images of the heart. In this paper, We propose a self supervised\ncontrastive learning method to segment the left ventricle from echocardiography\nwhen limited annotated images exist. Furthermore, we study the effect of\ncontrastive pretraining on two well-known segmentation networks, UNet and\nDeepLabV3. Our results show that contrastive pretraining helps improve the\nperformance on left ventricle segmentation, particularly when annotated data is\nscarce. We show how to achieve comparable results to state-of-the-art fully\nsupervised algorithms when we train our models in a self-supervised fashion\nfollowed by fine-tuning on just 5\\% of the data. We show that our solution\noutperforms what is currently published on a large public dataset\n(EchoNet-Dynamic) achieving a Dice score of 0.9252. We also compare the\nperformance of our solution on another smaller dataset (CAMUS) to demonstrate\nthe generalizability of our proposed solution. The code is available at\n(https://github.com/BioMedIA-MBZUAI/contrastive-echo).\n","authors":["Mohamed Saeed","Rand Muhtaseb","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2201.07219v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07051v1","updated":"2022-07-14T16:51:09Z","published":"2022-07-14T16:51:09Z","title":"Language models show human-like content effects on reasoning","summary":"  Abstract reasoning is a key ability for an intelligent system. Large language\nmodels achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect, and depends on our knowledge and beliefs about the content of the\nreasoning problem. For example, humans reason much more reliably about logical\nrules that are grounded in everyday situations than arbitrary rules about\nabstract attributes. The training experiences of language models similarly\nendow them with prior expectations that reflect human knowledge and beliefs. We\ntherefore hypothesized that language models would show human-like content\neffects on abstract reasoning problems. We explored this hypothesis across\nthree logical reasoning tasks: natural language inference, judging the logical\nvalidity of syllogisms, and the Wason selection task (Wason, 1968). We find\nthat state of the art large language models (with 7 or 70 billion parameters;\nHoffman et al., 2022) reflect many of the same patterns observed in humans\nacross these tasks -- like humans, models reason more effectively about\nbelievable situations than unrealistic or abstract ones. Our findings have\nimplications for understanding both these cognitive effects, and the factors\nthat contribute to language model performance.\n","authors":["Ishita Dasgupta","Andrew K. Lampinen","Stephanie C. Y. Chan","Antonia Creswell","Dharshan Kumaran","James L. McClelland","Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2207.07051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07049v1","updated":"2022-07-14T16:45:54Z","published":"2022-07-14T16:45:54Z","title":"How do tuna schools associate to dFADs? A study using echo-sounder buoys\n  to identify global patterns","summary":"  Based on the data gathered by echo-sounder buoys attached to drifting Fish\nAggregating Devices (dFADs) across tropical oceans, the current study applies a\nMachine Learning protocol to examine the temporal trends of tuna schools'\nassociation to drifting objects. Using a binary output, metrics typically used\nin the literature were adapted to account for the fact that the entire tuna\naggregation under the dFAD was considered. The median time it took tuna to\ncolonize the dFADs for the first time varied between 25 and 43 days, depending\non the ocean, and the longest soak and colonization times were registered in\nthe Pacific Ocean. The tuna schools' Continuous Residence Times were generally\nshorter than Continuous Absence Times (median values between 5 and 7 days, and\n9 and 11 days, respectively), in line with the results found by previous\nstudies. Using a regression output, two novel metrics, namely aggregation time\nand disaggregation time, were estimated to obtain further insight into the\nsymmetry of the aggregation process. Across all oceans, the time it took for\nthe tuna aggregation to depart from the dFADs was not significantly longer than\nthe time it took for the aggregation to form. The value of these results in the\ncontext of the \"ecological trap\" hypothesis is discussed, and further analyses\nto enrich and make use of this data source are proposed.\n","authors":["Manuel Navarro-García","Daniel Precioso","Kathryn Gavira-O'Neill","Alberto Torres-Barrán","David Gordo","Víctor Gallego","David Gómez-Ullate"],"pdf_url":"https://arxiv.org/pdf/2207.07049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07048v1","updated":"2022-07-14T16:44:59Z","published":"2022-07-14T16:44:59Z","title":"Leakage and the Reproducibility Crisis in ML-based Science","summary":"  The use of machine learning (ML) methods for prediction and forecasting has\nbecome widespread across the quantitative sciences. However, there are many\nknown methodological pitfalls, including data leakage, in ML-based science. In\nthis paper, we systematically investigate reproducibility issues in ML-based\nscience. We show that data leakage is indeed a widespread problem and has led\nto severe reproducibility failures. Specifically, through a survey of\nliterature in research communities that adopted ML methods, we find 17 fields\nwhere errors have been found, collectively affecting 329 papers and in some\ncases leading to wildly overoptimistic conclusions. Based on our survey, we\npresent a fine-grained taxonomy of 8 types of leakage that range from textbook\nerrors to open research problems.\n  We argue for fundamental methodological changes to ML-based science so that\ncases of leakage can be caught before publication. To that end, we propose\nmodel info sheets for reporting scientific claims based on ML models that would\naddress all types of leakage identified in our survey. To investigate the\nimpact of reproducibility errors and the efficacy of model info sheets, we\nundertake a reproducibility study in a field where complex ML models are\nbelieved to vastly outperform older statistical models such as Logistic\nRegression (LR): civil war prediction. We find that all papers claiming the\nsuperior performance of complex ML models compared to LR models fail to\nreproduce due to data leakage, and complex ML models don't perform\nsubstantively better than decades-old LR models. While none of these errors\ncould have been caught by reading the papers, model info sheets would enable\nthe detection of leakage in each case.\n","authors":["Sayash Kapoor","Arvind Narayanan"],"pdf_url":"https://arxiv.org/pdf/2207.07048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.03804v2","updated":"2022-07-14T16:44:11Z","published":"2021-10-07T21:39:33Z","title":"FOCUS: Familiar Objects in Common and Uncommon Settings","summary":"  Standard training datasets for deep learning often contain objects in common\nsettings (e.g., \"a horse on grass\" or \"a ship in water\") since they are usually\ncollected by randomly scraping the web. Uncommon and rare settings (e.g., \"a\nplane on water\", \"a car in snowy weather\") are thus severely under-represented\nin the training data. This can lead to an undesirable bias in model predictions\ntowards common settings and create a false sense of accuracy. In this paper, we\nintroduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset\nfor stress-testing the generalization power of deep image classifiers. By\nleveraging the power of modern search engines, we deliberately gather data\ncontaining objects in common and uncommon settings in a wide range of\nlocations, weather conditions, and time of day. We present a detailed analysis\nof the performance of various popular image classifiers on our dataset and\ndemonstrate a clear drop in performance when classifying images in uncommon\nsettings. By analyzing deep features of these models, we show that such errors\ncan be due to the use of spurious features in model predictions. We believe\nthat our dataset will aid researchers in understanding the inability of deep\nmodels to generalize well to uncommon settings and drive future work on\nimproving their distributional robustness.\n","authors":["Priyatham Kattakinda","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2110.03804v2.pdf","comment":"23 pages, 14 figures, 4 tables. Accepted to ICML 2022"},{"id":"http://arxiv.org/abs/2204.01737v3","updated":"2022-07-14T16:35:45Z","published":"2022-04-04T17:37:54Z","title":"Feature robustness and sex differences in medical imaging: a case study\n  in MRI-based Alzheimer's disease detection","summary":"  Convolutional neural networks have enabled significant improvements in\nmedical image-based diagnosis. It is, however, increasingly clear that these\nmodels are susceptible to performance degradation when facing spurious\ncorrelations and dataset shift, leading, e.g., to underperformance on\nunderrepresented patient groups. In this paper, we compare two classification\nschemes on the ADNI MRI dataset: a simple logistic regression model using\nmanually selected volumetric features, and a convolutional neural network\ntrained on 3D MRI data. We assess the robustness of the trained models in the\nface of varying dataset splits, training set sex composition, and stage of\ndisease. In contrast to earlier work in other imaging modalities, we do not\nobserve a clear pattern of improved model performance for the majority group in\nthe training dataset. Instead, while logistic regression is fully robust to\ndataset composition, we find that CNN performance is generally improved for\nboth male and female subjects when including more female subjects in the\ntraining dataset. We hypothesize that this might be due to inherent differences\nin the pathology of the two sexes. Moreover, in our analysis, the logistic\nregression model outperforms the 3D CNN, emphasizing the utility of manual\nfeature specification based on prior knowledge, and the need for more robust\nautomatic feature selection.\n","authors":["Eike Petersen","Aasa Feragen","Maria Luise da Costa Zemsch","Anders Henriksen","Oskar Eiler Wiese Christensen","Melanie Ganz"],"pdf_url":"https://arxiv.org/pdf/2204.01737v3.pdf","comment":"Accepted for presentation at MICCAI 2022"},{"id":"http://arxiv.org/abs/2207.07038v1","updated":"2022-07-14T16:28:54Z","published":"2022-07-14T16:28:54Z","title":"From Shapley back to Pearson: Hypothesis Testing via the Shapley Value","summary":"  Machine learning models, in particular artificial neural networks, are\nincreasingly used to inform decision making in high-stakes scenarios across a\nvariety of fields--from financial services, to public safety, and healthcare.\nWhile neural networks have achieved remarkable performance in many settings,\ntheir complex nature raises concerns on their reliability, trustworthiness, and\nfairness in real-world scenarios. As a result, several a-posteriori explanation\nmethods have been proposed to highlight the features that influence a model's\nprediction. Notably, the Shapley value--a game theoretic quantity that\nsatisfies several desirable properties--has gained popularity in the machine\nlearning explainability literature. More traditionally, however, feature\nimportance in statistical learning has been formalized by conditional\nindependence, and a standard way to test for it is via Conditional\nRandomization Tests (CRTs). So far, these two perspectives on interpretability\nand feature importance have been considered distinct and separate. In this\nwork, we show that Shapley-based explanation methods and conditional\nindependence testing for feature importance are closely related. More\nprecisely, we prove that evaluating a Shapley coefficient amounts to performing\na specific set of conditional independence tests, as implemented by a procedure\nsimilar to the CRT but for a different null hypothesis. Furthermore, the\nobtained game-theoretic values upper bound the $p$-values of such tests. As a\nresult, we grant large Shapley coefficients with a precise statistical sense of\nimportance with controlled type I error.\n","authors":["Jacopo Teneggi","Beepul Bharti","Yaniv Romano","Jeremias Sulam"],"pdf_url":"https://arxiv.org/pdf/2207.07038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.04320v2","updated":"2022-07-14T16:24:29Z","published":"2021-09-09T14:52:05Z","title":"Discovery of New Multi-Level Features for Domain Generalization via\n  Knowledge Corruption","summary":"  Machine learning models that can generalize to unseen domains are essential\nwhen applied in real-world scenarios involving strong domain shifts. We address\nthe challenging domain generalization (DG) problem, where a model trained on a\nset of source domains is expected to generalize well in unseen domains without\nany exposure to their data. The main challenge of DG is that the features\nlearned from the source domains are not necessarily present in the unseen\ntarget domains, leading to performance deterioration. We assume that learning a\nricher set of features is crucial to improve the transfer to a wider set of\nunknown domains. For this reason, we propose COLUMBUS, a method that enforces\nnew feature discovery via a targeted corruption of the most relevant input and\nmulti-level representations of the data. We conduct an extensive empirical\nevaluation to demonstrate the effectiveness of the proposed approach which\nachieves new state-of-the-art results by outperforming 18 DG algorithms on\nmultiple DG benchmark datasets in the DomainBed framework.\n","authors":["Ahmed Frikha","Denis Krompaß","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2109.04320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06046v2","updated":"2022-07-14T16:11:03Z","published":"2022-07-13T08:43:05Z","title":"DeepTIMe: Deep Time-Index Meta-Learning for Non-Stationary Time-Series\n  Forecasting","summary":"  Deep learning has been actively applied to time-series forecasting, leading\nto a deluge of new autoregressive model architectures. Yet, despite the\nattractive properties of time-index based models, such as being a continuous\nsignal function over time leading to smooth representations, little attention\nhas been given to them. Indeed, while naive deep time-index based models are\nfar more expressive than the manually predefined function representations of\nclassical time-index based models, they are inadequate for forecasting due to\nthe lack of inductive biases, and the non-stationarity of time-series. In this\npaper, we propose DeepTIMe, a deep time-index based model trained via a\nmeta-learning formulation which overcomes these limitations, yielding an\nefficient and accurate forecasting model. Extensive experiments on real world\ndatasets demonstrate that our approach achieves competitive results with\nstate-of-the-art methods, and is highly efficient. Code is available at\nhttps://github.com/salesforce/DeepTIMe.\n","authors":["Gerald Woo","Chenghao Liu","Doyen Sahoo","Akshat Kumar","Steven Hoi"],"pdf_url":"https://arxiv.org/pdf/2207.06046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.14015v2","updated":"2022-07-14T16:03:19Z","published":"2021-06-26T13:09:52Z","title":"Contextual Inverse Optimization: Offline and Online Learning","summary":"  We study the problems of offline and online contextual optimization with\nfeedback information, where instead of observing the loss, we observe,\nafter-the-fact, the optimal action an oracle with full knowledge of the\nobjective function would have taken. We aim to minimize regret, which is\ndefined as the difference between our losses and the ones incurred by an\nall-knowing oracle. In the offline setting, the decision-maker has information\navailable from past periods and needs to make one decision, while in the online\nsetting, the decision-maker optimizes decisions dynamically over time based a\nnew set of feasible actions and contextual functions in each period. For the\noffline setting, we characterize the optimal minimax policy, establishing the\nperformance that can be achieved as a function of the underlying geometry of\nthe information induced by the data. In the online setting, we leverage this\ngeometric characterization to optimize the cumulative regret. We develop an\nalgorithm that yields the first regret bound for this problem that is\nlogarithmic in the time horizon.\n","authors":["Omar Besbes","Yuri Fonseca","Ilan Lobel"],"pdf_url":"https://arxiv.org/pdf/2106.14015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07028v1","updated":"2022-07-14T15:59:55Z","published":"2022-07-14T15:59:55Z","title":"Early Detection of Ovarian Cancer by Wavelet Analysis of Protein Mass\n  Spectra","summary":"  Accurate and efficient detection of ovarian cancer at early stages is\ncritical to ensure proper treatments for patients. Among the first-line\nmodalities investigated in studies of early diagnosis are features distilled\nfrom protein mass spectra. This method, however, considers only a specific\nsubset of spectral responses and ignores the interplay among protein expression\nlevels, which can also contain diagnostic information. We propose a new\nmodality that automatically searches protein mass spectra for discriminatory\nfeatures by considering the self-similar nature of the spectra. Self-similarity\nis assessed by taking a wavelet decomposition of protein mass spectra and\nestimating the rate of level-wise decay in the energies of the resulting\nwavelet coefficients. Level-wise energies are estimated in a robust manner\nusing distance variance, and rates are estimated locally via a rolling window\napproach. This results in a collection of rates that can be used to\ncharacterize the interplay among proteins, which can be indicative of cancer\npresence. Discriminatory descriptors are then selected from these evolutionary\nrates and used as classifying features. The proposed wavelet-based features are\nused in conjunction with features proposed in the existing literature for early\nstage diagnosis of ovarian cancer using two datasets published by the American\nNational Cancer Institute. Including the wavelet-based features from the new\nmodality results in improvements in diagnostic performance for early-stage\novarian cancer detection. This demonstrates the ability of the proposed\nmodality to characterize new ovarian cancer diagnostic information.\n","authors":["Dixon Vimalajeewa","Scott Alan Bruce","Brani Vidakovic"],"pdf_url":"https://arxiv.org/pdf/2207.07028v1.pdf","comment":"Pages 18, 8 figures"},{"id":"http://arxiv.org/abs/2207.07027v1","updated":"2022-07-14T15:59:03Z","published":"2022-07-14T15:59:03Z","title":"MedFuse: Multi-modal fusion with clinical time-series data and chest\n  X-ray images","summary":"  Multi-modal fusion approaches aim to integrate information from different\ndata sources. Unlike natural datasets, such as in audio-visual applications,\nwhere samples consist of \"paired\" modalities, data in healthcare is often\ncollected asynchronously. Hence, requiring the presence of all modalities for a\ngiven sample is not realistic for clinical tasks and significantly limits the\nsize of the dataset during training. In this paper, we propose MedFuse, a\nconceptually simple yet promising LSTM-based fusion module that can accommodate\nuni-modal as well as multi-modal input. We evaluate the fusion method and\nintroduce new benchmark results for in-hospital mortality prediction and\nphenotype classification, using clinical time-series data in the MIMIC-IV\ndataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more\ncomplex multi-modal fusion strategies, MedFuse provides a performance\nimprovement by a large margin on the fully paired test set. It also remains\nrobust across the partially paired test set containing samples with missing\nchest X-ray images. We release our code for reproducibility and to enable the\nevaluation of competing models in the future.\n","authors":["Nasir Hayat","Krzysztof J. Geras","Farah E. Shamout"],"pdf_url":"https://arxiv.org/pdf/2207.07027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1907.06441v5","updated":"2022-07-14T15:57:45Z","published":"2019-07-15T11:32:31Z","title":"Noise-Stable Rigid Graphs for Euclidean Embedding","summary":"  We proposed a new criterion \\textit{noise-stability}, which revised the\nclassical rigidity theory, for evaluation of MDS algorithms which can\ntruthfully represent the fidelity of global structure reconstruction; then we\nproved the noise-stability of the cMDS algorithm in generic conditions, which\nprovides a rigorous theoretical guarantee for the precision and theoretical\nbounds for Euclidean embedding and its application in fields including wireless\nsensor network localization and satellite positioning.\n  Furthermore, we looked into previous work about minimum-cost globally rigid\nspanning subgraph, and proposed an algorithm to construct a minimum-cost\nnoise-stable spanning graph in the Euclidean space, which enabled reliable\nlocalization on sparse graphs of noisy distance constraints with linear numbers\nof edges and sublinear costs in total edge lengths. Additionally, this\nalgorithm also suggests a scheme to reconstruct point clouds from pairwise\ndistances at a minimum of $O(n)$ time complexity, down from $O(n^3)$ for cMDS.\n","authors":["Zishuo Zhao"],"pdf_url":"https://arxiv.org/pdf/1907.06441v5.pdf","comment":"low quality as is my undergraduate work"},{"id":"http://arxiv.org/abs/1909.08444v2","updated":"2022-07-14T15:57:11Z","published":"2019-09-16T19:54:49Z","title":"Musical Instrument Classification via Low-Dimensional Feature Vectors","summary":"  Music is a mysterious language that conveys feeling and thoughts via\ndifferent tones and timbre. For better understanding of timbre in music, we\nchose music data of 6 representative instruments, analysed their timbre\nfeatures and classified them. Instead of the current trend of Neural Network\nfor black-box classification, our project is based on a combination of MFCC and\nLPC, and augmented with a 6-dimensional feature vector designed by ourselves\nfrom observation and attempts. In our white-box model, we observed significant\npatterns of sound that distinguish different timbres, and discovered some\nconnection between objective data and subjective senses. With a totally\n32-dimensional feature vector and a naive all-pairs SVM, we achieved improved\nclassification accuracy compared to a single tool. We also attempted to analyze\nmusic pieces downloaded from the Internet, found out different performance on\ndifferent instruments, explored the reasons and suggested possible ways to\nimprove the performance.\n","authors":["Zishuo Zhao","Haoyun Wang"],"pdf_url":"https://arxiv.org/pdf/1909.08444v2.pdf","comment":"low quality as is my undergraduate work"},{"id":"http://arxiv.org/abs/2207.04198v2","updated":"2022-07-14T15:54:05Z","published":"2022-07-09T05:28:44Z","title":"Improved Binary Forward Exploration: Learning Rate Scheduling Method for\n  Stochastic Optimization","summary":"  A new gradient-based optimization approach by automatically scheduling the\nlearning rate has been proposed recently, which is called Binary Forward\nExploration (BFE). The Adaptive version of BFE has also been discussed\nthereafter. In this paper, the improved algorithms based on them will be\ninvestigated, in order to optimize the efficiency and robustness of the new\nmethodology. This improved approach provides a new perspective to scheduling\nthe update of learning rate and will be compared with the stochastic gradient\ndescent, aka SGD algorithm with momentum or Nesterov momentum and the most\nsuccessful adaptive learning rate algorithm e.g. Adam. The goal of this method\ndoes not aim to beat others but provide a different viewpoint to optimize the\ngradient descent process. This approach combines the advantages of the\nfirst-order and second-order optimizations in the aspects of speed and\nefficiency.\n","authors":["Xin Cao"],"pdf_url":"https://arxiv.org/pdf/2207.04198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.14452v2","updated":"2022-07-14T15:33:40Z","published":"2021-03-26T13:16:01Z","title":"Robot Program Parameter Inference via Differentiable Shadow Program\n  Inversion","summary":"  Challenging manipulation tasks can be solved effectively by combining\nindividual robot skills, which must be parameterized for the concrete physical\nenvironment and task at hand. This is time-consuming and difficult for human\nprogrammers, particularly for force-controlled skills. To this end, we present\nShadow Program Inversion (SPI), a novel approach to infer optimal skill\nparameters directly from data. SPI leverages unsupervised learning to train an\nauxiliary differentiable program representation (\"shadow program\") and realizes\nparameter inference via gradient-based model inversion. Our method enables the\nuse of efficient first-order optimizers to infer optimal parameters for\noriginally non-differentiable skills, including many skill variants currently\nused in production. SPI zero-shot generalizes across task objectives, meaning\nthat shadow programs do not need to be retrained to infer parameters for\ndifferent task variants. We evaluate our methods on three different robots and\nskill frameworks in industrial and household scenarios. Code and examples are\navailable at https://innolab.artiminds.com/icra2021.\n","authors":["Benjamin Alt","Darko Katic","Rainer Jäkel","Asil Kaan Bozcuoglu","Michael Beetz"],"pdf_url":"https://arxiv.org/pdf/2103.14452v2.pdf","comment":"7 pages, 7 figures, presented at IEEE International Conference on\n  Robotics and Automation (ICRA), Xi'an, China, 2021"},{"id":"http://arxiv.org/abs/2207.05225v3","updated":"2022-07-14T15:28:31Z","published":"2022-07-11T23:45:12Z","title":"Susceptibility of Continual Learning Against Adversarial Attacks","summary":"  The recent advances in continual (incremental or lifelong) learning have\nconcentrated on the prevention of forgetting that can lead to catastrophic\nconsequences, but there are two outstanding challenges that must be addressed.\nThe first is the evaluation of the robustness of the proposed methods. The\nsecond is ensuring the security of learned tasks remains largely unexplored.\nThis paper presents a comprehensive study of the susceptibility of the\ncontinually learned tasks (including both current and previously learned tasks)\nthat are vulnerable to forgetting. Such vulnerability of tasks against\nadversarial attacks raises profound issues in data integrity and privacy. We\nconsider all three scenarios (i.e, task-incremental leaning, domain-incremental\nlearning and class-incremental learning) of continual learning and explore\nthree regularization-based experiments, three replay-based experiments, and one\nhybrid technique based on the reply and exemplar approach. We examine the\nrobustness of these methods. In particular, we consider cases where we\ndemonstrate that any class belonging to the current or previously learned tasks\nis prone to misclassification. Our observations, we identify potential\nlimitations in continual learning approaches against adversarial attacks. Our\nempirical study recommends that the research community consider the robustness\nof the proposed continual learning approaches and invest extensive efforts in\nmitigating catastrophic forgetting.\n","authors":["Hikmat Khan","Pir Masoom Shah","Syed Farhan Alam Zaidi","Saif ul Islam"],"pdf_url":"https://arxiv.org/pdf/2207.05225v3.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2105.12639v4","updated":"2022-07-14T15:27:38Z","published":"2021-05-26T15:58:11Z","title":"Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy,\n  Uncertainty, and Robustness","summary":"  Neural network ensembles, such as Bayesian neural networks (BNNs), have shown\nsuccess in the areas of uncertainty estimation and robustness. However, a\ncrucial challenge prohibits their use in practice. BNNs require a large number\nof predictions to produce reliable results, leading to a significant increase\nin computational cost. To alleviate this issue, we propose spatial smoothing, a\nmethod that spatially ensembles neighboring feature map points of convolutional\nneural networks. By simply adding a few blur layers to the models, we\nempirically show that spatial smoothing improves accuracy, uncertainty\nestimation, and robustness of BNNs across a whole range of ensemble sizes. In\nparticular, BNNs incorporating spatial smoothing achieve high predictive\nperformance merely with a handful of ensembles. Moreover, this method also can\nbe applied to canonical deterministic neural networks to improve the\nperformances. A number of evidences suggest that the improvements can be\nattributed to the stabilized feature maps and the smoothing of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing them\nas special cases of spatial smoothing. These not only enhance accuracy, but\nalso improve uncertainty estimation and robustness by making the loss landscape\nsmoother in the same manner as spatial smoothing. The code is available at\nhttps://github.com/xxxnell/spatial-smoothing.\n","authors":["Namuk Park","Songkuk Kim"],"pdf_url":"https://arxiv.org/pdf/2105.12639v4.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2207.06991v1","updated":"2022-07-14T15:20:36Z","published":"2022-07-14T15:20:36Z","title":"Language Modelling with Pixels","summary":"  Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches, instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust to noisy text inputs than BERT,\nfurther confirming the benefits of modelling language with pixels.\n","authors":["Phillip Rust","Jonas F. Lotz","Emanuele Bugliarello","Elizabeth Salesky","Miryam de Lhoneux","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2207.06991v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2207.06983v1","updated":"2022-07-14T15:06:37Z","published":"2022-07-14T15:06:37Z","title":"Multitrack Music Transformer: Learning Long-Term Dependencies in Music\n  with Diverse Instruments","summary":"  Existing approaches for generating multitrack music with transformer models\nhave been limited to either a small set of instruments or short music segments.\nThis is partly due to the memory requirements of the lengthy input sequences\nnecessitated by existing representations for multitrack music. In this work, we\npropose a compact representation that allows a diverse set of instruments while\nkeeping a short sequence length. Using our proposed representation, we present\nthe Multitrack Music Transformer (MTMT) for learning long-term dependencies in\nmultitrack music. In a subjective listening test, our proposed model achieves\ncompetitive quality on unconditioned generation against two baseline models. We\nalso show that our proposed model can generate samples that are twice as long\nas those produced by the baseline models, and, further, can do so in half the\ninference time. Moreover, we propose a new measure for analyzing musical\nself-attentions and show that the trained model learns to pay less attention to\nnotes that form a dissonant interval with the current note, yet attending more\nto notes that are 4N beats away from current. Finally, our findings provide a\nnovel foundation for future work exploring longer-form multitrack music\ngeneration and improving self-attentions for music. All source code and audio\nsamples can be found at https://salu133445.github.io/mtmt/ .\n","authors":["Hao-Wen Dong","Ke Chen","Shlomo Dubnov","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2207.06983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11670v2","updated":"2022-07-14T15:03:07Z","published":"2022-03-22T12:41:55Z","title":"Improving Meta-learning for Low-resource Text Classification and\n  Generation via Memory Imitation","summary":"  Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.\n","authors":["Yingxiu Zhao","Zhiliang Tian","Huaxiu Yao","Yinhe Zheng","Dongkyu Lee","Yiping Song","Jian Sun","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11670v2.pdf","comment":"ACL 2022 Camera Ready; modified emails"},{"id":"http://arxiv.org/abs/2207.03075v3","updated":"2022-07-14T15:02:09Z","published":"2022-07-07T04:07:48Z","title":"Towards the Practical Utility of Federated Learning in the Medical\n  Domain","summary":"  Federated learning (FL) is an active area of research. One of the most\nsuitable areas for adopting FL is the medical domain, where patient privacy\nmust be respected. Previous research, however, does not fully consider who will\nmost likely use FL in the medical domain. It is not the hospitals who are eager\nto adopt FL, but the service providers such as IT companies who want to develop\nmachine learning models with real patient records. Moreover, service providers\nwould prefer to focus on maximizing the performance of the models at the lowest\ncost possible. In this work, we propose empirical benchmarks of FL methods\nconsidering both performance and monetary cost with three real-world datasets:\nelectronic health records, skin cancer images, and electrocardiogram datasets.\nWe also propose Federated learning with Proximal regularization eXcept local\nNormalization (FedPxN), which, using a simple combination of FedProx and FedBN,\noutperforms all other FL algorithms while consuming only slightly more power\nthan the most power efficient method.\n","authors":["Seongjun Yang","Hyeonji Hwang","Daeyoung Kim","Radhika Dua","Jong-Yeup Kim","Eunho Yang","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2207.03075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02628v3","updated":"2022-07-14T15:02:04Z","published":"2022-02-05T20:08:58Z","title":"Improved Certified Defenses against Data Poisoning with (Deterministic)\n  Finite Aggregation","summary":"  Data poisoning attacks aim at manipulating model behaviors through distorting\ntraining data. Previously, an aggregation-based certified defense, Deep\nPartition Aggregation (DPA), was proposed to mitigate this threat. DPA predicts\nthrough an aggregation of base classifiers trained on disjoint subsets of data,\nthus restricting its sensitivity to dataset distortions. In this work, we\npropose an improved certified defense against general poisoning attacks, namely\nFinite Aggregation. In contrast to DPA, which directly splits the training set\ninto disjoint subsets, our method first splits the training set into smaller\ndisjoint subsets and then combines duplicates of them to build larger (but not\ndisjoint) subsets for training base classifiers. This reduces the worst-case\nimpacts of poison samples and thus improves certified robustness bounds. In\naddition, we offer an alternative view of our method, bridging the designs of\ndeterministic and stochastic aggregation-based certified defenses. Empirically,\nour proposed Finite Aggregation consistently improves certificates on MNIST,\nCIFAR-10, and GTSRB, boosting certified fractions by up to 3.05%, 3.87% and\n4.77%, respectively, while keeping the same clean accuracies as DPA's,\neffectively establishing a new state of the art in (pointwise) certified\nrobustness against data poisoning.\n","authors":["Wenxiao Wang","Alexander Levine","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2202.02628v3.pdf","comment":"International Conference on Machine Learning (ICML), 2022"},{"id":"http://arxiv.org/abs/2206.11190v2","updated":"2022-07-14T14:42:03Z","published":"2022-06-22T16:17:21Z","title":"Learning Optimal Treatment Strategies for Sepsis Using Offline\n  Reinforcement Learning in Continuous Space","summary":"  Sepsis is a leading cause of death in the ICU. It is a disease requiring\ncomplex interventions in a short period of time, but its optimal treatment\nstrategy remains uncertain. Evidence suggests that the practices of currently\nused treatment strategies are problematic and may cause harm to patients. To\naddress this decision problem, we propose a new medical decision model based on\nhistorical data to help clinicians recommend the best reference option for\nreal-time treatment. Our model combines offline reinforcement learning and deep\nreinforcement learning to solve the problem of traditional reinforcement\nlearning in the medical field due to the inability to interact with the\nenvironment, while enabling our model to make decisions in a continuous\nstate-action space. We demonstrate that, on average, the treatments recommended\nby the model are more valuable and reliable than those recommended by\nclinicians. In a large validation dataset, we find out that the patients whose\nactual doses from clinicians matched the decisions made by AI has the lowest\nmortality rates. Our model provides personalized and clinically interpretable\ntreatment decisions for sepsis to improve patient care.\n","authors":["Zeyu Wang","Huiying Zhao","Peng Ren","Yuxi Zhou","Ming Sheng"],"pdf_url":"https://arxiv.org/pdf/2206.11190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06960v1","updated":"2022-07-14T14:39:30Z","published":"2022-07-14T14:39:30Z","title":"Forming Trees with Treeformers","summary":"  Popular models such as Transformers and LSTMs use tokens as its unit of\ninformation. That is, each token is encoded into a vector representation, and\nthose vectors are used directly in a computation. However, humans frequently\nconsider spans of tokens (i.e., phrases) instead of their constituent tokens.\nIn this paper we introduce Treeformer, an architecture inspired by the CKY\nalgorithm and Transformer which learns a composition operator and pooling\nfunction in order to construct hierarchical encodings for phrases and\nsentences. Our extensive experiments demonstrate the benefits of incorporating\na hierarchical structure into the Transformer, and show significant\nimprovements compared to a baseline Transformer in machine translation,\nabstractive summarization, and various natural language understanding tasks.\n","authors":["Nilay Patel","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2207.06960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06959v1","updated":"2022-07-14T14:30:59Z","published":"2022-07-14T14:30:59Z","title":"Spatiotemporal Propagation Learning for Network-Wide Flight Delay\n  Prediction","summary":"  Demystifying the delay propagation mechanisms among multiple airports is\nfundamental to precise and interpretable delay prediction, which is crucial\nduring decision-making for all aviation industry stakeholders. The principal\nchallenge lies in effectively leveraging the spatiotemporal dependencies and\nexogenous factors related to the delay propagation. However, previous works\nonly consider limited spatiotemporal patterns with few factors. To promote more\ncomprehensive propagation modeling for delay prediction, we propose\nSpatioTemporal Propagation Network (STPN), a space-time separable graph\nconvolutional network, which is novel in spatiotemporal dependency capturing.\nFrom the aspect of spatial relation modeling, we propose a multi-graph\nconvolution model considering both geographic proximity and airline schedule.\nFrom the aspect of temporal dependency capturing, we propose a multi-head\nself-attentional mechanism that can be learned end-to-end and explicitly reason\nmultiple kinds of temporal dependency of delay time series. We show that the\njoint spatial and temporal learning models yield a sum of the Kronecker\nproduct, which factors the spatiotemporal dependence into the sum of several\nspatial and temporal adjacency matrices. By this means, STPN allows cross-talk\nof spatial and temporal factors for modeling delay propagation. Furthermore, a\nsqueeze and excitation module is added to each layer of STPN to boost\nmeaningful spatiotemporal features. To this end, we apply STPN to multi-step\nahead arrival and departure delay prediction in large-scale airport networks.\nTo validate the effectiveness of our model, we experiment with two real-world\ndelay datasets, including U.S and China flight delays; and we show that STPN\noutperforms state-of-the-art methods. In addition, counterfactuals produced by\nSTPN show that it learns explainable delay propagation patterns.\n","authors":["Yuankai Wu","Hongyu Yang","Yi Lin","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2207.06959v1.pdf","comment":"14 pages,8 figures"},{"id":"http://arxiv.org/abs/2207.06958v1","updated":"2022-07-14T14:30:34Z","published":"2022-07-14T14:30:34Z","title":"Proceedings of the ICML 2022 Expressive Vocalizations Workshop and\n  Competition: Recognizing, Generating, and Personalizing Vocal Bursts","summary":"  This is the Proceedings of the ICML Expressive Vocalization (ExVo)\nCompetition. The ExVo competition focuses on understanding and generating vocal\nbursts: laughs, gasps, cries, and other non-verbal vocalizations that are\ncentral to emotional expression and communication. ExVo 2022, included three\ncompetition tracks using a large-scale dataset of 59,201 vocalizations from\n1,702 speakers. The first, ExVo-MultiTask, requires participants to train a\nmulti-task model to recognize expressed emotions and demographic traits from\nvocal bursts. The second, ExVo-Generate, requires participants to train a\ngenerative model that produces vocal bursts conveying ten different emotions.\nThe third, ExVo-FewShot, requires participants to leverage few-shot learning\nincorporating speaker identity to train a model for the recognition of 10\nemotions conveyed by vocal bursts.\n","authors":["Alice Baird","Panagiotis Tzirakis","Gauthier Gidel","Marco Jiralerspong","Eilif B. Muller","Kory Mathewson","Björn Schuller","Erik Cambria","Dacher Keltner","Alan Cowen"],"pdf_url":"https://arxiv.org/pdf/2207.06958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06950v1","updated":"2022-07-14T14:23:14Z","published":"2022-07-14T14:23:14Z","title":"Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA\n  Models","summary":"  Low-order functional ANOVA (fANOVA) models have been rediscovered in the\nmachine learning (ML) community under the guise of inherently interpretable\nmachine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and\nGAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting\nfunctional main effects and second-order interactions. We propose a new\nalgorithm, called GAMI-Tree, that is similar to EBM, but has a number of\nfeatures that lead to better performance. It uses model-based trees as base\nlearners and incorporates a new interaction filtering method that is better at\ncapturing the underlying interactions. In addition, our iterative training\nmethod converges to a model with better predictive performance, and the\nembedded purification ensures that interactions are hierarchically orthogonal\nto main effects. The algorithm does not need extensive tuning, and our\nimplementation is fast and efficient. We use simulated and real datasets to\ncompare the performance and interpretability of GAMI-Tree with EBM and\nGAMI-Net.\n","authors":["Linwei Hu","Jie Chen","Vijayan N. Nair"],"pdf_url":"https://arxiv.org/pdf/2207.06950v1.pdf","comment":"25 pages plus appendix"},{"id":"http://arxiv.org/abs/2207.06949v1","updated":"2022-07-14T14:22:36Z","published":"2022-07-14T14:22:36Z","title":"Seeking the Truth Beyond the Data. An Unsupervised Machine Learning\n  Approach","summary":"  Clustering is an unsupervised machine learning methodology where unlabeled\nelements/objects are grouped together aiming to the construction of\nwell-established clusters that their elements are classified according to their\nsimilarity. The goal of this process is to provide a useful aid to the\nresearcher that will help her/him to identify patterns among the data. Dealing\nwith large databases, such patterns may not be easily detectable without the\ncontribution of a clustering algorithm. This article provides a deep\ndescription of the most widely used clustering methodologies accompanied by\nuseful presentations concerning suitable parameter selection and\ninitializations. Simultaneously, this article not only represents a review\nhighlighting the major elements of examined clustering techniques but\nemphasizes the comparison of these algorithms' clustering efficiency based on 3\ndatasets, revealing their existing weaknesses and capabilities through accuracy\nand complexity, during the confrontation of discrete and continuous\nobservations. The produced results help us extract valuable conclusions about\nthe appropriateness of the examined clustering techniques in accordance with\nthe dataset's size.\n","authors":["Dimitrios Saligkaras","Vasileios E. Papageorgiou"],"pdf_url":"https://arxiv.org/pdf/2207.06949v1.pdf","comment":"This paper has been accepted for publication in the proceedings of\n  the 3rd International Scientific Forum on Computer and Energy Sciences (WFCES\n  2022)"},{"id":"http://arxiv.org/abs/2207.06946v1","updated":"2022-07-14T14:11:34Z","published":"2022-07-14T14:11:34Z","title":"Insurgency as Complex Network: Image Co-Appearance and Hierarchy in the\n  PKK","summary":"  Despite a growing recognition of the importance of insurgent group structure\non conflict outcomes, there is very little empirical research thereon. Though\nthis problem is rooted in the inaccessibility of data on militant group\nstructure, insurgents frequently publish large volumes of image data on the\ninternet. In this paper, I develop a new methodology that leverages this\nabundant but underutilized source of data by automating the creation of a\nsocial network graph based on co-appearance in photographs using deep learning.\nUsing a trove of 19,115 obituary images published online by the PKK, a Kurdish\nmilitant group in Turkey, I demonstrate that an individual's centrality in the\nresulting co-appearance network is closely correlated with their rank in the\ninsurgent group.\n","authors":["Ollie Ballinger"],"pdf_url":"https://arxiv.org/pdf/2207.06946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06944v1","updated":"2022-07-14T14:08:59Z","published":"2022-07-14T14:08:59Z","title":"Differentially Private Graph Learning via Sensitivity-Bounded\n  Personalized PageRank","summary":"  Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of\ngraph representations such as node ranking, labeling, and graph embedding.\nHowever, while data privacy is one of the most important recent concerns,\nexisting PPR algorithms are not designed to protect user privacy. PPR is highly\nsensitive to the input graph edges: the difference of only one edge may cause a\nbig change in the PPR vector, potentially leaking private user data.\n  In this work, we propose an algorithm which outputs an approximate PPR and\nhas provably bounded sensitivity to input edges. In addition, we prove that our\nalgorithm achieves similar accuracy to non-private algorithms when the input\ngraph has large degrees. Our sensitivity-bounded PPR directly implies private\nalgorithms for several tools of graph learning, such as, differentially private\n(DP) PPR ranking, DP node classification, and DP node embedding. To complement\nour theoretical analysis, we also empirically verify the practical performances\nof our algorithms.\n","authors":["Alessandro Epasto","Vahab Mirrokni","Bryan Perozzi","Anton Tsitsulin","Peilin Zhong"],"pdf_url":"https://arxiv.org/pdf/2207.06944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06940v1","updated":"2022-07-14T14:06:15Z","published":"2022-07-14T14:06:15Z","title":"PASHA: Efficient HPO with Progressive Resource Allocation","summary":"  Hyperparameter optimization (HPO) and neural architecture search (NAS) are\nmethods of choice to obtain the best-in-class machine learning models, but in\npractice they can be costly to run. When models are trained on large datasets,\ntuning them with HPO or NAS rapidly becomes prohibitively expensive for\npractitioners, even when efficient multi-fidelity methods are employed. We\npropose an approach to tackle the challenge of tuning machine learning models\ntrained on large datasets with limited computational resources. Our approach,\nnamed PASHA, is able to dynamically allocate maximum resources for the tuning\nprocedure depending on the need. The experimental comparison shows that PASHA\nidentifies well-performing hyperparameter configurations and architectures\nwhile consuming significantly fewer computational resources than solutions like\nASHA.\n","authors":["Ondrej Bohdal","Lukas Balles","Beyza Ermis","Cédric Archambeau","Giovanni Zappella"],"pdf_url":"https://arxiv.org/pdf/2207.06940v1.pdf","comment":"Shorter version accepted at AutoML Conference 2022 Workshop Track"},{"id":"http://arxiv.org/abs/2207.06936v1","updated":"2022-07-14T13:59:26Z","published":"2022-07-14T13:59:26Z","title":"Multi-Level Branched Regularization for Federated Learning","summary":"  A critical challenge of federated learning is data heterogeneity and\nimbalance across clients, which leads to inconsistency between local networks\nand unstable convergence of global models. To alleviate the limitations, we\npropose a novel architectural regularization technique that constructs multiple\nauxiliary branches in each local model by grafting local and global subnetworks\nat several different levels and that learns the representations of the main\npathway in the local model congruent to the auxiliary hybrid pathways via\nonline knowledge distillation. The proposed technique is effective to robustify\nthe global model even in the non-iid setting and is applicable to various\nfederated learning frameworks conveniently without incurring extra\ncommunication costs. We perform comprehensive empirical studies and demonstrate\nremarkable performance gains in terms of accuracy and efficiency compared to\nexisting methods. The source code is available at our project page.\n","authors":["Jinkyu Kim","Geeho Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2207.06936v1.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2003.07939v5","updated":"2022-07-14T13:26:47Z","published":"2020-03-17T21:01:17Z","title":"Neural Networks for Encoding Dynamic Security-Constrained Optimal Power\n  Flow","summary":"  This paper introduces a framework to capture previously intractable\noptimization constraints and transform them to a mixed-integer linear program,\nthrough the use of neural networks. We encode the feasible space of\noptimization problems characterized by both tractable and intractable\nconstraints, e.g. differential equations, to a neural network. Leveraging an\nexact mixed-integer reformulation of neural networks, we solve mixed-integer\nlinear programs that accurately approximate solutions to the originally\nintractable non-linear optimization problem. We apply our methods to the AC\noptimal power flow problem (AC-OPF), where directly including dynamic security\nconstraints renders the AC-OPF intractable. Our proposed approach has the\npotential to be significantly more scalable than traditional approaches. We\ndemonstrate our approach for power system operation considering N-1 security\nand small-signal stability, showing how it can efficiently obtain cost-optimal\nsolutions which at the same time satisfy both static and dynamic security\nconstraints.\n","authors":["Ilgiz Murzakhanov","Andreas Venzke","George S. Misyris","Spyros Chatzivasileiadis"],"pdf_url":"https://arxiv.org/pdf/2003.07939v5.pdf","comment":"In proceedings of the 11th Bulk Power Systems Dynamics and Control\n  Symposium (IREP 2022), July 25-30, 2022, Banff, Canada"},{"id":"http://arxiv.org/abs/2207.06888v1","updated":"2022-07-14T13:07:08Z","published":"2022-07-14T13:07:08Z","title":"Distance Learner: Incorporating Manifold Prior to Model Training","summary":"  The manifold hypothesis (real world data concentrates near low-dimensional\nmanifolds) is suggested as the principle behind the effectiveness of machine\nlearning algorithms in very high dimensional problems that are common in\ndomains such as vision and speech. Multiple methods have been proposed to\nexplicitly incorporate the manifold hypothesis as a prior in modern Deep Neural\nNetworks (DNNs), with varying success. In this paper, we propose a new method,\nDistance Learner, to incorporate this prior for DNN-based classifiers. Distance\nLearner is trained to predict the distance of a point from the underlying\nmanifold of each class, rather than the class label. For classification,\nDistance Learner then chooses the class corresponding to the closest predicted\nclass manifold. Distance Learner can also identify points as being out of\ndistribution (belonging to neither class), if the distance to the closest\nmanifold is higher than a threshold. We evaluate our method on multiple\nsynthetic datasets and show that Distance Learner learns much more meaningful\nclassification boundaries compared to a standard classifier. We also evaluate\nour method on the task of adversarial robustness, and find that it not only\noutperforms standard classifier by a large margin, but also performs at par\nwith classifiers trained via state-of-the-art adversarial training.\n","authors":["Aditya Chetan","Nipun Kwatra"],"pdf_url":"https://arxiv.org/pdf/2207.06888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06882v1","updated":"2022-07-14T13:00:41Z","published":"2022-07-14T13:00:41Z","title":"Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically\n  Ambiguous Settings for Low Resource Languages","summary":"  We leverage pre-trained language models to solve the task of complex NER for\ntwo low-resource languages: Chinese and Spanish. We use the technique of Whole\nWord Masking(WWM) to boost the performance of masked language modeling\nobjective on large and unsupervised corpora. We experiment with multiple neural\nnetwork architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on\ntop of a fine-tuned BERT layer. All our models outperform the baseline by a\nsignificant margin and our best performing model obtains a competitive position\non the evaluation leaderboard for the blind test set.\n","authors":["Amit Pandey","Swayatta Daw","Narendra Babu Unnam","Vikram Pudi"],"pdf_url":"https://arxiv.org/pdf/2207.06882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06881v1","updated":"2022-07-14T13:00:22Z","published":"2022-07-14T13:00:22Z","title":"Recurrent Memory Transformer","summary":"  Transformer-based models show their effectiveness across multiple domains and\ntasks. The self-attention allows to combine information from all sequence\nelements into context-aware representations. However, global and local\ninformation has to be stored mostly in the same element-wise representations.\nMoreover, the length of an input sequence is limited by quadratic computational\ncomplexity of self-attention.\n  In this work, we propose and study a memory-augmented segment-level recurrent\nTransformer (Recurrent Memory Transformer). Memory allows to store and process\nlocal and global information as well as to pass information between segments of\nthe long sequence with the help of recurrence. We implement a memory mechanism\nwith no changes to Transformer model by adding special memory tokens to the\ninput or output sequence. Then Transformer is trained to control both memory\noperations and sequence representations processing.\n  Results of experiments show that our model performs on par with the\nTransformer-XL on language modeling for smaller memory sizes and outperforms it\nfor tasks that require longer sequence processing. We show that adding memory\ntokens to Tr-XL is able to improve it performance. This makes Recurrent Memory\nTransformer a promising architecture for applications that require learning of\nlong-term dependencies and general purpose in memory processing, such as\nalgorithmic tasks and reasoning.\n","authors":["Aydar Bulatov","Yuri Kuratov","Mikhail S. Burtsev"],"pdf_url":"https://arxiv.org/pdf/2207.06881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13393v2","updated":"2022-07-14T12:58:05Z","published":"2022-06-20T11:38:55Z","title":"Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing\n  Framework for Alzheimer's Disease","summary":"  Cross-modal fusion of different types of neuroimaging data has shown great\npromise for predicting the progression of Alzheimer's Disease(AD). However,\nmost existing methods applied in neuroimaging can not efficiently fuse the\nfunctional and structural information from multi-modal neuroimages. In this\nwork, a novel cross-modal transformer generative adversarial network(CT-GAN) is\nproposed to fuse functional information contained in resting-state functional\nmagnetic resonance imaging (rs-fMRI) and structural information contained in\nDiffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match\nfunctional information to structural information efficiently and maximize the\ncapability of extracting complementary information from rs-fMRI and DTI. By\ncapturing the deep complementary information between structural features and\nfunctional features, the proposed CT-GAN can detect the AD-related brain\nconnectivity, which could be used as a bio-marker of AD. Experimental results\nshow that the proposed model can not only improve classification performance\nbut also detect the AD-related brain connectivity effectively.\n","authors":["Junren Pan","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2206.13393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.06197v3","updated":"2022-07-14T12:52:50Z","published":"2021-07-25T09:10:10Z","title":"A comparison of latent semantic analysis and correspondence analysis of\n  document-term matrices","summary":"  Latent semantic analysis (LSA) and correspondence analysis (CA) are two\ntechniques that use a singular value decomposition (SVD) for dimensionality\nreduction. LSA has been extensively used to obtain low-dimensional\nrepresentations that capture relationships among documents and terms. In this\narticle, we present a theoretical analysis and comparison of the two techniques\nin the context of document-term matrices. We show that CA has some attractive\nproperties as compared to LSA, for instance that effects of margins arising\nfrom differing document-lengths and term-frequencies are effectively\neliminated, so that the CA solution is optimally suited to focus on\nrelationships among documents and terms. A unifying framework is proposed that\nincludes both CA and LSA as special cases. We empirically compare CA to various\nLSA based methods on text categorization in English and authorship attribution\non historical Dutch texts, and find that CA performs significantly better. We\nalso apply CA to a long-standing question regarding the authorship of the Dutch\nnational anthem Wilhelmus and provide further support that it can be attributed\nto the author Datheen, amongst several contenders.\n","authors":["Qianqian Qi","David J. Hessen","Tejaswini Deoskar","Peter G. M. van der Heijden"],"pdf_url":"https://arxiv.org/pdf/2108.06197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06858v1","updated":"2022-07-14T12:22:19Z","published":"2022-07-14T12:22:19Z","title":"RSD-GAN: Regularized Sobolev Defense GAN Against Speech-to-Text\n  Adversarial Attacks","summary":"  This paper introduces a new synthesis-based defense algorithm for\ncounteracting with a varieties of adversarial attacks developed for challenging\nthe performance of the cutting-edge speech-to-text transcription systems. Our\nalgorithm implements a Sobolev-based GAN and proposes a novel regularizer for\neffectively controlling over the functionality of the entire generative model,\nparticularly the discriminator network during training. Our achieved results\nupon carrying out numerous experiments on the victim DeepSpeech, Kaldi, and\nLingvo speech transcription systems corroborate the remarkable performance of\nour defense approach against a comprehensive range of targeted and non-targeted\nadversarial attacks.\n","authors":["Mohammad Esmaeilpour","Nourhene Chaalia","Patrick Cardinal"],"pdf_url":"https://arxiv.org/pdf/2207.06858v1.pdf","comment":"Paper submitted to IEEE Signal Processing Letters Journal"},{"id":"http://arxiv.org/abs/2205.13274v2","updated":"2022-07-14T12:20:57Z","published":"2022-05-26T11:18:09Z","title":"Evaluating Multimodal Interactive Agents","summary":"  Creating agents that can interact naturally with humans is a common goal in\nartificial intelligence (AI) research. However, evaluating these interactions\nis challenging: collecting online human-agent interactions is slow and\nexpensive, yet faster proxy metrics often do not correlate well with\ninteractive evaluation. In this paper, we assess the merits of these existing\nevaluation metrics and present a novel approach to evaluation called the\nStandardised Test Suite (STS). The STS uses behavioural scenarios mined from\nreal human interaction data. Agents see replayed scenario context, receive an\ninstruction, and are then given control to complete the interaction offline.\nThese agent continuations are recorded and sent to human annotators to mark as\nsuccess or failure, and agents are ranked according to the proportion of\ncontinuations in which they succeed. The resulting STS is fast, controlled,\ninterpretable, and representative of naturalistic interactions. Altogether, the\nSTS consolidates much of what is desirable across many of our standard\nevaluation metrics, allowing us to accelerate research progress towards\nproducing agents that can interact naturally with humans. A video may be found\nat https://youtu.be/YR1TngGORGQ.\n","authors":["Josh Abramson","Arun Ahuja","Federico Carnevale","Petko Georgiev","Alex Goldin","Alden Hung","Jessica Landon","Timothy Lillicrap","Alistair Muldal","Blake Richards","Adam Santoro","Tamara von Glehn","Greg Wayne","Nathaniel Wong","Chen Yan"],"pdf_url":"https://arxiv.org/pdf/2205.13274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06856v1","updated":"2022-07-14T12:20:46Z","published":"2022-07-14T12:20:46Z","title":"Low-Precision Arithmetic for Fast Gaussian Processes","summary":"  Low-precision arithmetic has had a transformative effect on the training of\nneural networks, reducing computation, memory and energy requirements. However,\ndespite its promise, low-precision arithmetic has received little attention for\nGaussian processes (GPs), largely because GPs require sophisticated linear\nalgebra routines that are unstable in low-precision. We study the different\nfailure modes that can occur when training GPs in half precision. To circumvent\nthese failure modes, we propose a multi-faceted approach involving conjugate\ngradients with re-orthogonalization, mixed precision, and preconditioning. Our\napproach significantly improves the numerical stability and practical\nperformance of conjugate gradients in low-precision over a wide range of\nsettings, enabling GPs to train on $1.8$ million data points in $10$ hours on a\nsingle GPU, without any sparse approximations.\n","authors":["Wesley J. Maddox","Andres Potapczynski","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2207.06856v1.pdf","comment":"UAI 2022. Code available at https://github.com/AndPotap/halfpres_gps"},{"id":"http://arxiv.org/abs/2204.12835v4","updated":"2022-07-14T12:11:46Z","published":"2022-04-27T10:39:52Z","title":"Learning to Parallelize in a Shared-Memory Environment with Transformers","summary":"  In past years, the world has switched to many-core and multi-core shared\nmemory architectures. As a result, there is a growing need to utilize these\narchitectures by introducing shared memory parallelization schemes to software\napplications. OpenMP is the most comprehensive API that implements such\nschemes, characterized by a readable interface. Nevertheless, introducing\nOpenMP into code is challenging due to pervasive pitfalls in management of\nparallel shared memory. To facilitate the performance of this task, many\nsource-to-source (S2S) compilers have been created over the years, tasked with\ninserting OpenMP directives into code automatically. In addition to having\nlimited robustness to their input format, these compilers still do not achieve\nsatisfactory coverage and precision in locating parallelizable code and\ngenerating appropriate directives. In this work, we propose leveraging recent\nadvances in ML techniques, specifically in natural language processing (NLP),\nto replace S2S compilers altogether. We create a database (corpus), Open-OMP,\nspecifically for this goal. Open-OMP contains over 28,000 code snippets, half\nof which contain OpenMP directives while the other half do not need\nparallelization at all with high probability. We use the corpus to train\nsystems to automatically classify code segments in need of parallelization, as\nwell as suggest individual OpenMP clauses. We train several transformer models,\nnamed PragFormer, for these tasks, and show that they outperform\nstatistically-trained baselines and automatic S2S parallelization compilers in\nboth classifying the overall need for an OpenMP directive and the introduction\nof private and reduction clauses.\n  Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.\n","authors":["Re'em Harel","Yuval Pinter","Gal Oren"],"pdf_url":"https://arxiv.org/pdf/2204.12835v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16223v2","updated":"2022-07-14T11:55:38Z","published":"2022-03-30T11:57:16Z","title":"Hypergraphon Mean Field Games","summary":"  We propose an approach to modelling large-scale multi-agent dynamical systems\nallowing interactions among more than just pairs of agents using the theory of\nmean-field games and the notion of hypergraphons, which are obtained as limits\nof large hypergraphs. To the best of our knowledge, ours is the first work on\nmean field games on hypergraphs. Together with an extension to a multi-layer\nsetup, we obtain limiting descriptions for large systems of non-linear,\nweakly-interacting dynamical agents. On the theoretical side, we prove the\nwell-foundedness of the resulting hypergraphon mean field game, showing both\nexistence and approximate Nash properties. On the applied side, we extend\nnumerical and learning algorithms to compute the hypergraphon mean field\nequilibria. To verify our approach empirically, we consider an epidemic control\nproblem and a social rumor spreading model, where we give agents intrinsic\nmotivation to spread rumors to unaware agents.\n","authors":["Kai Cui","Wasiur R. KhudaBukhsh","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2203.16223v2.pdf","comment":"The following article has been submitted to Chaos. v2: Full-length\n  references"},{"id":"http://arxiv.org/abs/2207.06841v1","updated":"2022-07-14T11:54:58Z","published":"2022-07-14T11:54:58Z","title":"Deep Dictionary Learning with An Intra-class Constraint","summary":"  In recent years, deep dictionary learning (DDL)has attracted a great amount\nof attention due to its effectiveness for representation learning and visual\nrecognition.~However, most existing methods focus on unsupervised deep\ndictionary learning, failing to further explore the category information.~To\nmake full use of the category information of different samples, we propose a\nnovel deep dictionary learning model with an intra-class constraint (DDLIC) for\nvisual classification. Specifically, we design the intra-class compactness\nconstraint on the intermediate representation at different levels to encourage\nthe intra-class representations to be closer to each other, and eventually the\nlearned representation becomes more discriminative.~Unlike the traditional DDL\nmethods, during the classification stage, our DDLIC performs a layer-wise\ngreedy optimization in a similar way to the training stage. Experimental\nresults on four image datasets show that our method is superior to the\nstate-of-the-art methods.\n","authors":["Xia Yuan","Jianping Gou","Baosheng Yu","Jiali Yu","Zhang Yi"],"pdf_url":"https://arxiv.org/pdf/2207.06841v1.pdf","comment":"6 pages, 3 figures, 2 tables. It has been accepted in ICME2022"},{"id":"http://arxiv.org/abs/2201.03560v2","updated":"2022-07-14T11:54:22Z","published":"2022-01-10T16:14:27Z","title":"Iterative training of robust k-space interpolation networks for improved\n  image reconstruction with limited scan specific training samples","summary":"  Purpose: To evaluate an iterative learning approach for enhanced performance\nof Robust Artificial-neural-networks for K-space Interpolation (RAKI), when\nonly a limited amount of training data (auto-calibration signals, ACS) are\navailable for accelerated standard 2D imaging. Methods: In a first step, the\nRAKI model was optimized for the case of strongly limited training data amount.\nIn the iterative learning approach (termed iterative RAKI), the optimized RAKI\nmodel is initially trained using original and augmented ACS obtained from a\nlinear parallel imaging reconstruction. Subsequently, the RAKI convolution\nfilters are refined iteratively using original and augmented ACS extracted from\nthe previous RAKI reconstruction. Evaluation was carried out on 200\nretrospectively undersampled in-vivo datasets from the fastMRI neuro database\nwith different contrast settings. Results: For limited training data (18 and 22\nACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard\nRAKI by reducing residual artefacts and yields strong noise suppression when\ncompared to standard parallel imaging, underlined by quantitative\nreconstruction quality metrics. In combination with a phase constraint, further\nreconstruction improvements can be achieved. Additionally, iterative RAKI shows\nbetter performance than both GRAPPA and RAKI in case of pre-scan calibration\nwith varying contrast between training- and undersampled data. Conclusion: The\niterative learning approach with RAKI benefits from standard RAKIs well known\nnoise suppression feature but requires less original training data for the\naccurate reconstruction of standard 2D images thereby improving net\nacceleration.\n","authors":["Peter Dawood","Felix Breuer","Paul R. Burd","István Homolya","Johannes Oberberger","Peter M. Jakob","Martin Blaimer"],"pdf_url":"https://arxiv.org/pdf/2201.03560v2.pdf","comment":"Submitted to Magnetic Resonance in Medicine"},{"id":"http://arxiv.org/abs/2207.06835v1","updated":"2022-07-14T11:45:43Z","published":"2022-07-14T11:45:43Z","title":"Instance Selection Mechanisms for Human-in-the-Loop Systems in Few-Shot\n  Learning","summary":"  Business analytics and machine learning have become essential success factors\nfor various industries - with the downside of cost-intensive gathering and\nlabeling of data. Few-shot learning addresses this challenge and reduces data\ngathering and labeling costs by learning novel classes with very few labeled\ndata. In this paper, we design a human-in-the-loop (HITL) system for few-shot\nlearning and analyze an extensive range of mechanisms that can be used to\nacquire human expert knowledge for instances that have an uncertain prediction\noutcome. We show that the acquisition of human expert knowledge significantly\naccelerates the few-shot model performance given a negligible labeling effort.\nWe validate our findings in various experiments on a benchmark dataset in\ncomputer vision and real-world datasets. We further demonstrate the\ncost-effectiveness of HITL systems for few-shot learning. Overall, our work\naims at supporting researchers and practitioners in effectively adapting\nmachine learning models to novel classes at reduced costs.\n","authors":["Johannes Jakubik","Benedikt Blumenstiel","Michael Vössing","Patrick Hemmer"],"pdf_url":"https://arxiv.org/pdf/2207.06835v1.pdf","comment":"International Conference on Wirtschaftsinformatik, 14 pages"},{"id":"http://arxiv.org/abs/2207.06828v1","updated":"2022-07-14T11:32:42Z","published":"2022-07-14T11:32:42Z","title":"Pose-based Tremor Classification for Parkinson's Disease Diagnosis from\n  Video","summary":"  Parkinson's disease (PD) is a progressive neurodegenerative disorder that\nresults in a variety of motor dysfunction symptoms, including tremors,\nbradykinesia, rigidity and postural instability. The diagnosis of PD mainly\nrelies on clinical experience rather than a definite medical test, and the\ndiagnostic accuracy is only about 73-84% since it is challenged by the\nsubjective opinions or experiences of different medical experts. Therefore, an\nefficient and interpretable automatic PD diagnosis system is valuable for\nsupporting clinicians with more robust diagnostic decision-making. To this end,\nwe propose to classify Parkinson's tremor since it is one of the most\npredominant symptoms of PD with strong generalizability. Different from other\ncomputer-aided time and resource-consuming Parkinson's Tremor (PT)\nclassification systems that rely on wearable sensors, we propose SPAPNet, which\nonly requires consumer-grade non-intrusive video recording of camera-facing\nhuman movements as input to provide undiagnosed patients with low-cost PT\nclassification results as a PD warning sign. For the first time, we propose to\nuse a novel attention module with a lightweight pyramidal\nchannel-squeezing-fusion architecture to extract relevant PT information and\nfilter the noise efficiently. This design aids in improving both classification\nperformance and system interpretability. Experimental results show that our\nsystem outperforms state-of-the-arts by achieving a balanced accuracy of 90.9%\nand an F1-score of 90.6% in classifying PT with the non-PT class.\n","authors":["Haozheng Zhang","Edmond S. L. Ho","Xiatian Zhang","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2207.06828v1.pdf","comment":"MICCAI 2022"},{"id":"http://arxiv.org/abs/2206.00380v2","updated":"2022-07-14T11:28:50Z","published":"2022-06-01T10:30:59Z","title":"Strongly Augmented Contrastive Clustering","summary":"  Deep clustering has attracted increasing attention in recent years due to its\ncapability of joint representation learning and clustering via deep neural\nnetworks. In its latest developments, the contrastive learning has emerged as\nan effective technique to substantially enhance the deep clustering\nperformance. However, the existing contrastive learning based deep clustering\nalgorithms mostly focus on some carefully-designed augmentations (often with\nlimited transformations to preserve the structure), referred to as weak\naugmentations, but cannot go beyond the weak augmentations to explore the more\nopportunities in stronger augmentations (with more aggressive transformations\nor even severe distortions). In this paper, we present an end-to-end deep\nclustering approach termed Strongly Augmented Contrastive Clustering (SACC),\nwhich extends the conventional two-augmentation-view paradigm to multiple views\nand jointly leverages strong and weak augmentations for strengthened deep\nclustering. Particularly, we utilize a backbone network with triply-shared\nweights, where a strongly augmented view and two weakly augmented views are\nincorporated. Based on the representations produced by the backbone, the\nweak-weak view pair and the strong-weak view pairs are simultaneously exploited\nfor the instance-level contrastive learning (via an instance projector) and the\ncluster-level contrastive learning (via a cluster projector), which, together\nwith the backbone, can be jointly optimized in a purely unsupervised manner.\nExperimental results on five challenging image datasets have shown the\nsuperiority of our SACC approach over the state-of-the-art. The code is\navailable at https://github.com/dengxiaozhi/SACC.\n","authors":["Xiaozhi Deng","Dong Huang","Ding-Hua Chen","Chang-Dong Wang","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2206.00380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06819v1","updated":"2022-07-14T10:59:39Z","published":"2022-07-14T10:59:39Z","title":"Anomal-E: A Self-Supervised Network Intrusion Detection System based on\n  Graph Neural Networks","summary":"  This paper investigates Graph Neural Networks (GNNs) application for\nself-supervised network intrusion and anomaly detection. GNNs are a deep\nlearning approach for graph-based data that incorporate graph structures into\nlearning to generalise graph representations and output embeddings. As network\nflows are naturally graph-based, GNNs are a suitable fit for analysing and\nlearning network behaviour. The majority of current implementations of\nGNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled\nnetwork traffic which can not only restrict the amount and structure of input\ntraffic, but also the NIDSs potential to adapt to unseen attacks. To overcome\nthese restrictions, we present Anomal-E, a GNN approach to intrusion and\nanomaly detection that leverages edge features and graph topological structure\nin a self-supervised process. This approach is, to the best our knowledge, the\nfirst successful and practical approach to network intrusion detection that\nutilises network flows in a self-supervised, edge leveraging GNN. Experimental\nresults on two modern benchmark NIDS datasets not only clearly display the\nimprovement of using Anomal-E embeddings rather than raw features, but also the\npotential Anomal-E has for detection on wild network traffic.\n","authors":["Evan Caville","Wai Weng Lo","Siamak Layeghy","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2207.06819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10696v2","updated":"2022-07-14T10:43:45Z","published":"2022-06-21T19:31:25Z","title":"Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting\n  Epidemics","summary":"  Infectious diseases remain among the top contributors to human illness and\ndeath worldwide, among which many diseases produce epidemic waves of infection.\nThe unavailability of specific drugs and ready-to-use vaccines to prevent most\nof these epidemics makes the situation worse. These force public health\nofficials and policymakers to rely on early warning systems generated by\nreliable and accurate forecasts of epidemics. Accurate forecasts of epidemics\ncan assist stakeholders in tailoring countermeasures, such as vaccination\ncampaigns, staff scheduling, and resource allocation, to the situation at hand,\nwhich could translate to reductions in the impact of a disease. Unfortunately,\nmost of these past epidemics exhibit nonlinear and non-stationary\ncharacteristics due to their spreading fluctuations based on seasonal-dependent\nvariability and the nature of these epidemics. We analyse a wide variety of\nepidemic time series datasets using a maximal overlap discrete wavelet\ntransform (MODWT) based autoregressive neural network and call it EWNet model.\nMODWT techniques effectively characterize non-stationary behavior and seasonal\ndependencies in the epidemic time series and improve the nonlinear forecasting\nscheme of the autoregressive neural network in the proposed ensemble wavelet\nnetwork framework. From a nonlinear time series viewpoint, we explore the\nasymptotic stationarity of the proposed EWNet model to show the asymptotic\nbehavior of the associated Markov Chain. We also theoretically investigate the\neffect of learning stability and the choice of hidden neurons in the proposal.\nFrom a practical perspective, we compare our proposed EWNet framework with\nseveral statistical, machine learning, and deep learning models. Experimental\nresults show that the proposed EWNet is highly competitive compared to the\nstate-of-the-art epidemic forecasting methods.\n","authors":["Madhurima Panja","Tanujit Chakraborty","Uttam Kumar","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2206.10696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06812v1","updated":"2022-07-14T10:39:02Z","published":"2022-07-14T10:39:02Z","title":"Comparing the latent space of generative models","summary":"  Different encodings of datapoints in the latent space of latent-vector\ngenerative models may result in more or less effective and disentangled\ncharacterizations of the different explanatory factors of variation behind the\ndata. Many works have been recently devoted to the explorationof the latent\nspace of specific models, mostly focused on the study of how features are\ndisentangled and of how trajectories producing desired alterations of data in\nthe visible space can be found. In this work we address the more general\nproblem of comparing the latent spaces of different models, looking for\ntransformations between them. We confined the investigation to the familiar and\nlargely investigated case of generative models for the data manifold of human\nfaces. The surprising, preliminary result reported in this article is that\n(provided models have not been taught or explicitly conceived to act\ndifferently) a simple linear mapping is enough to pass from a latent space to\nanother while preserving most of the information.\n","authors":["Andrea Asperti","Valerio Tonelli"],"pdf_url":"https://arxiv.org/pdf/2207.06812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06810v1","updated":"2022-07-14T10:38:21Z","published":"2022-07-14T10:38:21Z","title":"In-memory Realization of In-situ Few-shot Continual Learning with a\n  Dynamically Evolving Explicit Memory","summary":"  Continually learning new classes from a few training examples without\nforgetting previous old classes demands a flexible architecture with an\ninevitably growing portion of storage, in which new examples and classes can be\nincrementally stored and efficiently retrieved. One viable architectural\nsolution is to tightly couple a stationary deep neural network to a dynamically\nevolving explicit memory (EM). As the centerpiece of this architecture, we\npropose an EM unit that leverages energy-efficient in-memory compute (IMC)\ncores during the course of continual learning operations. We demonstrate for\nthe first time how the EM unit can physically superpose multiple training\nexamples, expand to accommodate unseen classes, and perform similarity search\nduring inference, using operations on an IMC core based on phase-change memory\n(PCM). Specifically, the physical superposition of a few encoded training\nexamples is realized via in-situ progressive crystallization of PCM devices.\nThe classification accuracy achieved on the IMC core remains within a range of\n1.28%--2.5% compared to that of the state-of-the-art full-precision baseline\nsoftware model on both the CIFAR-100 and miniImageNet datasets when continually\nlearning 40 novel classes (from only five examples per class) on top of 60 old\nclasses.\n","authors":["Geethan Karunaratne","Michael Hersche","Jovin Langenegger","Giovanni Cherubini","Manuel Le Gallo-Bourdeau","Urs Egger","Kevin Brew","Sam Choi","INJO OK","Mary Claire Silvestre","Ning Li","Nicole Saulnier","Victor Chan","Ishtiaq Ahsan","Vijay Narayanan","Luca Benini","Abu Sebastian","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2207.06810v1.pdf","comment":"Accepted at the European Solid-state Devices and Circuits Conference\n  (ESSDERC), September 2022"},{"id":"http://arxiv.org/abs/2204.13784v3","updated":"2022-07-14T10:30:07Z","published":"2022-04-28T21:15:59Z","title":"AGIC: Approximate Gradient Inversion Attack on Federated Learning","summary":"  Federated learning is a private-by-design distributed learning paradigm where\nclients train local models on their own data before a central server aggregates\ntheir local updates to compute a global model. Depending on the aggregation\nmethod used, the local updates are either the gradients or the weights of local\nlearning models. Recent reconstruction attacks apply a gradient inversion\noptimization on the gradient update of a single minibatch to reconstruct the\nprivate data used by clients during training. As the state-of-the-art\nreconstruction attacks solely focus on single update, realistic adversarial\nscenarios are overlooked, such as observation across multiple updates and\nupdates trained from multiple mini-batches. A few studies consider a more\nchallenging adversarial scenario where only model updates based on multiple\nmini-batches are observable, and resort to computationally expensive simulation\nto untangle the underlying samples for each local step. In this paper, we\npropose AGIC, a novel Approximate Gradient Inversion Attack that efficiently\nand effectively reconstructs images from both model or gradient updates, and\nacross multiple epochs. In a nutshell, AGIC (i) approximates gradient updates\nof used training samples from model updates to avoid costly simulation\nprocedures, (ii) leverages gradient/model updates collected from multiple\nepochs, and (iii) assigns increasing weights to layers with respect to the\nneural network structure for reconstruction quality. We extensively evaluate\nAGIC on three datasets, CIFAR-10, CIFAR-100 and ImageNet. Our results show that\nAGIC increases the peak signal-to-noise ratio (PSNR) by up to 50% compared to\ntwo representative state-of-the-art gradient inversion attacks. Furthermore,\nAGIC is faster than the state-of-the-art simulation based attack, e.g., it is\n5x faster when attacking FedAvg with 8 local steps in between model updates.\n","authors":["Jin Xu","Chi Hong","Jiyue Huang","Lydia Y. Chen","Jérémie Decouchant"],"pdf_url":"https://arxiv.org/pdf/2204.13784v3.pdf","comment":"This paper is accepted at the 41st International Symposium on\n  Reliable Distributed Systems (SRDS 2022)"},{"id":"http://arxiv.org/abs/2204.13437v2","updated":"2022-07-14T10:29:40Z","published":"2022-04-28T12:08:53Z","title":"Regotron: Regularizing the Tacotron2 architecture via monotonic\n  alignment loss","summary":"  Recent deep learning Text-to-Speech (TTS) systems have achieved impressive\nperformance by generating speech close to human parity. However, they suffer\nfrom training stability issues as well as incorrect alignment of the\nintermediate acoustic representation with the input text sequence. In this\nwork, we introduce Regotron, a regularized version of Tacotron2 which aims to\nalleviate the training issues and at the same time produce monotonic\nalignments. Our method augments the vanilla Tacotron2 objective function with\nan additional term, which penalizes non-monotonic alignments in the\nlocation-sensitive attention mechanism. By properly adjusting this\nregularization term we show that the loss curves become smoother, and at the\nsame time Regotron consistently produces monotonic alignments in unseen\nexamples even at an early stage (13\\% of the total number of epochs) of its\ntraining process, whereas the fully converged Tacotron2 fails to do so.\nMoreover, our proposed regularization method has no additional computational\noverhead, while reducing common TTS mistakes and achieving slighlty improved\nspeech naturalness according to subjective mean opinion scores (MOS) collected\nfrom 50 evaluators.\n","authors":["Efthymios Georgiou","Kosmas Kritsis","Georgios Paraskevopoulos","Athanasios Katsamanis","Vassilis Katsouros","Alexandros Potamianos"],"pdf_url":"https://arxiv.org/pdf/2204.13437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06802v1","updated":"2022-07-14T10:27:25Z","published":"2022-07-14T10:27:25Z","title":"GrabQC: Graph based Query Contextualization for automated ICD coding","summary":"  Automated medical coding is a process of codifying clinical notes to\nappropriate diagnosis and procedure codes automatically from the standard\ntaxonomies such as ICD (International Classification of Diseases) and CPT\n(Current Procedure Terminology). The manual coding process involves the\nidentification of entities from the clinical notes followed by querying a\ncommercial or non-commercial medical codes Information Retrieval (IR) system\nthat follows the Centre for Medicare and Medicaid Services (CMS) guidelines. We\npropose to automate this manual process by automatically constructing a query\nfor the IR system using the entities auto-extracted from the clinical notes. We\npropose \\textbf{GrabQC}, a \\textbf{Gra}ph \\textbf{b}ased \\textbf{Q}uery\n\\textbf{C}ontextualization method that automatically extracts queries from the\nclinical text, contextualizes the queries using a Graph Neural Network (GNN)\nmodel and obtains the ICD Codes using an external IR system. We also propose a\nmethod for labelling the dataset for training the model. We perform experiments\non two datasets of clinical text in three different setups to assert the\neffectiveness of our approach. The experimental results show that our proposed\nmethod is better than the compared baselines in all three settings.\n","authors":["Jeshuren Chelladurai","Sudarsun Santhiappan","Balaraman Ravindran"],"pdf_url":"https://arxiv.org/pdf/2207.06802v1.pdf","comment":"25th Pacific-Asia Conference on Knowledge Discovery and Data Mining\n  (PAKDD 2021)"},{"id":"http://arxiv.org/abs/1908.07220v3","updated":"2022-07-14T09:45:36Z","published":"2019-08-20T08:36:14Z","title":"A Bayesian Lasso based Sparse Learning Model","summary":"  The Bayesian Lasso is constructed in the linear regression framework and\napplies the Gibbs sampling to estimate the regression parameters. This paper\ndevelops a new sparse learning model, named the Bayesian Lasso Sparse (BLS)\nmodel, that takes the hierarchical model formulation of the Bayesian Lasso. The\nmain difference from the original Bayesian Lasso lies in the estimation\nprocedure; the BLS method uses a learning algorithm based on the type-II\nmaximum likelihood procedure. Opposed to the Bayesian Lasso, the BLS provides\nsparse estimates of the regression parameters. The BLS method is also derived\nfor nonlinear supervised learning problems by introducing kernel functions. We\ncompare the BLS model to the well known Relevance Vector Machine, the Fast\nLaplace method, the Byesian Lasso, and the Lasso, on both simulated and real\ndata. The numerical results show that the BLS is sparse and precise, especially\nwhen dealing with noisy and irregular dataset.\n","authors":["Ingvild M. Helgøy","Yushu Li"],"pdf_url":"https://arxiv.org/pdf/1908.07220v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2201.03349v4","updated":"2022-07-14T09:41:17Z","published":"2022-01-10T14:05:02Z","title":"A Unified Granular-ball Learning Model of Pawlak Rough Set and\n  Neighborhood Rough Set","summary":"  Pawlak rough set and neighborhood rough set are the two most common rough set\ntheoretical models. Pawlak can use equivalence classes to represent knowledge,\nbut it cannot process continuous data; neighborhood rough sets can process\ncontinuous data, but it loses the ability of using equivalence classes to\nrepresent knowledge. To this end, this paper presents a granular-ball rough set\nbased on the granular-ball computing. The granular-ball rough set can\nsimultaneously represent Pawlak rough sets, and the neighborhood rough set, so\nas to realize the unified representation of the two. This makes the\ngranular-ball rough set not only can deal with continuous data, but also can\nuse equivalence classes for knowledge representation. In addition, we propose\nan implementation algorithms of granular-ball rough sets. The experimental\nresults on benchmark datasets demonstrate that, due to the combination of the\nrobustness and adaptability of the granular-ball computing, the learning\naccuracy of the granular-ball rough set has been greatly improved compared with\nthe Pawlak rough set and the traditional neighborhood rough set. The\ngranular-ball rough set also outperforms nine popular or the state-of-the-art\nfeature selection methods.\n","authors":["Shuyin Xia","Cheng Wang","Guoyin Wang","Weiping Ding","Xinbo Gao","Jianhang Yu","Yujia Zhai","Zizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2201.03349v4.pdf","comment":"12 pages, 18 figures"},{"id":"http://arxiv.org/abs/2207.06775v1","updated":"2022-07-14T09:34:22Z","published":"2022-07-14T09:34:22Z","title":"Strain-Minimizing Hyperbolic Network Embeddings with Landmarks","summary":"  We introduce L-hydra (landmarked hyperbolic distance recovery and\napproximation), a method for embedding network- or distance-based data into\nhyperbolic space, which requires only the distance measurements to a few\n'landmark nodes'. This landmark heuristic makes L-hydra applicable to\nlarge-scale graphs and improves upon previously introduced methods. As a\nmathematical justification, we show that a point configuration in d-dimensional\nhyperbolic space can be perfectly recovered (up to isometry) from distance\nmeasurements to just d+1 landmarks. We also show that L-hydra solves a\ntwo-stage strain-minimization problem, similar to our previous (unlandmarked)\nmethod 'hydra'. Testing on real network data, we show that L-hydra is an order\nof magnitude faster than existing hyperbolic embedding methods and scales\nlinearly in the number of nodes. While the embedding error of L-hydra is higher\nthan the error of existing methods, we introduce an extension, L-hydra+, which\noutperforms existing methods in both runtime and embedding quality.\n","authors":["Martin Keller-Ressel","Stephanie Nargang"],"pdf_url":"https://arxiv.org/pdf/2207.06775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06767v1","updated":"2022-07-14T09:24:55Z","published":"2022-07-14T09:24:55Z","title":"Semi-supervised cross-lingual speech emotion recognition","summary":"  Speech emotion recognition (SER) on a single language has achieved remarkable\nresults through deep learning approaches over the last decade. However,\ncross-lingual SER remains a challenge in real-world applications due to (i) a\nlarge difference between the source and target domain distributions, (ii) the\navailability of few labeled and many unlabeled utterances for the new language.\nTaking into account previous aspects, we propose a Semi-Supervised Learning\n(SSL) method for cross-lingual emotion recognition when a few labels from the\nnew language are available. Based on a Convolutional Neural Network (CNN), our\nmethod adapts to a new language by exploiting a pseudo-labeling strategy for\nthe unlabeled utterances. In particular, the use of a hard and soft\npseudo-labels approach is investigated. We thoroughly evaluate the performance\nof the method in a speaker-independent setup on both the source and the new\nlanguage and show its robustness across five languages belonging to different\nlinguistic strains.\n","authors":["Mirko Agarla","Simone Bianco","Luigi Celona","Paolo Napoletano","Alexey Petrovsky","Flavio Piccoli","Raimondo Schettini","Ivan Shanin"],"pdf_url":"https://arxiv.org/pdf/2207.06767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.10808v2","updated":"2022-07-14T09:15:18Z","published":"2022-02-22T10:58:26Z","title":"Combating Distribution Shift for Accurate Time Series Forecasting via\n  Hypernetworks","summary":"  Time series forecasting has widespread applications in urban life ranging\nfrom air quality monitoring to traffic analysis. However, accurate time series\nforecasting is challenging because real-world time series suffer from the\ndistribution shift problem, where their statistical properties change over\ntime. Despite extensive solutions to distribution shifts in domain adaptation\nor generalization, they fail to function effectively in unknown,\nconstantly-changing distribution shifts, which are common in time series. In\nthis paper, we propose Hyper Time- Series Forecasting (HTSF), a\nhypernetwork-based framework for accurate time series forecasting under\ndistribution shift. HTSF jointly learns the time-varying distributions and the\ncorresponding forecasting models in an end-to-end fashion. Specifically, HTSF\nexploits the hyper layers to learn the best characterization of the\ndistribution shifts, generating the model parameters for the main layers to\nmake accurate predictions. We implement HTSF as an extensible framework that\ncan incorporate diverse time series forecasting models such as RNNs and\nTransformers. Extensive experiments on 9 benchmarks demonstrate that HTSF\nachieves state-of-the-art performances.\n","authors":["Wenying Duan","Xiaoxi He","Lu Zhou","Lothar Thiele","Hong Rao"],"pdf_url":"https://arxiv.org/pdf/2202.10808v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.06759v1","updated":"2022-07-14T09:10:30Z","published":"2022-07-14T09:10:30Z","title":"Work In Progress: Safety and Robustness Verification of\n  Autoencoder-Based Regression Models using the NNV Tool","summary":"  This work in progress paper introduces robustness verification for\nautoencoder-based regression neural network (NN) models, following\nstate-of-the-art approaches for robustness verification of image classification\nNNs. Despite the ongoing progress in developing verification methods for safety\nand robustness in various deep neural networks (DNNs), robustness checking of\nautoencoder models has not yet been considered. We explore this open space of\nresearch and check ways to bridge the gap between existing DNN verification\nmethods by extending existing robustness analysis methods for such autoencoder\nnetworks. While classification models using autoencoders work more or less\nsimilar to image classification NNs, the functionality of regression models is\ndistinctly different. We introduce two definitions of robustness evaluation\nmetrics for autoencoder-based regression models, specifically the percentage\nrobustness and un-robustness grade. We also modified the existing Imagestar\napproach, adjusting the variables to take care of the specific input types for\nregression networks. The approach is implemented as an extension of NNV, then\napplied and evaluated on a dataset, with a case study experiment shown using\nthe same dataset. As per the authors' understanding, this work in progress\npaper is the first to show possible reachability analysis of autoencoder-based\nNNs.\n","authors":["Neelanjana Pal","Taylor T Johnson"],"pdf_url":"https://arxiv.org/pdf/2207.06759v1.pdf","comment":"In Proceedings SNR 2021, arXiv:2207.04391"},{"id":"http://arxiv.org/abs/2110.13854v2","updated":"2022-07-14T09:10:00Z","published":"2021-10-26T16:57:46Z","title":"Interpretable Decision Trees Through MaxSAT","summary":"  We present an approach to improve the accuracy-interpretability trade-off of\nMachine Learning (ML) Decision Trees (DTs). In particular, we apply Maximum\nSatisfiability technology to compute Minimum Pure DTs (MPDTs). We improve the\nruntime of previous approaches and, show that these MPDTs can outperform the\naccuracy of DTs generated with the ML framework sklearn.\n","authors":["Josep Alos","Carlos Ansotegui","Eduard Torres"],"pdf_url":"https://arxiv.org/pdf/2110.13854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06755v1","updated":"2022-07-14T09:08:38Z","published":"2022-07-14T09:08:38Z","title":"Verification of Sigmoidal Artificial Neural Networks using iSAT","summary":"  This paper presents an approach for verifying the behaviour of nonlinear\nArtificial Neural Networks (ANNs) found in cyber-physical safety-critical\nsystems. We implement a dedicated interval constraint propagator for the\nsigmoid function into the SMT solver iSAT and compare this approach with a\ncompositional approach encoding the sigmoid function by basic arithmetic\nfeatures available in iSAT and an approximating approach. Our experimental\nresults show that the dedicated and the compositional approach clearly\noutperform the approximating approach. Throughout all our benchmarks, the\ndedicated approach showed an equal or better performance compared to the\ncompositional approach.\n","authors":["Dominik Grundt","Sorin Liviu Jurj","Willem Hagemann","Paul Kröger","Martin Fränzle"],"pdf_url":"https://arxiv.org/pdf/2207.06755v1.pdf","comment":"In Proceedings SNR 2021, arXiv:2207.04391"},{"id":"http://arxiv.org/abs/2207.06741v1","updated":"2022-07-14T08:48:26Z","published":"2022-07-14T08:48:26Z","title":"Differentiable Logics for Neural Network Training and Verification","summary":"  The rising popularity of neural networks (NNs) in recent years and their\nincreasing prevalence in real-world applications have drawn attention to the\nimportance of their verification. While verification is known to be\ncomputationally difficult theoretically, many techniques have been proposed for\nsolving it in practice. It has been observed in the literature that by default\nneural networks rarely satisfy logical constraints that we want to verify. A\ngood course of action is to train the given NN to satisfy said constraint prior\nto verifying them. This idea is sometimes referred to as continuous\nverification, referring to the loop between training and verification. Usually\ntraining with constraints is implemented by specifying a translation for a\ngiven formal logic language into loss functions. These loss functions are then\nused to train neural networks. Because for training purposes these functions\nneed to be differentiable, these translations are called differentiable logics\n(DL). This raises several research questions. What kind of differentiable\nlogics are possible? What difference does a specific choice of DL make in the\ncontext of continuous verification? What are the desirable criteria for a DL\nviewed from the point of view of the resulting loss function? In this extended\nabstract we will discuss and answer these questions.\n","authors":["Natalia Slusarz","Ekaterina Komendantskaya","Matthew L. Daggitt","Robert Stewart"],"pdf_url":"https://arxiv.org/pdf/2207.06741v1.pdf","comment":"FOMLAS'22 paper"},{"id":"http://arxiv.org/abs/2110.11526v3","updated":"2022-07-14T07:33:29Z","published":"2021-10-21T23:49:23Z","title":"Wide Neural Networks Forget Less Catastrophically","summary":"  A primary focus area in continual learning research is alleviating the\n\"catastrophic forgetting\" problem in neural networks by designing new\nalgorithms that are more robust to the distribution shifts. While the recent\nprogress in continual learning literature is encouraging, our understanding of\nwhat properties of neural networks contribute to catastrophic forgetting is\nstill limited. To address this, instead of focusing on continual learning\nalgorithms, in this work, we focus on the model itself and study the impact of\n\"width\" of the neural network architecture on catastrophic forgetting, and show\nthat width has a surprisingly significant effect on forgetting. To explain this\neffect, we study the learning dynamics of the network from various perspectives\nsuch as gradient orthogonality, sparsity, and lazy training regime. We provide\npotential explanations that are consistent with the empirical results across\ndifferent architectures and continual learning benchmarks.\n","authors":["Seyed Iman Mirzadeh","Arslan Chaudhry","Dong Yin","Huiyi Hu","Razvan Pascanu","Dilan Gorur","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2110.11526v3.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2207.06709v1","updated":"2022-07-14T07:32:15Z","published":"2022-07-14T07:32:15Z","title":"problexity -- an open-source Python library for binary classification\n  problem complexity assessment","summary":"  The classification problem's complexity assessment is an essential element of\nmany topics in the supervised learning domain. It plays a significant role in\nmeta-learning -- becoming the basis for determining meta-attributes or\nmulti-criteria optimization -- allowing the evaluation of the training set\nresampling without needing to rebuild the recognition model. The tools\ncurrently available for the academic community, which would enable the\ncalculation of problem complexity measures, are available only as libraries of\nthe C++ and R languages. This paper describes the software module that allows\nfor the estimation of 22 complexity measures for the Python language --\ncompatible with the scikit-learn programming interface -- allowing for the\nimplementation of research using them in the most popular programming\nenvironment of the machine learning community.\n","authors":["Joanna Komorniczak","Pawel Ksieniewicz"],"pdf_url":"https://arxiv.org/pdf/2207.06709v1.pdf","comment":"20 pages, 1 figure"},{"id":"http://arxiv.org/abs/2206.08523v3","updated":"2022-07-14T07:10:17Z","published":"2022-06-17T03:11:18Z","title":"A Spatio-Temporal Neural Network Forecasting Approach for Emulation of\n  Firefront Models","summary":"  Computational simulations of wildfire spread typically employ empirical\nrate-of-spread calculations under various conditions (such as terrain, fuel\ntype, weather). Small perturbations in conditions can often lead to significant\nchanges in fire spread (such as speed and direction), necessitating a\ncomputationally expensive large set of simulations to quantify uncertainty.\nModel emulation seeks alternative representations of physical models using\nmachine learning, aiming to provide more efficient and/or simplified surrogate\nmodels. We propose a dedicated spatio-temporal neural network based framework\nfor model emulation, able to capture the complex behaviour of fire spread\nmodels. The proposed approach can approximate forecasts at fine spatial and\ntemporal resolutions that are often challenging for neural network based\napproaches. Furthermore, the proposed approach is robust even with small\ntraining sets, due to novel data augmentation methods. Empirical experiments\nshow good agreement between simulated and emulated firefronts, with an average\nJaccard score of 0.76.\n","authors":["Andrew Bolt","Carolyn Huston","Petra Kuhnert","Joel Janek Dabrowski","James Hilton","Conrad Sanderson"],"pdf_url":"https://arxiv.org/pdf/2206.08523v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06410v1","updated":"2022-07-14T07:04:29Z","published":"2022-07-14T07:04:29Z","title":"MDEAW: A Multimodal Dataset for Emotion Analysis through EDA and PPG\n  signals from wireless wearable low-cost off-the-shelf Devices","summary":"  We present MDEAW, a multimodal database consisting of Electrodermal Activity\n(EDA) and Photoplethysmography (PPG) signals recorded during the exams for the\ncourse taught by the teacher at Eurecat Academy, Sabadell, Barcelona in order\nto elicit the emotional reactions to the students in a classroom scenario.\nSignals from 10 students were recorded along with the students' self-assessment\nof their affective state after each stimulus, in terms of 6 basic emotion\nstates. All the signals were captured using portable, wearable, wireless,\nlow-cost, and off-the-shelf equipment that has the potential to allow the use\nof affective computing methods in everyday applications. A baseline for\nstudent-wise affect recognition using EDA and PPG-based features, as well as\ntheir fusion, was established through ReMECS, Fed-ReMECS, and Fed-ReMECS-U.\nThese results indicate the prospects of using low-cost devices for affective\nstate recognition applications. The proposed database will be made publicly\navailable in order to allow researchers to achieve a more thorough evaluation\nof the suitability of these capturing devices for emotion state recognition\napplications.\n","authors":["Arijit Nandi","Fatos Xhafa","Laia Subirats","Santi Fort"],"pdf_url":"https://arxiv.org/pdf/2207.06410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08344v5","updated":"2022-07-14T06:43:48Z","published":"2022-03-16T01:32:21Z","title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild","summary":"  We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n","authors":["Takehiko Ohkawa","Yu-Jhe Li","Qichen Fu","Ryosuke Furuta","Kris M. Kitani","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2203.08344v5.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06687v1","updated":"2022-07-14T06:34:21Z","published":"2022-07-14T06:34:21Z","title":"Improved OOD Generalization via Conditional Invariant Regularizer","summary":"  Recently, generalization on out-of-distribution (OOD) data with correlation\nshift has attracted great attention. The correlation shift is caused by the\nspurious attributes that correlate to the class label, as the correlation\nbetween them may vary in training and test data. For such a problem, we show\nthat given the class label, the conditionally independent models of spurious\nattributes are OOD generalizable. Based on this, a metric Conditional Spurious\nVariation (CSV) which controls OOD generalization error, is proposed to measure\nsuch conditional independence. To improve the OOD generalization, we regularize\nthe training process with the proposed CSV. Under mild assumptions, our\ntraining objective can be formulated as a nonconvex-concave mini-max problem.\nAn algorithm with provable convergence rate is proposed to solve the problem.\nExtensive empirical results verify our algorithm's efficacy in improving OOD\ngeneralization.\n","authors":["Mingyang Yi","Ruoyu Wang","Jiachen Sun","Zhenguo Li","Zhi-Ming Ma"],"pdf_url":"https://arxiv.org/pdf/2207.06687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.12747v2","updated":"2022-07-14T06:31:25Z","published":"2022-06-25T22:48:27Z","title":"HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network","summary":"  Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in\nthe worst scenario, they may lead to adverse drug reactions (ADRs). Predicting\nall DDIs is a challenging and critical problem. Most existing computational\nmodels integrate drug-centric information from different sources and leverage\nthem as features in machine learning classifiers to predict DDIs. However,\nthese models have a high chance of failure, especially for the new drugs when\nall the information is not available. This paper proposes a novel Hypergraph\nNeural Network (HyGNN) model based on only the SMILES string of drugs,\navailable for any drug, for the DDI prediction problem. To capture the drug\nsimilarities, we create a hypergraph from drugs' chemical substructures\nextracted from the SMILES strings. Then, we develop HyGNN consisting of a novel\nattention-based hypergraph edge encoder to get the representation of drugs as\nhyperedges and a decoder to predict the interactions between drug pairs.\nFurthermore, we conduct extensive experiments to evaluate our model and compare\nit with several state-of-the-art methods. Experimental results demonstrate that\nour proposed HyGNN model effectively predicts DDIs and impressively outperforms\nthe baselines with a maximum ROC-AUC and PR-AUC of 97.9% and 98.1%,\nrespectively.\n","authors":["Khaled Mohammed Saifuddin","Briana Bumgardner","Farhan Tanvir","Esra Akbas"],"pdf_url":"https://arxiv.org/pdf/2206.12747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06684v1","updated":"2022-07-14T06:23:38Z","published":"2022-07-14T06:23:38Z","title":"Subgraph Frequency Distribution Estimation using Graph Neural Networks","summary":"  Small subgraphs (graphlets) are important features to describe fundamental\nunits of a large network. The calculation of the subgraph frequency\ndistributions has a wide application in multiple domains including biology and\nengineering. Unfortunately due to the inherent complexity of this task, most of\nthe existing methods are computationally intensive and inefficient. In this\nwork, we propose GNNS, a novel representational learning framework that\nutilizes graph neural networks to sample subgraphs efficiently for estimating\ntheir frequency distribution. Our framework includes an inference model and a\ngenerative model that learns hierarchical embeddings of nodes, subgraphs, and\ngraph types. With the learned model and embeddings, subgraphs are sampled in a\nhighly scalable and parallel way and the frequency distribution estimation is\nthen performed based on these sampled subgraphs. Eventually, our methods\nachieve comparable accuracy and a significant speedup by three orders of\nmagnitude compared to existing methods.\n","authors":["Zhongren Chen","Xinyue Xu","Shengyi Jiang","Hao Wang","Lu Mi"],"pdf_url":"https://arxiv.org/pdf/2207.06684v1.pdf","comment":"accepted by KDD 2022 Workshop on Deep Learning on Graphs"},{"id":"http://arxiv.org/abs/2207.06680v1","updated":"2022-07-14T06:17:00Z","published":"2022-07-14T06:17:00Z","title":"Equivariant Hypergraph Diffusion Neural Operators","summary":"  Hypergraph neural networks (HNNs) using neural networks to encode hypergraphs\nprovide a promising way to model higher-order relations in data and further\nsolve relevant prediction tasks built upon such higher-order relations.\nHowever, higher-order relations in practice contain complex patterns and are\noften highly irregular. So, it is often challenging to design an HNN that\nsuffices to express those relations while keeping computational efficiency.\nInspired by hypergraph diffusion algorithms, this work proposes a new HNN\narchitecture named ED-HNN, which provably represents any continuous equivariant\nhypergraph diffusion operators that can model a wide range of higher-order\nrelations. ED-HNN can be implemented efficiently by combining star expansions\nof hypergraphs with standard message passing neural networks. ED-HNN further\nshows great superiority in processing heterophilic hypergraphs and constructing\ndeep models. We evaluate ED-HNN for node classification on nine real-world\nhypergraph datasets. ED-HNN uniformly outperforms the best baselines over these\nnine datasets and achieves more than 2\\%$\\uparrow$ in prediction accuracy over\nfour datasets therein.\n","authors":["Peihao Wang","Shenghao Yang","Yunyu Liu","Zhangyang Wang","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2207.06680v1.pdf","comment":"Code: https://github.com/Graph-COM/ED-HNN"},{"id":"http://arxiv.org/abs/2206.04838v3","updated":"2022-07-14T06:16:24Z","published":"2022-06-10T01:47:49Z","title":"In Defense of Core-set: A Density-aware Core-set Selection for Active\n  Learning","summary":"  Active learning enables the efficient construction of a labeled dataset by\nlabeling informative samples from an unlabeled dataset. In a real-world active\nlearning scenario, considering the diversity of the selected samples is crucial\nbecause many redundant or highly similar samples exist. Core-set approach is\nthe promising diversity-based method selecting diverse samples based on the\ndistance between samples. However, the approach poorly performs compared to the\nuncertainty-based approaches that select the most difficult samples where\nneural models reveal low confidence. In this work, we analyze the feature space\nthrough the lens of the density and, interestingly, observe that locally sparse\nregions tend to have more informative samples than dense regions. Motivated by\nour analysis, we empower the core-set approach with the density-awareness and\npropose a density-aware core-set (DACS). The strategy is to estimate the\ndensity of the unlabeled samples and select diverse samples mainly from sparse\nregions. To reduce the computational bottlenecks in estimating the density, we\nalso introduce a new density approximation based on locality-sensitive hashing.\nExperimental results clearly demonstrate the efficacy of DACS in both\nclassification and regression tasks and specifically show that DACS can produce\nstate-of-the-art performance in a practical scenario. Since DACS is weakly\ndependent on neural architectures, we present a simple yet effective\ncombination method to show that the existing methods can be beneficially\ncombined with DACS.\n","authors":["Yeachan Kim","Bonggun Shin"],"pdf_url":"https://arxiv.org/pdf/2206.04838v3.pdf","comment":"Proceedings of the ACM SIGKDD Conference on Knowledge Discovery &\n  Data Mining, 2022 (KDD'22)"},{"id":"http://arxiv.org/abs/2207.06679v1","updated":"2022-07-14T06:16:17Z","published":"2022-07-14T06:16:17Z","title":"Learning to Prove Trigonometric Identities","summary":"  Automatic theorem proving with deep learning methods has attracted attentions\nrecently. In this paper, we construct an automatic proof system for\ntrigonometric identities. We define the normalized form of trigonometric\nidentities, design a set of rules for the proof and put forward a method which\ncan generate theoretically infinite trigonometric identities. Our goal is not\nonly to complete the proof, but to complete the proof in as few steps as\npossible. For this reason, we design a model to learn proof data generated by\nrandom BFS (rBFS), and it is proved theoretically and experimentally that the\nmodel can outperform rBFS after a simple imitation learning. After further\nimprovement through reinforcement learning, we get AutoTrig, which can give\nproof steps for identities in almost as short steps as BFS (theoretically\nshortest method), with a time cost of only one-thousandth. In addition,\nAutoTrig also beats Sympy, Matlab and human in the synthetic dataset, and\nperforms well in many generalization tasks.\n","authors":["Zhou Liu","Yujun Li","Zhengying Liu","Lin Li","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2207.06679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06678v1","updated":"2022-07-14T06:11:32Z","published":"2022-07-14T06:11:32Z","title":"Deep Learning Methods for Protein Family Classification on PDB\n  Sequencing Data","summary":"  Composed of amino acid chains that influence how they fold and thus dictating\ntheir function and features, proteins are a class of macromolecules that play a\ncentral role in major biological processes and are required for the structure,\nfunction, and regulation of the body's tissues. Understanding protein functions\nis vital to the development of therapeutics and precision medicine, and hence\nthe ability to classify proteins and their functions based on measurable\nfeatures is crucial; indeed, the automatic inference of a protein's properties\nfrom its sequence of amino acids, known as its primary structure, remains an\nimportant open problem within the field of bioinformatics, especially given the\nrecent advancements in sequencing technologies and the extensive number of\nknown but uncategorized proteins with unknown properties. In this work, we\ndemonstrate and compare the performance of several deep learning frameworks,\nincluding novel bi-directional LSTM and convolutional models, on widely\navailable sequencing data from the Protein Data Bank (PDB) of the Research\nCollaboratory for Structural Bioinformatics (RCSB), as well as benchmark this\nperformance against classical machine learning approaches, including k-nearest\nneighbors and multinomial regression classifiers, trained on experimental data.\nOur results show that our deep learning models deliver superior performance to\nclassical machine learning methods, with the convolutional architecture\nproviding the most impressive inference performance.\n","authors":["Aaron Wang"],"pdf_url":"https://arxiv.org/pdf/2207.06678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.03092v2","updated":"2022-07-14T06:07:30Z","published":"2020-12-05T18:13:14Z","title":"Several Approximation Algorithms for Sparse Best Rank-1 Approximation to\n  Higher-Order Tensors","summary":"  Sparse tensor best rank-1 approximation (BR1Approx), which is a sparsity\ngeneralization of the dense tensor BR1Approx, and is a higher-order extension\nof the sparse matrix BR1Approx, is one of the most important problems in sparse\ntensor decomposition and related problems arising from statistics and machine\nlearning. By exploiting the multilinearity as well as the sparsity structure of\nthe problem, four approximation algorithms are proposed, which are easily\nimplemented, of low computational complexity, and can serve as initial\nprocedures for iterative algorithms. In addition, theoretically guaranteed\nworst-case approximation lower bounds are proved for all the algorithms. We\nprovide numerical experiments on synthetic and real data to illustrate the\neffectiveness of the proposed algorithms.\n","authors":["Xianpeng Mao","Yuning Yang"],"pdf_url":"https://arxiv.org/pdf/2012.03092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06676v1","updated":"2022-07-14T06:04:30Z","published":"2022-07-14T06:04:30Z","title":"A Meta-learning Formulation of the Autoencoder Problem","summary":"  A rapidly growing area of research is the use of machine learning approaches\nsuch as autoencoders for dimensionality reduction of data and models in\nscientific applications. We show that the canonical formulation of autoencoders\nsuffers from several deficiencies that can hinder their performance. Using a\nmeta-learning approach, we reformulate the autoencoder problem as a bi-level\noptimization procedure that explicitly solves the dimensionality reduction\ntask. We prove that the new formulation corrects the identified deficiencies\nwith canonical autoencoders, provide a practical way to solve it, and showcase\nthe strength of this formulation with a simple numerical illustration.\n","authors":["Andrey A. Popov","Arash Sarshar","Austin Chennault","Adrian Sandu"],"pdf_url":"https://arxiv.org/pdf/2207.06676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.03378v3","updated":"2022-07-14T05:50:36Z","published":"2021-09-08T00:38:44Z","title":"Rethinking Multidimensional Discriminator Output for Generative\n  Adversarial Networks","summary":"  The study of multidimensional discriminator (critic) output for Generative\nAdversarial Networks has been underexplored in the literature. In this paper,\nwe generalize the Wasserstein GAN framework to take advantage of\nmultidimensional critic output and explore its properties. We also introduce a\nsquare-root velocity transformation (SRVT) block which favors training in the\nmultidimensional setting. Proofs of properties are based on our proposed\nmaximal p-centrality discrepancy, which is bounded above by p-Wasserstein\ndistance and fits the Wasserstein GAN framework with multidimensional critic\noutput n. Especially when n = 1 and p = 1, the proposed discrepancy equals\n1-Wasserstein distance. Theoretical analysis and empirical evidence show that\nhigh-dimensional critic output has its advantage on distinguishing real and\nfake distributions, and benefits faster convergence and diversity of results.\n","authors":["Mengyu Dai","Haibin Hang","Anuj Srivastava"],"pdf_url":"https://arxiv.org/pdf/2109.03378v3.pdf","comment":"Frontiers in Adversarial Machine Learning ICML 2022"},{"id":"http://arxiv.org/abs/2111.12485v2","updated":"2022-07-14T05:39:09Z","published":"2021-11-24T13:29:17Z","title":"Graph Modularity: Towards Understanding the Cross-Layer Transition of\n  Feature Representations in Deep Neural Networks","summary":"  There are good arguments to support the claim that deep neural networks\n(DNNs) capture better feature representations than the previous hand-crafted\nfeature engineering, which leads to a significant performance improvement. In\nthis paper, we move a tiny step towards understanding the dynamics of feature\nrepresentations over layers. Specifically, we model the process of class\nseparation of intermediate representations in pre-trained DNNs as the evolution\nof communities in dynamic graphs. Then, we introduce modularity, a generic\nmetric in graph theory, to quantify the evolution of communities. In the\npreliminary experiment, we find that modularity roughly tends to increase as\nthe layer goes deeper and the degradation and plateau arise when the model\ncomplexity is great relative to the dataset. Through an asymptotic analysis, we\nprove that modularity can be broadly used for different applications. For\nexample, modularity provides new insights to quantify the difference between\nfeature representations. More crucially, we demonstrate that the degradation\nand plateau in modularity curves represent redundant layers in DNNs and can be\npruned with minimal impact on performance, which provides theoretical guidance\nfor layer pruning. Our code is available at\nhttps://github.com/yaolu-zjut/Dynamic-Graphs-Construction.\n","authors":["Yao Lu","Wen Yang","Yunzhe Zhang","Zuohui Chen","Jinyin Chen","Qi Xuan","Zhen Wang","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2111.12485v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06667v1","updated":"2022-07-14T05:38:14Z","published":"2022-07-14T05:38:14Z","title":"Large-scale Knowledge Distillation with Elastic Heterogeneous Computing\n  Resources","summary":"  Although more layers and more parameters generally improve the accuracy of\nthe models, such big models generally have high computational complexity and\nrequire big memory, which exceed the capacity of small devices for inference\nand incurs long training time. In addition, it is difficult to afford long\ntraining time and inference time of big models even in high performance\nservers, as well. As an efficient approach to compress a large deep model (a\nteacher model) to a compact model (a student model), knowledge distillation\nemerges as a promising approach to deal with the big models. Existing knowledge\ndistillation methods cannot exploit the elastic available computing resources\nand correspond to low efficiency. In this paper, we propose an Elastic Deep\nLearning framework for knowledge Distillation, i.e., EDL-Dist. The advantages\nof EDL-Dist are three-fold. First, the inference and the training process is\nseparated. Second, elastic available computing resources can be utilized to\nimprove the efficiency. Third, fault-tolerance of the training and inference\nprocesses is supported. We take extensive experimentation to show that the\nthroughput of EDL-Dist is up to 3.125 times faster than the baseline method\n(online knowledge distillation) while the accuracy is similar or higher.\n","authors":["Ji Liu","Daxiang Dong","Xi Wang","An Qin","Xingjian Li","Patrick Valduriez","Dejing Dou","Dianhai Yu"],"pdf_url":"https://arxiv.org/pdf/2207.06667v1.pdf","comment":"To appear in Concurrency and Computation: Practice and Experience, 16\n  pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2105.13001v3","updated":"2022-07-14T04:52:20Z","published":"2021-05-27T08:36:54Z","title":"Estimating Instance-dependent Bayes-label Transition Matrix using a Deep\n  Neural Network","summary":"  In label-noise learning, estimating the transition matrix is a hot topic as\nthe matrix plays an important role in building statistically consistent\nclassifiers. Traditionally, the transition from clean labels to noisy labels\n(i.e., clean-label transition matrix (CLTM)) has been widely exploited to learn\na clean label classifier by employing the noisy data. Motivated by that\nclassifiers mostly output Bayes optimal labels for prediction, in this paper,\nwe study to directly model the transition from Bayes optimal labels to noisy\nlabels (i.e., Bayes-label transition matrix (BLTM)) and learn a classifier to\npredict Bayes optimal labels. Note that given only noisy data, it is ill-posed\nto estimate either the CLTM or the BLTM. But favorably, Bayes optimal labels\nhave less uncertainty compared with the clean labels, i.e., the class\nposteriors of Bayes optimal labels are one-hot vectors while those of clean\nlabels are not. This enables two advantages to estimate the BLTM, i.e., (a) a\nset of examples with theoretically guaranteed Bayes optimal labels can be\ncollected out of noisy data; (b) the feasible solution space is much smaller.\nBy exploiting the advantages, we estimate the BLTM parametrically by employing\na deep neural network, leading to better generalization and superior\nclassification performance.\n","authors":["Shuo Yang","Erkun Yang","Bo Han","Yang Liu","Min Xu","Gang Niu","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2105.13001v3.pdf","comment":"ICML 22 camera ready"},{"id":"http://arxiv.org/abs/2110.10545v4","updated":"2022-07-14T04:50:39Z","published":"2021-10-20T12:59:23Z","title":"Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting\n  Model Hubs","summary":"  Model hubs with many pre-trained models (PTMs) have become a cornerstone of\ndeep learning. Although built at a high cost, they remain\n\\emph{under-exploited} -- practitioners usually pick one PTM from the provided\nmodel hub by popularity and then fine-tune the PTM to solve the target task.\nThis na\\\"ive but common practice poses two obstacles to full exploitation of\npre-trained model hubs: first, the PTM selection by popularity has no\noptimality guarantee, and second, only one PTM is used while the remaining PTMs\nare ignored. An alternative might be to consider all possible combinations of\nPTMs and extensively fine-tune each combination, but this would not only be\nprohibitive computationally but may also lead to statistical over-fitting. In\nthis paper, we propose a new paradigm for exploiting model hubs that is\nintermediate between these extremes. The paradigm is characterized by two\naspects: (1) We use an evidence maximization procedure to estimate the maximum\nvalue of label evidence given features extracted by pre-trained models. This\nprocedure can rank all the PTMs in a model hub for various types of PTMs and\ntasks \\emph{before fine-tuning}. (2) The best ranked PTM can either be\nfine-tuned and deployed if we have no preference for the model's architecture\nor the target PTM can be tuned by the top $K$ ranked PTMs via a Bayesian\nprocedure that we propose. This procedure, which we refer to as\n\\emph{B-Tuning}, not only improves upon specialized methods designed for tuning\nhomogeneous PTMs, but also applies to the challenging problem of tuning\nheterogeneous PTMs where it yields a new level of benchmark performance.\n","authors":["Kaichao You","Yong Liu","Ziyang Zhang","Jianmin Wang","Michael I. Jordan","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2110.10545v4.pdf","comment":"47 pages, camera-ready version for JMLR 2022"},{"id":"http://arxiv.org/abs/2112.02086v2","updated":"2022-07-14T04:42:44Z","published":"2021-12-03T18:53:16Z","title":"Data-Free Neural Architecture Search via Recursive Label Calibration","summary":"  This paper aims to explore the feasibility of neural architecture search\n(NAS) given only a pre-trained model without using any original training data.\nThis is an important circumstance for privacy protection, bias avoidance, etc.,\nin real-world scenarios. To achieve this, we start by synthesizing usable data\nthrough recovering the knowledge from a pre-trained deep neural network. Then\nwe use the synthesized data and their predicted soft-labels to guide neural\narchitecture search. We identify that the NAS task requires the synthesized\ndata (we target at image domain here) with enough semantics, diversity, and a\nminimal domain gap from the natural images. For semantics, we propose recursive\nlabel calibration to produce more informative outputs. For diversity, we\npropose a regional update strategy to generate more diverse and\nsemantically-enriched synthetic data. For minimal domain gap, we use input and\nfeature-level regularization to mimic the original data distribution in latent\nspace. We instantiate our proposed framework with three popular NAS algorithms:\nDARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the\narchitectures discovered by searching with our synthetic data achieve accuracy\nthat is comparable to, or even higher than, architectures discovered by\nsearching from the original ones, for the first time, deriving the conclusion\nthat NAS can be done effectively with no need of access to the original or\ncalled natural data if the synthesis method is well designed.\n","authors":["Zechun Liu","Zhiqiang Shen","Yun Long","Eric Xing","Kwang-Ting Cheng","Chas Leichner"],"pdf_url":"https://arxiv.org/pdf/2112.02086v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.06652v1","updated":"2022-07-14T04:29:54Z","published":"2022-07-14T04:29:54Z","title":"Every Preference Changes Differently: Neural Multi-Interest Preference\n  Model with Temporal Dynamics for Recommendation","summary":"  User embeddings (vectorized representations of a user) are essential in\nrecommendation systems. Numerous approaches have been proposed to construct a\nrepresentation for the user in order to find similar items for retrieval tasks,\nand they have been proven effective in industrial recommendation systems as\nwell. Recently people have discovered the power of using multiple embeddings to\nrepresent a user, with the hope that each embedding represents the user's\ninterest in a certain topic. With multi-interest representation, it's important\nto model the user's preference over the different topics and how the preference\nchange with time. However, existing approaches either fail to estimate the\nuser's affinity to each interest or unreasonably assume every interest of every\nuser fades with an equal rate with time, thus hurting the recall of candidate\nretrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,\nan approach that not only produces multi-interest for users by using the user's\nsequential engagement more effectively but also automatically learns a set of\nweights to represent the preference over each embedding so that the candidates\ncan be retrieved from each interest proportionally. Extensive experiments have\nbeen done on various industrial-scale datasets to demonstrate the effectiveness\nof our approach.\n","authors":["Hui Shi","Yupeng Gu","Yitong Zhou","Bo Zhao","Sicun Gao","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.06652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06651v1","updated":"2022-07-14T04:20:08Z","published":"2022-07-14T04:20:08Z","title":"Have we been Naive to Select Machine Learning Models? Noisy Data are\n  here to Stay!","summary":"  The model selection procedure is usually a single-criterion decision making\nin which we select the model that maximizes a specific metric in a specific\nset, such as the Validation set performance. We claim this is very naive and\ncan perform poor selections of over-fitted models due to the over-searching\nphenomenon, which over-estimates the performance on that specific set.\nFuthermore, real world data contains noise that should not be ignored by the\nmodel selection procedure and must be taken into account when performing model\nselection. Also, we have defined four theoretical optimality conditions that we\ncan pursue to better select the models and analyze them by using a\nmulti-criteria decision-making algorithm (TOPSIS) that considers proxies to the\noptimality conditions to select reasonable models.\n","authors":["Felipe Costa Farias","Teresa Bernarda Ludermir","Carmelo José Albanez Bastos-Filho"],"pdf_url":"https://arxiv.org/pdf/2207.06651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07238v1","updated":"2022-07-14T23:59:06Z","published":"2022-07-14T23:59:06Z","title":"Emotion Recognition in Conversation using Probabilistic Soft Logic","summary":"  Creating agents that can both appropriately respond to conversations and\nunderstand complex human linguistic tendencies and social cues has been a long\nstanding challenge in the NLP community. A recent pillar of research revolves\naround emotion recognition in conversation (ERC); a sub-field of emotion\nrecognition that focuses on conversations or dialogues that contain two or more\nutterances. In this work, we explore an approach to ERC that exploits the use\nof neural embeddings along with complex structures in dialogues. We implement\nour approach in a framework called Probabilistic Soft Logic (PSL), a\ndeclarative templating language that uses first-order like logical rules, that\nwhen combined with data, define a particular class of graphical model.\nAdditionally, PSL provides functionality for the incorporation of results from\nneural models into PSL models. This allows our model to take advantage of\nadvanced neural methods, such as sentence embeddings, and logical reasoning\nover the structure of a dialogue. We compare our method with state-of-the-art\npurely neural ERC systems, and see almost a 20% improvement. With these\nresults, we provide an extensive qualitative and quantitative analysis over the\nDailyDialog conversation dataset.\n","authors":["Eriq Augustine","Pegah Jandaghi","Alon Albalak","Connor Pryor","Charles Dickens","William Wang","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2207.07238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07235v1","updated":"2022-07-14T23:54:54Z","published":"2022-07-14T23:54:54Z","title":"Single Model Uncertainty Estimation via Stochastic Data Centering","summary":"  We are interested in estimating the uncertainties of deep neural networks,\nwhich play an important role in many scientific and engineering problems. In\nthis paper, we present a striking new finding that an ensemble of neural\nnetworks with the same weight initialization, trained on datasets that are\nshifted by a constant bias gives rise to slightly inconsistent trained models,\nwhere the differences in predictions are a strong indicator of epistemic\nuncertainties. Using the neural tangent kernel (NTK), we demonstrate that this\nphenomena occurs in part because the NTK is not shift-invariant. Since this is\nachieved via a trivial input transformation, we show that it can therefore be\napproximated using just a single neural network -- using a technique that we\ncall $\\Delta-$UQ -- that estimates uncertainty around prediction by\nmarginalizing out the effect of the biases. We show that $\\Delta-$UQ's\nuncertainty estimates are superior to many of the current methods on a variety\nof benchmarks -- outlier rejection, calibration under distribution shift, and\nsequential design optimization of black box functions.\n","authors":["Jayaraman J. Thiagarajan","Rushil Anirudh","Vivek Narayanaswamy","Peer-Timo Bremer"],"pdf_url":"https://arxiv.org/pdf/2207.07235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07232v1","updated":"2022-07-14T23:40:22Z","published":"2022-07-14T23:40:22Z","title":"Lipschitz Bound Analysis of Neural Networks","summary":"  Lipschitz Bound Estimation is an effective method of regularizing deep neural\nnetworks to make them robust against adversarial attacks. This is useful in a\nvariety of applications ranging from reinforcement learning to autonomous\nsystems. In this paper, we highlight the significant gap in obtaining a\nnon-trivial Lipschitz bound certificate for Convolutional Neural Networks\n(CNNs) and empirically support it with extensive graphical analysis. We also\nshow that unrolling Convolutional layers or Toeplitz matrices can be employed\nto convert Convolutional Neural Networks (CNNs) to a Fully Connected Network.\nFurther, we propose a simple algorithm to show the existing 20x-50x gap in a\nparticular data distribution between the actual lipschitz constant and the\nobtained tight bound. We also ran sets of thorough experiments on various\nnetwork architectures and benchmark them on datasets like MNIST and CIFAR-10.\nAll these proposals are supported by extensive testing, graphs, histograms and\ncomparative analysis.\n","authors":["Sarosij Bose"],"pdf_url":"https://arxiv.org/pdf/2207.07232v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2201.01763v3","updated":"2022-07-14T23:05:52Z","published":"2022-01-05T18:50:50Z","title":"Robust Self-Supervised Audio-Visual Speech Recognition","summary":"  Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.\n","authors":["Bowen Shi","Wei-Ning Hsu","Abdelrahman Mohamed"],"pdf_url":"https://arxiv.org/pdf/2201.01763v3.pdf","comment":"Interspeech 2022"},{"id":"http://arxiv.org/abs/2111.01516v2","updated":"2022-07-14T22:46:58Z","published":"2021-11-02T11:44:41Z","title":"FedFly: Towards Migration in Edge-based Distributed Federated Learning","summary":"  Federated learning (FL) is a privacy-preserving distributed machine learning\ntechnique that trains models while keeping all the original data generated on\ndevices locally. Since devices may be resource constrained, offloading can be\nused to improve FL performance by transferring computational workload from\ndevices to edge servers. However, due to mobility, devices participating in FL\nmay leave the network during training and need to connect to a different edge\nserver. This is challenging because the offloaded computations from edge server\nneed to be migrated. In line with this assertion, we present FedFly, which is,\nto the best of our knowledge, the first work to migrate a deep neural network\n(DNN) when devices move between edge servers during FL training. Our empirical\nresults on the CIFAR10 dataset, with both balanced and imbalanced data\ndistribution, support our claims that FedFly can reduce training time by up to\n33% when a device moves after 50% of the training is completed, and by up to\n45% when 90% of the training is completed when compared to state-of-the-art\noffloading approach in FL. FedFly has negligible overhead of up to two seconds\nand does not compromise accuracy. Finally, we highlight a number of open\nresearch issues for further investigation. FedFly can be downloaded from\nhttps://github.com/qub-blesson/FedFly.\n","authors":["Rehmat Ullah","Di Wu","Paul Harvey","Peter Kilpatrick","Ivor Spence","Blesson Varghese"],"pdf_url":"https://arxiv.org/pdf/2111.01516v2.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2207.07223v1","updated":"2022-07-14T22:46:43Z","published":"2022-07-14T22:46:43Z","title":"Accelerated Federated Learning with Decoupled Adaptive Optimization","summary":"  The federated learning (FL) framework enables edge clients to collaboratively\nlearn a shared inference model while keeping privacy of training data on\nclients. Recently, many heuristics efforts have been made to generalize\ncentralized adaptive optimization methods, such as SGDM, Adam, AdaGrad, etc.,\nto federated settings for improving convergence and accuracy. However, there is\nstill a paucity of theoretical principles on where to and how to design and\nutilize adaptive optimization methods in federated settings. This work aims to\ndevelop novel adaptive optimization methods for FL from the perspective of\ndynamics of ordinary differential equations (ODEs). First, an analytic\nframework is established to build a connection between federated optimization\nmethods and decompositions of ODEs of corresponding centralized optimizers.\nSecond, based on this analytic framework, a momentum decoupling adaptive\noptimization method, FedDA, is developed to fully utilize the global momentum\non each local iteration and accelerate the training convergence. Last but not\nleast, full batch gradients are utilized to mimic centralized optimization in\nthe end of the training process to ensure the convergence and overcome the\npossible inconsistency caused by adaptive optimization methods.\n","authors":["Jiayin Jin","Jiaxiang Ren","Yang Zhou","Lingjuan Lyu","Ji Liu","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2207.07223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08220v2","updated":"2022-07-14T22:43:44Z","published":"2021-10-15T17:31:10Z","title":"Combining Diverse Feature Priors","summary":"  To improve model generalization, model designers often restrict the features\nthat their models use, either implicitly or explicitly. In this work, we\nexplore the design space of leveraging such feature priors by viewing them as\ndistinct perspectives on the data. Specifically, we find that models trained\nwith diverse sets of feature priors have less overlapping failure modes, and\ncan thus be combined more effectively. Moreover, we demonstrate that jointly\ntraining such models on additional (unlabeled) data allows them to correct each\nother's mistakes, which, in turn, leads to better generalization and resilience\nto spurious correlations. Code available at\nhttps://github.com/MadryLab/copriors\n","authors":["Saachi Jain","Dimitris Tsipras","Aleksander Madry"],"pdf_url":"https://arxiv.org/pdf/2110.08220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07222v1","updated":"2022-07-14T22:36:10Z","published":"2022-07-14T22:36:10Z","title":"Assortment Optimization with Customer Choice Modeling in a Crowdfunding\n  Setting","summary":"  Crowdfunding, which is the act of raising funds from a large number of\npeople's contributions, is among the most popular research topics in economic\ntheory. Due to the fact that crowdfunding platforms (CFPs) have facilitated the\nprocess of raising funds by offering several features, we should take their\nexistence and survival in the marketplace into account. In this study, we\ninvestigated the significant role of platform features in a customer behavioral\nchoice model. In particular, we proposed a multinomial logit model to describe\nthe customers' (backers') behavior in a crowdfunding setting. We proceed by\ndiscussing the revenue-sharing model in these platforms. For this purpose, we\nconclude that an assortment optimization problem could be of major importance\nin order to maximize the platforms' revenue. We were able to derive a\nreasonable amount of data in some cases and implement two well-known machine\nlearning methods such as multivariate regression and classification problems to\npredict the best assortments the platform could offer to every arriving\ncustomer. We compared the results of these two methods and investigated how\nwell they perform in all cases.\n","authors":["Fatemeh Nosrat"],"pdf_url":"https://arxiv.org/pdf/2207.07222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06500v2","updated":"2022-07-14T22:14:17Z","published":"2021-10-13T05:15:00Z","title":"Differentially Private Fine-tuning of Language Models","summary":"  We give simpler, sparser, and faster algorithms for differentially private\nfine-tuning of large-scale pre-trained language models, which achieve the\nstate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.\nWe propose a meta-framework for this problem, inspired by the recent success of\nhighly parameter-efficient methods for fine-tuning. Our experiments show that\ndifferentially private adaptations of these approaches outperform previous\nprivate algorithms in three important dimensions: utility, privacy, and the\ncomputational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models.\nFor example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using\nRoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of\n$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large\nachieves an accuracy of $90.2\\%$. Our findings are similar for natural language\ngeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,\nGPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8\nrespectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the\nnon-private baseline is $48.1$. All our experiments suggest that larger models\nare better suited for private fine-tuning: while they are well known to achieve\nsuperior accuracy non-privately, we find that they also better maintain their\naccuracy when privacy is introduced.\n","authors":["Da Yu","Saurabh Naik","Arturs Backurs","Sivakanth Gopi","Huseyin A. Inan","Gautam Kamath","Janardhan Kulkarni","Yin Tat Lee","Andre Manoel","Lukas Wutschitz","Sergey Yekhanin","Huishuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2110.06500v2.pdf","comment":"ICLR 2022. Code available at\n  https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.07087v1","updated":"2022-07-14T17:40:00Z","published":"2022-07-14T17:40:00Z","title":"Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated\n  Neural Text Retrievers","summary":"  Prompt tuning attempts to update few task-specific parameters in pre-trained\nmodels. It has achieved comparable performance to fine-tuning of the full\nparameter set on both language understanding and generation tasks. In this\nwork, we study the problem of prompt tuning for neural text retrievers. We\nintroduce parameter-efficient prompt tuning for text retrieval across\nin-domain, cross-domain, and cross-topic settings. Through an extensive\nanalysis, we show that the strategy can mitigate the two issues --\nparameter-inefficiency and weak generalizability -- faced by fine-tuning based\nretrieval methods. Notably, it can significantly improve the out-of-domain\nzero-shot generalization of the retrieval models. By updating only 0.1% of the\nmodel parameters, the prompt tuning strategy can help retrieval models achieve\nbetter generalization performance than traditional methods in which all\nparameters are updated. Finally, to facilitate research on retrievers'\ncross-topic generalizability, we curate and release an academic retrieval\ndataset with 18K query-results pairs in 87 topics, making it the largest\ntopic-specific one to date.\n","authors":["Weng Lam Tam","Xiao Liu","Kaixuan Ji","Lilong Xue","Xingjian Zhang","Yuxiao Dong","Jiahua Liu","Maodi Hu","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2207.07087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.06197v3","updated":"2022-07-14T12:52:50Z","published":"2021-07-25T09:10:10Z","title":"A comparison of latent semantic analysis and correspondence analysis of\n  document-term matrices","summary":"  Latent semantic analysis (LSA) and correspondence analysis (CA) are two\ntechniques that use a singular value decomposition (SVD) for dimensionality\nreduction. LSA has been extensively used to obtain low-dimensional\nrepresentations that capture relationships among documents and terms. In this\narticle, we present a theoretical analysis and comparison of the two techniques\nin the context of document-term matrices. We show that CA has some attractive\nproperties as compared to LSA, for instance that effects of margins arising\nfrom differing document-lengths and term-frequencies are effectively\neliminated, so that the CA solution is optimally suited to focus on\nrelationships among documents and terms. A unifying framework is proposed that\nincludes both CA and LSA as special cases. We empirically compare CA to various\nLSA based methods on text categorization in English and authorship attribution\non historical Dutch texts, and find that CA performs significantly better. We\nalso apply CA to a long-standing question regarding the authorship of the Dutch\nnational anthem Wilhelmus and provide further support that it can be attributed\nto the author Datheen, amongst several contenders.\n","authors":["Qianqian Qi","David J. Hessen","Tejaswini Deoskar","Peter G. M. van der Heijden"],"pdf_url":"https://arxiv.org/pdf/2108.06197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06674v1","updated":"2022-07-14T05:59:58Z","published":"2022-07-14T05:59:58Z","title":"Reinforced Path Reasoning for Counterfactual Explainable Recommendation","summary":"  Counterfactual explanations interpret the recommendation mechanism via\nexploring how minimal alterations on items or users affect the recommendation\ndecisions. Existing counterfactual explainable approaches face huge search\nspace and their explanations are either action-based (e.g., user click) or\naspect-based (i.e., item description). We believe item attribute-based\nexplanations are more intuitive and persuadable for users since they explain by\nfine-grained item demographic features (e.g., brand). Moreover, counterfactual\nexplanation could enhance recommendations by filtering out negative items.\n  In this work, we propose a novel Counterfactual Explainable Recommendation\n(CERec) to generate item attribute-based counterfactual explanations meanwhile\nto boost recommendation performance. Our CERec optimizes an explanation policy\nupon uniformly searching candidate counterfactuals within a reinforcement\nlearning environment. We reduce the huge search space with an adaptive path\nsampler by using rich context information of a given knowledge graph. We also\ndeploy the explanation policy to a recommendation model to enhance the\nrecommendation. Extensive explainability and recommendation evaluations\ndemonstrate CERec's ability to provide explanations consistent with user\npreferences and maintain improved recommendations. We release our code at\nhttps://github.com/Chrystalii/CERec.\n","authors":["Xiangmeng Wang","Qian Li","Dianer Yu","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2207.06674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06652v1","updated":"2022-07-14T04:29:54Z","published":"2022-07-14T04:29:54Z","title":"Every Preference Changes Differently: Neural Multi-Interest Preference\n  Model with Temporal Dynamics for Recommendation","summary":"  User embeddings (vectorized representations of a user) are essential in\nrecommendation systems. Numerous approaches have been proposed to construct a\nrepresentation for the user in order to find similar items for retrieval tasks,\nand they have been proven effective in industrial recommendation systems as\nwell. Recently people have discovered the power of using multiple embeddings to\nrepresent a user, with the hope that each embedding represents the user's\ninterest in a certain topic. With multi-interest representation, it's important\nto model the user's preference over the different topics and how the preference\nchange with time. However, existing approaches either fail to estimate the\nuser's affinity to each interest or unreasonably assume every interest of every\nuser fades with an equal rate with time, thus hurting the recall of candidate\nretrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,\nan approach that not only produces multi-interest for users by using the user's\nsequential engagement more effectively but also automatically learns a set of\nweights to represent the preference over each embedding so that the candidates\ncan be retrieved from each interest proportionally. Extensive experiments have\nbeen done on various industrial-scale datasets to demonstrate the effectiveness\nof our approach.\n","authors":["Hui Shi","Yupeng Gu","Yitong Zhou","Bo Zhao","Sicun Gao","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.06652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11966v2","updated":"2022-07-14T20:41:04Z","published":"2022-04-25T21:04:46Z","title":"Estimating and Penalizing Induced Preference Shifts in Recommender\n  Systems","summary":"  The content that a recommender system (RS) shows to users influences them.\nTherefore, when choosing a recommender to deploy, one is implicitly also\nchoosing to induce specific internal states in users. Even more, systems\ntrained via long-horizon optimization will have direct incentives to manipulate\nusers: in this work, we focus on the incentive to shift user preferences so\nthey are easier to satisfy. We argue that - before deployment - system\ndesigners should: estimate the shifts a recommender would induce; evaluate\nwhether such shifts would be undesirable; and perhaps even actively optimize to\navoid problematic shifts. These steps involve two challenging ingredients:\nestimation requires anticipating how hypothetical algorithms would influence\nuser preferences if deployed - we do this by using historical user interaction\ndata to train a predictive user model which implicitly contains their\npreference dynamics; evaluation and optimization additionally require metrics\nto assess whether such influences are manipulative or otherwise unwanted - we\nuse the notion of \"safe shifts\", that define a trust region within which\nbehavior is safe: for instance, the natural way in which users would shift\nwithout interference from the system could be deemed \"safe\". In simulated\nexperiments, we show that our learned preference dynamics model is effective in\nestimating user preferences and how they would respond to new recommenders.\nAdditionally, we show that recommenders that optimize for staying in the trust\nregion can avoid manipulative behaviors while still generating engagement.\n","authors":["Micah Carroll","Anca Dragan","Stuart Russell","Dylan Hadfield-Menell"],"pdf_url":"https://arxiv.org/pdf/2204.11966v2.pdf","comment":"Accepted to ICML 2022 (Spotlight)"},{"id":"http://arxiv.org/abs/2207.07187v1","updated":"2022-07-14T20:15:11Z","published":"2022-07-14T20:15:11Z","title":"NASRec: Weight Sharing Neural Architecture Search for Recommender\n  Systems","summary":"  The rise of deep neural networks provides an important driver in optimizing\nrecommender systems. However, the success of recommender systems lies in\ndelicate architecture fabrication, and thus calls for Neural Architecture\nSearch (NAS) to further improve its modeling. We propose NASRec, a paradigm\nthat trains a single supernet and efficiently produces abundant\nmodels/sub-architectures by weight sharing. To overcome the data multi-modality\nand architecture heterogeneity challenges in recommendation domain, NASRec\nestablishes a large supernet (i.e., search space) to search the full\narchitectures, with the supernet incorporating versatile operator choices and\ndense connectivity minimizing human prior for flexibility. The scale and\nheterogeneity in NASRec impose challenges in search, such as training\ninefficiency, operator-imbalance, and degraded rank correlation. We tackle\nthese challenges by proposing single-operator any-connection sampling,\noperator-balancing interaction modules, and post-training fine-tuning. Our\nresults on three Click-Through Rates (CTR) prediction benchmarks show that\nNASRec can outperform both manually designed models and existing NAS methods,\nachieving state-of-the-art performance.\n","authors":["Tunhou Zhang","Dehua Cheng","Yuchen He","Zhengxing Chen","Xiaoliang Dai","Liang Xiong","Feng Yan","Hai Li","Yiran Chen","Wei Wen"],"pdf_url":"https://arxiv.org/pdf/2207.07187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07149v1","updated":"2022-07-14T18:17:03Z","published":"2022-07-14T18:17:03Z","title":"Bug Fix Time Optimization Using Matrix Factorization and Iterative\n  Gale-Shaply Algorithms","summary":"  Bug triage is an essential task in software maintenance phase. It assigns\ndevelopers (fixers) to bug reports to fix them. This process is performed\nmanually by a triager, who analyzes developers profiles and submitted bug\nreports to make suitable assignments. Bug triaging process is time consuming\nthus automating this process is essential to improve the quality of software.\nPrevious work addressed triaging problem either as an information retrieval or\nclassification problem. This paper tackles this problem as a resource\nallocation problem, that aims at the best assignments of developers to bug\nreports, that reduces the total fixing time of the newly submitted bug reports,\nin addition to the even distribution of bug reports over developers. In this\npaper, a combination of matrix factorization and Gale Shapely algorithm,\nsupported by the differential evolution is firstly introduced to optimize the\ntotal fix time and normalize developers work load. Matrix factorization is used\nto establish a recommendation system for Gale-Shapley to make assignment\ndecisions. Differential evolution provides the best set of weights to build\ndevelopers score profiles. The proposed approach is assessed over three\nrepositories, Linux, Apache and Eclipse. Experimental results show that the\nproposed approach reduces the bug fixing time, in comparison to the manual\ntriage, by 80.67%, 23.61% and 60.22% over Linux, Eclipse and Apache\nrespectively. Moreover, the workload for the developers is uniform.\n","authors":["Madonna Mayez","Khaled Nagaty","Abeer Hamdy"],"pdf_url":"https://arxiv.org/pdf/2207.07149v1.pdf","comment":"14 page, 7 figures, 8 tables, 10 equations"},{"id":"http://arxiv.org/abs/2207.07910v1","updated":"2022-07-14T07:49:28Z","published":"2022-07-14T07:49:28Z","title":"Improving Multi-Interest Network with Stable Learning","summary":"  Modeling users' dynamic preferences from historical behaviors lies at the\ncore of modern recommender systems. Due to the diverse nature of user\ninterests, recent advances propose the multi-interest networks to encode\nhistorical behaviors into multiple interest vectors. In real scenarios, the\ncorresponding items of captured interests are usually retrieved together to get\nexposure and collected into training data, which produces dependencies among\ninterests. Unfortunately, multi-interest networks may incorrectly concentrate\non subtle dependencies among captured interests. Misled by these dependencies,\nthe spurious correlations between irrelevant interests and targets are\ncaptured, resulting in the instability of prediction results when training and\ntest distributions do not match. In this paper, we introduce the widely used\nHilbert-Schmidt Independence Criterion (HSIC) to measure the degree of\nindependence among captured interests and empirically show that the continuous\nincrease of HSIC may harm model performance. Based on this, we propose a novel\nmulti-interest network, named DEep Stable Multi-Interest Learning (DESMIL),\nwhich tries to eliminate the influence of subtle dependencies among captured\ninterests via learning weights for training samples and make model concentrate\nmore on underlying true causation. We conduct extensive experiments on public\nrecommendation datasets, a large-scale industrial dataset and the synthetic\ndatasets which simulate the out-of-distribution data. Experimental results\ndemonstrate that our proposed DESMIL outperforms state-of-the-art models by a\nsignificant margin. Besides, we also conduct comprehensive model analysis to\nreveal the reason why DESMIL works to a certain extent.\n","authors":["Zhaocheng Liu","Yingtao Luo","Di Zeng","Qiang Liu","Daqing Chang","Dongying Kong","Zhi Chen"],"pdf_url":"https://arxiv.org/pdf/2207.07910v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.07092v1","updated":"2022-07-14T17:44:49Z","published":"2022-07-14T17:44:49Z","title":"Explaining Image Enhancement Black-Box Methods through a Path Planning\n  Based Algorithm","summary":"  Nowadays, image-to-image translation methods, are the state of the art for\nthe enhancement of natural images. Even if they usually show high performance\nin terms of accuracy, they often suffer from several limitations such as the\ngeneration of artifacts and the scalability to high resolutions. Moreover,\ntheir main drawback is the completely black-box approach that does not allow to\nprovide the final user with any insight about the enhancement processes\napplied. In this paper we present a path planning algorithm which provides a\nstep-by-step explanation of the output produced by state of the art enhancement\nmethods, overcoming black-box limitation. This algorithm, called eXIE, uses a\nvariant of the A* algorithm to emulate the enhancement process of another\nmethod through the application of an equivalent sequence of enhancing\noperators. We applied eXIE to explain the output of several state-of-the-art\nmodels trained on the Five-K dataset, obtaining sequences of enhancing\noperators able to produce very similar results in terms of performance and\novercoming the huge limitation of poor interpretability of the best performing\nalgorithms.\n","authors":["Marco Cotogni","Claudio Cusano"],"pdf_url":"https://arxiv.org/pdf/2207.07092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07059v1","updated":"2022-07-14T16:58:47Z","published":"2022-07-14T16:58:47Z","title":"Semi-Supervised Temporal Action Detection with Proposal-Free Masking","summary":"  Existing temporal action detection (TAD) methods rely on a large number of\ntraining data with segment-level annotations. Collecting and annotating such a\ntraining set is thus highly expensive and unscalable. Semi-supervised TAD\n(SS-TAD) alleviates this problem by leveraging unlabeled videos freely\navailable at scale. However, SS-TAD is also a much more challenging problem\nthan supervised TAD, and consequently much under-studied. Prior SS-TAD methods\ndirectly combine an existing proposal-based TAD method and a SSL method. Due to\ntheir sequential localization (e.g, proposal generation) and classification\ndesign, they are prone to proposal error propagation. To overcome this\nlimitation, in this work we propose a novel Semi-supervised Temporal action\ndetection model based on PropOsal-free Temporal mask (SPOT) with a parallel\nlocalization (mask generation) and classification architecture. Such a novel\ndesign effectively eliminates the dependence between localization and\nclassification by cutting off the route for error propagation in-between. We\nfurther introduce an interaction mechanism between classification and\nlocalization for prediction refinement, and a new pretext task for\nself-supervised model pre-training. Extensive experiments on two standard\nbenchmarks show that our SPOT outperforms state-of-the-art alternatives, often\nby a large margin. The PyTorch implementation of SPOT is available at\nhttps://github.com/sauradip/SPOT\n","authors":["Sauradip Nag","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2207.07059v1.pdf","comment":"ECCV 2022; Code available at https://github.com/sauradip/SPOT"},{"id":"http://arxiv.org/abs/2207.06983v1","updated":"2022-07-14T15:06:37Z","published":"2022-07-14T15:06:37Z","title":"Multitrack Music Transformer: Learning Long-Term Dependencies in Music\n  with Diverse Instruments","summary":"  Existing approaches for generating multitrack music with transformer models\nhave been limited to either a small set of instruments or short music segments.\nThis is partly due to the memory requirements of the lengthy input sequences\nnecessitated by existing representations for multitrack music. In this work, we\npropose a compact representation that allows a diverse set of instruments while\nkeeping a short sequence length. Using our proposed representation, we present\nthe Multitrack Music Transformer (MTMT) for learning long-term dependencies in\nmultitrack music. In a subjective listening test, our proposed model achieves\ncompetitive quality on unconditioned generation against two baseline models. We\nalso show that our proposed model can generate samples that are twice as long\nas those produced by the baseline models, and, further, can do so in half the\ninference time. Moreover, we propose a new measure for analyzing musical\nself-attentions and show that the trained model learns to pay less attention to\nnotes that form a dissonant interval with the current note, yet attending more\nto notes that are 4N beats away from current. Finally, our findings provide a\nnovel foundation for future work exploring longer-form multitrack music\ngeneration and improving self-attentions for music. All source code and audio\nsamples can be found at https://salu133445.github.io/mtmt/ .\n","authors":["Hao-Wen Dong","Ke Chen","Shlomo Dubnov","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2207.06983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01832v2","updated":"2022-07-14T08:59:41Z","published":"2021-12-03T10:41:12Z","title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video\n  Retrieval","summary":"  In this paper we revisit \\emph{feature fusion}, an old-fashioned topic, in\nthe new context of text-to-video retrieval. Different from previous research\nthat considers feature fusion only at one end, let it be video or text, we aim\nfor feature fusion for both ends within a unified framework. We hypothesize\nthat optimizing the convex combination of the features is preferred to modeling\ntheir correlations by computationally heavy multi-head self attention. We\npropose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature\nfusion at both early and late stages and at both video and text ends, making it\na powerful method for exploiting diverse (off-the-shelf) features. The\ninterpretability of LAFF can be used for feature selection. Extensive\nexperiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and\nTRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video\nretrieval.\n","authors":["Fan Hu","Aozhu Chen","Ziyue Wang","Fangming Zhou","Jianfeng Dong","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2112.01832v2.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.06717v1","updated":"2022-07-14T07:59:45Z","published":"2022-07-14T07:59:45Z","title":"Layout-Aware Information Extraction for Document-Grounded Dialogue:\n  Dataset, Method and Demonstration","summary":"  Building document-grounded dialogue systems have received growing interest as\ndocuments convey a wealth of human knowledge and commonly exist in enterprises.\nWherein, how to comprehend and retrieve information from documents is a\nchallenging research problem. Previous work ignores the visual property of\ndocuments and treats them as plain text, resulting in incomplete modality. In\nthis paper, we propose a Layout-aware document-level Information Extraction\ndataset, LIE, to facilitate the study of extracting both structural and\nsemantic knowledge from visually rich documents (VRDs), so as to generate\naccurate responses in dialogue systems. LIE contains 62k annotations of three\nextraction tasks from 4,061 pages in product and official documents, becoming\nthe largest VRD-based information extraction dataset to the best of our\nknowledge. We also develop benchmark methods that extend the token-based\nlanguage model to consider layout features like humans. Empirical results show\nthat layout is critical for VRD-based extraction, and system demonstration also\nverifies that the extracted knowledge can help locate the answers that users\ncare about.\n","authors":["Zhenyu Zhang","Bowen Yu","Haiyang Yu","Tingwen Liu","Cheng Fu","Jingyang Li","Chengguang Tang","Jian Sun","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2207.06717v1.pdf","comment":"Accepted to ACM Multimedia (MM) Industry Track 2022"},{"id":"http://arxiv.org/abs/2203.13696v2","updated":"2022-07-14T01:30:07Z","published":"2022-03-25T15:04:51Z","title":"Speech-enhanced and Noise-aware Networks for Robust Speech Recognition","summary":"  Compensation for channel mismatch and noise interference is essential for\nrobust automatic speech recognition. Enhanced speech has been introduced into\nthe multi-condition training of acoustic models to improve their generalization\nability. In this paper, a noise-aware training framework based on two cascaded\nneural structures is proposed to jointly optimize speech enhancement and speech\nrecognition. The feature enhancement module is composed of a multi-task\nautoencoder, where noisy speech is decomposed into clean speech and noise. By\nconcatenating its enhanced, noise-aware, and noisy features for each frame, the\nacoustic-modeling module maps each feature-augmented frame into a triphone\nstate by optimizing the lattice-free maximum mutual information and cross\nentropy between the predicted and actual state sequences. On top of the\nfactorized time delay neural network (TDNN-F) and its convolutional variant\n(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error\nrate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared\nwith the best existing systems that use bigram and trigram language models for\ndecoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction\nof 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based\nsystem also outperforms the baseline CNN-TDNNF system on the AMI task.\n","authors":["Hung-Shin Lee","Pin-Yuan Chen","Yao-Fei Cheng","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2203.13696v2.pdf","comment":"submitted to ISCSLP 2022"},{"id":"http://arxiv.org/abs/2207.06580v1","updated":"2022-07-14T00:46:51Z","published":"2022-07-14T00:46:51Z","title":"Temporal Action Detection with Global Segmentation Mask Learning","summary":"  Existing temporal action detection (TAD) methods rely on generating an\noverwhelmingly large number of proposals per video. This leads to complex model\ndesigns due to proposal generation and/or per-proposal action instance\nevaluation and the resultant high computational cost. In this work, for the\nfirst time, we propose a proposal-free Temporal Action detection model with\nGlobal Segmentation mask (TAGS). Our core idea is to learn a global\nsegmentation mask of each action instance jointly at the full video length. The\nTAGS model differs significantly from the conventional proposal-based methods\nby focusing on global temporal representation learning to directly detect local\nstart and end points of action instances without proposals. Further, by\nmodeling TAD holistically rather than locally at the individual proposal level,\nTAGS needs a much simpler model architecture with lower computational cost.\nExtensive experiments show that despite its simpler design, TAGS outperforms\nexisting TAD methods, achieving new state-of-the-art performance on two\nbenchmarks. Importantly, it is ~ 20x faster to train and ~1.6x more efficient\nfor inference. Our PyTorch implementation of TAGS is available at\nhttps://github.com/sauradip/TAGS .\n","authors":["Sauradip Nag","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2207.06580v1.pdf","comment":"ECCV 2022; Code available at https://github.com/sauradip/TAGS"},{"id":"http://arxiv.org/abs/2207.07165v1","updated":"2022-07-14T18:51:17Z","published":"2022-07-14T18:51:17Z","title":"Estimating Emotion Contagion on Social Media via Localized Diffusion in\n  Dynamic Graphs","summary":"  We present a computational approach for estimating emotion contagion on\nsocial media networks. Built on a foundation of psychology literature, our\napproach estimates the degree to which the perceivers' emotional states\n(positive or negative) start to match those of the expressors, based on the\nlatter's content. We use a combination of deep learning and social network\nanalysis to model emotion contagion as a diffusion process in dynamic social\nnetwork graphs, taking into consideration key aspects like causality,\nhomophily, and interference. We evaluate our approach on user behavior data\nobtained from a popular social media platform for sharing short videos. We\nanalyze the behavior of 48 users over a span of 8 weeks (over 200k audio-visual\nshort posts analyzed) and estimate how contagious the users with whom they\nengage with are on social media. As per the theory of diffusion, we account for\nthe videos a user watches during this time (inflow) and the daily engagements;\nliking, sharing, downloading or creating new videos (outflow) to estimate\ncontagion. To validate our approach and analysis, we obtain human feedback on\nthese 48 social media platform users with an online study by collecting\nresponses of about 150 participants. We report users who interact with more\nnumber of creators on the platform are 12% less prone to contagion, and those\nwho consume more content of `negative' sentiment are 23% more prone to\ncontagion. We will publicly release our code upon acceptance.\n","authors":["Trisha Mittal","Puneet Mathur","Rohan Chandra","Apurva Bhatt","Vikram Gupta","Debdoot Mukherjee","Aniket Bera","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2207.07165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07162v1","updated":"2022-07-14T18:41:00Z","published":"2022-07-14T18:41:00Z","title":"Audio-guided Album Cover Art Generation with Genetic Algorithms","summary":"  Over 60,000 songs are released on Spotify every day, and the competition for\nthe listener's attention is immense. In that regard, the importance of\ncaptivating and inviting cover art cannot be underestimated, because it is\ndeeply entangled with a song's character and the artist's identity, and remains\none of the most important gateways to lead people to discover music. However,\ndesigning cover art is a highly creative, lengthy and sometimes expensive\nprocess that can be daunting, especially for non-professional artists. For this\nreason, we propose a novel deep-learning framework to generate cover art guided\nby audio features. Inspired by VQGAN-CLIP, our approach is highly flexible\nbecause individual components can easily be replaced without the need for any\nretraining. This paper outlines the architectural details of our models and\ndiscusses the optimization challenges that emerge from them. More specifically,\nwe will exploit genetic algorithms to overcome bad local minima and adversarial\nexamples. We find that our framework can generate suitable cover art for most\ngenres, and that the visual features adapt themselves to audio feature changes.\nGiven these results, we believe that our framework paves the road for\nextensions and more advanced applications in audio-guided visual generation\ntasks.\n","authors":["James Marien","Sam Leroux","Bart Dhoedt","Cedric De Boom"],"pdf_url":"https://arxiv.org/pdf/2207.07162v1.pdf","comment":"8 pages, 6 figures, 4 tables"}]},"2022-07-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2110.08214v3","updated":"2022-07-15T16:18:36Z","published":"2021-10-15T17:20:28Z","title":"From Start to Finish: Latency Reduction Strategies for Incremental\n  Speech Synthesis in Simultaneous Speech-to-Speech Translation","summary":"  Speech-to-speech translation (S2ST) converts input speech to speech in\nanother language. A challenge of delivering S2ST in real time is the\naccumulated delay between the translation and speech synthesis modules. While\nrecently incremental text-to-speech (iTTS) models have shown large quality\nimprovements, they typically require additional future text inputs to reach\noptimal performance. In this work, we minimize the initial waiting time of iTTS\nby adapting the upstream speech translator to generate high-quality pseudo\nlookahead for the speech synthesizer. After mitigating the initial delay, we\ndemonstrate that the duration of synthesized speech also plays a crucial role\non latency. We formalize this as a latency metric and then present a simple yet\neffective duration-scaling approach for latency reduction. Our approaches\nconsistently reduce latency by 0.2-0.5 second without sacrificing speech\ntranslation quality.\n","authors":["Danni Liu","Changhan Wang","Hongyu Gong","Xutai Ma","Yun Tang","Juan Pino"],"pdf_url":"https://arxiv.org/pdf/2110.08214v3.pdf","comment":"Accepted by Interspeech 2022"},{"id":"http://arxiv.org/abs/2207.07568v1","updated":"2022-07-15T16:15:46Z","published":"2022-07-15T16:15:46Z","title":"Reasoning about Actions over Visual and Linguistic Modalities: A Survey","summary":"  'Actions' play a vital role in how humans interact with the world and enable\nthem to achieve desired goals. As a result, most common sense (CS) knowledge\nfor humans revolves around actions. While 'Reasoning about Actions & Change'\n(RAC) has been widely studied in the Knowledge Representation community, it has\nrecently piqued the interest of NLP and computer vision researchers. This paper\nsurveys existing tasks, benchmark datasets, various techniques and models, and\ntheir respective performance concerning advancements in RAC in the vision and\nlanguage domain. Towards the end, we summarize our key takeaways, discuss the\npresent challenges facing this research area, and outline potential directions\nfor future research.\n","authors":["Shailaja Keyur Sampat","Maitreya Patel","Subhasish Das","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2207.07568v1.pdf","comment":"7 pages, 3 figures; This survey will be periodically updated with the\n  latest works in this area"},{"id":"http://arxiv.org/abs/2207.04906v2","updated":"2022-07-15T15:06:40Z","published":"2022-07-11T14:33:13Z","title":"HLT-MT: High-resource Language-specific Training for Multilingual Neural\n  Machine Translation","summary":"  Multilingual neural machine translation (MNMT) trained in multiple language\npairs has attracted considerable attention due to fewer model parameters and\nlower training costs by sharing knowledge among multiple languages.\nNonetheless, multilingual training is plagued by language interference\ndegeneration in shared parameters because of the negative interference among\ndifferent translation directions, especially on high-resource languages. In\nthis paper, we propose the multilingual translation model with the\nhigh-resource language-specific training (HLT-MT) to alleviate the negative\ninterference, which adopts the two-stage training with the language-specific\nselection mechanism. Specifically, we first train the multilingual model only\nwith the high-resource pairs and select the language-specific modules at the\ntop of the decoder to enhance the translation quality of high-resource\ndirections. Next, the model is further trained on all available corpora to\ntransfer knowledge from high-resource languages (HRLs) to low-resource\nlanguages (LRLs). Experimental results show that HLT-MT outperforms various\nstrong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic\nexperiments validate the effectiveness of our method in mitigating the negative\ninterference in multilingual training.\n","authors":["Jian Yang","Yuwei Yin","Shuming Ma","Dongdong Zhang","Zhoujun Li","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2207.04906v2.pdf","comment":"7 pages, 7 figures, IJCAI-ECAI 2022"},{"id":"http://arxiv.org/abs/2207.05564v2","updated":"2022-07-15T14:51:13Z","published":"2022-07-12T14:35:07Z","title":"Linear-time calculation of the expected sum of edge lengths in planar\n  linearizations of trees","summary":"  Dependency graphs have proven to be a very successful model to represent the\nsyntactic structure of sentences of human languages. In these graphs, widely\naccepted to be trees, vertices are words and arcs connect\nsyntactically-dependent words. The tendency of these dependencies to be short\nhas been demonstrated using random baselines for the sum of the lengths of the\nedges or its variants. A ubiquitous baseline is the expected sum in projective\norderings (wherein edges do not cross and the root word of the sentence is not\ncovered by any edge). It was shown that said expected value can be computed in\n$O(n)$ time. In this article we focus on planar orderings (where the root word\ncan be covered) and present two main results. First, we show the relationship\nbetween the expected sum in planar arrangements and the expected sum in\nprojective arrangements. Second, we also derive a $O(n)$-time algorithm to\ncalculate the expected value of the sum of edge lengths. These two results stem\nfrom another contribution of the present article, namely a characterization of\nplanarity that, given a sentence, yields either the number of planar\npermutations or an efficient algorithm to generate uniformly random planar\npermutations of the words. Our research paves the way for replicating past\nresearch on dependency distance minimization using random planar linearizations\nas random baseline.\n","authors":["Lluís Alemany-Puig","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2207.05564v2.pdf","comment":"Updated with comments from a colleague"},{"id":"http://arxiv.org/abs/2205.10586v2","updated":"2022-07-15T13:39:29Z","published":"2022-05-21T13:09:01Z","title":"Calibration of Natural Language Understanding Models with Venn--ABERS\n  Predictors","summary":"  Transformers, currently the state-of-the-art in natural language\nunderstanding (NLU) tasks, are prone to generate uncalibrated predictions or\nextreme probabilities, making the process of taking different decisions based\non their output relatively difficult. In this paper we propose to build several\ninductive Venn--ABERS predictors (IVAP), which are guaranteed to be well\ncalibrated under minimal assumptions, based on a selection of pre-trained\ntransformers. We test their performance over a set of diverse NLU tasks and\nshow that they are capable of producing well-calibrated probabilistic\npredictions that are uniformly spread over the [0,1] interval -- all while\nretaining the original model's predictive accuracy.\n","authors":["Patrizio Giovannotti"],"pdf_url":"https://arxiv.org/pdf/2205.10586v2.pdf","comment":"Accepted at the 11th Symposium on Conformal and Probabilistic\n  Prediction with Applications - COPA 2022"},{"id":"http://arxiv.org/abs/2203.12886v4","updated":"2022-07-15T11:48:34Z","published":"2022-03-24T07:15:24Z","title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool\n  Children","summary":"  Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about a children's growth and development. The\ncoronavirus pandemic has highlighted the necessity of online assessment for\npreschool children. One of the areas that should be tested is the ability to\nspeak. Because of the differences between children's and adults' voices,\nemploying Automatic Speech Recognition(ASR) systems is difficult since they are\npre-trained on adults' voices. We constructed an ASR for our cognitive test\nsystem to solve this issue using the Wav2Vec 2.0 model with a new pre-training\nobjective called Random Frequency Pitch(RFP). In addition, we used our new\ndataset to fine-tune our model for Meaningless Words(MW) and Rapid Automatic\nNaming(RAN) tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on\nthe Persian section of the CommonVoice dataset. Furthermore, our novel\nmethodology produces positive outcomes in zero- and few-shot scenarios.\n","authors":["Amirhossein Abaskohi","Fatemeh Mortazavi","Hadi Moradi"],"pdf_url":"https://arxiv.org/pdf/2203.12886v4.pdf","comment":"8 pages, 5 figures, 4 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2204.03286v2","updated":"2022-07-15T09:29:36Z","published":"2022-04-07T08:33:06Z","title":"Entailment Graph Learning with Textual Entailment and Soft Transitivity","summary":"  Typed entailment graphs try to learn the entailment relations between\npredicates from text and model them as edges between predicate nodes. The\nconstruction of entailment graphs usually suffers from severe sparsity and\nunreliability of distributional similarity. We propose a two-stage method,\nEntailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns\nlocal entailment relations by recognizing possible textual entailment between\ntemplate sentences formed by typed CCG-parsed predicates. Based on the\ngenerated local graph, EGT2 then uses three novel soft transitivity constraints\nto consider the logical transitivity in entailment structures. Experiments on\nbenchmark datasets show that EGT2 can well model the transitivity in entailment\ngraph to alleviate the sparsity issue, and lead to significant improvement over\ncurrent state-of-the-art methods.\n","authors":["Zhibin Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2204.03286v2.pdf","comment":"9 pages, 2 figures, accepted to ACL 2022 (main conference). 10 pages\n  for version 2"},{"id":"http://arxiv.org/abs/2204.03117v2","updated":"2022-07-15T08:51:00Z","published":"2022-04-06T22:18:12Z","title":"BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based\n  Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to align aspects and corresponding sentiments for\naspect-specific sentiment polarity inference. It is challenging because a\nsentence may contain multiple aspects or complicated (e.g., conditional,\ncoordinating, or adversative) relations. Recently, exploiting dependency syntax\ninformation with graph neural networks has been the most popular trend. Despite\nits success, methods that heavily rely on the dependency tree pose challenges\nin accurately modeling the alignment of the aspects and their words indicative\nof sentiment, since the dependency tree may provide noisy signals of unrelated\nassociations (e.g., the \"conj\" relation between \"great\" and \"dreadful\" in\nFigure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax\naware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully\nexploits the syntax information (e.g., phrase segmentation and hierarchical\nstructure) of the constituent tree of a sentence to model the sentiment-aware\ncontext of every single aspect (called intra-context) and the sentiment\nrelations across aspects (called inter-context) for learning. Experiments on\nfour benchmark datasets demonstrate that BiSyn-GAT+ outperforms the\nstate-of-the-art methods consistently.\n","authors":["Shuo Liang","Wei Wei","Xian-Ling Mao","Fei Wang","Zhiyong He"],"pdf_url":"https://arxiv.org/pdf/2204.03117v2.pdf","comment":"Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2112.04478v2","updated":"2022-07-15T08:31:45Z","published":"2021-12-08T18:58:16Z","title":"Prompting Visual-Language Models for Efficient Video Understanding","summary":"  Image-based visual-language (I-VL) pre-training has shown great success for\nlearning joint visual-textual representations from large-scale web data,\nrevealing remarkable ability for zero-shot generalisation. This paper presents\na simple but strong baseline to efficiently adapt the pre-trained I-VL model,\nand exploit its powerful ability for resource-hungry video understanding tasks,\nwith minimal training. Specifically, we propose to optimise a few random\nvectors, termed as continuous prompt vectors, that convert video-related tasks\ninto the same format as the pre-training objectives. In addition, to bridge the\ngap between static images and videos, temporal information is encoded with\nlightweight Transformers stacking on top of frame-wise visual features.\nExperimentally, we conduct extensive ablation studies to analyse the critical\ncomponents. On 10 public benchmarks of action recognition, action localisation,\nand text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,\nwe achieve competitive or state-of-the-art performance to existing methods,\ndespite optimising significantly fewer parameters.\n","authors":["Chen Ju","Tengda Han","Kunhao Zheng","Ya Zhang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2112.04478v2.pdf","comment":"ECCV 2022. Project page: https://ju-chen.github.io/efficient-prompt/"},{"id":"http://arxiv.org/abs/2203.17152v4","updated":"2022-07-15T08:22:20Z","published":"2022-03-31T16:24:51Z","title":"Perceptual Contrast Stretching on Target Feature for Speech Enhancement","summary":"  Speech enhancement (SE) performance has improved considerably owing to the\nuse of deep learning models as a base function. Herein, we propose a perceptual\ncontrast stretching (PCS) approach to further improve SE performance. The PCS\nis derived based on the critical band importance function and is applied to\nmodify the targets of the SE model. Specifically, the contrast of target\nfeatures is stretched based on perceptual importance, thereby improving the\noverall SE performance. Compared with post-processing-based implementations,\nincorporating PCS into the training phase preserves performance and reduces\nonline computation. Notably, PCS can be combined with different SE model\narchitectures and training criteria. Furthermore, PCS does not affect the\ncausality or convergence of SE model training. Experimental results on the\nVoiceBank-DEMAND dataset show that the proposed method can achieve\nstate-of-the-art performance on both causal (PESQ score = 3.07) and noncausal\n(PESQ score = 3.35) SE tasks.\n","authors":["Rong Chao","Cheng Yu","Szu-Wei Fu","Xugang Lu","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2203.17152v4.pdf","comment":"Accepted by Interspeech 2022"},{"id":"http://arxiv.org/abs/2206.11349v2","updated":"2022-07-15T07:15:31Z","published":"2022-05-31T08:43:07Z","title":"Prompt Injection: Parameterization of Fixed Inputs","summary":"  Recent works have shown that attaching prompts to the input is effective at\nconditioning Language Models (LM) to perform specific tasks. However, prompts\nare always included in the input text during inference, thus incurring\nsubstantial computational and memory overhead. Also, there is currently no\nstraightforward method of utilizing prompts that are longer than the maximum\ninput length of the LMs without incurring additional costs during inference. We\npropose Prompt Injection (PI), a novel formulation of injecting the prompt into\nthe parameters of an LM to be an efficient alternative to attaching fixed\nprompts to the input. We show that in scenarios with long fixed prompts, PI can\nbe up to 280 times more efficient in terms of total FLOPs than previous\napproaches. We further explore methodologies for PI and show promising results\nin persona-dependent conversation, semantic parsing, and zero-shot learning\nwith task instructions. Through these explorations, we show that PI can be a\npromising direction for conditioning language models, especially in scenarios\nwith long and fixed prompts.\n","authors":["Eunbi Choi","Yongrae Jo","Joel Jang","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2206.11349v2.pdf","comment":"PING results in Table 2 updated (bug fixed)"},{"id":"http://arxiv.org/abs/2207.00748v2","updated":"2022-07-15T07:02:55Z","published":"2022-07-02T06:23:25Z","title":"Sequence-aware multimodal page classification of Brazilian legal\n  documents","summary":"  The Brazilian Supreme Court receives tens of thousands of cases each\nsemester. Court employees spend thousands of hours to execute the initial\nanalysis and classification of those cases -- which takes effort away from\nposterior, more complex stages of the case management workflow. In this paper,\nwe explore multimodal classification of documents from Brazil's Supreme Court.\nWe train and evaluate our methods on a novel multimodal dataset of 6,510\nlawsuits (339,478 pages) with manual annotation assigning each page to one of\nsix classes. Each lawsuit is an ordered sequence of pages, which are stored\nboth as an image and as a corresponding text extracted through optical\ncharacter recognition. We first train two unimodal classifiers: a ResNet\npre-trained on ImageNet is fine-tuned on the images, and a convolutional\nnetwork with filters of multiple kernel sizes is trained from scratch on\ndocument texts. We use them as extractors of visual and textual features, which\nare then combined through our proposed Fusion Module. Our Fusion Module can\nhandle missing textual or visual input by using learned embeddings for missing\ndata. Moreover, we experiment with bi-directional Long Short-Term Memory\n(biLSTM) networks and linear-chain conditional random fields to model the\nsequential nature of the pages. The multimodal approaches outperform both\ntextual and visual classifiers, especially when leveraging the sequential\nnature of the pages.\n","authors":["Pedro H. Luz de Araujo","Ana Paula G. S. de Almeida","Fabricio A. Braz","Nilton C. da Silva","Flavio de Barros Vidal","Teofilo E. de Campos"],"pdf_url":"https://arxiv.org/pdf/2207.00748v2.pdf","comment":"11 pages, 6 figures. This preprint, which was originally written on 8\n  April 2021, has not undergone peer review or any post-submission improvements\n  or corrections. The Version of Record of this article is published in the\n  International Journal on Document Analysis and Recognition, and is available\n  online at https://doi.org/10.1007/s10032-022-00406-7 and\n  https://rdcu.be/cRvvV"},{"id":"http://arxiv.org/abs/2207.06414v2","updated":"2022-07-15T06:38:06Z","published":"2022-07-13T09:15:26Z","title":"Modeling Long-term Dependencies and Short-term Correlations in Patient\n  Journey Data with Temporal Attention Networks for Health Prediction","summary":"  Building models for health prediction based on Electronic Health Records\n(EHR) has become an active research area. EHR patient journey data consists of\npatient time-ordered clinical events/visits from patients. Most existing\nstudies focus on modeling long-term dependencies between visits, without\nexplicitly taking short-term correlations between consecutive visits into\naccount, where irregular time intervals, incorporated as auxiliary information,\nare fed into health prediction models to capture latent progressive patterns of\npatient journeys. We present a novel deep neural network with four modules to\ntake into account the contributions of various variables for health prediction:\ni) the Stacked Attention module strengthens the deep semantics in clinical\nevents within each patient journey and generates visit embeddings, ii) the\nShort-Term Temporal Attention module models short-term correlations between\nconsecutive visit embeddings while capturing the impact of time intervals\nwithin those visit embeddings, iii) the Long-Term Temporal Attention module\nmodels long-term dependencies between visit embeddings while capturing the\nimpact of time intervals within those visit embeddings, iv) and finally, the\nCoupled Attention module adaptively aggregates the outputs of Short-Term\nTemporal Attention and Long-Term Temporal Attention modules to make health\npredictions. Experimental results on MIMIC-III demonstrate superior predictive\naccuracy of our model compared to existing state-of-the-art methods, as well as\nthe interpretability and robustness of this approach. Furthermore, we found\nthat modeling short-term correlations contributes to local priors generation,\nleading to improved predictive modeling of patient journeys.\n","authors":["Yuxi Liu","Zhenhao Zhang","Antonio Jimeno Yepes","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2207.06414v2.pdf","comment":"10 pages, 4 figures, accepted at ACM BCB 2022"},{"id":"http://arxiv.org/abs/2207.07308v1","updated":"2022-07-15T06:21:35Z","published":"2022-07-15T06:21:35Z","title":"Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet\n  Text","summary":"  The wide use of social media and digital technologies facilitates sharing\nvarious news and information about events and activities. Despite sharing\npositive information misleading and false information is also spreading on\nsocial media. There have been efforts in identifying such misleading\ninformation both manually by human experts and automatic tools. Manual effort\ndoes not scale well due to the high volume of information, containing factual\nclaims, are appearing online. Therefore, automatically identifying check-worthy\nclaims can be very useful for human experts. In this study, we describe our\nparticipation in Subtask-1A: Check-worthiness of tweets (English, Dutch and\nSpanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing\nsteps and applied different models to identify whether a given text is worthy\nof fact checking or not. We use the oversampling technique to balance the\ndataset and applied SVM and Random Forest (RF) with TF-IDF representations. We\nalso used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models\nfor the experiments. We used BERT-m for the official submissions and our\nsystems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,\nrespectively. In further experiments, our evaluation shows that transformer\nmodels (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and\nEnglish languages where a different scenario is observed for Spanish.\n","authors":["Prerona Tarannum","Firoj Alam","Md. Arid Hasan","Sheak Rashed Haider Noori"],"pdf_url":"https://arxiv.org/pdf/2207.07308v1.pdf","comment":"Accepted in CLEF 2022"},{"id":"http://arxiv.org/abs/2203.04904v3","updated":"2022-07-15T04:16:21Z","published":"2022-03-09T17:26:53Z","title":"Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning","summary":"  Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n","authors":["Zhenhailong Wang","Hang Yu","Manling Li","Han Zhao","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2203.04904v3.pdf","comment":"4 pages, 4 figures, under review"},{"id":"http://arxiv.org/abs/2207.07278v1","updated":"2022-07-15T03:58:04Z","published":"2022-07-15T03:58:04Z","title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified\n  Learning Scheme and Dynamic Range Minimization","summary":"  With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.\n","authors":["Mengyin Liu","Chao Zhu","Hongyu Gao","Weibo Gu","Hongfa Wang","Wei Liu","Xu-cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2207.07278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07276v1","updated":"2022-07-15T03:52:00Z","published":"2022-07-15T03:52:00Z","title":"A Flexible Schema-Guided Dialogue Management Framework: From Friendly\n  Peer to Virtual Standardized Cancer Patient","summary":"  A schema-guided approach to dialogue management has been shown in recent work\nto be effective in creating robust customizable virtual agents capable of\nacting as friendly peers or task assistants. However, successful applications\nof these methods in open-ended, mixed-initiative domains remain elusive --\nparticularly within medical domains such as virtual standardized patients,\nwhere such complex interactions are commonplace -- and require more extensive\nand flexible dialogue management capabilities than previous systems provide. In\nthis paper, we describe a general-purpose schema-guided dialogue management\nframework used to develop SOPHIE, a virtual standardized cancer patient that\nallows a doctor to conveniently practice for interactions with patients. We\nconduct a crowdsourced evaluation of conversations between medical students and\nSOPHIE. Our agent is judged to produce responses that are natural, emotionally\nappropriate, and consistent with her role as a cancer patient. Furthermore, it\nsignificantly outperforms an end-to-end neural model fine-tuned on a human\nstandardized patient corpus, attesting to the advantages of a schema-guided\napproach.\n","authors":["Benjamin Kane","Catherine Giugno","Lenhart Schubert","Kurtis Haut","Caleb Wohn","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2207.07276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07255v1","updated":"2022-07-15T02:08:41Z","published":"2022-07-15T02:08:41Z","title":"Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights","summary":"  Investigating cooperativity of interlocutors is central in studying\npragmatics of dialogue. Models of conversation that only assume cooperative\nagents fail to explain the dynamics of strategic conversations. Thus, we\ninvestigate the ability of agents to identify non-cooperative interlocutors\nwhile completing a concurrent visual-dialogue task. Within this novel setting,\nwe study the optimality of communication strategies for achieving this\nmulti-task objective. We use the tools of learning theory to develop a\ntheoretical model for identifying non-cooperative interlocutors and apply this\ntheory to analyze different communication strategies. We also introduce a\ncorpus of non-cooperative conversations about images in the GuessWhat?! dataset\nproposed by De Vries et al. (2017). We use reinforcement learning to implement\nmultiple communication strategies in this context and find empirical results\nvalidate our theory.\n","authors":["Anthony Sicilia","Tristan Maidment","Pat Healy","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2207.07255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07243v1","updated":"2022-07-15T00:35:59Z","published":"2022-07-15T00:35:59Z","title":"LineCap: Line Charts for Data Visualization Captioning Models","summary":"  Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.\n","authors":["Anita Mahinpei","Zona Kostic","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2207.07243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07706v1","updated":"2022-07-15T19:04:43Z","published":"2022-07-15T19:04:43Z","title":"Probing Semantic Grounding in Language Models of Code with\n  Representational Similarity Analysis","summary":"  Representational Similarity Analysis is a method from cognitive neuroscience,\nwhich helps in comparing representations from two different sources of data. In\nthis paper, we propose using Representational Similarity Analysis to probe the\nsemantic grounding in language models of code. We probe representations from\nthe CodeBERT model for semantic grounding by using the data from the IBM\nCodeNet dataset. Through our experiments, we show that current pre-training\nmethods do not induce semantic grounding in language models of code, and\ninstead focus on optimizing form-based patterns. We also show that even a\nlittle amount of fine-tuning on semantically relevant tasks increases the\nsemantic grounding in CodeBERT significantly. Our ablations with the input\nmodality to the CodeBERT model show that using bimodal inputs (code and natural\nlanguage) over unimodal inputs (only code) gives better semantic grounding and\nsample efficiency during semantic fine-tuning. Finally, our experiments with\nsemantic perturbations in code reveal that CodeBERT is able to robustly\ndistinguish between semantically correct and incorrect code.\n","authors":["Shounak Naik","Rajaswa Patil","Swati Agarwal","Veeky Baths"],"pdf_url":"https://arxiv.org/pdf/2207.07706v1.pdf","comment":"Under review at ADMA 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2207.07646v1","updated":"2022-07-15T17:59:11Z","published":"2022-07-15T17:59:11Z","title":"Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision\n  and Language Models","summary":"  Utilizing vision and language models (VLMs) pre-trained on large-scale\nimage-text pairs is becoming a promising paradigm for open-vocabulary visual\nrecognition. In this work, we extend this paradigm by leveraging motion and\naudio that naturally exist in video. We present \\textbf{MOV}, a simple yet\neffective method for \\textbf{M}ultimodal \\textbf{O}pen-\\textbf{V}ocabulary\nvideo classification. In MOV, we directly use the vision encoder from\npre-trained VLMs with minimal modifications to encode video, optical flow and\naudio spectrogram. We design a cross-modal fusion mechanism to aggregate\ncomplimentary multimodal information. Experiments on Kinetics-700 and VGGSound\nshow that introducing flow or audio modality brings large performance gains\nover the pre-trained VLM and existing methods. Specifically, MOV greatly\nimproves the accuracy on base classes, while generalizes better on novel\nclasses. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video\nclassification benchmarks, significantly outperforming both traditional\nzero-shot methods and recent methods based on VLMs. Code and models will be\nreleased.\n","authors":["Rui Qian","Yeqing Li","Zheng Xu","Ming-Hsuan Yang","Serge Belongie","Yin Cui"],"pdf_url":"https://arxiv.org/pdf/2207.07646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07635v1","updated":"2022-07-15T17:50:51Z","published":"2022-07-15T17:50:51Z","title":"Is a Caption Worth a Thousand Images? A Controlled Study for\n  Representation Learning","summary":"  The development of CLIP [Radford et al., 2021] has sparked a debate on\nwhether language supervision can result in vision models with more transferable\nrepresentations than traditional image-only methods. Our work studies this\nquestion through a carefully controlled comparison of two approaches in terms\nof their ability to learn representations that generalize to downstream\nclassification tasks. We find that when the pre-training dataset meets certain\ncriteria -- it is sufficiently large and contains descriptive captions with low\nvariability -- image-only methods do not match CLIP's transfer performance,\neven when they are trained with more image data. However, contrary to what one\nmight expect, there are practical settings in which these criteria are not met,\nwherein added supervision through captions is actually detrimental. Motivated\nby our findings, we devise simple prescriptions to enable CLIP to better\nleverage the language information present in existing pre-training datasets.\n","authors":["Shibani Santurkar","Yann Dubois","Rohan Taori","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2207.07635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07629v1","updated":"2022-07-15T17:42:49Z","published":"2022-07-15T17:42:49Z","title":"GUSOT: Green and Unsupervised Single Object Tracking for Long Video\n  Sequences","summary":"  Supervised and unsupervised deep trackers that rely on deep learning\ntechnologies are popular in recent years. Yet, they demand high computational\ncomplexity and a high memory cost. A green unsupervised single-object tracker,\ncalled GUSOT, that aims at object tracking for long videos under a\nresource-constrained environment is proposed in this work. Built upon a\nbaseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT\ncontains two additional new modules: 1) lost object recovery, and 2)\ncolor-saliency-based shape proposal. They help resolve the tracking loss\nproblem and offer a more flexible object proposal, respectively. Thus, they\nenable GUSOT to achieve higher tracking accuracy in the long run. We conduct\nexperiments on the large-scale dataset LaSOT with long video sequences, and\nshow that GUSOT offers a lightweight high-performance tracking solution that\nfinds applications in mobile and edge computing platforms.\n","authors":["Zhiruo Zhou","Hongyu Fu","Suya You","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2207.07629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07622v1","updated":"2022-07-15T17:34:05Z","published":"2022-07-15T17:34:05Z","title":"Brain MRI study for glioma segmentation using convolutional neural\n  networks and original post-processing techniques with low computational\n  demand","summary":"  Gliomas are brain tumors composed of different highly heterogeneous\nhistological subregions. Image analysis techniques to identify relevant tumor\nsubstructures have high potential for improving patient diagnosis, treatment\nand prognosis. However, due to the high heterogeneity of gliomas, the\nsegmentation task is currently a major challenge in the field of medical image\nanalysis. In the present work, the database of the Brain Tumor Segmentation\n(BraTS) Challenge 2018, composed of multimodal MRI scans of gliomas, was\nstudied. A segmentation methodology based on the design and application of\nconvolutional neural networks (CNNs) combined with original post-processing\ntechniques with low computational demand was proposed. The post-processing\ntechniques were the main responsible for the results obtained in the\nsegmentations. The segmented regions were the whole tumor, the tumor core, and\nthe enhancing tumor core, obtaining averaged Dice coefficients equal to 0.8934,\n0.8376, and 0.8113, respectively. These results reached the state of the art in\nglioma segmentation determined by the winners of the challenge.\n","authors":["José Gerardo Suárez-García Javier Miguel Hernández-López","Eduardo Moreno-Barbosa","Benito de Celis-Alonso"],"pdf_url":"https://arxiv.org/pdf/2207.07622v1.pdf","comment":"34 pages, 12 tables, 23 figures"},{"id":"http://arxiv.org/abs/2207.07621v1","updated":"2022-07-15T17:32:37Z","published":"2022-07-15T17:32:37Z","title":"MegaPortraits: One-shot Megapixel Neural Head Avatars","summary":"  In this work, we advance the neural head avatar technology to the megapixel\nresolution while focusing on the particularly challenging task of cross-driving\nsynthesis, i.e., when the appearance of the driving image is substantially\ndifferent from the animated source image. We propose a set of new neural\narchitectures and training methods that can leverage both medium-resolution\nvideo data and high-resolution image data to achieve the desired levels of\nrendered image quality and generalization to novel views and motion. We\ndemonstrate that suggested architectures and methods produce convincing\nhigh-resolution neural avatars, outperforming the competitors in the\ncross-driving scenario. Lastly, we show how a trained high-resolution neural\navatar model can be distilled into a lightweight student model which runs in\nreal-time and locks the identities of neural avatars to several dozens of\npre-defined source images. Real-time operation and identity lock are essential\nfor many practical applications head avatar systems.\n","authors":["Nikita Drobyshev","Jenya Chelishev","Taras Khakhulin","Aleksei Ivakhnenko","Victor Lempitsky","Egor Zakharov"],"pdf_url":"https://arxiv.org/pdf/2207.07621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07619v1","updated":"2022-07-15T17:28:52Z","published":"2022-07-15T17:28:52Z","title":"A Non-Anatomical Graph Structure for isolated hand gesture separation in\n  continuous gesture sequences","summary":"  Continuous Hand Gesture Recognition (CHGR) has been extensively studied by\nresearchers in the last few decades. Recently, one model has been presented to\ndeal with the challenge of the boundary detection of isolated gestures in a\ncontinuous gesture video [17]. To enhance the model performance and also\nreplace the handcrafted feature extractor in the presented model in [17], we\npropose a GCN model and combine it with the stacked Bi-LSTM and Attention\nmodules to push the temporal information in the video stream. Considering the\nbreakthroughs of GCN models for skeleton modality, we propose a two-layer GCN\nmodel to empower the 3D hand skeleton features. Finally, the class\nprobabilities of each isolated gesture are fed to the post-processing module,\nborrowed from [17]. Furthermore, we replace the anatomical graph structure with\nsome non-anatomical graph structures. Due to the lack of a large dataset,\nincluding both the continuous gesture sequences and the corresponding isolated\ngestures, three public datasets in Dynamic Hand Gesture Recognition (DHGR),\nRKS-PERSIANSIGN, and ASLVID, are used for evaluation. Experimental results show\nthe superiority of the proposed model in dealing with isolated gesture\nboundaries detection in continuous gesture sequences\n","authors":["Razieh Rastgoo","Kourosh Kiani","Sergio Escalera"],"pdf_url":"https://arxiv.org/pdf/2207.07619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.09717v8","updated":"2022-07-15T17:27:20Z","published":"2021-08-22T13:21:58Z","title":"EKTVQA: Generalized use of External Knowledge to empower Scene Text in\n  Text-VQA","summary":"  The open-ended question answering task of Text-VQA often requires reading and\nreasoning about rarely seen or completely unseen scene-text content of an\nimage. We address this zero-shot nature of the problem by proposing the\ngeneralized use of external knowledge to augment our understanding of the scene\ntext. We design a framework to extract, validate, and reason with knowledge\nusing a standard multimodal transformer for vision language understanding\ntasks. Through empirical evidence and qualitative results, we demonstrate how\nexternal knowledge can highlight instance-only cues and thus help deal with\ntraining data bias, improve answer entity type correctness, and detect\nmultiword named entities. We generate results comparable to the\nstate-of-the-art on three publicly available datasets, under the constraints of\nsimilar upstream OCR systems and training data.\n","authors":["Arka Ujjal Dey","Ernest Valveny","Gaurav Harit"],"pdf_url":"https://arxiv.org/pdf/2108.09717v8.pdf","comment":"Accepted at IEEE Access"},{"id":"http://arxiv.org/abs/2207.07611v1","updated":"2022-07-15T17:10:48Z","published":"2022-07-15T17:10:48Z","title":"Position Prediction as an Effective Pretraining Strategy","summary":"  Transformers have gained increasing popularity in a wide range of\napplications, including Natural Language Processing (NLP), Computer Vision and\nSpeech Recognition, because of their powerful representational capacity.\nHowever, harnessing this representational capacity effectively requires a large\namount of data, strong regularization, or both, to mitigate overfitting.\nRecently, the power of the Transformer has been unlocked by self-supervised\npretraining strategies based on masked autoencoders which rely on\nreconstructing masked inputs, directly, or contrastively from unmasked content.\nThis pretraining strategy which has been used in BERT models in NLP, Wav2Vec\nmodels in Speech and, recently, in MAE models in Vision, forces the model to\nlearn about relationships between the content in different parts of the input\nusing autoencoding related objectives. In this paper, we propose a novel, but\nsurprisingly simple alternative to content reconstruction~-- that of predicting\nlocations from content, without providing positional information for it. Doing\nso requires the Transformer to understand the positional relationships between\ndifferent parts of the input, from their content alone. This amounts to an\nefficient implementation where the pretext task is a classification problem\namong all possible positions for each input token. We experiment on both Vision\nand Speech benchmarks, where our approach brings improvements over strong\nsupervised training baselines and is comparable to modern\nunsupervised/self-supervised pretraining methods. Our method also enables\nTransformers trained without position embeddings to outperform ones trained\nwith full position information.\n","authors":["Shuangfei Zhai","Navdeep Jaitly","Jason Ramapuram","Dan Busbridge","Tatiana Likhomanenko","Joseph Yitan Cheng","Walter Talbott","Chen Huang","Hanlin Goh","Joshua Susskind"],"pdf_url":"https://arxiv.org/pdf/2207.07611v1.pdf","comment":"Accepted to ICML 2022"},{"id":"http://arxiv.org/abs/2207.07609v1","updated":"2022-07-15T17:07:07Z","published":"2022-07-15T17:07:07Z","title":"DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and\n  Interconnected Self-driving","summary":"  Vehicle-to-Everything (V2X) network has enabled collaborative perception in\nautonomous driving, which is a promising solution to the fundamental defect of\nstand-alone intelligence including blind zones and long-range perception.\nHowever, the lack of datasets has severely blocked the development of\ncollaborative perception algorithms. In this work, we release DOLPHINS: Dataset\nfor cOllaborative Perception enabled Harmonious and INterconnected\nSelf-driving, as a new simulated large-scale various-scenario multi-view\nmulti-modality autonomous driving dataset, which provides a ground-breaking\nbenchmark platform for interconnected autonomous driving. DOLPHINS outperforms\ncurrent datasets in six dimensions: temporally-aligned images and point clouds\nfrom both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle\n(V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6\ntypical scenarios with dynamic weather conditions make the most various\ninterconnected autonomous driving dataset; meticulously selected viewpoints\nproviding full coverage of the key areas and every object; 42376 frames and\n292549 objects, as well as the corresponding 3D annotations, geo-positions, and\ncalibrations, compose the largest dataset for collaborative perception; Full-HD\nimages and 64-line LiDARs construct high-resolution data with sufficient\ndetails; well-organized APIs and open-source codes ensure the extensibility of\nDOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and\nmulti-view collaborative perception tasks on DOLPHINS. The experiment results\nshow that the raw-level fusion scheme through V2X communication can help to\nimprove the precision as well as to reduce the necessity of expensive LiDAR\nequipment on vehicles when RSUs exist, which may accelerate the popularity of\ninterconnected self-driving vehicles. DOLPHINS is now available on\nhttps://dolphins-dataset.net/.\n","authors":["Ruiqing Mao","Jingyu Guo","Yukuan Jia","Yuxuan Sun","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2207.07609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07604v1","updated":"2022-07-15T17:02:55Z","published":"2022-07-15T17:02:55Z","title":"Image and Texture Independent Deep Learning Noise Estimation using\n  Multiple Frames","summary":"  In this study, a novel multiple-frame based image and texture independent\nconvolutional Neural Network (CNN) noise estimator is introduced. The estimator\nworks.\n","authors":["Hikmet Kirmizitas","Nurettin Besli"],"pdf_url":"https://arxiv.org/pdf/2207.07604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07601v1","updated":"2022-07-15T16:57:43Z","published":"2022-07-15T16:57:43Z","title":"ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal\n  Feature Learning","summary":"  Many existing autonomous driving paradigms involve a multi-stage discrete\npipeline of tasks. To better predict the control signals and enhance user\nsafety, an end-to-end approach that benefits from joint spatial-temporal\nfeature learning is desirable. While there are some pioneering works on\nLiDAR-based input or implicit design, in this paper we formulate the problem in\nan interpretable vision-based setting. In particular, we propose a\nspatial-temporal feature learning scheme towards a set of more representative\nfeatures for perception, prediction and planning tasks simultaneously, which is\ncalled ST-P3. Specifically, an egocentric-aligned accumulation technique is\nproposed to preserve geometry information in 3D space before the bird's eye\nview transformation for perception; a dual pathway modeling is devised to take\npast motion variations into account for future prediction; a temporal-based\nrefinement unit is introduced to compensate for recognizing vision-based\nelements for planning. To the best of our knowledge, we are the first to\nsystematically investigate each part of an interpretable end-to-end\nvision-based autonomous driving system. We benchmark our approach against\nprevious state-of-the-arts on both open-loop nuScenes dataset as well as\nclosed-loop CARLA simulation. The results show the effectiveness of our method.\nSource code, model and protocol details are made publicly available at\nhttps://github.com/OpenPerceptionX/ST-P3.\n","authors":["Shengchao Hu","Li Chen","Penghao Wu","Hongyang Li","Junchi Yan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2207.07601v1.pdf","comment":"Project page: https://github.com/OpenPerceptionX/ST-P3"},{"id":"http://arxiv.org/abs/2207.07596v1","updated":"2022-07-15T16:50:11Z","published":"2022-07-15T16:50:11Z","title":"Mobile Keystroke Biometrics Using Transformers","summary":"  Behavioural biometrics have proven to be effective against identity theft as\nwell as be considered user-friendly authentication methods. One of the most\npopular traits in the literature is keystroke dynamics due to the large\ndeployment of computers and mobile devices in our society. This paper focuses\non improving keystroke biometric systems on the free-text scenario. This\nscenario is characterised as very challenging due to the uncontrolled text\nconditions, the influential of the user's emotional and physical state, and the\nin-use application. To overcome these drawbacks, methods based on deep learning\nsuch as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks\n(RNNs) have been proposed in the literature, outperforming traditional machine\nlearning methods. However, these architectures still have aspects that need to\nbe reviewed and improved. To the best of our knowledge, this is the first study\nthat proposes keystroke biometric systems based on Transformers. The proposed\nTransformer architecture has achieved Equal Error Rate (EER) values of 3.84% in\nthe popular Aalto mobile keystroke database using only 5 enrolment sessions,\noutperforming in large margin other state-of-the-art approaches in the\nliterature.\n","authors":["Giuseppe Stragapede","Paula Delgado-Santos","Ruben Tolosana","Ruben Vera-Rodriguez","Richard Guest","Aythami Morales"],"pdf_url":"https://arxiv.org/pdf/2207.07596v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2205.01931v2","updated":"2022-07-15T16:23:53Z","published":"2022-05-04T08:06:55Z","title":"Self-supervised learning in non-small cell lung cancer discovers novel\n  morphological clusters linked to patient outcome and molecular phenotypes","summary":"  Histopathological images provide the definitive source of cancer diagnosis,\ncontaining information used by pathologists to identify and subclassify\nmalignant disease, and to guide therapeutic choices. These images contain vast\namounts of information, much of which is currently unavailable to human\ninterpretation. Supervised deep learning approaches have been powerful for\nclassification tasks, but they are inherently limited by the cost and quality\nof annotations. Therefore, we developed Histomorphological Phenotype Learning,\nan unsupervised methodology, which requires no annotations and operates via the\nself-discovery of discriminatory image features in small image tiles. Tiles are\ngrouped into morphologically similar clusters which appear to represent\nrecurrent modes of tumor growth emerging under natural selection. These\nclusters have distinct features which can be identified using orthogonal\nmethods. Applied to lung cancer tissues, we show that they align closely with\npatient outcomes, with histopathologically recognised tumor types and growth\npatterns, and with transcriptomic measures of immunophenotype.\n","authors":["Adalberto Claudio Quiros","Nicolas Coudray","Anna Yeaton","Xinyu Yang","Luis Chiriboga","Afreen Karimkhan","Navneet Narula","Harvey Pass","Andre L. Moreira","John Le Quesne","Aristotelis Tsirigos","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2205.01931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.15000v2","updated":"2022-07-15T16:17:50Z","published":"2021-11-29T22:38:13Z","title":"Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable\n  Prototypes","summary":"  We present a deformable prototypical part network (Deformable ProtoPNet), an\ninterpretable image classifier that integrates the power of deep learning and\nthe interpretability of case-based reasoning. This model classifies input\nimages by comparing them with prototypes learned during training, yielding\nexplanations in the form of \"this looks like that.\" However, while previous\nmethods use spatially rigid prototypes, we address this shortcoming by\nproposing spatially flexible prototypes. Each prototype is made up of several\nprototypical parts that adaptively change their relative spatial positions\ndepending on the input image. Consequently, a Deformable ProtoPNet can\nexplicitly capture pose variations and context, improving both model accuracy\nand the richness of explanations provided. Compared to other case-based\ninterpretable models using prototypes, our approach achieves state-of-the-art\naccuracy and gives an explanation with greater context. The code is available\nat https://github.com/jdonnelly36/Deformable-ProtoPNet.\n","authors":["Jon Donnelly","Alina Jade Barnett","Chaofan Chen"],"pdf_url":"https://arxiv.org/pdf/2111.15000v2.pdf","comment":"This was published in CVPR 2022"},{"id":"http://arxiv.org/abs/2207.07553v1","updated":"2022-07-15T15:51:08Z","published":"2022-07-15T15:51:08Z","title":"CheXplaining in Style: Counterfactual Explanations for Chest X-rays\n  using StyleGAN","summary":"  Deep learning models used in medical image analysis are prone to raising\nreliability concerns due to their black-box nature. To shed light on these\nblack-box models, previous works predominantly focus on identifying the\ncontribution of input features to the diagnosis, i.e., feature attribution. In\nthis work, we explore counterfactual explanations to identify what patterns the\nmodels rely on for diagnosis. Specifically, we investigate the effect of\nchanging features within chest X-rays on the classifier's output to understand\nits decision mechanism. We leverage a StyleGAN-based approach (StyleEx) to\ncreate counterfactual explanations for chest X-rays by manipulating specific\nlatent directions in their latent space. In addition, we propose EigenFind to\nsignificantly reduce the computation time of generated explanations. We\nclinically evaluate the relevancy of our counterfactual explanations with the\nhelp of radiologists. Our code is publicly available.\n","authors":["Matan Atad","Vitalii Dmytrenko","Yitong Li","Xinyue Zhang","Matthias Keicher","Jan Kirschke","Bene Wiestler","Ashkan Khakzar","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2207.07553v1.pdf","comment":"Accepted to the ICML 2022 Interpretable Machine Learning in\n  Healthcare (IMLH) Workshop ----- Project website:\n  http://github.com/CAMP-eXplain-AI/Style-CheXplain"},{"id":"http://arxiv.org/abs/2207.07539v1","updated":"2022-07-15T15:31:16Z","published":"2022-07-15T15:31:16Z","title":"3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models","summary":"  3D point cloud models are widely applied in safety-critical scenes, which\ndelivers an urgent need to obtain more solid proofs to verify the robustness of\nmodels. Existing verification method for point cloud model is time-expensive\nand computationally unattainable on large networks. Additionally, they cannot\nhandle the complete PointNet model with joint alignment network (JANet) that\ncontains multiplication layers, which effectively boosts the performance of 3D\nmodels. This motivates us to design a more efficient and general framework to\nverify various architectures of point cloud models. The key challenges in\nverifying the large-scale complete PointNet models are addressed as dealing\nwith the cross-non-linearity operations in the multiplication layers and the\nhigh computational complexity of high-dimensional point cloud inputs and added\nlayers. Thus, we propose an efficient verification framework, 3DVerifier, to\ntackle both challenges by adopting a linear relaxation function to bound the\nmultiplication layer and combining forward and backward propagation to compute\nthe certified bounds of the outputs of the point cloud models. Our\ncomprehensive experiments demonstrate that 3DVerifier outperforms existing\nverification algorithms for 3D models in terms of both efficiency and accuracy.\nNotably, our approach achieves an orders-of-magnitude improvement in\nverification efficiency for the large network, and the obtained certified\nbounds are also significantly tighter than the state-of-the-art verifiers. We\nrelease our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use\nby the community.\n","authors":["Ronghui Mu","Wenjie Ruan","Leandro S. Marcolino","Qiang Ni"],"pdf_url":"https://arxiv.org/pdf/2207.07539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07522v1","updated":"2022-07-15T15:14:53Z","published":"2022-07-15T15:14:53Z","title":"Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow\n  Estimation","summary":"  Scene flow estimation, which extracts point-wise motion between scenes, is\nbecoming a crucial task in many computer vision tasks. However, all of the\nexisting estimation methods utilize only the unidirectional features,\nrestricting the accuracy and generality. This paper presents a novel scene flow\nestimation architecture using bidirectional flow embedding layers. The proposed\nbidirectional layer learns features along both forward and backward directions,\nenhancing the estimation performance. In addition, hierarchical feature\nextraction and warping improve the performance and reduce computational\noverhead. Experimental results show that the proposed architecture achieved a\nnew state-of-the-art record by outperforming other approaches with large margin\nin both FlyingThings3D and KITTI benchmarks. Codes are available at\nhttps://github.com/cwc1260/BiFlow.\n","authors":["Wencan Cheng","Jong Hwan Ko"],"pdf_url":"https://arxiv.org/pdf/2207.07522v1.pdf","comment":"Accepted as a conference paper at European Conference on Computer\n  Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2207.07517v1","updated":"2022-07-15T15:02:38Z","published":"2022-07-15T15:02:38Z","title":"On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution\n  Detection","summary":"  The ability to detect Out-of-Distribution (OOD) data is important in\nsafety-critical applications of deep learning. The aim is to separate\nIn-Distribution (ID) data drawn from the training distribution from OOD data\nusing a measure of uncertainty extracted from a deep neural network. Deep\nEnsembles are a well-established method of improving the quality of uncertainty\nestimates produced by deep neural networks, and have been shown to have\nsuperior OOD detection performance compared to single models. An existing\nintuition in the literature is that the diversity of Deep Ensemble predictions\nindicates distributional shift, and so measures of diversity such as Mutual\nInformation (MI) should be used for OOD detection. We show experimentally that\nthis intuition is not valid on ImageNet-scale OOD detection -- using MI leads\nto 30-40% worse %FPR@95 compared to single-model entropy on some OOD datasets.\nWe suggest an alternative explanation for Deep Ensembles' better OOD detection\nperformance -- OOD detection is binary classification and we are ensembling\ndiverse classifiers. As such we show that practically, even better OOD\ndetection performance can be achieved for Deep Ensembles by averaging\ntask-specific detection scores such as Energy over the ensemble.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07506v1","updated":"2022-07-15T14:39:57Z","published":"2022-07-15T14:39:57Z","title":"Augmenting Softmax Information for Selective Classification with\n  Out-of-Distribution Data","summary":"  Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05963v3","updated":"2022-07-15T14:34:17Z","published":"2022-06-13T08:19:54Z","title":"ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual\n  Simultaneous Localization and Mapping","summary":"  In this paper, a novel solution is introduced for visual Simultaneous\nLocalization and Mapping (vSLAM) that is built up of Deep Learning components.\nThe proposed architecture is a highly modular framework in which each component\noffers state of the art results in their respective fields of vision-based deep\nlearning solutions. The paper shows that with the synergic integration of these\nindividual building blocks, a functioning and efficient all-through deep neural\n(ATDN) vSLAM system can be created. The Embedding Distance Loss function is\nintroduced and using it the ATDN architecture is trained. The resulting system\nmanaged to achieve 4.4% translation and 0.0176 deg/m rotational error on a\nsubset of the KITTI dataset. The proposed architecture can be used for\nefficient and low-latency autonomous driving (AD) aiding database creation as\nwell as a basis for autonomous vehicle (AV) control.\n","authors":["Mátyás Szántó","György R. Bogár","László Vajta"],"pdf_url":"https://arxiv.org/pdf/2206.05963v3.pdf","comment":"Published in Periodica Polytechnica Electrical Engineering 11 pages"},{"id":"http://arxiv.org/abs/2203.03041v4","updated":"2022-07-15T14:28:49Z","published":"2022-03-06T20:09:19Z","title":"Highly Accurate Dichotomous Image Segmentation","summary":"  We present a systematic study on a new task called dichotomous image\nsegmentation (DIS) , which aims to segment highly accurate objects from natural\nimages. To this end, we collected the first large-scale DIS dataset, called\nDIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images\ncovering camouflaged, salient, or meticulous objects in various backgrounds.\nDIS is annotated with extremely fine-grained labels. Besides, we introduce a\nsimple intermediate supervision baseline (IS-Net) using both feature-level and\nmask-level guidance for DIS model training. IS-Net outperforms various\ncutting-edge baselines on the proposed DIS5K, making it a general self-learned\nsupervision network that can facilitate future research in DIS. Further, we\ndesign a new metric called human correction efforts (HCE) which approximates\nthe number of mouse clicking operations required to correct the false positives\nand false negatives. HCE is utilized to measure the gap between models and\nreal-world applications and thus can complement existing metrics. Finally, we\nconduct the largest-scale benchmark, evaluating 16 representative segmentation\nmodels, providing a more insightful discussion regarding object complexities,\nand showing several potential applications (e.g., background removal, art\ndesign, 3D reconstruction). Hoping these efforts can open up promising\ndirections for both academic and industries. Project page:\nhttps://xuebinqin.github.io/dis/index.html.\n","authors":["Xuebin Qin","Hang Dai","Xiaobin Hu","Deng-Ping Fan","Ling Shao","and Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.03041v4.pdf","comment":"29 pages, 18 figures, ECCV 2022"},{"id":"http://arxiv.org/abs/2207.07469v1","updated":"2022-07-15T13:25:47Z","published":"2022-07-15T13:25:47Z","title":"USegScene: Unsupervised Learning of Depth, Optical Flow and Ego-Motion\n  with Semantic Guidance and Coupled Networks","summary":"  In this paper we propose USegScene, a framework for semantically guided\nunsupervised learning of depth, optical flow and ego-motion estimation for\nstereo camera images using convolutional neural networks. Our framework\nleverages semantic information for improved regularization of depth and optical\nflow maps, multimodal fusion and occlusion filling considering dynamic rigid\nobject motions as independent SE(3) transformations. Furthermore, complementary\nto pure photo-metric matching, we propose matching of semantic features,\npixel-wise classes and object instance borders between the consecutive images.\nIn contrast to previous methods, we propose a network architecture that jointly\npredicts all outputs using shared encoders and allows passing information\nacross the task-domains, e.g., the prediction of optical flow can benefit from\nthe prediction of the depth. Furthermore, we explicitly learn the depth and\noptical flow occlusion maps inside the network, which are leveraged in order to\nimprove the predictions in therespective regions. We present results on the\npopular KITTI dataset and show that our approach outperforms other methods by a\nlarge margin.\n","authors":["Johan Vertens","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2207.07469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07466v1","updated":"2022-07-15T13:23:24Z","published":"2022-07-15T13:23:24Z","title":"DeepSolar tracker: towards unsupervised assessment with open-source data\n  of the accuracy of deep learning-based distributed PV mapping","summary":"  Photovoltaic (PV) energy is key to mitigating the current energy crisis.\nHowever, distributed PV generation, which amounts to half of the PV energy\ngeneration, makes it increasingly difficult for transmission system operators\n(TSOs) to balance the load and supply and avoid grid congestions. Indeed, in\nthe absence of measurements, estimating the distributed PV generation is tough.\nIn recent years, many remote sensing-based approaches have been proposed to map\ndistributed PV installations. However, to be applicable in industrial settings,\none needs to assess the accuracy of the mapping over the whole deployment area.\nWe build on existing work to propose an automated PV registry pipeline. This\npipeline automatically generates a dataset recording all distributed PV\ninstallations' location, area, installed capacity, and tilt angle. It only\nrequires aerial orthoimagery and topological data, both of which are freely\naccessible online. In order to assess the accuracy of the registry, we propose\nan unsupervised method based on the {\\it Registre national d'installation}\n(RNI), that centralizes all individual PV systems aggregated at communal level,\nenabling practitioners to assess the accuracy of the registry and eventually\nremove outliers. We deploy our model on 9 French {\\it d\\'epartements} covering\nmore than 50 000 square kilometers, providing the largest mapping of\ndistributed PV panels with this level of detail to date. We then demonstrate\nhow practitioners can use our unsupervised accuracy assessment method to assess\nthe accuracy of the outputs. In particular, we show how it can easily identify\noutliers in the detections. Overall, our approach paves the way for a safer\nintegration of deep learning-based pipelines for remote PV mapping. Code is\navailable at {\\tt https://github.com/gabrielkasmi/dsfrance}.\n","authors":["Gabriel Kasmi","Laurent Dubus","Philippe Blanc","Yves-Marie Saint-Drenan"],"pdf_url":"https://arxiv.org/pdf/2207.07466v1.pdf","comment":"10 pages, 3 figures, 3 tables. Accepted for Workshop on Machine\n  Learning for Earth Observation (MACLEAN), in Conjunction with the ECML/PKDD\n  2022"},{"id":"http://arxiv.org/abs/2112.09131v2","updated":"2022-07-15T13:15:16Z","published":"2021-12-16T18:59:53Z","title":"HODOR: High-level Object Descriptors for Object Re-segmentation in Video\n  Learned from Static Images","summary":"  Existing state-of-the-art methods for Video Object Segmentation (VOS) learn\nlow-level pixel-to-pixel correspondences between frames to propagate object\nmasks across video. This requires a large amount of densely annotated video\ndata, which is costly to annotate, and largely redundant since frames within a\nvideo are highly correlated. In light of this, we propose HODOR: a novel method\nthat tackles VOS by effectively leveraging annotated static images for\nunderstanding object appearance and scene context. We encode object instances\nand scene information from an image frame into robust high-level descriptors\nwhich can then be used to re-segment those objects in different frames. As a\nresult, HODOR achieves state-of-the-art performance on the DAVIS and\nYouTube-VOS benchmarks compared to existing methods trained without video\nannotations. Without any architectural modification, HODOR can also learn from\nvideo context around single annotated video frames by utilizing cyclic\nconsistency, whereas other methods rely on dense, temporally consistent\nannotations. Source code is available at: https://github.com/Ali2500/HODOR\n","authors":["Ali Athar","Jonathon Luiten","Alexander Hermans","Deva Ramanan","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2112.09131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07454v1","updated":"2022-07-15T13:03:47Z","published":"2022-07-15T13:03:47Z","title":"Multi-Object Tracking and Segmentation via Neural Message Passing","summary":"  Graphs offer a natural way to formulate Multiple Object Tracking (MOT) and\nMultiple Object Tracking and Segmentation (MOTS) within the\ntracking-by-detection paradigm. However, they also introduce a major challenge\nfor learning methods, as defining a model that can operate on such structured\ndomain is not trivial. In this work, we exploit the classical network flow\nformulation of MOT to define a fully differentiable framework based on Message\nPassing Networks (MPNs). By operating directly on the graph domain, our method\ncan reason globally over an entire set of detections and exploit contextual\nfeatures. It then jointly predicts both final solutions for the data\nassociation problem and segmentation masks for all objects in the scene while\nexploiting synergies between the two tasks. We achieve state-of-the-art results\nfor both tracking and segmentation in several publicly available datasets. Our\ncode is available at github.com/ocetintas/MPNTrackSeg.\n","authors":["Guillem Braso","Orcun Cetintas","Laura Leal-Taixe"],"pdf_url":"https://arxiv.org/pdf/2207.07454v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:1912.07515"},{"id":"http://arxiv.org/abs/2012.04708v2","updated":"2022-07-15T13:02:08Z","published":"2020-12-08T19:54:20Z","title":"ODFNet: Using orientation distribution functions to characterize 3D\n  point clouds","summary":"  Learning new representations of 3D point clouds is an active research area in\n3D vision, as the order-invariant point cloud structure still presents\nchallenges to the design of neural network architectures. Recent works explored\nlearning either global or local features or both for point clouds, however none\nof the earlier methods focused on capturing contextual shape information by\nanalysing local orientation distribution of points. In this paper, we leverage\non point orientation distributions around a point in order to obtain an\nexpressive local neighborhood representation for point clouds. We achieve this\nby dividing the spherical neighborhood of a given point into predefined cone\nvolumes, and statistics inside each volume are used as point features. In this\nway, a local patch can be represented by not only the selected point's nearest\nneighbors, but also considering a point density distribution defined along\nmultiple orientations around the point. We are then able to construct an\norientation distribution function (ODF) neural network that involves an\nODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet\nmodel achieves state-of the-art accuracy for object classification on\nModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS\ndatasets.\n","authors":["Yusuf H. Sahin","Alican Mertan","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2012.04708v2.pdf","comment":"The paper is under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2207.07418v1","updated":"2022-07-15T11:57:14Z","published":"2022-07-15T11:57:14Z","title":"LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds\n  Representing Laparoscopic Scenes","summary":"  The semantic segmentation of surgical scenes is a prerequisite for task\nautomation in robot assisted interventions. We propose LapSeg3D, a novel\nDNN-based approach for the voxel-wise annotation of point clouds representing\nsurgical scenes. As the manual annotation of training data is highly time\nconsuming, we introduce a semi-autonomous clustering-based pipeline for the\nannotation of the gallbladder, which is used to generate segmented labels for\nthe DNN. When evaluated against manually annotated data, LapSeg3D achieves an\nF1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo\nporcine livers. We show LapSeg3D to generalize accurately across different\ngallbladders and datasets recorded with different RGB-D camera systems.\n","authors":["Benjamin Alt","Christian Kunz","Darko Katic","Rayan Younis","Rainer Jäkel","Beat Peter Müller-Stich","Martin Wagner","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2207.07418v1.pdf","comment":"6 pages, 5 figures, accepted at the 2022 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan"},{"id":"http://arxiv.org/abs/2204.08721v2","updated":"2022-07-15T11:00:23Z","published":"2022-04-19T07:47:50Z","title":"Multimodal Token Fusion for Vision Transformers","summary":"  Many adaptations of transformers have emerged to address the single-modal\nvision tasks, where self-attention modules are stacked to handle input sources\nlike images. Intuitively, feeding multiple modalities of data to vision\ntransformers could improve the performance, yet the inner-modal attentive\nweights may also be diluted, which could thus undermine the final performance.\nIn this paper, we propose a multimodal token fusion method (TokenFusion),\ntailored for transformer-based vision tasks. To effectively fuse multiple\nmodalities, TokenFusion dynamically detects uninformative tokens and\nsubstitutes these tokens with projected and aggregated inter-modal features.\nResidual positional alignment is also adopted to enable explicit utilization of\nthe inter-modal alignments after fusion. The design of TokenFusion allows the\ntransformer to learn correlations among multimodal features, while the\nsingle-modal transformer architecture remains largely intact. Extensive\nexperiments are conducted on a variety of homogeneous and heterogeneous\nmodalities and demonstrate that TokenFusion surpasses state-of-the-art methods\nin three typical vision tasks: multimodal image-to-image translation, RGB-depth\nsemantic segmentation, and 3D object detection with point cloud and images. Our\ncode is available at https://github.com/yikaiw/TokenFusion.\n","authors":["Yikai Wang","Xinghao Chen","Lele Cao","Wenbing Huang","Fuchun Sun","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2204.08721v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2207.06831v2","updated":"2022-07-15T10:50:36Z","published":"2022-07-14T11:40:32Z","title":"iColoriT: Towards Propagating Local Hint to the Right Region in\n  Interactive Colorization by Leveraging Vision Transformer","summary":"  Point-interactive image colorization aims to colorize grayscale images when a\nuser provides the colors for specific locations. It is essential for\npoint-interactive colorization methods to appropriately propagate user-provided\ncolors (i.e., user hints) in the entire image to obtain a reasonably colorized\nimage with minimal user effort. However, existing approaches often produce\npartially colorized results due to the inefficient design of stacking\nconvolutional layers to propagate hints to distant relevant regions. To address\nthis problem, we present iColoriT, a novel point-interactive colorization\nVision Transformer capable of propagating user hints to relevant regions,\nleveraging the global receptive field of Transformers. The self-attention\nmechanism of Transformers enables iColoriT to selectively colorize relevant\nregions with only a few local hints. Our approach colorizes images in real-time\nby utilizing pixel shuffling, an efficient upsampling technique that replaces\nthe decoder architecture. Also, in order to mitigate the artifacts caused by\npixel shuffling with large upsampling ratios, we present the local stabilizing\nlayer. Extensive quantitative and qualitative results demonstrate that our\napproach highly outperforms existing methods for point-interactive\ncolorization, producing accurately colorized images with a user's minimal\neffort.\n","authors":["Sanghyeon Lee","Jooyeol Yun","Minho Park","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2207.06831v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.12175v3","updated":"2022-07-15T10:47:06Z","published":"2021-12-22T19:11:53Z","title":"Recur, Attend or Convolve? On Whether Temporal Modeling Matters for\n  Cross-Domain Robustness in Action Recognition","summary":"  Most action recognition models today are highly parameterized, and evaluated\non datasets with predominantly spatially distinct classes. It has also been\nshown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward\ntexture rather than shape in still image recognition tasks. Taken together,\nthis raises suspicion that large video models partly learn spurious\ncorrelations rather than to track relevant shapes over time to infer\ngeneralizable semantics from their movement. A natural way to avoid parameter\nexplosion when learning visual patterns over time is to make use of recurrence.\nIn this article, we empirically study whether the choice of low-level temporal\nmodeling has consequences for texture bias and cross-domain robustness. In\norder to enable a light-weight and systematic assessment of the ability to\ncapture temporal structure, not revealed from single frames, we provide the\nTemporal Shape (TS) dataset, as well as modified domains of Diving48 allowing\nfor the investigation of texture bias for video models. We find that across a\nvariety of model sizes, convolutional-recurrent and attention-based models show\nbetter out-of-domain robustness on TS than 3D CNNs. In domain shift experiments\non Diving48, our experiments indicate that 3D CNNs and attention-based models\nexhibit more texture bias than convolutional-recurrent models. Moreover,\nqualitative examples suggest that convolutional-recurrent models learn more\ncorrect class attributes from the diving data when compared to the other two\ntypes of models at the same global validation performance.\n","authors":["Sofia Broomé","Ernest Pokropek","Boyu Li","Hedvig Kjellström"],"pdf_url":"https://arxiv.org/pdf/2112.12175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2003.07311v7","updated":"2022-07-15T10:39:38Z","published":"2020-03-16T16:27:49Z","title":"clDice -- A Novel Topology-Preserving Loss Function for Tubular\n  Structure Segmentation","summary":"  Accurate segmentation of tubular, network-like structures, such as vessels,\nneurons, or roads, is relevant to many fields of research. For such structures,\nthe topology is their most important characteristic; particularly preserving\nconnectedness: in the case of vascular networks, missing a connected vessel\nentirely alters the blood-flow dynamics. We introduce a novel similarity\nmeasure termed centerlineDice (short clDice), which is calculated on the\nintersection of the segmentation masks and their (morphological) skeleta. We\ntheoretically prove that clDice guarantees topology preservation up to homotopy\nequivalence for binary 2D and 3D segmentation. Extending this, we propose a\ncomputationally efficient, differentiable loss function (soft-clDice) for\ntraining arbitrary neural segmentation networks. We benchmark the soft-clDice\nloss on five public datasets, including vessels, roads and neurons (2D and 3D).\nTraining on soft-clDice leads to segmentation with more accurate connectivity\ninformation, higher graph similarity, and better volumetric scores.\n","authors":["Suprosanna Shit","Johannes C. Paetzold","Anjany Sekuboyina","Ivan Ezhov","Alexander Unger","Andrey Zhylka","Josien P. W. Pluim","Ulrich Bauer","Bjoern H. Menze"],"pdf_url":"https://arxiv.org/pdf/2003.07311v7.pdf","comment":"* The authors Suprosanna Shit and Johannes C. Paetzold contributed\n  equally to the work"},{"id":"http://arxiv.org/abs/2203.13694v2","updated":"2022-07-15T10:26:37Z","published":"2022-03-25T15:00:38Z","title":"Implicit Neural Representations for Variable Length Human Motion\n  Generation","summary":"  We propose an action-conditional human motion generation method using\nvariational implicit neural representations (INR). The variational formalism\nenables action-conditional distributions of INRs, from which one can easily\nsample representations to generate novel human motion sequences. Our method\noffers variable-length sequence generation by construction because a part of\nINR is optimized for a whole sequence of arbitrary length with temporal\nembeddings. In contrast, previous works reported difficulties with modeling\nvariable-length sequences. We confirm that our method with a Transformer\ndecoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC\ndatasets in terms of realism and diversity of generated motions. Surprisingly,\neven our method with an MLP decoder consistently outperforms the\nstate-of-the-art Transformer-based auto-encoder. In particular, we show that\nvariable-length motions generated by our method are better than fixed-length\nmotions generated by the state-of-the-art method in terms of realism and\ndiversity. Code at https://github.com/PACerv/ImplicitMotion.\n","authors":["Pablo Cervantes","Yusuke Sekikawa","Ikuro Sato","Koichi Shinoda"],"pdf_url":"https://arxiv.org/pdf/2203.13694v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.03574v2","updated":"2022-07-15T10:19:22Z","published":"2022-06-18T04:14:38Z","title":"Demystifying the Adversarial Robustness of Random Transformation\n  Defenses","summary":"  Neural networks' lack of robustness against attacks raises concerns in\nsecurity-sensitive settings such as autonomous vehicles. While many\ncountermeasures may look promising, only a few withstand rigorous evaluation.\nDefenses using random transformations (RT) have shown impressive results,\nparticularly BaRT (Raff et al., 2019) on ImageNet. However, this type of\ndefense has not been rigorously evaluated, leaving its robustness properties\npoorly understood. Their stochastic properties make evaluation more challenging\nand render many proposed attacks on deterministic models inapplicable. First,\nwe show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation\nis ineffective and likely overestimates its robustness. We then attempt to\nconstruct the strongest possible RT defense through the informed selection of\ntransformations and Bayesian optimization for tuning their parameters.\nFurthermore, we create the strongest possible attack to evaluate our RT\ndefense. Our new attack vastly outperforms the baseline, reducing the accuracy\nby 83% compared to the 19% reduction by the commonly used EoT attack\n($4.3\\times$ improvement). Our result indicates that the RT defense on the\nImagenette dataset (a ten-class subset of ImageNet) is not robust against\nadversarial examples. Extending the study further, we use our new attack to\nadversarially train RT defense (called AdvRT), resulting in a large robustness\ngain. Code is available at\nhttps://github.com/wagner-group/demystify-random-transform.\n","authors":["Chawin Sitawarin","Zachary Golan-Strieb","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2207.03574v2.pdf","comment":"ICML 2022 (short presentation), AAAI 2022 AdvML Workshop (best paper,\n  oral presentation)"},{"id":"http://arxiv.org/abs/2207.07381v1","updated":"2022-07-15T10:00:43Z","published":"2022-07-15T10:00:43Z","title":"A Dual-Masked Auto-Encoder for Robust Motion Capture with\n  Spatial-Temporal Skeletal Token Completion","summary":"  Multi-person motion capture can be challenging due to ambiguities caused by\nsevere occlusion, fast body movement, and complex interactions. Existing\nframeworks build on 2D pose estimations and triangulate to 3D coordinates via\nreasoning the appearance, trajectory, and geometric consistencies among\nmulti-camera observations. However, 2D joint detection is usually incomplete\nand with wrong identity assignments due to limited observation angle, which\nleads to noisy 3D triangulation results. To overcome this issue, we propose to\nexplore the short-range autoregressive characteristics of skeletal motion using\ntransformer. First, we propose an adaptive, identity-aware triangulation module\nto reconstruct 3D joints and identify the missing joints for each identity. To\ngenerate complete 3D skeletal motion, we then propose a Dual-Masked\nAuto-Encoder (D-MAE) which encodes the joint status with both\nskeletal-structural and temporal position encoding for trajectory completion.\nD-MAE's flexible masking and encoding mechanism enable arbitrary skeleton\ndefinitions to be conveniently deployed under the same framework. In order to\ndemonstrate the proposed model's capability in dealing with severe data loss\nscenarios, we contribute a high-accuracy and challenging motion capture dataset\nof multi-person interactions with severe occlusion. Evaluations on both\nbenchmark and our new dataset demonstrate the efficiency of our proposed model,\nas well as its advantage against the other state-of-the-art methods.\n","authors":["Junkun Jiang","Jie Chen","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2207.07381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13388v2","updated":"2022-07-15T09:46:16Z","published":"2022-02-27T16:03:38Z","title":"PanoFlow: Learning 360° Optical Flow for Surrounding Temporal\n  Understanding","summary":"  Optical flow estimation is a basic task in self-driving and robotics systems,\nwhich enables to temporally interpret traffic scenes. Autonomous vehicles\nclearly benefit from the ultra-wide Field of View (FoV) offered by 360{\\deg}\npanoramic sensors. However, due to the unique imaging process of panoramic\ncameras, models designed for pinhole images do not directly generalize\nsatisfactorily to 360{\\deg} panoramic images. In this paper, we put forward a\nnovel network framework--PanoFlow, to learn optical flow for panoramic images.\nTo overcome the distortions introduced by equirectangular projection in\npanoramic transformation, we design a Flow Distortion Augmentation (FDA)\nmethod, which contains radial flow distortion (FDA-R) or equirectangular flow\ndistortion (FDA-E). We further look into the definition and properties of\ncyclic optical flow for panoramic videos, and hereby propose a Cyclic Flow\nEstimation (CFE) method by leveraging the cyclicity of spherical images to\ninfer 360{\\deg} optical flow and converting large displacement to relatively\nsmall displacement. PanoFlow is applicable to any existing flow estimation\nmethod and benefits from the progress of narrow-FoV flow estimation. In\naddition, we create and release a synthetic panoramic dataset Flow360 based on\nCARLA to facilitate training and quantitative analysis. PanoFlow achieves\nstate-of-the-art performance on the public OmniFlowNet and the established\nFlow360 benchmarks. Our proposed approach reduces the End-Point-Error (EPE) on\nFlow360 by 27.3%. On OmniFlowNet, PanoFlow achieves an EPE of 3.17 pixels, a\n55.5% error reduction from the best published result. We also qualitatively\nvalidate our method via a collection vehicle and a public real-world OmniPhotos\ndataset, indicating strong potential and robustness for real-world navigation\napplications. Code and dataset are publicly available at\nhttps://github.com/MasterHow/PanoFlow.\n","authors":["Hao Shi","Yifan Zhou","Kailun Yang","Xiaoting Yin","Ze Wang","Yaozu Ye","Zhe Yin","Shi Meng","Peng Li","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2202.13388v2.pdf","comment":"Code and dataset are publicly available at\n  https://github.com/MasterHow/PanoFlow"},{"id":"http://arxiv.org/abs/2207.07372v1","updated":"2022-07-15T09:38:56Z","published":"2022-07-15T09:38:56Z","title":"3D Instances as 1D Kernels","summary":"  We introduce a 3D instance representation, termed instance kernels, where\ninstances are represented by one-dimensional vectors that encode the semantic,\npositional, and shape information of 3D instances. We show that instance\nkernels enable easy mask inference by simply scanning kernels over the entire\nscenes, avoiding the heavy reliance on proposals or heuristic clustering\nalgorithms in standard 3D instance segmentation pipelines. The idea of instance\nkernel is inspired by recent success of dynamic convolutions in 2D/3D instance\nsegmentation. However, we find it non-trivial to represent 3D instances due to\nthe disordered and unstructured nature of point cloud data, e.g., poor instance\nlocalization can significantly degrade instance representation. To remedy this,\nwe construct a novel 3D instance encoding paradigm. First, potential instance\ncentroids are localized as candidates. Then, a candidate merging scheme is\ndevised to simultaneously aggregate duplicated candidates and collect context\naround the merged centroids to form the instance kernels. Once instance kernels\nare available, instance masks can be reconstructed via dynamic convolutions\nwhose weights are conditioned on instance kernels. The whole pipeline is\ninstantiated with a dynamic kernel network (DKNet). Results show that DKNet\noutperforms the state of the arts on both ScanNetV2 and S3DIS datasets with\nbetter instance localization. Code is available:\nhttps://github.com/W1zheng/DKNet.\n","authors":["Yizheng Wu","Min Shi","Shuaiyuan Du","Hao Lu","Zhiguo Cao","Weicai Zhong"],"pdf_url":"https://arxiv.org/pdf/2207.07372v1.pdf","comment":"Appearing in ECCV, 2022"},{"id":"http://arxiv.org/abs/2207.07370v1","updated":"2022-07-15T09:35:29Z","published":"2022-07-15T09:35:29Z","title":"CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with\n  Modality-Correlated Cross-Attention for Brain Tumor Segmentation","summary":"  Brain tumor segmentation (BTS) in magnetic resonance image (MRI) is crucial\nfor brain tumor diagnosis, cancer management and research purposes. With the\ngreat success of the ten-year BraTS challenges as well as the advances of CNN\nand Transformer algorithms, a lot of outstanding BTS models have been proposed\nto tackle the difficulties of BTS in different technical aspects. However,\nexisting studies hardly consider how to fuse the multi-modality images in a\nreasonable manner. In this paper, we leverage the clinical knowledge of how\nradiologists diagnose brain tumors from multiple MRI modalities and propose a\nclinical knowledge-driven brain tumor segmentation model, called CKD-TransBTS.\nInstead of directly concatenating all the modalities, we re-organize the input\nmodalities by separating them into two groups according to the imaging\nprinciple of MRI. A dual-branch hybrid encoder with the proposed\nmodality-correlated cross-attention block (MCCA) is designed to extract the\nmulti-modality image features. The proposed model inherits the strengths from\nboth Transformer and CNN with the local feature representation ability for\nprecise lesion boundaries and long-range feature extraction for 3D volumetric\nimages. To bridge the gap between Transformer and CNN features, we propose a\nTrans&CNN Feature Calibration block (TCFC) in the decoder. We compare the\nproposed model with five CNN-based models and six transformer-based models on\nthe BraTS 2021 challenge dataset. Extensive experiments demonstrate that the\nproposed model achieves state-of-the-art brain tumor segmentation performance\ncompared with all the competitors.\n","authors":["Jianwei Lin","Jiatai Lin","Cheng Lu","Hao Chen","Huan Lin","Bingchao Zhao","Zhenwei Shi","Bingjiang Qiu","Xipeng Pan","Zeyan Xu","Biao Huang","Changhong Liang","Guoqiang Han","Zaiyi Liu","Chu Han"],"pdf_url":"https://arxiv.org/pdf/2207.07370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12840v2","updated":"2022-07-15T09:32:28Z","published":"2022-05-24T05:50:49Z","title":"SALAD: Source-free Active Label-Agnostic Domain Adaptation for\n  Classification, Segmentation and Detection","summary":"  We present a novel method, SALAD, for the challenging vision task of adapting\na pre-trained \"source\" domain network to a \"target\" domain, with a small budget\nfor annotation in the \"target\" domain and a shift in the label space. Further,\nthe task assumes that the source data is not available for adaptation, due to\nprivacy concerns or otherwise. We postulate that such systems need to jointly\noptimize the dual task of (i) selecting fixed number of samples from the target\ndomain for annotation and (ii) transfer of knowledge from the pre-trained\nnetwork to the target domain. To do this, SALAD consists of a novel Guided\nAttention Transfer Network (GATN) and an active learning function, HAL. The\nGATN enables feature distillation from pre-trained network to the target\nnetwork, complemented with the target samples mined by HAL using\ntransfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it\nis task-agnostic, and can be applied across various visual tasks such as\nclassification, segmentation and detection; (ii) it can handle shifts in output\nlabel space from the pre-trained source network to the target domain; (iii) it\ndoes not require access to source data for adaptation. We conduct extensive\nexperiments across 3 visual tasks, viz. digits classification (MNIST, SVHN,\nVISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document\nlayout detection (PubLayNet to DSSE). We show that our source-free approach,\nSALAD, results in an improvement of 0.5%-31.3%(across datasets and tasks) over\nprior adaptation methods that assume access to large amounts of annotated\nsource data for adaptation.\n","authors":["Divya Kothandaraman","Sumit Shekhar","Abhilasha Sancheti","Manoj Ghuhan","Tripti Shukla","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2205.12840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07368v1","updated":"2022-07-15T09:30:32Z","published":"2022-07-15T09:30:32Z","title":"Trainable Joint Bilateral Filters for Enhanced Prediction Stability in\n  Low-dose CT","summary":"  Low-dose computed tomography (CT) denoising algorithms aim to enable reduced\npatient dose in routine CT acquisitions while maintaining high image quality.\nRecently, deep learning~(DL)-based methods were introduced, outperforming\nconventional denoising algorithms on this task due to their high model\ncapacity. However, for the transition of DL-based denoising to clinical\npractice, these data-driven approaches must generalize robustly beyond the seen\ntraining data. We, therefore, propose a hybrid denoising approach consisting of\na set of trainable joint bilateral filters (JBFs) combined with a convolutional\nDL-based denoising network to predict the guidance image. Our proposed\ndenoising pipeline combines the high model capacity enabled by DL-based feature\nextraction with the reliability of the conventional JBF. The pipeline's ability\nto generalize is demonstrated by training on abdomen CT scans without metal\nimplants and testing on abdomen scans with metal implants as well as on head CT\ndata. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in\nour pipeline, the denoising performance is improved by $10\\,\\%$/$82\\,\\%$ (RMSE)\nand $3\\,\\%$/$81\\,\\%$ (PSNR) in regions containing metal and by $6\\,\\%$/$78\\,\\%$\n(RMSE) and $2\\,\\%$/$4\\,\\%$ (PSNR) on head CT data, compared to the respective\nvanilla model. Concluding, the proposed trainable JBFs limit the error bound of\ndeep neural networks to facilitate the applicability of DL-based denoisers in\nlow-dose CT pipelines.\n","authors":["Fabian Wagner","Mareike Thies","Felix Denzinger","Mingxuan Gu","Mayank Patwari","Stefan Ploner","Noah Maul","Laura Pfaff","Yixing Huang","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2207.07368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07361v1","updated":"2022-07-15T09:20:13Z","published":"2022-07-15T09:20:13Z","title":"Registration based Few-Shot Anomaly Detection","summary":"  This paper considers few-shot anomaly detection (FSAD), a practical yet\nunder-studied setting for anomaly detection (AD), where only a limited number\nof normal images are provided for each category at training. So far, existing\nFSAD studies follow the one-model-per-category learning paradigm used for\nstandard AD, and the inter-category commonality has not been explored. Inspired\nby how humans detect anomalies, i.e., comparing an image in question to normal\nimages, we here leverage registration, an image alignment task that is\ninherently generalizable across categories, as the proxy task, to train a\ncategory-agnostic anomaly detection model. During testing, the anomalies are\nidentified by comparing the registered features of the test image and its\ncorresponding support (normal) images. As far as we know, this is the first\nFSAD method that trains a single generalizable model and requires no\nre-training or parameter fine-tuning for new categories. Experimental results\nhave shown that the proposed method outperforms the state-of-the-art FSAD\nmethods by 3%-8% in AUC on the MVTec and MPDD benchmarks.\n","authors":["Chaoqin Huang","Haoyan Guan","Aofan Jiang","Ya Zhang","Michael Spratling","Yan-Feng Wang"],"pdf_url":"https://arxiv.org/pdf/2207.07361v1.pdf","comment":"ECCV 2022 Oral; Code is available at\n  https://github.com/MediaBrain-SJTU/RegAD"},{"id":"http://arxiv.org/abs/2207.07351v1","updated":"2022-07-15T09:03:57Z","published":"2022-07-15T09:03:57Z","title":"Diverse Human Motion Prediction via Gumbel-Softmax Sampling from an\n  Auxiliary Space","summary":"  Diverse human motion prediction aims at predicting multiple possible future\npose sequences from a sequence of observed poses. Previous approaches usually\nemploy deep generative networks to model the conditional distribution of data,\nand then randomly sample outcomes from the distribution. While different\nresults can be obtained, they are usually the most likely ones which are not\ndiverse enough. Recent work explicitly learns multiple modes of the conditional\ndistribution via a deterministic network, which however can only cover a fixed\nnumber of modes within a limited range. In this paper, we propose a novel\nsampling strategy for sampling very diverse results from an imbalanced\nmultimodal distribution learned by a deep generative model. Our method works by\ngenerating an auxiliary space and smartly making randomly sampling from the\nauxiliary space equivalent to the diverse sampling from the target\ndistribution. We propose a simple yet effective network architecture that\nimplements this novel sampling strategy, which incorporates a Gumbel-Softmax\ncoefficient matrix sampling method and an aggressive diversity promoting hinge\nloss function. Extensive experiments demonstrate that our method significantly\nimproves both the diversity and accuracy of the samplings compared with\nprevious state-of-the-art sampling approaches. Code and pre-trained models are\navailable at https://github.com/Droliven/diverse_sampling.\n","authors":["Lingwei Dang","Yongwei Nie","Chengjiang Long","Qing Zhang","Guiqing Li"],"pdf_url":"https://arxiv.org/pdf/2207.07351v1.pdf","comment":"Paper and Supp of our work accepted by ACM MM 2022"},{"id":"http://arxiv.org/abs/2207.07347v1","updated":"2022-07-15T08:48:40Z","published":"2022-07-15T08:48:40Z","title":"Feasibility of Inconspicuous GAN-generated Adversarial Patches against\n  Object Detection","summary":"  Standard approaches for adversarial patch generation lead to noisy\nconspicuous patterns, which are easily recognizable by humans. Recent research\nhas proposed several approaches to generate naturalistic patches using\ngenerative adversarial networks (GANs), yet only a few of them were evaluated\non the object detection use case. Moreover, the state of the art mostly focuses\non suppressing a single large bounding box in input by overlapping it with the\npatch directly. Suppressing objects near the patch is a different, more complex\ntask. In this work, we have evaluated the existing approaches to generate\ninconspicuous patches. We have adapted methods, originally developed for\ndifferent computer vision tasks, to the object detection use case with YOLOv3\nand the COCO dataset. We have evaluated two approaches to generate naturalistic\npatches: by incorporating patch generation into the GAN training process and by\nusing the pretrained GAN. For both cases, we have assessed a trade-off between\nperformance and naturalistic patch appearance. Our experiments have shown, that\nusing a pre-trained GAN helps to gain realistic-looking patches while\npreserving the performance similar to conventional adversarial patches.\n","authors":["Svetlana Pavlitskaya","Bianca-Marina Codău","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2207.07347v1.pdf","comment":"Accepted for publication at the IJCAI 2022 AISafety workshop"},{"id":"http://arxiv.org/abs/2207.05518v2","updated":"2022-07-15T08:47:30Z","published":"2022-07-12T13:22:29Z","title":"Tracking Objects as Pixel-wise Distributions","summary":"  Multi-object tracking (MOT) requires detecting and associating objects\nthrough frames. Unlike tracking via detected bounding boxes or tracking objects\nas points, we propose tracking objects as pixel-wise distributions. We\ninstantiate this idea on a transformer-based architecture, P3AFormer, with\npixel-wise propagation, prediction, and association. P3AFormer propagates\npixel-wise features guided by flow information to pass messages between frames.\nFurthermore, P3AFormer adopts a meta-architecture to produce multi-scale object\nfeature maps. During inference, a pixel-wise association procedure is proposed\nto recover object connections through frames based on the pixel-wise\nprediction. P3AFormer yields 81.2\\% in terms of MOTA on the MOT17 benchmark --\nthe first among all transformer networks to reach 80\\% MOTA in literature.\nP3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.\n","authors":["Zelin Zhao","Ze Wu","Yueqing Zhuang","Boxun Li","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2207.05518v2.pdf","comment":"Accepted in ECCV22 as an oral presentation paper. The code&project\n  page is at\n  https://github.com/dvlab-research/ECCV22-P3AFormer-Tracking-Objects-as-Pixel-wise-Distributions"},{"id":"http://arxiv.org/abs/2204.12266v2","updated":"2022-07-15T08:40:19Z","published":"2022-04-26T12:44:55Z","title":"Attentive Fine-Grained Structured Sparsity for Image Restoration","summary":"  Image restoration tasks have witnessed great performance improvement in\nrecent years by developing large deep models. Despite the outstanding\nperformance, the heavy computation demanded by the deep models has restricted\nthe application of image restoration. To lift the restriction, it is required\nto reduce the size of the networks while maintaining accuracy. Recently, N:M\nstructured pruning has appeared as one of the effective and practical pruning\napproaches for making the model efficient with the accuracy constraint.\nHowever, it fails to account for different computational complexities and\nperformance requirements for different layers of an image restoration network.\nTo further optimize the trade-off between the efficiency and the restoration\naccuracy, we propose a novel pruning method that determines the pruning ratio\nfor N:M structured sparsity at each layer. Extensive experimental results on\nsuper-resolution and deblurring tasks demonstrate the efficacy of our method\nwhich outperforms previous pruning methods significantly. PyTorch\nimplementation for the proposed methods will be publicly available at\nhttps://github.com/JungHunOh/SLS_CVPR2022.\n","authors":["Junghun Oh","Heewon Kim","Seungjun Nah","Cheeun Hong","Jonghyun Choi","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2204.12266v2.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2207.07340v1","updated":"2022-07-15T08:35:44Z","published":"2022-07-15T08:35:44Z","title":"DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel\n  Splitting in the Frequency Domain","summary":"  With the wide application of face recognition systems, there is rising\nconcern that original face images could be exposed to malicious intents and\nconsequently cause personal privacy breaches. This paper presents DuetFace, a\nnovel privacy-preserving face recognition method that employs collaborative\ninference in the frequency domain. Starting from a counterintuitive discovery\nthat face recognition can achieve surprisingly good performance with only\nvisually indistinguishable high-frequency channels, this method designs a\ncredible split of frequency channels by their cruciality for visualization and\noperates the server-side model on non-crucial channels. However, the model\ndegrades in its attention to facial features due to the missing visual\ninformation. To compensate, the method introduces a plug-in interactive block\nto allow attention transfer from the client-side by producing a feature mask.\nThe mask is further refined by deriving and overlaying a facial region of\ninterest (ROI). Extensive experiments on multiple datasets validate the\neffectiveness of the proposed method in protecting face images from undesired\nvisual inspection, reconstruction, and identification while maintaining high\ntask availability and performance. Results show that the proposed method\nachieves a comparable recognition accuracy and computation cost to the\nunprotected ArcFace and outperforms the state-of-the-art privacy-preserving\nmethods. The source code is available at\nhttps://github.com/Tencent/TFace/tree/master/recognition/tasks/duetface.\n","authors":["Yuxi Mi","Yuge Huang","Jiazhen Ji","Hongquan Liu","Xingkun Xu","Shouhong Ding","Shuigeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2207.07340v1.pdf","comment":"Accepted to ACM Multimedia 2022"},{"id":"http://arxiv.org/abs/2112.04478v2","updated":"2022-07-15T08:31:45Z","published":"2021-12-08T18:58:16Z","title":"Prompting Visual-Language Models for Efficient Video Understanding","summary":"  Image-based visual-language (I-VL) pre-training has shown great success for\nlearning joint visual-textual representations from large-scale web data,\nrevealing remarkable ability for zero-shot generalisation. This paper presents\na simple but strong baseline to efficiently adapt the pre-trained I-VL model,\nand exploit its powerful ability for resource-hungry video understanding tasks,\nwith minimal training. Specifically, we propose to optimise a few random\nvectors, termed as continuous prompt vectors, that convert video-related tasks\ninto the same format as the pre-training objectives. In addition, to bridge the\ngap between static images and videos, temporal information is encoded with\nlightweight Transformers stacking on top of frame-wise visual features.\nExperimentally, we conduct extensive ablation studies to analyse the critical\ncomponents. On 10 public benchmarks of action recognition, action localisation,\nand text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,\nwe achieve competitive or state-of-the-art performance to existing methods,\ndespite optimising significantly fewer parameters.\n","authors":["Chen Ju","Tengda Han","Kunhao Zheng","Ya Zhang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2112.04478v2.pdf","comment":"ECCV 2022. Project page: https://ju-chen.github.io/efficient-prompt/"},{"id":"http://arxiv.org/abs/2207.07335v1","updated":"2022-07-15T08:21:53Z","published":"2022-07-15T08:21:53Z","title":"Learning Parallax Transformer Network for Stereo Image JPEG Artifacts\n  Removal","summary":"  Under stereo settings, the performance of image JPEG artifacts removal can be\nfurther improved by exploiting the additional information provided by a second\nview. However, incorporating this information for stereo image JPEG artifacts\nremoval is a huge challenge, since the existing compression artifacts make\npixel-level view alignment difficult. In this paper, we propose a novel\nparallax transformer network (PTNet) to integrate the information from stereo\nimage pairs for stereo image JPEG artifacts removal. Specifically, a\nwell-designed symmetric bi-directional parallax transformer module is proposed\nto match features with similar textures between different views instead of\npixel-level view alignment. Due to the issues of occlusions and boundaries, a\nconfidence-based cross-view fusion module is proposed to achieve better feature\nfusion for both views, where the cross-view features are weighted with\nconfidence maps. Especially, we adopt a coarse-to-fine design for the\ncross-view interaction, leading to better performance. Comprehensive\nexperimental results demonstrate that our PTNet can effectively remove\ncompression artifacts and achieves superior performance than other testing\nstate-of-the-art methods.\n","authors":["Xuhao Jiang","Weimin Tan","Ri Cheng","Shili Zhou","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2207.07335v1.pdf","comment":"11 pages, 12 figures, ACM MM2022"},{"id":"http://arxiv.org/abs/2101.07555v3","updated":"2022-07-15T08:10:38Z","published":"2021-01-19T10:40:38Z","title":"JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative\n  Adversarial Networks","summary":"  The paper proposes a solution based on Generative Adversarial Network (GAN)\nfor solving jigsaw puzzles. The problem assumes that an image is divided into\nequal square pieces, and asks to recover the image according to information\nprovided by the pieces. Conventional jigsaw puzzle solvers often determine the\nrelationships based on the boundaries of pieces, which ignore the important\nsemantic information. In this paper, we propose JigsawGAN, a GAN-based\nauxiliary learning method for solving jigsaw puzzles with unpaired images (with\nno prior knowledge of the initial images). We design a multi-task pipeline that\nincludes, (1) a classification branch to classify jigsaw permutations, and (2)\na GAN branch to recover features to images in correct orders. The\nclassification branch is constrained by the pseudo-labels generated according\nto the shuffled pieces. The GAN branch concentrates on the image semantic\ninformation, where the generator produces the natural images to fool the\ndiscriminator, while the discriminator distinguishes whether a given image\nbelongs to the synthesized or the real target domain. These two branches are\nconnected by a flow-based warp module that is applied to warp features to\ncorrect the order according to the classification results. The proposed method\ncan solve jigsaw puzzles more efficiently by utilizing both semantic\ninformation and boundary information simultaneously. Qualitative and\nquantitative comparisons against several representative jigsaw puzzle solvers\ndemonstrate the superiority of our method.\n","authors":["Ru Li","Shuaicheng Liu","Guangfu Wang","Guanghui Liu","Bing Zeng"],"pdf_url":"https://arxiv.org/pdf/2101.07555v3.pdf","comment":"Accepted by IEEE Transactions on Image Processing (TIP)"},{"id":"http://arxiv.org/abs/2207.07333v1","updated":"2022-07-15T08:05:41Z","published":"2022-07-15T08:05:41Z","title":"Rain Rate Estimation with SAR using NEXRAD measurements with\n  Convolutional Neural Networks","summary":"  Remote sensing of rainfall events is critical for both operational and\nscientific needs, including for example weather forecasting, extreme flood\nmitigation, water cycle monitoring, etc. Ground-based weather radars, such as\nNOAA's Next-Generation Radar (NEXRAD), provide reflectivity and precipitation\nmeasurements of rainfall events. However, the observation range of such radars\nis limited to a few hundred kilometers, prompting the exploration of other\nremote sensing methods, paricularly over the open ocean, that represents large\nareas not covered by land-based radars. For a number of decades, C-band SAR\nimagery such a such as Sentinel-1 imagery has been known to exhibit rainfall\nsignatures over the sea surface. However, the development of SAR-derived\nrainfall products remains a challenge. Here we propose a deep learning approach\nto extract rainfall information from SAR imagery. We demonstrate that a\nconvolutional neural network, such as U-Net, trained on a colocated and\npreprocessed Sentinel-1/NEXRAD dataset clearly outperforms state-of-the-art\nfiltering schemes. Our results indicate high performance in segmenting\nprecipitation regimes, delineated by thresholds at 1, 3, and 10 mm/h. Compared\nto current methods that rely on Koch filters to draw binary rainfall maps,\nthese multi-threshold learning-based models can provide rainfall estimation for\nhigher wind speeds and thus may be of great interest for data assimilation\nweather forecasting or for improving the qualification of SAR-derived wind\nfield data.\n","authors":["Aurélien Colin","Pierre Tandeo","Charles Peureux","Romain Husson","Nicolas Longepe","Ronan Fablet"],"pdf_url":"https://arxiv.org/pdf/2207.07333v1.pdf","comment":"25 pages, 10 figures"},{"id":"http://arxiv.org/abs/2207.07332v1","updated":"2022-07-15T08:04:10Z","published":"2022-07-15T08:04:10Z","title":"Stereo Co-capture System for Recording and Tracking Fish with Frame- and\n  Event Cameras","summary":"  This work introduces a co-capture system for multi-animal visual data\nacquisition using conventional cameras and event cameras. Event cameras offer\nmultiple advantages over frame-based cameras, such as a high temporal\nresolution and temporal redundancy suppression, which enable us to efficiently\ncapture the fast and erratic movements of fish. We furthermore present an\nevent-based multi-animal tracking algorithm, which proves the feasibility of\nthe approach and sets the baseline for further exploration of combining the\nadvantages of event cameras and conventional cameras for multi-animal tracking.\n","authors":["Friedhelm Hamann","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2207.07332v1.pdf","comment":"4 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2102.01850v2","updated":"2022-07-15T07:54:33Z","published":"2021-02-03T03:09:14Z","title":"UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging\n  with Unpaired Data","summary":"  The paper proposes a method to effectively fuse multi-exposure inputs and\ngenerate high-quality high dynamic range (HDR) images with unpaired datasets.\nDeep learning-based HDR image generation methods rely heavily on paired\ndatasets. The ground truth images play a leading role in generating reasonable\nHDR images. Datasets without ground truth are hard to be applied to train deep\nneural networks. Recently, Generative Adversarial Networks (GAN) have\ndemonstrated their potentials of translating images from source domain X to\ntarget domain Y in the absence of paired examples. In this paper, we propose a\nGAN-based network for solving such problems while generating enjoyable HDR\nresults, named UPHDR-GAN. The proposed method relaxes the constraint of the\npaired dataset and learns the mapping from the LDR domain to the HDR domain.\nAlthough the pair data are missing, UPHDR-GAN can properly handle the ghosting\nartifacts caused by moving objects or misalignments with the help of the\nmodified GAN loss, the improved discriminator network and the useful\ninitialization phase. The proposed method preserves the details of important\nregions and improves the total image perceptual quality. Qualitative and\nquantitative comparisons against the representative methods demonstrate the\nsuperiority of the proposed UPHDR-GAN.\n","authors":["Ru Li","Chuan Wang","Jue Wang","Guanghui Liu","Heng-Yu Zhang","Bing Zeng","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2102.01850v2.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2202.11312v2","updated":"2022-07-15T07:33:48Z","published":"2022-02-23T05:12:45Z","title":"Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative\n  Characterization of SLAM Datasets","summary":"  Reliability of SLAM systems is considered one of the critical requirements in\nmodern autonomous systems. This directed the efforts to developing many\nstate-of-the-art systems, creating challenging datasets, and introducing\nrigorous metrics to measure SLAM performance. However, the link between\ndatasets and performance in the robustness/resilience context has rarely been\nexplored. In order to fill this void, characterization of the operating\nconditions of SLAM systems is essential in order to provide an environment for\nquantitative measurement of robustness and resilience. In this paper, we argue\nthat for proper evaluation of SLAM performance, the characterization of SLAM\ndatasets serves as a critical first step. The study starts by reviewing\nprevious efforts for quantitative characterization of SLAM datasets. Then, the\nproblem of perturbation characterization is discussed and the linkage to SLAM\nrobustness/resilience is established. After that, we propose a novel, generic\nand extendable framework for quantitative analysis and comparison of SLAM\ndatasets. Additionally, a description of different characterization parameters\nis provided. Finally, we demonstrate the application of our framework by\npresenting the characterization results of three SLAM datasets: KITTI,\nEuroC-MAV, and TUM-VI highlighting the level of insights achieved by the\nproposed framework.\n","authors":["Islam Ali","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2202.11312v2.pdf","comment":"7 Pages, Accepted to IROS 2022, updating to the latest submitted\n  version"},{"id":"http://arxiv.org/abs/2207.05385v2","updated":"2022-07-15T07:27:52Z","published":"2022-07-12T08:29:51Z","title":"Controllable Shadow Generation Using Pixel Height Maps","summary":"  Shadows are essential for realistic image compositing. Physics-based shadow\nrendering methods require 3D geometries, which are not always available. Deep\nlearning-based shadow synthesis methods learn a mapping from the light\ninformation to an object's shadow without explicitly modeling the shadow\ngeometry. Still, they lack control and are prone to visual artifacts. We\nintroduce pixel heigh, a novel geometry representation that encodes the\ncorrelations between objects, ground, and camera pose. The pixel height can be\ncalculated from 3D geometries, manually annotated on 2D images, and can also be\npredicted from a single-view RGB image by a supervised approach. It can be used\nto calculate hard shadows in a 2D image based on the projective geometry,\nproviding precise control of the shadows' direction and shape. Furthermore, we\npropose a data-driven soft shadow generator to apply softness to a hard shadow\nbased on a softness input parameter. Qualitative and quantitative evaluations\ndemonstrate that the proposed pixel height significantly improves the quality\nof the shadow generation while allowing for controllability.\n","authors":["Yichen Sheng","Yifan Liu","Jianming Zhang","Wei Yin","A. Cengiz Oztireli","He Zhang","Zhe Lin","Eli Shechtman","Bedrich Benes"],"pdf_url":"https://arxiv.org/pdf/2207.05385v2.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2207.07317v1","updated":"2022-07-15T07:16:10Z","published":"2022-07-15T07:16:10Z","title":"Enhancement by Your Aesthetic: An Intelligible Unsupervised Personalized\n  Enhancer for Low-Light Images","summary":"  Low-light image enhancement is an inherently subjective process whose targets\nvary with the user's aesthetic. Motivated by this, several personalized\nenhancement methods have been investigated. However, the enhancement process\nbased on user preferences in these techniques is invisible, i.e., a \"black\nbox\". In this work, we propose an intelligible unsupervised personalized\nenhancer (iUPEnhancer) for low-light images, which establishes the correlations\nbetween the low-light and the unpaired reference images with regard to three\nuser-friendly attributions (brightness, chromaticity, and noise). The proposed\niUP-Enhancer is trained with the guidance of these correlations and the\ncorresponding unsupervised loss functions. Rather than a \"black box\" process,\nour iUP-Enhancer presents an intelligible enhancement process with the above\nattributions. Extensive experiments demonstrate that the proposed algorithm\nproduces competitive qualitative and quantitative results while maintaining\nexcellent flexibility and scalability. This can be validated by personalization\nwith single/multiple references, cross-attribution references, or merely\nadjusting parameters.\n","authors":["Naishan Zheng","Jie Huang","Qi Zhu","Man Zhou","Feng Zhao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2207.07317v1.pdf","comment":"Accepted to ACM MM 2022"},{"id":"http://arxiv.org/abs/2207.07316v1","updated":"2022-07-15T07:15:36Z","published":"2022-07-15T07:15:36Z","title":"Privacy-Preserving Face Recognition with Learnable Privacy Budgets in\n  Frequency Domain","summary":"  Face recognition technology has been used in many fields due to its high\nrecognition accuracy, including the face unlocking of mobile devices, community\naccess control systems, and city surveillance. As the current high accuracy is\nguaranteed by very deep network structures, facial images often need to be\ntransmitted to third-party servers with high computational power for inference.\nHowever, facial images visually reveal the user's identity information. In this\nprocess, both untrusted service providers and malicious users can significantly\nincrease the risk of a personal privacy breach. Current privacy-preserving\napproaches to face recognition are often accompanied by many side effects, such\nas a significant increase in inference time or a noticeable decrease in\nrecognition accuracy. This paper proposes a privacy-preserving face recognition\nmethod using differential privacy in the frequency domain. Due to the\nutilization of differential privacy, it offers a guarantee of privacy in\ntheory. Meanwhile, the loss of accuracy is very slight. This method first\nconverts the original image to the frequency domain and removes the direct\ncomponent termed DC. Then a privacy budget allocation method can be learned\nbased on the loss of the back-end face recognition network within the\ndifferential privacy framework. Finally, it adds the corresponding noise to the\nfrequency domain features. Our method performs very well with several classical\nface recognition test sets according to the extensive experiments.\n","authors":["Jiazhen Ji","Huan Wang","Yuge Huang","Jiaxiang Wu","Xingkun Xu","Shouhong Ding","ShengChuan Zhang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2207.07316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12636v4","updated":"2022-07-15T07:01:47Z","published":"2021-07-27T07:17:12Z","title":"Exploring Sequence Feature Alignment for Domain Adaptive Detection\n  Transformers","summary":"  Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.\n","authors":["Wen Wang","Yang Cao","Jing Zhang","Fengxiang He","Zheng-Jun Zha","Yonggang Wen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2107.12636v4.pdf","comment":"Fix a typo in Eq. 13"},{"id":"http://arxiv.org/abs/2207.07311v1","updated":"2022-07-15T06:58:41Z","published":"2022-07-15T06:58:41Z","title":"Towards Privacy-Preserving Person Re-identification via Person Identify\n  Shift","summary":"  Recently privacy concerns of person re-identification (ReID) raise more and\nmore attention and preserving the privacy of the pedestrian images used by ReID\nmethods become essential. De-identification (DeID) methods alleviate privacy\nissues by removing the identity-related of the ReID data. However, most of the\nexisting DeID methods tend to remove all personal identity-related information\nand compromise the usability of de-identified data on the ReID task. In this\npaper, we aim to develop a technique that can achieve a good trade-off between\nprivacy protection and data usability for person ReID. To achieve this, we\npropose a novel de-identification method designed explicitly for person ReID,\nnamed Person Identify Shift (PIS). PIS removes the absolute identity in a\npedestrian image while preserving the identity relationship between image\npairs. By exploiting the interpolation property of variational auto-encoder,\nPIS shifts each pedestrian image from the current identity to another with a\nnew identity, resulting in images still preserving the relative identities.\nExperimental results show that our method has a better trade-off between\nprivacy-preserving and model performance than existing de-identification\nmethods and can defend against human and model attacks for data privacy.\n","authors":["Shuguang Dou","Xinyang Jiang","Qingsong Zhao","Dongsheng Li","Cairong Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.07311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07303v1","updated":"2022-07-15T06:07:11Z","published":"2022-07-15T06:07:11Z","title":"Towards Better Dermoscopic Image Feature Representation Learning for\n  Melanoma Classification","summary":"  Deep learning-based melanoma classification with dermoscopic images has\nrecently shown great potential in automatic early-stage melanoma diagnosis.\nHowever, limited by the significant data imbalance and obvious extraneous\nartifacts, i.e., the hair and ruler markings, discriminative feature extraction\nfrom dermoscopic images is very challenging. In this study, we seek to resolve\nthese problems respectively towards better representation learning for lesion\nfeatures. Specifically, a GAN-based data augmentation (GDA) strategy is adapted\nto generate synthetic melanoma-positive images, in conjunction with the\nproposed implicit hair denoising (IHD) strategy. Wherein the hair-related\nrepresentations are implicitly disentangled via an auxiliary classifier network\nand reversely sent to the melanoma-feature extraction backbone for better\nmelanoma-specific representation learning. Furthermore, to train the IHD\nmodule, the hair noises are additionally labeled on the ISIC2020 dataset,\nmaking it the first large-scale dermoscopic dataset with annotation of\nhair-like artifacts. Extensive experiments demonstrate the superiority of the\nproposed framework as well as the effectiveness of each component. The improved\ndataset publicly avaliable at https://github.com/kirtsy/DermoscopicDataset.\n","authors":["ChengHui Yu","MingKang Tang","ShengGe Yang","MingQing Wang","Zhe Xu","JiangPeng Yan","HanMo Chen","Yu Yang","Xiao-Jun Zeng","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2207.07303v1.pdf","comment":"ICONIP 2021 conference"},{"id":"http://arxiv.org/abs/2207.07301v1","updated":"2022-07-15T05:56:13Z","published":"2022-07-15T05:56:13Z","title":"Robust Deep Compressive Sensing with Recurrent-Residual Structural\n  Constraints","summary":"  Existing deep compressive sensing (CS) methods either ignore adaptive online\noptimization or depend on costly iterative optimizer during reconstruction.\nThis work explores a novel image CS framework with recurrent-residual\nstructural constraint, termed as R$^2$CS-NET. The R$^2$CS-NET first\nprogressively optimizes the acquired samplings through a novel recurrent neural\nnetwork. The cascaded residual convolutional network then fully reconstructs\nthe image from optimized latent representation. As the first deep CS framework\nefficiently bridging adaptive online optimization, the R$^2$CS-NET integrates\nthe robustness of online optimization with the efficiency and nonlinear\ncapacity of deep learning methods. Signal correlation has been addressed\nthrough the network architecture. The adaptive sensing nature further makes it\nan ideal candidate for color image CS via leveraging channel correlation.\nNumerical experiments verify the proposed recurrent latent optimization design\nnot only fulfills the adaptation motivation, but also outperforms classic long\nshort-term memory (LSTM) architecture in the same scenario. The overall\nframework demonstrates hardware implementation feasibility, with leading\nrobustness and generalization capability among existing deep CS benchmarks.\n","authors":["Jun Niu"],"pdf_url":"https://arxiv.org/pdf/2207.07301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.13027v5","updated":"2022-07-15T05:47:19Z","published":"2021-03-24T07:21:53Z","title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers","summary":"  Data mixing augmentation have proved to be effective for improving the\ngeneralization ability of deep neural networks. While early methods mix samples\nby hand-crafted policies (e.g., linear interpolation), recent methods utilize\nsaliency information to match the mixed samples and labels via complex offline\noptimization. However, there arises a trade-off between precise mixing policies\nand optimization complexity. To address this challenge, we propose a novel\nautomatic mixup (AutoMix) framework, where the mixup policy is parameterized\nand serves the ultimate classification goal directly. Specifically, AutoMix\nreformulates the mixup classification into two sub-tasks (i.e., mixed sample\ngeneration and mixup classification) with corresponding sub-networks and solves\nthem in a bi-level optimization framework. For the generation, a learnable\nlightweight mixup generator, Mix Block, is designed to generate mixed samples\nby modeling patch-wise relationships under the direct supervision of the\ncorresponding mixed labels. To prevent the degradation and instability of\nbi-level optimization, we further introduce a momentum pipeline to train\nAutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks\nprove the superiority of AutoMix compared with state-of-the-arts in various\nclassification scenarios and downstream tasks.\n","authors":["Zicheng Liu","Siyuan Li","Di Wu","Zihan Liu","Zhiyuan Chen","Lirong Wu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2103.13027v5.pdf","comment":"ECCV 2022 (Oral representation paper). The source code is available\n  at https://github.com/Westlake-AI/openmixup"},{"id":"http://arxiv.org/abs/2205.08390v2","updated":"2022-07-15T05:09:37Z","published":"2022-05-17T14:11:07Z","title":"HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer\n  Diagnosis in Ultrasound Images","summary":"  Ultrasonography is an important routine examination for breast cancer\ndiagnosis, due to its non-invasive, radiation-free and low-cost properties.\nHowever, the diagnostic accuracy of breast cancer is still limited due to its\ninherent limitations. It would be a tremendous success if we can precisely\ndiagnose breast cancer by breast ultrasound images (BUS). Many learning-based\ncomputer-aided diagnostic methods have been proposed to achieve breast cancer\ndiagnosis/lesion classification. However, most of them require a pre-define ROI\nand then classify the lesion inside the ROI. Conventional classification\nbackbones, such as VGG16 and ResNet50, can achieve promising classification\nresults with no ROI requirement. But these models lack interpretability, thus\nrestricting their use in clinical practice. In this study, we propose a novel\nROI-free model for breast cancer diagnosis in ultrasound images with\ninterpretable feature representations. We leverage the anatomical prior\nknowledge that malignant and benign tumors have different spatial relationships\nbetween different tissue layers, and propose a HoVer-Transformer to formulate\nthis prior knowledge. The proposed HoVer-Trans block extracts the inter- and\nintra-layer spatial information horizontally and vertically. We conduct and\nrelease an open dataset GDPH&SYSUCC for breast cancer diagnosis in BUS. The\nproposed model is evaluated in three datasets by comparing with four CNN-based\nmodels and two vision transformer models via five-fold cross validation. It\nachieves state-of-the-art classification performance with the best model\ninterpretability. In the meanwhile, our proposed model outperforms two senior\nsonographers on the breast cancer diagnosis when only one BUS image is given.\n","authors":["Yuhao Mo","Chu Han","Yu Liu","Min Liu","Zhenwei Shi","Jiatai Lin","Bingchao Zhao","Chunwang Huang","Bingjiang Qiu","Yanfen Cui","Lei Wu","Xipeng Pan","Zeyan Xu","Xiaomei Huang","Zaiyi Liu","Ying Wang","Changhong Liang"],"pdf_url":"https://arxiv.org/pdf/2205.08390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09091v2","updated":"2022-07-15T04:41:31Z","published":"2022-03-17T05:25:36Z","title":"deepNIR: Datasets for generating synthetic NIR images and improved fruit\n  detection system using deep learning techniques","summary":"  This paper presents datasets utilised for synthetic near-infrared (NIR) image\ngeneration and bounding-box level fruit detection systems. It is undeniable\nthat high-calibre machine learning frameworks such as Tensorflow or Pytorch,\nand large-scale ImageNet or COCO datasets with the aid of accelerated GPU\nhardware have pushed the limit of machine learning techniques for more than\ndecades. Among these breakthroughs, a high-quality dataset is one of the\nessential building blocks that can lead to success in model generalisation and\nthe deployment of data-driven deep neural networks. In particular, synthetic\ndata generation tasks often require more training samples than other supervised\napproaches. Therefore, in this paper, we share the NIR+RGB datasets that are\nre-processed from two public datasets (i.e., nirscene and SEN12MS) and our\nnovel NIR+RGB sweet pepper(capsicum) dataset. We quantitatively and\nqualitatively demonstrate that these NIR+RGB datasets are sufficient to be used\nfor synthetic NIR image generation. We achieved Frechet Inception Distance\n(FID) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper\ndatasets respectively. In addition, we release manual annotations of 11 fruit\nbounding boxes that can be exported as various formats using cloud service.\nFour newly added fruits [blueberry, cherry, kiwi, and wheat] compound 11 novel\nbounding box datasets on top of our previous work presented in the deepFruits\nproject [apple, avocado, capsicum, mango, orange, rockmelon, strawberry]. The\ntotal number of bounding box instances of the dataset is 162k and it is ready\nto use from cloud service. For the evaluation of the dataset, Yolov5 single\nstage detector is exploited and reported impressive\nmean-average-precision,mAP[0.5:0.95] results of[min:0.49, max:0.812]. We hope\nthese datasets are useful and serve as a baseline for the future studies.\n","authors":["Inkyu Sa","JongYoon Lim","Ho Seok Ahn","Bruce MacDonald"],"pdf_url":"https://arxiv.org/pdf/2203.09091v2.pdf","comment":"35 pages, 27 figures, published in MDPI Remote Sensing journal"},{"id":"http://arxiv.org/abs/2207.07288v1","updated":"2022-07-15T04:39:47Z","published":"2022-07-15T04:39:47Z","title":"WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation","summary":"  Existing few-shot image generation approaches typically employ fusion-based\nstrategies, either on the image or the feature level, to produce new images.\nHowever, previous approaches struggle to synthesize high-frequency signals with\nfine details, deteriorating the synthesis quality. To address this, we propose\nWaveGAN, a frequency-aware model for few-shot image generation. Concretely, we\ndisentangle encoded features into multiple frequency components and perform\nlow-frequency skip connections to preserve outline and structural information.\nThen we alleviate the generator's struggles of synthesizing fine details by\nemploying high-frequency skip connections, thus providing informative frequency\ninformation to the generator. Moreover, we utilize a frequency L1-loss on the\ngenerated and real images to further impede frequency information loss.\nExtensive experiments demonstrate the effectiveness and advancement of our\nmethod on three datasets. Noticeably, we achieve new state-of-the-art with FID\n42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822\nrespectively on Flower, Animal Faces, and VGGFace. GitHub:\nhttps://github.com/kobeshegu/ECCV2022_WaveGAN\n","authors":["Mengping Yang","Zhe Wang","Ziqiu Chi","Wenyi Feng"],"pdf_url":"https://arxiv.org/pdf/2207.07288v1.pdf","comment":"Accepted by ECCV2022, Code\n  Link:https://github.com/kobeshegu/ECCV2022_WaveGAN"},{"id":"http://arxiv.org/abs/2207.07285v1","updated":"2022-07-15T04:23:42Z","published":"2022-07-15T04:23:42Z","title":"X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text\n  Retrieval","summary":"  Video-text retrieval has been a crucial and fundamental task in multi-modal\nresearch. The development of video-text retrieval has been considerably\npromoted by large-scale multi-modal contrastive pre-training, which primarily\nfocuses on coarse-grained or fine-grained contrast. However, cross-grained\ncontrast, which is the contrast between coarse-grained representations and\nfine-grained representations, has rarely been explored in prior research.\nCompared with fine-grained or coarse-grained contrasts, cross-grained contrast\ncalculate the correlation between coarse-grained features and each fine-grained\nfeature, and is able to filter out the unnecessary fine-grained features guided\nby the coarse-grained feature during similarity calculation, thus improving the\naccuracy of retrieval. To this end, this paper presents a novel multi-grained\ncontrastive model, namely X-CLIP, for video-text retrieval. However, another\nchallenge lies in the similarity aggregation problem, which aims to aggregate\nfine-grained and cross-grained similarity matrices to instance-level\nsimilarity. To address this challenge, we propose the Attention Over Similarity\nMatrix (AOSM) module to make the model focus on the contrast between essential\nframes and words, thus lowering the impact of unnecessary frames and words on\nretrieval results. With multi-grained contrast and the proposed AOSM module,\nX-CLIP achieves outstanding performance on five widely-used video-text\nretrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1\nR@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous\nstate-of-theart by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on\nthese benchmarks, demonstrating the superiority of multi-grained contrast and\nAOSM.\n","authors":["Yiwei Ma","Guohai Xu","Xiaoshuai Sun","Ming Yan","Ji Zhang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2207.07285v1.pdf","comment":"13 pages, 6 figures, ACMMM22"},{"id":"http://arxiv.org/abs/2207.07284v1","updated":"2022-07-15T04:18:06Z","published":"2022-07-15T04:18:06Z","title":"Parameterization of Cross-Token Relations with Relative Positional\n  Encoding for Vision MLP","summary":"  Vision multi-layer perceptrons (MLPs) have shown promising performance in\ncomputer vision tasks, and become the main competitor of CNNs and vision\nTransformers. They use token-mixing layers to capture cross-token interactions,\nas opposed to the multi-head self-attention mechanism used by Transformers.\nHowever, the heavily parameterized token-mixing layers naturally lack\nmechanisms to capture local information and multi-granular non-local relations,\nthus their discriminative power is restrained. To tackle this issue, we propose\na new positional spacial gating unit (PoSGU). It exploits the attention\nformulations used in the classical relative positional encoding (RPE), to\nefficiently encode the cross-token relations for token mixing. It can\nsuccessfully reduce the current quadratic parameter complexity $O(N^2)$ of\nvision MLPs to $O(N)$ and $O(1)$. We experiment with two RPE mechanisms, and\nfurther propose a group-wise extension to improve their expressive power with\nthe accomplishment of multi-granular contexts. These then serve as the key\nbuilding blocks of a new type of vision MLP, referred to as PosMLP. We evaluate\nthe effectiveness of the proposed approach by conducting thorough experiments,\ndemonstrating an improved or comparable performance with reduced parameter\ncomplexity. For instance, for a model trained on ImageNet1K, we achieve a\nperformance improvement from 72.14\\% to 74.02\\% and a learnable parameter\nreduction from $19.4M$ to $18.2M$. Code could be found at\n\\href{https://github.com/Zhicaiwww/PosMLP}{https://github.com/Zhicaiwww/PosMLP}.\n","authors":["Zhicai Wang","Yanbin Hao","Xingyu Gao","Hao Zhang","Shuo Wang","Tingting Mu","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2207.07284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.04904v3","updated":"2022-07-15T04:16:21Z","published":"2022-03-09T17:26:53Z","title":"Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning","summary":"  Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n","authors":["Zhenhailong Wang","Hang Yu","Manling Li","Han Zhao","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2203.04904v3.pdf","comment":"4 pages, 4 figures, under review"},{"id":"http://arxiv.org/abs/2207.07278v1","updated":"2022-07-15T03:58:04Z","published":"2022-07-15T03:58:04Z","title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified\n  Learning Scheme and Dynamic Range Minimization","summary":"  With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.\n","authors":["Mengyin Liu","Chao Zhu","Hongyu Gao","Weibo Gu","Hongfa Wang","Wei Liu","Xu-cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2207.07278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07106v2","updated":"2022-07-15T03:34:49Z","published":"2022-07-14T17:58:02Z","title":"Benchmarking Omni-Vision Representation through the Lens of Visual\n  Realms","summary":"  Though impressive performance has been achieved in specific visual realms\n(e.g. faces, dogs, and places), an omni-vision representation generalizing to\nmany natural visual domains is highly desirable. But, existing benchmarks are\nbiased and inefficient to evaluate the omni-vision representation -- these\nbenchmarks either only include several specific realms, or cover most realms at\nthe expense of subsuming numerous datasets that have extensive realm\noverlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It\nincludes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.\nWithout semantic overlapping, these datasets cover most visual realms\ncomprehensively and meanwhile efficiently. In addition, we propose a new\nsupervised contrastive learning framework, namely Relational Contrastive\nlearning (ReCo), for a better omni-vision representation. Beyond pulling two\ninstances from the same concept closer -- the typical supervised contrastive\nlearning framework -- ReCo also pulls two instances from the same semantic\nrealm closer, encoding the semantic relation between concepts, and facilitating\nomni-vision representation learning. We benchmark ReCo and other advances in\nomni-vision representation studies that are different in architectures (from\nCNNs to transformers) and in learning paradigms (from supervised learning to\nself-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo\nto other supervised contrastive learning methods and reveal multiple practical\nobservations to facilitate future research.\n","authors":["Yuanhan Zhang","Zhenfei Yin","Jing Shao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2207.07106v2.pdf","comment":"In ECCV 2022; The project page at\n  https://zhangyuanhan-ai.github.io/OmniBenchmark"},{"id":"http://arxiv.org/abs/2207.07269v1","updated":"2022-07-15T03:31:15Z","published":"2022-07-15T03:31:15Z","title":"Weakly Supervised Video Salient Object Detection via Point Supervision","summary":"  Video salient object detection models trained on pixel-wise dense annotation\nhave achieved excellent performance, yet obtaining pixel-by-pixel annotated\ndatasets is laborious. Several works attempt to use scribble annotations to\nmitigate this problem, but point supervision as a more labor-saving annotation\nmethod (even the most labor-saving method among manual annotation methods for\ndense prediction), has not been explored. In this paper, we propose a strong\nbaseline model based on point supervision. To infer saliency maps with temporal\ninformation, we mine inter-frame complementary information from short-term and\nlong-term perspectives, respectively. Specifically, we propose a hybrid token\nattention module, which mixes optical flow and image information from\northogonal directions, adaptively highlighting critical optical flow\ninformation (channel dimension) and critical token information (spatial\ndimension). To exploit long-term cues, we develop the Long-term Cross-Frame\nAttention module (LCFA), which assists the current frame in inferring salient\nobjects based on multi-frame tokens. Furthermore, we label two point-supervised\ndatasets, P-DAVIS and P-DAVSOD, by relabeling the DAVIS and the DAVSOD dataset.\nExperiments on the six benchmark datasets illustrate our method outperforms the\nprevious state-of-the-art weakly supervised methods and even is comparable with\nsome fully supervised approaches. Source code and datasets are available.\n","authors":["Shuyong Gao","Haozhe Xing","Wei Zhang","Yan Wang","Qianyu Guo","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.07269v1.pdf","comment":"accepted by ACM MM 2022"},{"id":"http://arxiv.org/abs/2203.06841v2","updated":"2022-07-15T03:28:15Z","published":"2022-03-14T03:40:35Z","title":"STDAN: Deformable Attention Network for Space-Time Video\n  Super-Resolution","summary":"  The target of space-time video super-resolution (STVSR) is to increase the\nspatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)\nvideos. Recent approaches based on deep learning have made significant\nimprovements, but most of them only use two adjacent frames, that is,\nshort-term features, to synthesize the missing frame embedding, which cannot\nfully explore the information flow of consecutive input LR frames. In addition,\nexisting STVSR models hardly exploit the temporal contexts explicitly to assist\nhigh-resolution (HR) frame reconstruction. To address these issues, in this\npaper, we propose a deformable attention network called STDAN for STVSR. First,\nwe devise a long-short term feature interpolation (LSTFI) module, which is\ncapable of excavating abundant content from more neighboring input frames for\nthe interpolation process through a bidirectional RNN structure. Second, we put\nforward a spatial-temporal deformable feature aggregation (STDFA) module, in\nwhich spatial and temporal contexts in dynamic video frames are adaptively\ncaptured and aggregated to enhance SR reconstruction. Experimental results on\nseveral datasets demonstrate that our approach outperforms state-of-the-art\nSTVSR methods. The code is available at\nhttps://github.com/littlewhitesea/STDAN.\n","authors":["Hai Wang","Xiaoyu Xiang","Yapeng Tian","Wenming Yang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2203.06841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07268v1","updated":"2022-07-15T03:27:13Z","published":"2022-07-15T03:27:13Z","title":"Lightweight Vision Transformer with Cross Feature Attention","summary":"  Recent advances in vision transformers (ViTs) have achieved great performance\nin visual recognition tasks. Convolutional neural networks (CNNs) exploit\nspatial inductive bias to learn visual representations, but these networks are\nspatially local. ViTs can learn global representations with their\nself-attention mechanism, but they are usually heavy-weight and unsuitable for\nmobile devices. In this paper, we propose cross feature attention (XFA) to\nbring down computation cost for transformers, and combine efficient mobile CNNs\nto form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can\nserve as a general-purpose backbone to learn both global and local\nrepresentation. Experimental results show that XFormer outperforms numerous CNN\nand ViT-based models across different tasks and datasets. On ImageNet1K\ndataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,\nwhich is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT\n(ViT-based) for similar number of parameters. Our model also performs well when\ntransferring to object detection and semantic segmentation tasks. On MS COCO\ndataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -> 33.2 AP) in YOLOv3\nframework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with\nonly a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,\nsurpassing state-of-the-art lightweight segmentation networks.\n","authors":["Youpeng Zhao","Huadong Tang","Yingying Jiang","Yong A","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2207.07268v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2207.07267v1","updated":"2022-07-15T03:16:43Z","published":"2022-07-15T03:16:43Z","title":"ScaleNet: Searching for the Model to Scale","summary":"  Recently, community has paid increasing attention on model scaling and\ncontributed to developing a model family with a wide spectrum of scales.\nCurrent methods either simply resort to a one-shot NAS manner to construct a\nnon-structural and non-scalable model family or rely on a manual yet fixed\nscaling strategy to scale an unnecessarily best base model. In this paper, we\nbridge both two components and propose ScaleNet to jointly search base model\nand scaling strategy so that the scaled large model can have more promising\nperformance. Concretely, we design a super-supernet to embody models with\ndifferent spectrum of sizes (e.g., FLOPs). Then, the scaling strategy can be\nlearned interactively with the base model via a Markov chain-based evolution\nalgorithm and generalized to develop even larger models. To obtain a decent\nsuper-supernet, we design a hierarchical sampling strategy to enhance its\ntraining sufficiency and alleviate the disturbance. Experimental results show\nour scaled networks enjoy significant performance superiority on various FLOPs,\nbut with at least 2.53x reduction on search cost. Codes are available at\nhttps://github.com/luminolx/ScaleNet.\n","authors":["Jiyang Xie","Xiu Su","Shan You","Zhanyu Ma","Fei Wang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2207.07267v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2203.06585v2","updated":"2022-07-15T03:10:58Z","published":"2022-03-13T06:23:18Z","title":"CVFNet: Real-time 3D Object Detection by Learning Cross View Features","summary":"  In recent years 3D object detection from LiDAR point clouds has made great\nprogress thanks to the development of deep learning technologies. Although\nvoxel or point based methods are popular in 3D object detection, they usually\ninvolve time-consuming operations such as 3D convolutions on voxels or ball\nquery among points, making the resulting network inappropriate for time\ncritical applications. On the other hand, 2D view-based methods feature high\ncomputing efficiency while usually obtaining inferior performance than the\nvoxel or point based methods. In this work, we present a real-time view-based\nsingle stage 3D object detector, namely CVFNet to fulfill this task. To\nstrengthen the cross-view feature learning under the condition of demanding\nefficiency, our framework extracts the features of different views and fuses\nthem in an efficient progressive way. We first propose a novel Point-Range\nfeature fusion module that deeply integrates point and range view features in\nmultiple stages. Then, a special Slice Pillar is designed to well maintain the\n3D geometry when transforming the obtained deep point-view features into bird's\neye view. To better balance the ratio of samples, a sparse pillar detection\nhead is presented to focus the detection on the nonempty grids. We conduct\nexperiments on the popular KITTI and NuScenes benchmark, and state-of-the-art\nperformances are achieved in terms of both accuracy and speed.\n","authors":["Jiaqi Gu","Zhiyu Xiang","Pan Zhao","Tingming Bai","Lingxuan Wang","Xijun Zhao","Zhiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.06585v2.pdf","comment":"7 pages, 5 figures, accepted by IROS 2022"},{"id":"http://arxiv.org/abs/2205.03650v2","updated":"2022-07-15T02:15:30Z","published":"2022-05-07T13:13:55Z","title":"Distilling Inter-Class Distance for Semantic Segmentation","summary":"  Knowledge distillation is widely adopted in semantic segmentation to reduce\nthe computation cost.The previous knowledge distillation methods for semantic\nsegmentation focus on pixel-wise feature alignment and intra-class feature\nvariation distillation, neglecting to transfer the knowledge of the inter-class\ndistance in the feature space, which is important for semantic segmentation. To\naddress this issue, we propose an Inter-class Distance Distillation (IDD)\nmethod to transfer the inter-class distance in the feature space from the\nteacher network to the student network. Furthermore, semantic segmentation is a\nposition-dependent task,thus we exploit a position information distillation\nmodule to help the student network encode more position information. Extensive\nexperiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show\nthat our method is helpful to improve the accuracy of semantic segmentation\nmodels and achieves the state-of-the-art performance. E.g. it boosts the\nbenchmark model(\"PSPNet+ResNet18\") by 7.50% in accuracy on the Cityscapes\ndataset.\n","authors":["Zhengbo Zhang","Chunluan Zhou","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2205.03650v2.pdf","comment":"IJCAI-ECAI2022 Long Oral"},{"id":"http://arxiv.org/abs/2207.07253v1","updated":"2022-07-15T01:59:14Z","published":"2022-07-15T01:59:14Z","title":"Decoupling Recognition from Detection: Single Shot Self-Reliant Scene\n  Text Spotter","summary":"  Typical text spotters follow the two-stage spotting strategy: detect the\nprecise boundary for a text instance first and then perform text recognition\nwithin the located text region. While such strategy has achieved substantial\nprogress, there are two underlying limitations. 1) The performance of text\nrecognition depends heavily on the precision of text detection, resulting in\nthe potential error propagation from detection to recognition. 2) The RoI\ncropping which bridges the detection and recognition brings noise from\nbackground and leads to information loss when pooling or interpolating from\nfeature maps. In this work we propose the single shot Self-Reliant Scene Text\nSpotter (SRSTS), which circumvents these limitations by decoupling recognition\nfrom detection. Specifically, we conduct text detection and recognition in\nparallel and bridge them by the shared positive anchor point. Consequently, our\nmethod is able to recognize the text instances correctly even though the\nprecise text boundaries are challenging to detect. Additionally, our method\nreduces the annotation cost for text detection substantially. Extensive\nexperiments on regular-shaped benchmark and arbitrary-shaped benchmark\ndemonstrate that our SRSTS compares favorably to previous state-of-the-art\nspotters in terms of both accuracy and efficiency.\n","authors":["Jingjing Wu","Pengyuan Lyu","Guangming Lu","Chengquan Zhang","Kun Yao","Wenjie Pei"],"pdf_url":"https://arxiv.org/pdf/2207.07253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06694v2","updated":"2022-07-15T01:59:00Z","published":"2022-07-14T06:49:59Z","title":"Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text\n  Spotting","summary":"  End-to-end text spotting has attached great attention recently due to its\nbenefits on global optimization and high maintainability for real applications.\nHowever, the input scale has always been a tough trade-off since recognizing a\nsmall text instance usually requires enlarging the whole image, which brings\nhigh computational costs. In this paper, to address this problem, we propose a\nnovel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting\nframework, which aims to infer images in different small but recognizable\nresolutions and achieve a better balance between accuracy and efficiency.\nConcretely, we adopt a resolution selector to dynamically decide the input\nresolutions for different images, which is constraint by both inference\naccuracy and computational cost. Another sequential knowledge distillation\nstrategy is conducted on the text recognition branch, making the low-res input\nobtains comparable performance to a high-res image. The proposed method can be\noptimized end-to-end and adopted in any current text spotting framework to\nimprove the practicability. Extensive experiments on several text spotting\nbenchmarks show that the proposed method vastly improves the usability of\nlow-res models. The code is available at\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/.\n","authors":["Ying Chen","Liang Qiao","Zhanzhan Cheng","Shiliang Pu","Yi Niu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2207.06694v2.pdf","comment":"Accept by ECCV2022"},{"id":"http://arxiv.org/abs/2207.07243v1","updated":"2022-07-15T00:35:59Z","published":"2022-07-15T00:35:59Z","title":"LineCap: Line Charts for Data Visualization Captioning Models","summary":"  Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.\n","authors":["Anita Mahinpei","Zona Kostic","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2207.07243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07241v1","updated":"2022-07-15T00:16:25Z","published":"2022-07-15T00:16:25Z","title":"Classification of Bark Beetle-Induced Forest Tree Mortality using Deep\n  Learning","summary":"  Bark beetle outbreaks can dramatically impact forest ecosystems and services\naround the world. For the development of effective forest policies and\nmanagement plans, the early detection of infested trees is essential. Despite\nthe visual symptoms of bark beetle infestation, this task remains challenging,\nconsidering overlapping tree crowns and non-homogeneity in crown foliage\ndiscolouration. In this work, a deep learning based method is proposed to\neffectively classify different stages of bark beetle attacks at the individual\ntree level. The proposed method uses RetinaNet architecture (exploiting a\nrobust feature extraction backbone pre-trained for tree crown detection) to\ntrain a shallow subnetwork for classifying the different attack stages of\nimages captured by unmanned aerial vehicles (UAVs). Moreover, various data\naugmentation strategies are examined to address the class imbalance problem,\nand consequently, the affine transformation is selected to be the most\neffective one for this purpose. Experimental evaluations demonstrate the\neffectiveness of the proposed method by achieving an average accuracy of\n98.95%, considerably outperforming the baseline method by approximately 10%.\n","authors":["Rudraksh Kapil","Seyed Mojtaba Marvasti-Zadeh","Devin Goodsman","Nilanjan Ray","Nadir Erbilgin"],"pdf_url":"https://arxiv.org/pdf/2207.07241v1.pdf","comment":"Extended abstract submitted to VAIB Worskhop at ICPR 2022. 4 pages, 6\n  figures. The code and results are publicly available at\n  https://github.com/rudrakshkapil09/BarkBeetle-DamageClassification-DL"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2207.07646v1","updated":"2022-07-15T17:59:11Z","published":"2022-07-15T17:59:11Z","title":"Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision\n  and Language Models","summary":"  Utilizing vision and language models (VLMs) pre-trained on large-scale\nimage-text pairs is becoming a promising paradigm for open-vocabulary visual\nrecognition. In this work, we extend this paradigm by leveraging motion and\naudio that naturally exist in video. We present \\textbf{MOV}, a simple yet\neffective method for \\textbf{M}ultimodal \\textbf{O}pen-\\textbf{V}ocabulary\nvideo classification. In MOV, we directly use the vision encoder from\npre-trained VLMs with minimal modifications to encode video, optical flow and\naudio spectrogram. We design a cross-modal fusion mechanism to aggregate\ncomplimentary multimodal information. Experiments on Kinetics-700 and VGGSound\nshow that introducing flow or audio modality brings large performance gains\nover the pre-trained VLM and existing methods. Specifically, MOV greatly\nimproves the accuracy on base classes, while generalizes better on novel\nclasses. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video\nclassification benchmarks, significantly outperforming both traditional\nzero-shot methods and recent methods based on VLMs. Code and models will be\nreleased.\n","authors":["Rui Qian","Yeqing Li","Zheng Xu","Ming-Hsuan Yang","Serge Belongie","Yin Cui"],"pdf_url":"https://arxiv.org/pdf/2207.07646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07645v1","updated":"2022-07-15T17:58:27Z","published":"2022-07-15T17:58:27Z","title":"A Probabilistic Autoencoder for Type Ia Supernovae Spectral Time Series","summary":"  We construct a physically-parameterized probabilistic autoencoder (PAE) to\nlearn the intrinsic diversity of type Ia supernovae (SNe Ia) from a sparse set\nof spectral time series. The PAE is a two-stage generative model, composed of\nan Auto-Encoder (AE) which is interpreted probabilistically after training\nusing a Normalizing Flow (NF). We demonstrate that the PAE learns a\nlow-dimensional latent space that captures the nonlinear range of features that\nexists within the population, and can accurately model the spectral evolution\nof SNe Ia across the full range of wavelength and observation times directly\nfrom the data. By introducing a correlation penalty term and multi-stage\ntraining setup alongside our physically-parameterized network we show that\nintrinsic and extrinsic modes of variability can be separated during training,\nremoving the need for the additional models to perform magnitude\nstandardization. We then use our PAE in a number of downstream tasks on SNe Ia\nfor increasingly precise cosmological analyses, including automatic detection\nof SN outliers, the generation of samples consistent with the data\ndistribution, and solving the inverse problem in the presence of noisy and\nincomplete data to constrain cosmological distance measurements. We find that\nthe optimal number of intrinsic model parameters appears to be three, in line\nwith previous studies, and show that we can standardize our test sample of SNe\nIa with an RMS of $0.091 \\pm 0.010$ mag, which corresponds to $0.074 \\pm 0.010$\nmag if peculiar velocity contributions are removed. Trained models and codes\nare released at\n\\href{https://github.com/georgestein/suPAErnova}{github.com/georgestein/suPAErnova}\n","authors":["George Stein","Uros Seljak","Vanessa Bohm","G. Aldering","P. Antilogus","C. Aragon","S. Bailey","C. Baltay","S. Bongard","K. Boone","C. Buton","Y. Copin","S. Dixon","D. Fouchez","E. Gangler","R. Gupta","B. Hayden","W. Hillebrandt","M. Karmen","A. G. Kim","M. Kowalski","D. Kusters","P. F. Leget","F. Mondon","J. Nordin","R. Pain","E. Pecontal","R. Pereira","S. Perlmutter","K. A. Ponder","D. Rabinowitz","M. Rigault","D. Rubin","K. Runge","C. Saunders","G. Smadja","N. Suzuki","C. Tao","R. C. Thomas","M. Vincenzi"],"pdf_url":"https://arxiv.org/pdf/2207.07645v1.pdf","comment":"23 pages, 8 Figures, 1 Table. Accepted to ApJ"},{"id":"http://arxiv.org/abs/2207.07635v1","updated":"2022-07-15T17:50:51Z","published":"2022-07-15T17:50:51Z","title":"Is a Caption Worth a Thousand Images? A Controlled Study for\n  Representation Learning","summary":"  The development of CLIP [Radford et al., 2021] has sparked a debate on\nwhether language supervision can result in vision models with more transferable\nrepresentations than traditional image-only methods. Our work studies this\nquestion through a carefully controlled comparison of two approaches in terms\nof their ability to learn representations that generalize to downstream\nclassification tasks. We find that when the pre-training dataset meets certain\ncriteria -- it is sufficiently large and contains descriptive captions with low\nvariability -- image-only methods do not match CLIP's transfer performance,\neven when they are trained with more image data. However, contrary to what one\nmight expect, there are practical settings in which these criteria are not met,\nwherein added supervision through captions is actually detrimental. Motivated\nby our findings, we devise simple prescriptions to enable CLIP to better\nleverage the language information present in existing pre-training datasets.\n","authors":["Shibani Santurkar","Yann Dubois","Rohan Taori","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2207.07635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.13450v2","updated":"2022-07-15T17:38:45Z","published":"2021-12-26T21:06:28Z","title":"Acoustic scene classification using auditory datasets","summary":"  The approach used not only challenges some of the fundamental mathematical\ntechniques used so far in early experiments of the same trend but also\nintroduces new scopes and new horizons for interesting results. The physics\ngoverning spectrograms have been optimized in the project along with exploring\nhow it handles the intense requirements of the problem at hand. Major\ncontributions and developments brought under the light, through this project\ninvolve using better mathematical techniques and problem-specific machine\nlearning methods. Improvised data analysis and data augmentation for audio\ndatasets like frequency masking and random frequency-time stretching are used\nin the project and hence are explained in this paper. In the used methodology,\nthe audio transforms principle were also tried and explored, and indeed the\ninsights gained were used constructively in the later stages of the project.\nUsing a deep learning principle is surely one of them. Also, in this paper, the\npotential scopes and upcoming research openings in both short and long term\ntunnel of time has been presented. Although much of the results gained are\ndomain-specific as of now, they are surely potent enough to produce novel\nsolutions in various different domains of diverse backgrounds.\n","authors":["Jayesh Kumpawat","Shubhajit Dey"],"pdf_url":"https://arxiv.org/pdf/2112.13450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07626v1","updated":"2022-07-15T17:38:01Z","published":"2022-07-15T17:38:01Z","title":"Computing-In-Memory Neural Network Accelerators for Safety-Critical\n  Systems: Can Small Device Variations Be Disastrous?","summary":"  Computing-in-Memory (CiM) architectures based on emerging non-volatile memory\n(NVM) devices have demonstrated great potential for deep neural network (DNN)\nacceleration thanks to their high energy efficiency. However, NVM devices\nsuffer from various non-idealities, especially device-to-device variations due\nto fabrication defects and cycle-to-cycle variations due to the stochastic\nbehavior of devices. As such, the DNN weights actually mapped to NVM devices\ncould deviate significantly from the expected values, leading to large\nperformance degradation. To address this issue, most existing works focus on\nmaximizing average performance under device variations. This objective would\nwork well for general-purpose scenarios. But for safety-critical applications,\nthe worst-case performance must also be considered. Unfortunately, this has\nbeen rarely explored in the literature. In this work, we formulate the problem\nof determining the worst-case performance of CiM DNN accelerators under the\nimpact of device variations. We further propose a method to effectively find\nthe specific combination of device variation in the high-dimensional space that\nleads to the worst-case performance. We find that even with very small device\nvariations, the accuracy of a DNN can drop drastically, causing concerns when\ndeploying CiM accelerators in safety-critical applications. Finally, we show\nthat surprisingly none of the existing methods used to enhance average DNN\nperformance in CiM accelerators are very effective when extended to enhance the\nworst-case performance, and further research down the road is needed to address\nthis problem.\n","authors":["Zheyu Yan","Xiaobo Sharon Hu","Yiyu Shi"],"pdf_url":"https://arxiv.org/pdf/2207.07626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07624v1","updated":"2022-07-15T17:37:42Z","published":"2022-07-15T17:37:42Z","title":"Feed-Forward Source-Free Latent Domain Adaptation via Cross-Attention","summary":"  We study the highly practical but comparatively under-studied problem of\nlatent-domain adaptation, where a source model should be adapted to a target\ndataset that contains a mixture of unlabelled domain-relevant and\ndomain-irrelevant examples. Furthermore, motivated by the requirements for data\nprivacy and the need for embedded and resource-constrained devices of all kinds\nto adapt to local data distributions, we focus on the setting of feed-forward\nsource-free domain adaptation, where adaptation should not require access to\nthe source dataset, and also be back propagation-free. Our solution is to\nmeta-learn a network capable of embedding the mixed-relevance target dataset\nand dynamically adapting inference for target examples using cross-attention.\nThe resulting framework leads to consistent improvement on strong ERM\nbaselines. We also show that our framework sometimes even improves on the upper\nbound of domain-supervised adaptation, where only domain-relevant instances are\nprovided for adaptation. This suggests that human annotated domain labels may\nnot always be optimal, and raises the possibility of doing better through\nautomated instance selection.\n","authors":["Ondrej Bohdal","Da Li","Shell Xu Hu","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2207.07624v1.pdf","comment":"Shorter version accepted at the First Workshop of Pre-training:\n  Perspectives, Pitfalls, and Paths Forward at ICML 2022"},{"id":"http://arxiv.org/abs/2207.07612v1","updated":"2022-07-15T17:11:26Z","published":"2022-07-15T17:11:26Z","title":"Blessing of Nonconvexity in Deep Linear Models: Depth Flattens the\n  Optimization Landscape Around the True Solution","summary":"  This work characterizes the effect of depth on the optimization landscape of\nlinear regression, showing that, despite their nonconvexity, deeper models have\nmore desirable optimization landscape. We consider a robust and\nover-parameterized setting, where a subset of measurements are grossly\ncorrupted with noise and the true linear model is captured via an $N$-layer\nlinear neural network. On the negative side, we show that this problem\n\\textit{does not} have a benign landscape: given any $N\\geq 1$, with constant\nprobability, there exists a solution corresponding to the ground truth that is\nneither local nor global minimum. However, on the positive side, we prove that,\nfor any $N$-layer model with $N\\geq 2$, a simple sub-gradient method becomes\noblivious to such ``problematic'' solutions; instead, it converges to a\nbalanced solution that is not only close to the ground truth but also enjoys a\nflat local landscape, thereby eschewing the need for \"early stopping\". Lastly,\nwe empirically verify that the desirable optimization landscape of deeper\nmodels extends to other robust learning tasks, including deep matrix recovery\nand deep ReLU networks with $\\ell_1$-loss.\n","authors":["Jianhao Ma","Salar Fattahi"],"pdf_url":"https://arxiv.org/pdf/2207.07612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07611v1","updated":"2022-07-15T17:10:48Z","published":"2022-07-15T17:10:48Z","title":"Position Prediction as an Effective Pretraining Strategy","summary":"  Transformers have gained increasing popularity in a wide range of\napplications, including Natural Language Processing (NLP), Computer Vision and\nSpeech Recognition, because of their powerful representational capacity.\nHowever, harnessing this representational capacity effectively requires a large\namount of data, strong regularization, or both, to mitigate overfitting.\nRecently, the power of the Transformer has been unlocked by self-supervised\npretraining strategies based on masked autoencoders which rely on\nreconstructing masked inputs, directly, or contrastively from unmasked content.\nThis pretraining strategy which has been used in BERT models in NLP, Wav2Vec\nmodels in Speech and, recently, in MAE models in Vision, forces the model to\nlearn about relationships between the content in different parts of the input\nusing autoencoding related objectives. In this paper, we propose a novel, but\nsurprisingly simple alternative to content reconstruction~-- that of predicting\nlocations from content, without providing positional information for it. Doing\nso requires the Transformer to understand the positional relationships between\ndifferent parts of the input, from their content alone. This amounts to an\nefficient implementation where the pretext task is a classification problem\namong all possible positions for each input token. We experiment on both Vision\nand Speech benchmarks, where our approach brings improvements over strong\nsupervised training baselines and is comparable to modern\nunsupervised/self-supervised pretraining methods. Our method also enables\nTransformers trained without position embeddings to outperform ones trained\nwith full position information.\n","authors":["Shuangfei Zhai","Navdeep Jaitly","Jason Ramapuram","Dan Busbridge","Tatiana Likhomanenko","Joseph Yitan Cheng","Walter Talbott","Chen Huang","Hanlin Goh","Joshua Susskind"],"pdf_url":"https://arxiv.org/pdf/2207.07611v1.pdf","comment":"Accepted to ICML 2022"},{"id":"http://arxiv.org/abs/2207.07605v1","updated":"2022-07-15T17:04:41Z","published":"2022-07-15T17:04:41Z","title":"Algorithms to estimate Shapley value feature attributions","summary":"  Feature attributions based on the Shapley value are popular for explaining\nmachine learning models; however, their estimation is complex from both a\ntheoretical and computational standpoint. We disentangle this complexity into\ntwo factors: (1)~the approach to removing feature information, and (2)~the\ntractable estimation strategy. These two factors provide a natural lens through\nwhich we can better understand and compare 24 distinct algorithms. Based on the\nvarious feature removal approaches, we describe the multiple types of Shapley\nvalue feature attributions and methods to calculate each one. Then, based on\nthe tractable estimation strategies, we characterize two distinct families of\napproaches: model-agnostic and model-specific approximations. For the\nmodel-agnostic approximations, we benchmark a wide class of estimation\napproaches and tie them to alternative yet equivalent characterizations of the\nShapley value. For the model-specific approximations, we clarify the\nassumptions crucial to each method's tractability for linear, tree, and deep\nmodels. Finally, we identify gaps in the literature and promising future\nresearch directions.\n","authors":["Hugh Chen","Ian C. Covert","Scott M. Lundberg","Su-In Lee"],"pdf_url":"https://arxiv.org/pdf/2207.07605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.14522v3","updated":"2022-07-15T17:04:24Z","published":"2020-11-30T03:21:05Z","title":"Feature Learning in Infinite-Width Neural Networks","summary":"  As its width tends to infinity, a deep neural network's behavior under\ngradient descent can become simplified and predictable (e.g. given by the\nNeural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK\nparametrization). However, we show that the standard and NTK parametrizations\nof a neural network do not admit infinite-width limits that can learn features,\nwhich is crucial for pretraining and transfer learning such as with BERT. We\npropose simple modifications to the standard parametrization to allow for\nfeature learning in the limit. Using the *Tensor Programs* technique, we derive\nexplicit formulas for such limits. On Word2Vec and few-shot learning on\nOmniglot via MAML, two canonical tasks that rely crucially on feature learning,\nwe compute these limits exactly. We find that they outperform both NTK\nbaselines and finite-width networks, with the latter approaching the\ninfinite-width feature learning performance as width increases.\n  More generally, we classify a natural space of neural network\nparametrizations that generalizes standard, NTK, and Mean Field\nparametrizations. We show 1) any parametrization in this space either admits\nfeature learning or has an infinite-width training dynamics given by kernel\ngradient descent, but not both; 2) any such infinite-width limit can be\ncomputed using the Tensor Programs technique. Code for our experiments can be\nfound at github.com/edwardjhu/TP4.\n","authors":["Greg Yang","Edward J. Hu"],"pdf_url":"https://arxiv.org/pdf/2011.14522v3.pdf","comment":"4th paper in the Tensor Programs series. Appearing in ICML 2021"},{"id":"http://arxiv.org/abs/2202.04104v2","updated":"2022-07-15T16:43:14Z","published":"2022-02-08T19:13:13Z","title":"Teaching Networks to Solve Optimization Problems","summary":"  Leveraging machine learning to facilitate the optimization process is an\nemerging field that holds the promise to bypass the fundamental computational\nbottleneck caused by classic iterative solvers in critical applications\nrequiring near-real-time optimization. The majority of existing approaches\nfocus on learning data-driven optimizers that lead to fewer iterations in\nsolving an optimization. In this paper, we take a different approach and\npropose to replace the iterative solvers altogether with a trainable parametric\nset function, that outputs the optimal arguments/parameters of an optimization\nproblem in a single feed forward. We denote our method as Learning to Optimize\nthe Optimization Process (LOOP). We show the feasibility of learning such\nparametric (set) functions to solve various classic optimization problems\nincluding linear/nonlinear regression, principal component analysis,\ntransport-based coreset, and quadratic programming in supply management\napplications. In addition, we propose two alternative approaches for learning\nsuch parametric functions, with and without a solver in the LOOP. Finally,\nthrough various numerical experiments, we show that the trained solvers could\nbe orders of magnitude faster than the classic iterative solvers while\nproviding near optimal solutions.\n","authors":["Xinran Liu","Yuzhe Lu","Ali Abbasi","Meiyi Li","Javad Mohammadi","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2202.04104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07589v1","updated":"2022-07-15T16:38:14Z","published":"2022-07-15T16:38:14Z","title":"A two-step machine learning approach to statistical post-processing of\n  weather forecasts for power generation","summary":"  By the end of 2021, the renewable energy share of the global electricity\ncapacity reached 38.3% and the new installations are dominated by wind and\nsolar energy, showing global increases of 12.7% and 18.5%, respectively.\nHowever, both wind and photovoltaic energy sources are highly volatile making\nplanning difficult for grid operators, so accurate forecasts of the\ncorresponding weather variables are essential for reliable electricity\npredictions. The most advanced approach in weather prediction is the ensemble\nmethod, which opens the door for probabilistic forecasting; though ensemble\nforecast are often underdispersive and subject to systematic bias. Hence, they\nrequire some form of statistical post-processing, where parametric models\nprovide full predictive distributions of the weather variables at hand. We\npropose a general two-step machine learning-based approach to calibrating\nensemble weather forecasts, where in the first step improved point forecasts\nare generated, which are then together with various ensemble statistics serve\nas input features of the neural network estimating the parameters of the\npredictive distribution. In two case studies based of 100m wind speed and\nglobal horizontal irradiance forecasts of the operational ensemble pre diction\nsystem of the Hungarian Meteorological Service, the predictive performance of\nthis novel method is compared with the forecast skill of the raw ensemble and\nthe state-of-the-art parametric approaches. Both case studies confirm that at\nleast up to 48h statistical post-processing substantially improves the\npredictive performance of the raw ensemble for all considered forecast\nhorizons. The investigated variants of the proposed two-step method outperform\nin skill their competitors and the suggested new approach is well applicable\nfor different weather quantities and for a fair range of predictive\ndistributions.\n","authors":["Ágnes Baran","Sándor Baran"],"pdf_url":"https://arxiv.org/pdf/2207.07589v1.pdf","comment":"25 pages, 12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2106.09613v2","updated":"2022-07-15T16:38:06Z","published":"2021-06-17T15:47:50Z","title":"Meta-Calibration: Learning of Model Calibration Using Differentiable\n  Expected Calibration Error","summary":"  Calibration of neural networks is a topical problem that is becoming more and\nmore important as neural networks increasingly underpin real-world\napplications. The problem is especially noticeable when using modern neural\nnetworks, for which there is a significant difference between the confidence of\nthe model and the probability of correct prediction. Various strategies have\nbeen proposed to improve calibration, yet accurate calibration remains\nchallenging. We propose a novel framework with two contributions: introducing a\ndifferentiable surrogate for expected calibration error (DECE) that allows\ncalibration quality to be directly optimised, and a meta-learning framework\nthat uses DECE to optimise for validation set calibration with respect to model\nhyper-parameters. The results show that we achieve competitive performance with\nstate-of-the-art calibration approaches. Our framework opens up a new avenue\nand toolset for tackling calibration, which we believe will inspire further\nwork in this important challenge.\n","authors":["Ondrej Bohdal","Yongxin Yang","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2106.09613v2.pdf","comment":"Shorter version was presented at ICML 2021 Workshop on Uncertainty\n  and Robustness in Deep Learning"},{"id":"http://arxiv.org/abs/2205.01931v2","updated":"2022-07-15T16:23:53Z","published":"2022-05-04T08:06:55Z","title":"Self-supervised learning in non-small cell lung cancer discovers novel\n  morphological clusters linked to patient outcome and molecular phenotypes","summary":"  Histopathological images provide the definitive source of cancer diagnosis,\ncontaining information used by pathologists to identify and subclassify\nmalignant disease, and to guide therapeutic choices. These images contain vast\namounts of information, much of which is currently unavailable to human\ninterpretation. Supervised deep learning approaches have been powerful for\nclassification tasks, but they are inherently limited by the cost and quality\nof annotations. Therefore, we developed Histomorphological Phenotype Learning,\nan unsupervised methodology, which requires no annotations and operates via the\nself-discovery of discriminatory image features in small image tiles. Tiles are\ngrouped into morphologically similar clusters which appear to represent\nrecurrent modes of tumor growth emerging under natural selection. These\nclusters have distinct features which can be identified using orthogonal\nmethods. Applied to lung cancer tissues, we show that they align closely with\npatient outcomes, with histopathologically recognised tumor types and growth\npatterns, and with transcriptomic measures of immunophenotype.\n","authors":["Adalberto Claudio Quiros","Nicolas Coudray","Anna Yeaton","Xinyu Yang","Luis Chiriboga","Afreen Karimkhan","Navneet Narula","Harvey Pass","Andre L. Moreira","John Le Quesne","Aristotelis Tsirigos","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2205.01931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07572v1","updated":"2022-07-15T16:22:07Z","published":"2022-07-15T16:22:07Z","title":"Outlier detection of vital sign trajectories from COVID-19 patients","summary":"  There is growing interest in continuous wearable vital sign sensors for\nmonitoring patients remotely at home. These monitors are usually coupled to an\nalerting system, which is triggered when vital sign measurements fall outside a\npredefined normal range. Trends in vital signs, such as an increasing heart\nrate, are often indicative of deteriorating health, but are rarely incorporated\ninto alerting systems. In this work, we present a novel outlier detection\nalgorithm to identify such abnormal vital sign trends. We introduce a\ndistance-based measure to compare vital sign trajectories. For each patient in\nour dataset, we split vital sign time series into 180 minute, non-overlapping\nepochs. We then calculated a distance between all pairs of epochs using the\ndynamic time warp distance. Each epoch was characterized by its mean pairwise\ndistance (average link distance) to all other epochs, with large distances\nconsidered as outliers. We applied this method to a pilot dataset collected\nover 1561 patient-hours from 8 patients who had recently been discharged from\nhospital after contracting COVID-19. We show that outlier epochs correspond\nwell with patients who were subsequently readmitted to hospital. We also show,\ndescriptively, how epochs transition from normal to abnormal for one such\npatient.\n","authors":["Sara Summerton","Ann Tivey","Rohan Shotton","Gavin Brown","Oliver C. Redfern","Rachel Oakley","John Radford","David C. Wong"],"pdf_url":"https://arxiv.org/pdf/2207.07572v1.pdf","comment":"4 pages, 4 figures, 1 table. Submitted to IEEE BHI 2022, decision\n  pending"},{"id":"http://arxiv.org/abs/2207.07570v1","updated":"2022-07-15T16:19:23Z","published":"2022-07-15T16:19:23Z","title":"The Nature of Temporal Difference Errors in Multi-step Distributional\n  Reinforcement Learning","summary":"  We study the multi-step off-policy learning approach to distributional RL.\nDespite the apparent similarity between value-based RL and distributional RL,\nour study reveals intriguing and fundamental differences between the two cases\nin the multi-step setting. We identify a novel notion of path-dependent\ndistributional TD error, which is indispensable for principled multi-step\ndistributional RL. The distinction from the value-based case bears important\nimplications on concepts such as backward-view algorithms. Our work provides\nthe first theoretical guarantees on multi-step off-policy distributional RL\nalgorithms, including results that apply to the small number of existing\napproaches to multi-step distributional RL. In addition, we derive a novel\nalgorithm, Quantile Regression-Retrace, which leads to a deep RL agent\nQR-DQN-Retrace that shows empirical improvements over QR-DQN on the Atari-57\nbenchmark. Collectively, we shed light on how unique challenges in multi-step\ndistributional RL can be addressed both in theory and practice.\n","authors":["Yunhao Tang","Mark Rowland","Rémi Munos","Bernardo Ávila Pires","Will Dabney","Marc G. Bellemare"],"pdf_url":"https://arxiv.org/pdf/2207.07570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.15000v2","updated":"2022-07-15T16:17:50Z","published":"2021-11-29T22:38:13Z","title":"Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable\n  Prototypes","summary":"  We present a deformable prototypical part network (Deformable ProtoPNet), an\ninterpretable image classifier that integrates the power of deep learning and\nthe interpretability of case-based reasoning. This model classifies input\nimages by comparing them with prototypes learned during training, yielding\nexplanations in the form of \"this looks like that.\" However, while previous\nmethods use spatially rigid prototypes, we address this shortcoming by\nproposing spatially flexible prototypes. Each prototype is made up of several\nprototypical parts that adaptively change their relative spatial positions\ndepending on the input image. Consequently, a Deformable ProtoPNet can\nexplicitly capture pose variations and context, improving both model accuracy\nand the richness of explanations provided. Compared to other case-based\ninterpretable models using prototypes, our approach achieves state-of-the-art\naccuracy and gives an explanation with greater context. The code is available\nat https://github.com/jdonnelly36/Deformable-ProtoPNet.\n","authors":["Jon Donnelly","Alina Jade Barnett","Chaofan Chen"],"pdf_url":"https://arxiv.org/pdf/2111.15000v2.pdf","comment":"This was published in CVPR 2022"},{"id":"http://arxiv.org/abs/2207.07560v1","updated":"2022-07-15T16:06:33Z","published":"2022-07-15T16:06:33Z","title":"Skill-based Model-based Reinforcement Learning","summary":"  Model-based reinforcement learning (RL) is a sample-efficient way of learning\ncomplex behaviors by leveraging a learned single-step dynamics model to plan\nactions in imagination. However, planning every action for long-horizon tasks\nis not practical, akin to a human planning out every muscle movement. Instead,\nhumans efficiently plan with high-level skills to solve complex tasks. From\nthis intuition, we propose a Skill-based Model-based RL framework (SkiMo) that\nenables planning in the skill space using a skill dynamics model, which\ndirectly predicts the skill outcomes, rather than predicting all small details\nin the intermediate states, step by step. For accurate and efficient long-term\nplanning, we jointly learn the skill dynamics model and a skill repertoire from\nprior experience. We then harness the learned skill dynamics model to\naccurately simulate and plan over long horizons in the skill space, which\nenables efficient downstream learning of long-horizon, sparse reward tasks.\nExperimental results in navigation and manipulation domains show that SkiMo\nextends the temporal horizon of model-based approaches and improves the sample\nefficiency for both model-based RL and skill-based RL. Code and videos are\navailable at \\url{https://clvrai.com/skimo}\n","authors":["Lucy Xiaoyang Shi","Joseph J. Lim","Youngwoon Lee"],"pdf_url":"https://arxiv.org/pdf/2207.07560v1.pdf","comment":"Website: \\url{https://clvrai.com/skimo}"},{"id":"http://arxiv.org/abs/2203.10681v3","updated":"2022-07-15T16:03:20Z","published":"2022-03-21T00:23:09Z","title":"Online Continual Learning for Embedded Devices","summary":"  Real-time on-device continual learning is needed for new applications such as\nhome robots, user personalization on smartphones, and augmented/virtual reality\nheadsets. However, this setting poses unique challenges: embedded devices have\nlimited memory and compute capacity and conventional machine learning models\nsuffer from catastrophic forgetting when updated on non-stationary data\nstreams. While several online continual learning models have been developed,\ntheir effectiveness for embedded applications has not been rigorously studied.\nIn this paper, we first identify criteria that online continual learners must\nmeet to effectively perform real-time, on-device learning. We then study the\nefficacy of several online continual learning methods when used with mobile\nneural networks. We measure their performance, memory usage, compute\nrequirements, and ability to generalize to out-of-domain inputs.\n","authors":["Tyler L. Hayes","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2203.10681v3.pdf","comment":"To appear in the Conference on Lifelong Learning Agents (CoLLAs-2022)"},{"id":"http://arxiv.org/abs/2207.02808v2","updated":"2022-07-15T16:00:54Z","published":"2022-07-06T16:54:36Z","title":"Improved conformalized quantile regression","summary":"  Conformalized quantile regression is a procedure that inherits the advantages\nof conformal prediction and quantile regression. That is, we use quantile\nregression to estimate the true conditional quantile and then apply a conformal\nstep on a calibration set to ensure marginal coverage. In this way, we get\nadaptive prediction intervals that account for heteroscedasticity. However, the\naforementioned conformal step lacks adaptiveness as described in (Romano et\nal., 2019). To overcome this limitation, instead of applying a single conformal\nstep after estimating conditional quantiles with quantile regression, we\npropose to cluster the explanatory variables weighted by their permutation\nimportance with an optimized k-means and apply k conformal steps. To show that\nthis improved version outperforms the classic version of conformalized quantile\nregression and is more adaptive to heteroscedasticity, we extensively compare\nthe prediction intervals of both in open datasets.\n","authors":["Martim Sousa","Ana Maria Tomé","José Moreira"],"pdf_url":"https://arxiv.org/pdf/2207.02808v2.pdf","comment":"10 pages, 17 figures"},{"id":"http://arxiv.org/abs/2207.07553v1","updated":"2022-07-15T15:51:08Z","published":"2022-07-15T15:51:08Z","title":"CheXplaining in Style: Counterfactual Explanations for Chest X-rays\n  using StyleGAN","summary":"  Deep learning models used in medical image analysis are prone to raising\nreliability concerns due to their black-box nature. To shed light on these\nblack-box models, previous works predominantly focus on identifying the\ncontribution of input features to the diagnosis, i.e., feature attribution. In\nthis work, we explore counterfactual explanations to identify what patterns the\nmodels rely on for diagnosis. Specifically, we investigate the effect of\nchanging features within chest X-rays on the classifier's output to understand\nits decision mechanism. We leverage a StyleGAN-based approach (StyleEx) to\ncreate counterfactual explanations for chest X-rays by manipulating specific\nlatent directions in their latent space. In addition, we propose EigenFind to\nsignificantly reduce the computation time of generated explanations. We\nclinically evaluate the relevancy of our counterfactual explanations with the\nhelp of radiologists. Our code is publicly available.\n","authors":["Matan Atad","Vitalii Dmytrenko","Yitong Li","Xinyue Zhang","Matthias Keicher","Jan Kirschke","Bene Wiestler","Ashkan Khakzar","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2207.07553v1.pdf","comment":"Accepted to the ICML 2022 Interpretable Machine Learning in\n  Healthcare (IMLH) Workshop ----- Project website:\n  http://github.com/CAMP-eXplain-AI/Style-CheXplain"},{"id":"http://arxiv.org/abs/2207.07543v1","updated":"2022-07-15T15:37:03Z","published":"2022-07-15T15:37:03Z","title":"Pick your Neighbor: Local Gauss-Southwell Rule for Fast Asynchronous\n  Decentralized Optimization","summary":"  In decentralized optimization environments, each agent $i$ in a network of\n$n$ optimization nodes possesses a private function $f_i$, and nodes\ncommunicate with their neighbors to cooperatively minimize the aggregate\nobjective $\\sum_{i=1}^n f_i$. In this setting, synchronizing the nodes' updates\nincurs significant communication overhead and computational costs, so much of\nthe recent literature has focused on the analysis and design of asynchronous\noptimization algorithms where agents activate and communicate at arbitrary\ntimes, without requiring a global synchronization enforcer. Nonetheless, in\nmost of the work on the topic, active nodes select a neighbor to contact based\non a fixed probability (e.g., uniformly at random), a choice that ignores the\noptimization landscape at the moment of activation. Instead, in this work we\nintroduce an optimization-aware selection rule that chooses the neighbor with\nthe highest dual cost improvement (a quantity related to a consensus-based\ndualization of the problem at hand). This scheme is related to the coordinate\ndescent (CD) method with a Gauss-Southwell (GS) rule for coordinate updates; in\nour setting however, only a subset of coordinates is accessible at each\niteration (because each node is constrained to communicate only with its direct\nneighbors), so the existing literature on GS methods does not apply. To\novercome this difficulty, we develop a new analytical framework for smooth and\nstrongly convex $f_i$ that covers the class of set-wise CD algorithms -- a\nclass that directly applies to decentralized scenarios, but is not limited to\nthem -- and we show that the proposed set-wise GS rule achieves a speedup by a\nfactor of up to the maximum degree in the network (which is of the order of\n$\\Theta(n)$ in highly connected graphs). The speedup predicted by our\ntheoretical analysis is subsequently validated in numerical experiments with\nsynthetic data.\n","authors":["Marina Costantini","Nikolaos Liakopoulos","Panayotis Mertikopoulos","Thrasyvoulos Spyropoulos"],"pdf_url":"https://arxiv.org/pdf/2207.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.05466v2","updated":"2022-07-15T15:36:03Z","published":"2022-07-12T11:25:53Z","title":"A Benchmark dataset for predictive maintenance","summary":"  The paper describes the MetroPT data set, an outcome of a Predictive\nMaintenance project with an urban metro public transportation service in Porto,\nPortugal. The data was collected between 2020 and 2022 that aimed to develop\nmachine learning methods for online anomaly detection and failure prediction.\nBy capturing several analogic sensor signals (pressure, temperature, current\nconsumption), digital signals (control signals, discrete signals), and GPS\ninformation (latitude, longitude, and speed), we provide a framework that can\nbe easily used and developed for the new machine learning methods. We believe\nthis dataset contains some interesting characteristics and can be a good\nbenchmark for predictive maintenance models.\n","authors":["Bruno Veloso","João Gama","Rita P. Ribeiro","Pedro M. Pereira"],"pdf_url":"https://arxiv.org/pdf/2207.05466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02643v2","updated":"2022-07-15T15:33:25Z","published":"2022-07-06T13:06:31Z","title":"Effective and Efficient Training for Sequential Recommendation using\n  Recency Sampling","summary":"  Many modern sequential recommender systems use deep neural networks, which\ncan effectively estimate the relevance of items but require a lot of time to\ntrain. Slow training increases expenses, hinders product development timescales\nand prevents the model from being regularly updated to adapt to changing user\npreferences. Training such sequential models involves appropriately sampling\npast user interactions to create a realistic training objective. The existing\ntraining objectives have limitations. For instance, next item prediction never\nuses the beginning of the sequence as a learning target, thereby potentially\ndiscarding valuable data. On the other hand, the item masking used by BERT4Rec\nis only weakly related to the goal of the sequential recommendation; therefore,\nit requires much more time to obtain an effective model. Hence, we propose a\nnovel Recency-based Sampling of Sequences training objective that addresses\nboth limitations. We apply our method to various recent and state-of-the-art\nmodel architectures - such as GRU4Rec, Caser, and SASRec. We show that the\nmodels enhanced with our method can achieve performances exceeding or very\nclose to stateof-the-art BERT4Rec, but with much less training time.\n","authors":["Aleksandr Petrov","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2207.02643v2.pdf","comment":"This full research paper is accepted at 16th ACM Conference on\n  Recommender Systems (ACM RecSys)"},{"id":"http://arxiv.org/abs/2207.07539v1","updated":"2022-07-15T15:31:16Z","published":"2022-07-15T15:31:16Z","title":"3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models","summary":"  3D point cloud models are widely applied in safety-critical scenes, which\ndelivers an urgent need to obtain more solid proofs to verify the robustness of\nmodels. Existing verification method for point cloud model is time-expensive\nand computationally unattainable on large networks. Additionally, they cannot\nhandle the complete PointNet model with joint alignment network (JANet) that\ncontains multiplication layers, which effectively boosts the performance of 3D\nmodels. This motivates us to design a more efficient and general framework to\nverify various architectures of point cloud models. The key challenges in\nverifying the large-scale complete PointNet models are addressed as dealing\nwith the cross-non-linearity operations in the multiplication layers and the\nhigh computational complexity of high-dimensional point cloud inputs and added\nlayers. Thus, we propose an efficient verification framework, 3DVerifier, to\ntackle both challenges by adopting a linear relaxation function to bound the\nmultiplication layer and combining forward and backward propagation to compute\nthe certified bounds of the outputs of the point cloud models. Our\ncomprehensive experiments demonstrate that 3DVerifier outperforms existing\nverification algorithms for 3D models in terms of both efficiency and accuracy.\nNotably, our approach achieves an orders-of-magnitude improvement in\nverification efficiency for the large network, and the obtained certified\nbounds are also significantly tighter than the state-of-the-art verifiers. We\nrelease our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use\nby the community.\n","authors":["Ronghui Mu","Wenjie Ruan","Leandro S. Marcolino","Qiang Ni"],"pdf_url":"https://arxiv.org/pdf/2207.07539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.03445v2","updated":"2022-07-15T15:27:49Z","published":"2021-09-08T06:06:28Z","title":"Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning","summary":"  The stochastic approximation algorithm is a widely used probabilistic method\nfor finding a zero of a vector-valued funtion, when only noisy measurements of\nthe function are available. In the literature to date, one can make a\ndistinction between \"synchronous\" updating, whereby every component of the\ncurrent guess is updated at each time, and `\"synchronous\" updating, whereby\nonly one component is updated. In principle, it is also possible to update, at\neach time instant, some but not all components of $\\theta_t$, which might be\ntermed as \"batch asynchronous stochastic approximation\" (BASA). Also, one can\nalso make a distinction between using a \"local\" clock versus a \"global\" clock.\n  In this paper, we propose a unified formulation of batch asynchronous\nstochastic approximation (BASA) algorithms, and develop a general methodology\nfor proving that such algorithms converge, irrespective of whether global or\nlocal clocks are used. These convergence proofs make use of weaker hypotheses\nthan existing results. For example: existing convergence proofs when a local\nclock is used require that the measurement noise is an i.i.d sequence. Here, it\nis assumed that the measurement errors form a martingale difference sequence.\nAlso, all results to date assume that the stochastic step sizes satisfy a\nprobabilistic analog of the Robbins-Monro conditions. We replace this by a\npurely deterministic condition on the irreducibility of the underlying Markov\nprocesses.\n  As specific applications to Reinforcement Learning, we introduce ``batch''\nversions of the temporal difference algorithm $TD(0)$ for value iteration, and\nthe $Q$-learning algorithm for finding the optimal action-value function, and\nalso permit the use of local clocks instead of a global clock. In all cases, we\nestablish the convergence of these algorithms, under milder conditions than in\nthe existing literature.\n","authors":["Rajeeva L. Karandikar","M. Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2109.03445v2.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2207.07533v1","updated":"2022-07-15T15:27:27Z","published":"2022-07-15T15:27:27Z","title":"Selection of the Most Probable Best","summary":"  We consider an expected-value ranking and selection problem where all k\nsolutions' simulation outputs depend on a common uncertain input model. Given\nthat the uncertainty of the input model is captured by a probability simplex on\na finite support, we define the most probable best (MPB) to be the solution\nwhose probability of being optimal is the largest. To devise an efficient\nsampling algorithm to find the MPB, we first derive a lower bound to the large\ndeviation rate of the probability of falsely selecting the MPB, then formulate\nan optimal computing budget allocation (OCBA) problem to find the optimal\nstatic sampling ratios for all solution-input model pairs that maximize the\nlower bound. We devise a series of sequential algorithms that apply\ninterpretable and computationally efficient sampling rules and prove their\nsampling ratios achieve the optimality conditions for the OCBA problem as the\nsimulation budget increases. The algorithms are benchmarked against a\nstate-of-the-art sequential sampling algorithm designed for contextual ranking\nand selection problems and demonstrated to have superior empirical performances\nat finding the MPB.\n","authors":["Taeho Kim","Kyoung-kuk Kim","Eunhye Song"],"pdf_url":"https://arxiv.org/pdf/2207.07533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07528v1","updated":"2022-07-15T15:20:28Z","published":"2022-07-15T15:20:28Z","title":"Modeling Quality and Machine Learning Pipelines through Extended Feature\n  Models","summary":"  The recently increased complexity of Machine Learning (ML) methods, led to\nthe necessity to lighten both the research and industry development processes.\nML pipelines have become an essential tool for experts of many domains, data\nscientists and researchers, allowing them to easily put together several ML\nmodels to cover the full analytic process starting from raw datasets. Over the\nyears, several solutions have been proposed to automate the building of ML\npipelines, most of them focused on semantic aspects and characteristics of the\ninput dataset. However, an approach taking into account the new quality\nconcerns needed by ML systems (like fairness, interpretability, privacy, etc.)\nis still missing. In this paper, we first identify, from the literature, key\nquality attributes of ML systems. Further, we propose a new engineering\napproach for quality ML pipeline by properly extending the Feature Models\nmeta-model. The presented approach allows to model ML pipelines, their quality\nrequirements (on the whole pipeline and on single phases), and quality\ncharacteristics of algorithms used to implement each pipeline phase. Finally,\nwe demonstrate the expressiveness of our model considering the classification\nproblem.\n","authors":["Giordano d'Aloisio","Antinisca Di Marco","Giovanni Stilo"],"pdf_url":"https://arxiv.org/pdf/2207.07528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07524v1","updated":"2022-07-15T15:16:08Z","published":"2022-07-15T15:16:08Z","title":"Heuristic-free Optimization of Force-Controlled Robot Search Strategies\n  in Stochastic Environments","summary":"  In both industrial and service domains, a central benefit of the use of\nrobots is their ability to quickly and reliably execute repetitive tasks.\nHowever, even relatively simple peg-in-hole tasks are typically subject to\nstochastic variations, requiring search motions to find relevant features such\nas holes. While search improves robustness, it comes at the cost of increased\nruntime: More exhaustive search will maximize the probability of successfully\nexecuting a given task, but will significantly delay any downstream tasks. This\ntrade-off is typically resolved by human experts according to simple\nheuristics, which are rarely optimal. This paper introduces an automatic,\ndata-driven and heuristic-free approach to optimize robot search strategies. By\ntraining a neural model of the search strategy on a large set of simulated\nstochastic environments, conditioning it on few real-world examples and\ninverting the model, we can infer search strategies which adapt to the\ntime-variant characteristics of the underlying probability distributions, while\nrequiring very few real-world measurements. We evaluate our approach on two\ndifferent industrial robots in the context of spiral and probe search for THT\nelectronics assembly.\n","authors":["Benjamin Alt","Darko Katic","Rainer Jäkel","Michael Beetz"],"pdf_url":"https://arxiv.org/pdf/2207.07524v1.pdf","comment":"7 pages, 5 figures, accepted to the 2022 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan For\n  code and data, see https://github.com/benjaminalt/dpse"},{"id":"http://arxiv.org/abs/2207.07520v1","updated":"2022-07-15T15:09:07Z","published":"2022-07-15T15:09:07Z","title":"Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual\n  Reality with Redirected Walking","summary":"  Full-immersive multiuser Virtual Reality (VR) envisions supporting\nunconstrained mobility of the users in the virtual worlds, while at the same\ntime constraining their physical movements inside VR setups through redirected\nwalking. For enabling delivery of high data rate video content in real-time,\nthe supporting wireless networks will leverage highly directional communication\nlinks that will \"track\" the users for maintaining the Line-of-Sight (LoS)\nconnectivity. Recurrent Neural Networks (RNNs) and in particular Long\nShort-Term Memory (LSTM) networks have historically presented themselves as a\nsuitable candidate for near-term movement trajectory prediction for natural\nhuman mobility, and have also recently been shown as applicable in predicting\nVR users' mobility under the constraints of redirected walking. In this work,\nwe extend these initial findings by showing that Gated Recurrent Unit (GRU)\nnetworks, another candidate from the RNN family, generally outperform the\ntraditionally utilized LSTMs. Second, we show that context from a virtual world\ncan enhance the accuracy of the prediction if used as an additional input\nfeature in comparison to the more traditional utilization of solely the\nhistorical physical movements of the VR users. Finally, we show that the\nprediction system trained on a static number of coexisting VR users be scaled\nto a multi-user system without significant accuracy degradation.\n","authors":["Filip Lemic","Jakob Struye","Jeroen Famaey"],"pdf_url":"https://arxiv.org/pdf/2207.07520v1.pdf","comment":"7 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2207.04906v2","updated":"2022-07-15T15:06:40Z","published":"2022-07-11T14:33:13Z","title":"HLT-MT: High-resource Language-specific Training for Multilingual Neural\n  Machine Translation","summary":"  Multilingual neural machine translation (MNMT) trained in multiple language\npairs has attracted considerable attention due to fewer model parameters and\nlower training costs by sharing knowledge among multiple languages.\nNonetheless, multilingual training is plagued by language interference\ndegeneration in shared parameters because of the negative interference among\ndifferent translation directions, especially on high-resource languages. In\nthis paper, we propose the multilingual translation model with the\nhigh-resource language-specific training (HLT-MT) to alleviate the negative\ninterference, which adopts the two-stage training with the language-specific\nselection mechanism. Specifically, we first train the multilingual model only\nwith the high-resource pairs and select the language-specific modules at the\ntop of the decoder to enhance the translation quality of high-resource\ndirections. Next, the model is further trained on all available corpora to\ntransfer knowledge from high-resource languages (HRLs) to low-resource\nlanguages (LRLs). Experimental results show that HLT-MT outperforms various\nstrong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic\nexperiments validate the effectiveness of our method in mitigating the negative\ninterference in multilingual training.\n","authors":["Jian Yang","Yuwei Yin","Shuming Ma","Dongdong Zhang","Zhoujun Li","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2207.04906v2.pdf","comment":"7 pages, 7 figures, IJCAI-ECAI 2022"},{"id":"http://arxiv.org/abs/2207.07517v1","updated":"2022-07-15T15:02:38Z","published":"2022-07-15T15:02:38Z","title":"On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution\n  Detection","summary":"  The ability to detect Out-of-Distribution (OOD) data is important in\nsafety-critical applications of deep learning. The aim is to separate\nIn-Distribution (ID) data drawn from the training distribution from OOD data\nusing a measure of uncertainty extracted from a deep neural network. Deep\nEnsembles are a well-established method of improving the quality of uncertainty\nestimates produced by deep neural networks, and have been shown to have\nsuperior OOD detection performance compared to single models. An existing\nintuition in the literature is that the diversity of Deep Ensemble predictions\nindicates distributional shift, and so measures of diversity such as Mutual\nInformation (MI) should be used for OOD detection. We show experimentally that\nthis intuition is not valid on ImageNet-scale OOD detection -- using MI leads\nto 30-40% worse %FPR@95 compared to single-model entropy on some OOD datasets.\nWe suggest an alternative explanation for Deep Ensembles' better OOD detection\nperformance -- OOD detection is binary classification and we are ensembling\ndiverse classifiers. As such we show that practically, even better OOD\ndetection performance can be achieved for Deep Ensembles by averaging\ntask-specific detection scores such as Energy over the ensemble.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07512v1","updated":"2022-07-15T14:57:33Z","published":"2022-07-15T14:57:33Z","title":"Sparse Relational Reasoning with Object-Centric Representations","summary":"  We investigate the composability of soft-rules learned by relational neural\narchitectures when operating over object-centric (slot-based) representations,\nunder a variety of sparsity-inducing constraints. We find that increasing\nsparsity, especially on features, improves the performance of some models and\nleads to simpler relations. Additionally, we observe that object-centric\nrepresentations can be detrimental when not all objects are fully captured; a\nfailure mode to which CNNs are less prone. These findings demonstrate the\ntrade-offs between interpretability and performance, even for models designed\nto tackle relational tasks.\n","authors":["Alex F. Spies","Alessandra Russo","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2207.07512v1.pdf","comment":"ICML 2022, DyNN Workshop"},{"id":"http://arxiv.org/abs/2207.05738v2","updated":"2022-07-15T14:47:03Z","published":"2022-07-12T17:57:17Z","title":"PAC Reinforcement Learning for Predictive State Representations","summary":"  In this paper we study online Reinforcement Learning (RL) in partially\nobservable dynamical systems. We focus on the Predictive State Representations\n(PSRs) model, which is an expressive model that captures other well-known\nmodels such as Partially Observable Markov Decision Processes (POMDP). PSR\nrepresents the states using a set of predictions of future observations and is\ndefined entirely using observable quantities. We develop a novel model-based\nalgorithm for PSRs that can learn a near optimal policy in sample complexity\nscaling polynomially with respect to all the relevant parameters of the\nsystems. Our algorithm naturally works with function approximation to extend to\nsystems with potentially large state and observation spaces. We show that given\na realizable model class, the sample complexity of learning the near optimal\npolicy only scales polynomially with respect to the statistical complexity of\nthe model class, without any explicit polynomial dependence on the size of the\nstate and observation spaces. Notably, our work is the first work that shows\npolynomial sample complexities to compete with the globally optimal policy in\nPSRs. Finally, we demonstrate how our general theorem can be directly used to\nderive sample complexity bounds for special models including $m$-step weakly\nrevealing and $m$-step decodable tabular POMDPs, POMDPs with low-rank latent\ntransition, and POMDPs with linear emission and latent transition.\n","authors":["Wenhao Zhan","Masatoshi Uehara","Wen Sun","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2207.05738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07506v1","updated":"2022-07-15T14:39:57Z","published":"2022-07-15T14:39:57Z","title":"Augmenting Softmax Information for Selective Classification with\n  Out-of-Distribution Data","summary":"  Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07497v1","updated":"2022-07-15T14:34:22Z","published":"2022-07-15T14:34:22Z","title":"Low-bit Shift Network for End-to-End Spoken Language Understanding","summary":"  Deep neural networks (DNN) have achieved impressive success in multiple\ndomains. Over the years, the accuracy of these models has increased with the\nproliferation of deeper and more complex architectures. Thus, state-of-the-art\nsolutions are often computationally expensive, which makes them unfit to be\ndeployed on edge computing platforms. In order to mitigate the high\ncomputation, memory, and power requirements of inferring convolutional neural\nnetworks (CNNs), we propose the use of power-of-two quantization, which\nquantizes continuous parameters into low-bit power-of-two values. This reduces\ncomputational complexity by removing expensive multiplication operations and\nwith the use of low-bit weights. ResNet is adopted as the building block of our\nsolution and the proposed model is evaluated on a spoken language understanding\n(SLU) task. Experimental results show improved performance for shift neural\nnetwork architectures, with our low-bit quantization achieving 98.76 \\% on the\ntest set which is comparable performance to its full-precision counterpart and\nstate-of-the-art solutions.\n","authors":["Anderson R. Avila","Khalil Bibi","Rui Heng Yang","Xinlin Li","Chao Xing","Xiao Chen"],"pdf_url":"https://arxiv.org/pdf/2207.07497v1.pdf","comment":"Accepted at INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2206.15177v2","updated":"2022-07-15T14:31:51Z","published":"2022-06-30T10:29:21Z","title":"A note on large deviations for interacting particle dynamics for finding\n  mixed equilibria in zero-sum games","summary":"  Finding equilibria points in continuous minimax games has become a key\nproblem within machine learning, in part due to its connection to the training\nof generative adversarial networks. Because of existence and robustness issues,\nrecent developments have shifted from pure equilibria to focusing on mixed\nequilibria points. In this note we consider a method proposed by Domingo-Enrich\net al. for finding mixed equilibria in two-layer zero-sum games. The method is\nbased on entropic regularisation and the two competing strategies are\nrepresented by two sets of interacting particles. We show that the sequence of\nempirical measures of the particle system satisfies a large deviation principle\nas the number of particles grows to infinity, and how this implies convergence\nof the empirical measure and the associated Nikaid\\^o-Isoda error,\ncomplementing existing law of large numbers results.\n","authors":["Viktor Nilsson","Pierre Nyquist"],"pdf_url":"https://arxiv.org/pdf/2206.15177v2.pdf","comment":"Revised section 3"},{"id":"http://arxiv.org/abs/2207.07493v1","updated":"2022-07-15T14:28:41Z","published":"2022-07-15T14:28:41Z","title":"Communication-Efficient Diffusion Strategy for Performance Improvement\n  of Federated Learning with Non-IID Data","summary":"  Federated learning (FL) is a novel learning paradigm that addresses the\nprivacy leakage challenge of centralized learning. However, in FL, users with\nnon-independent and identically distributed (non-IID) characteristics can\ndeteriorate the performance of the global model. Specifically, the global model\nsuffers from the weight divergence challenge owing to non-IID data. To address\nthe aforementioned challenge, we propose a novel diffusion strategy of the\nmachine learning (ML) model (FedDif) to maximize the FL performance with\nnon-IID data. In FedDif, users spread local models to neighboring users over\nD2D communications. FedDif enables the local model to experience different\ndistributions before parameter aggregation. Furthermore, we theoretically\ndemonstrate that FedDif can circumvent the weight divergence challenge. On the\ntheoretical basis, we propose the communication-efficient diffusion strategy of\nthe ML model, which can determine the trade-off between the learning\nperformance and communication cost based on auction theory. The performance\nevaluation results show that FedDif improves the test accuracy of the global\nmodel by 11% compared to the baseline FL with non-IID settings. Moreover,\nFedDif improves communication efficiency in perspective of the number of\ntransmitted sub-frames and models by 2.77 folds than the latest methods\n","authors":["Seyoung Ahn","Soohyeong Kim","Yongseok Kwon","Joohan Park","Jiseung Youn","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2207.07493v1.pdf","comment":"18 pages, 6 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2207.07483v1","updated":"2022-07-15T14:09:04Z","published":"2022-07-15T14:09:04Z","title":"A Systematic Review and Replicability Study of BERT4Rec for Sequential\n  Recommendation","summary":"  BERT4Rec is an effective model for sequential recommendation based on the\nTransformer architecture. In the original publication, BERT4Rec claimed\nsuperiority over other available sequential recommendation approaches (e.g.\nSASRec), and it is now frequently being used as a state-of-the art baseline for\nsequential recommendations. However, not all subsequent publications confirmed\nthis result and proposed other models that were shown to outperform BERT4Rec in\neffectiveness. In this paper we systematically review all publications that\ncompare BERT4Rec with another popular Transformer-based model, namely SASRec,\nand show that BERT4Rec results are not consistent within these publications. To\nunderstand the reasons behind this inconsistency, we analyse the available\nimplementations of BERT4Rec and show that we fail to reproduce results of the\noriginal BERT4Rec publication when using their default configuration\nparameters. However, we are able to replicate the reported results with the\noriginal code if training for a much longer amount of time (up to 30x) compared\nto the default configuration. We also propose our own implementation of\nBERT4Rec based on the Hugging Face Transformers library, which we demonstrate\nreplicates the originally reported results on 3 out 4 datasets, while requiring\nup to 95% less training time to converge. Overall, from our systematic review\nand detailed experiments, we conclude that BERT4Rec does indeed exhibit\nstate-of-the-art effectiveness for sequential recommendation, but only when\ntrained for a sufficient amount of time. Additionally, we show that our\nimplementation can further benefit from adapting other Transformer\narchitectures that are available in the Hugging Face Transformers library (e.g.\nusing disentangled attention, as provided by DeBERTa, or larger hidden layer\nsize cf. ALBERT).\n","authors":["Aleksandr Petrov","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2207.07483v1.pdf","comment":"This paper is accepted at the Reproducibility track of the ACM RecSys\n  '22 conference"},{"id":"http://arxiv.org/abs/2207.07482v1","updated":"2022-07-15T14:05:44Z","published":"2022-07-15T14:05:44Z","title":"The Mechanical Neural Network(MNN) -- A physical implementation of a\n  multilayer perceptron for education and hands-on experimentation","summary":"  In this paper the Mechanical Neural Network(MNN) is introduced, a physical\nimplementation of a multilayer perceptron(MLP) with ReLU activation functions,\ntwo input neurons, four hidden neurons and two output neurons. This physical\nmodel of a MLP is used in education to give a hands on experience and allow\nstudents to experience the effect of changing the parameters of the network on\nthe output. Neurons are small wooden levers which are connected by threads.\nStudents can adapt the weights between the neurons by moving the clamps\nconnecting a neuron via a thread to the next. The MNN can model real valued\nfunctions and logical operators including XOR.\n","authors":["Axel Schaffland"],"pdf_url":"https://arxiv.org/pdf/2207.07482v1.pdf","comment":"short video (30sec): https://youtu.be/zMxh3Io3hFE, full presentation\n  video: https://youtu.be/cEzk8JKDzy4; 8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2207.07068v2","updated":"2022-07-15T14:05:36Z","published":"2022-07-14T17:16:45Z","title":"Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey","summary":"  This paper provides a comprehensive survey of bias mitigation methods for\nachieving fairness in Machine Learning (ML) models. We collect a total of 234\npublications concerning bias mitigation for ML classifiers. These methods can\nbe distinguished based on their intervention procedure (i.e., pre-processing,\nin-processing, post-processing) and the technology they apply. We investigate\nhow existing bias mitigation methods are evaluated in the literature. In\nparticular, we consider datasets, metrics and benchmarking. Based on the\ngathered insights (e.g., what is the most popular fairness metric? How many\ndatasets are used for evaluating bias mitigation methods?). We hope to support\npractitioners in making informed choices when developing and evaluating new\nbias mitigation methods.\n","authors":["Max Hort","Zhenpeng Chen","Jie M. Zhang","Federica Sarro","Mark Harman"],"pdf_url":"https://arxiv.org/pdf/2207.07068v2.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.16004v4","updated":"2022-07-15T13:59:54Z","published":"2022-03-30T02:27:01Z","title":"Theory of Acceleration of Decision Making by Correlated Time Sequences","summary":"  Photonic accelerators have been intensively studied to provide enhanced\ninformation processing capability to benefit from the unique attributes of\nphysical processes. Recently, it has been reported that chaotically oscillating\nultrafast time series from a laser, called laser chaos, provide the ability to\nsolve multi-armed bandit (MAB) problems or decision-making problems at GHz\norder. Furthermore, it has been confirmed that the negatively correlated\ntime-domain structure of laser chaos contributes to the acceleration of\ndecision-making. However, the underlying mechanism of why decision-making is\naccelerated by correlated time series is unknown. In this study, we demonstrate\na theoretical model to account for accelerating decision-making by correlated\ntime sequence. We first confirm the effectiveness of the negative\nautocorrelation inherent in time series for solving two-armed bandit problems\nusing Fourier transform surrogate methods. We propose a theoretical model that\nconcerns the correlated time series subjected to the decision-making system and\nthe internal status of the system therein in a unified manner, inspired by\ncorrelated random walks. We demonstrate that the performance derived\nanalytically by the theory agrees well with the numerical simulations, which\nconfirms the validity of the proposed model and leads to optimal system design.\nThe present study paves the way for improving the effectiveness of correlated\ntime series for decision-making, impacting artificial intelligence and other\napplications.\n","authors":["Norihiro Okada","Tomoki Yamagami","Nicolas Chauvet","Yusuke Ito","Mikio Hasegawa","Makoto Naruse"],"pdf_url":"https://arxiv.org/pdf/2203.16004v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07475v1","updated":"2022-07-15T13:42:21Z","published":"2022-07-15T13:42:21Z","title":"Stable Invariant Models via Koopman Spectra","summary":"  Weight-tied models have attracted attention in the modern development of\nneural networks. The deep equilibrium model (DEQ) represents infinitely deep\nneural networks with weight-tying, and recent studies have shown the potential\nof this type of approach. DEQs are needed to iteratively solve root-finding\nproblems in training and are built on the assumption that the underlying\ndynamics determined by the models converge to a fixed point. In this paper, we\npresent the stable invariant model (SIM), a new class of deep models that in\nprinciple approximates DEQs under stability and extends the dynamics to more\ngeneral ones converging to an invariant set (not restricted in a fixed point).\nThe key ingredient in deriving SIMs is a representation of the dynamics with\nthe spectra of the Koopman and Perron--Frobenius operators. This perspective\napproximately reveals stable dynamics with DEQs and then derives two variants\nof SIMs. We also propose an implementation of SIMs that can be learned in the\nsame way as feedforward models. We illustrate the empirical performance of SIMs\nwith experiments and demonstrate that SIMs achieve comparative or superior\nperformance against DEQs in several learning tasks.\n","authors":["Takuya Konishi","Yoshinobu Kawahara"],"pdf_url":"https://arxiv.org/pdf/2207.07475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10586v2","updated":"2022-07-15T13:39:29Z","published":"2022-05-21T13:09:01Z","title":"Calibration of Natural Language Understanding Models with Venn--ABERS\n  Predictors","summary":"  Transformers, currently the state-of-the-art in natural language\nunderstanding (NLU) tasks, are prone to generate uncalibrated predictions or\nextreme probabilities, making the process of taking different decisions based\non their output relatively difficult. In this paper we propose to build several\ninductive Venn--ABERS predictors (IVAP), which are guaranteed to be well\ncalibrated under minimal assumptions, based on a selection of pre-trained\ntransformers. We test their performance over a set of diverse NLU tasks and\nshow that they are capable of producing well-calibrated probabilistic\npredictions that are uniformly spread over the [0,1] interval -- all while\nretaining the original model's predictive accuracy.\n","authors":["Patrizio Giovannotti"],"pdf_url":"https://arxiv.org/pdf/2205.10586v2.pdf","comment":"Accepted at the 11th Symposium on Conformal and Probabilistic\n  Prediction with Applications - COPA 2022"},{"id":"http://arxiv.org/abs/2207.07465v1","updated":"2022-07-15T13:18:48Z","published":"2022-07-15T13:18:48Z","title":"Creating an Explainable Intrusion Detection System Using Self Organizing\n  Maps","summary":"  Modern Artificial Intelligence (AI) enabled Intrusion Detection Systems (IDS)\nare complex black boxes. This means that a security analyst will have little to\nno explanation or clarification on why an IDS model made a particular\nprediction. A potential solution to this problem is to research and develop\nExplainable Intrusion Detection Systems (X-IDS) based on current capabilities\nin Explainable Artificial Intelligence (XAI). In this paper, we create a Self\nOrganizing Maps (SOMs) based X-IDS system that is capable of producing\nexplanatory visualizations. We leverage SOM's explainability to create both\nglobal and local explanations. An analyst can use global explanations to get a\ngeneral idea of how a particular IDS model computes predictions. Local\nexplanations are generated for individual datapoints to explain why a certain\nprediction value was computed. Furthermore, our SOM based X-IDS was evaluated\non both explanation generation and traditional accuracy tests using the NSL-KDD\nand the CIC-IDS-2017 datasets.\n","authors":["Jesse Ables","Thomas Kirby","William Anderson","Sudip Mittal","Shahram Rahimi","Ioana Banicescu","Maria Seale"],"pdf_url":"https://arxiv.org/pdf/2207.07465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07458v1","updated":"2022-07-15T13:08:15Z","published":"2022-07-15T13:08:15Z","title":"Joint Application of the Target Trial Causal Framework and Machine\n  Learning Modeling to Optimize Antibiotic Therapy: Use Case on Acute Bacterial\n  Skin and Skin Structure Infections due to Methicillin-resistant\n  Staphylococcus aureus","summary":"  Bacterial infections are responsible for high mortality worldwide.\nAntimicrobial resistance underlying the infection, and multifaceted patient's\nclinical status can hamper the correct choice of antibiotic treatment.\nRandomized clinical trials provide average treatment effect estimates but are\nnot ideal for risk stratification and optimization of therapeutic choice, i.e.,\nindividualized treatment effects (ITE). Here, we leverage large-scale\nelectronic health record data, collected from Southern US academic clinics, to\nemulate a clinical trial, i.e., 'target trial', and develop a machine learning\nmodel of mortality prediction and ITE estimation for patients diagnosed with\nacute bacterial skin and skin structure infection (ABSSSI) due to\nmethicillin-resistant Staphylococcus aureus (MRSA). ABSSSI-MRSA is a\nchallenging condition with reduced treatment options - vancomycin is the\npreferred choice, but it has non-negligible side effects. First, we use\npropensity score matching to emulate the trial and create a treatment\nrandomized (vancomycin vs. other antibiotics) dataset. Next, we use this data\nto train various machine learning methods (including boosted/LASSO logistic\nregression, support vector machines, and random forest) and choose the best\nmodel in terms of area under the receiver characteristic (AUC) through\nbootstrap validation. Lastly, we use the models to calculate ITE and identify\npossible averted deaths by therapy change. The out-of-bag tests indicate that\nSVM and RF are the most accurate, with AUC of 81% and 78%, respectively, but\nBLR/LASSO is not far behind (76%). By calculating the counterfactuals using the\nBLR/LASSO, vancomycin increases the risk of death, but it shows a large\nvariation (odds ratio 1.2, 95% range 0.4-3.8) and the contribution to outcome\nprobability is modest. Instead, the RF exhibits stronger changes in ITE,\nsuggesting more complex treatment heterogeneity.\n","authors":["Inyoung Jun","Simone Marini","Christina A. Boucher","J. Glenn Morris","Jiang Bian","Mattia Prosperi"],"pdf_url":"https://arxiv.org/pdf/2207.07458v1.pdf","comment":"This is the Proceedings of the KDD workshop on Applied Data Science\n  for Healthcare (DSHealth 2022), which was held on Washington D.C, August 14\n  2022"},{"id":"http://arxiv.org/abs/2012.04708v2","updated":"2022-07-15T13:02:08Z","published":"2020-12-08T19:54:20Z","title":"ODFNet: Using orientation distribution functions to characterize 3D\n  point clouds","summary":"  Learning new representations of 3D point clouds is an active research area in\n3D vision, as the order-invariant point cloud structure still presents\nchallenges to the design of neural network architectures. Recent works explored\nlearning either global or local features or both for point clouds, however none\nof the earlier methods focused on capturing contextual shape information by\nanalysing local orientation distribution of points. In this paper, we leverage\non point orientation distributions around a point in order to obtain an\nexpressive local neighborhood representation for point clouds. We achieve this\nby dividing the spherical neighborhood of a given point into predefined cone\nvolumes, and statistics inside each volume are used as point features. In this\nway, a local patch can be represented by not only the selected point's nearest\nneighbors, but also considering a point density distribution defined along\nmultiple orientations around the point. We are then able to construct an\norientation distribution function (ODF) neural network that involves an\nODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet\nmodel achieves state-of the-art accuracy for object classification on\nModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS\ndatasets.\n","authors":["Yusuf H. Sahin","Alican Mertan","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2012.04708v2.pdf","comment":"The paper is under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2207.07418v1","updated":"2022-07-15T11:57:14Z","published":"2022-07-15T11:57:14Z","title":"LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds\n  Representing Laparoscopic Scenes","summary":"  The semantic segmentation of surgical scenes is a prerequisite for task\nautomation in robot assisted interventions. We propose LapSeg3D, a novel\nDNN-based approach for the voxel-wise annotation of point clouds representing\nsurgical scenes. As the manual annotation of training data is highly time\nconsuming, we introduce a semi-autonomous clustering-based pipeline for the\nannotation of the gallbladder, which is used to generate segmented labels for\nthe DNN. When evaluated against manually annotated data, LapSeg3D achieves an\nF1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo\nporcine livers. We show LapSeg3D to generalize accurately across different\ngallbladders and datasets recorded with different RGB-D camera systems.\n","authors":["Benjamin Alt","Christian Kunz","Darko Katic","Rayan Younis","Rainer Jäkel","Beat Peter Müller-Stich","Martin Wagner","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2207.07418v1.pdf","comment":"6 pages, 5 figures, accepted at the 2022 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan"},{"id":"http://arxiv.org/abs/2207.07417v1","updated":"2022-07-15T11:55:09Z","published":"2022-07-15T11:55:09Z","title":"Low Rank Approximation for General Tensor Networks","summary":"  We study the problem of approximating a given tensor with $q$ modes $A \\in\n\\mathbb{R}^{n \\times \\ldots \\times n}$ with an arbitrary tensor network of rank\n$k$ -- that is, a graph $G = (V, E)$, where $|V| = q$, together with a\ncollection of tensors $\\{U_v \\mid v \\in V\\}$ which are contracted in the manner\nspecified by $G$ to obtain a tensor $T$. For each mode of $U_v$ corresponding\nto an edge incident to $v$, the dimension is $k$, and we wish to find $U_v$\nsuch that the Frobenius norm distance between $T$ and $A$ is minimized. This\ngeneralizes a number of well-known tensor network decompositions, such as the\nTensor Train, Tensor Ring, Tucker, and PEPS decompositions. We approximate $A$\nby a binary tree network $T'$ with $O(q)$ cores, such that the dimension on\neach edge of this network is at most $\\widetilde{O}(k^{O(dt)} \\cdot\nq/\\varepsilon)$, where $d$ is the maximum degree of $G$ and $t$ is its\ntreewidth, such that $\\|A - T'\\|_F^2 \\leq (1 + \\varepsilon) \\|A - T\\|_F^2$. The\nrunning time of our algorithm is $O(q \\cdot \\text{nnz}(A)) + n \\cdot\n\\text{poly}(k^{dt}q/\\varepsilon)$, where $\\text{nnz}(A)$ is the number of\nnonzero entries of $A$. Our algorithm is based on a new dimensionality\nreduction technique for tensor decomposition which may be of independent\ninterest.\n  We also develop fixed-parameter tractable $(1 + \\varepsilon)$-approximation\nalgorithms for Tensor Train and Tucker decompositions, improving the running\ntime of Song, Woodruff and Zhong (SODA, 2019) and avoiding the use of generic\npolynomial system solvers. We show that our algorithms have a nearly optimal\ndependence on $1/\\varepsilon$ assuming that there is no $O(1)$-approximation\nalgorithm for the $2 \\to 4$ norm with better running time than brute force.\nFinally, we give additional results for Tucker decomposition with robust loss\nfunctions, and fixed-parameter tractable CP decomposition.\n","authors":["Arvind V. Mahankali","David P. Woodruff","Ziyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.07417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07411v1","updated":"2022-07-15T11:39:37Z","published":"2022-07-15T11:39:37Z","title":"Plex: Towards Reliability using Pretrained Large Model Extensions","summary":"  A recent trend in artificial intelligence is the use of pretrained models for\nlanguage and vision tasks, which have achieved extraordinary performance but\nalso puzzling failures. Probing these models' abilities in diverse ways is\ntherefore critical to the field. In this paper, we explore the reliability of\nmodels, where we define a reliable model as one that not only achieves strong\npredictive performance but also performs well consistently over many\ndecision-making tasks involving uncertainty (e.g., selective prediction, open\nset recognition), robust generalization (e.g., accuracy and proper scoring\nrules such as log-likelihood on in- and out-of-distribution datasets), and\nadaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of\ntasks over 40 datasets in order to evaluate different aspects of reliability on\nboth vision and language domains. To improve reliability, we developed ViT-Plex\nand T5-Plex, pretrained large model extensions for vision and language\nmodalities, respectively. Plex greatly improves the state-of-the-art across\nreliability tasks, and simplifies the traditional protocol as it improves the\nout-of-the-box performance and does not require designing scores or tuning the\nmodel for each task. We demonstrate scaling effects over model sizes up to 1B\nparameters and pretraining dataset sizes up to 4B examples. We also demonstrate\nPlex's capabilities on challenging tasks including zero-shot open set\nrecognition, active learning, and uncertainty in conversational language\nunderstanding.\n","authors":["Dustin Tran","Jeremiah Liu","Michael W. Dusenberry","Du Phan","Mark Collier","Jie Ren","Kehang Han","Zi Wang","Zelda Mariet","Huiyi Hu","Neil Band","Tim G. J. Rudner","Karan Singhal","Zachary Nado","Joost van Amersfoort","Andreas Kirsch","Rodolphe Jenatton","Nithum Thain","Honglin Yuan","Kelly Buchanan","Kevin Murphy","D. Sculley","Yarin Gal","Zoubin Ghahramani","Jasper Snoek","Balaji Lakshminarayanan"],"pdf_url":"https://arxiv.org/pdf/2207.07411v1.pdf","comment":"Code available at https://goo.gle/plex-code"},{"id":"http://arxiv.org/abs/2207.07408v1","updated":"2022-07-15T11:28:11Z","published":"2022-07-15T11:28:11Z","title":"pathGCN: Learning General Graph Spatial Operators from Paths","summary":"  Graph Convolutional Networks (GCNs), similarly to Convolutional Neural\nNetworks (CNNs), are typically based on two main operations - spatial and\npoint-wise convolutions. In the context of GCNs, differently from CNNs, a\npre-determined spatial operator based on the graph Laplacian is often chosen,\nallowing only the point-wise operations to be learnt. However, learning a\nmeaningful spatial operator is critical for developing more expressive GCNs for\nimproved performance. In this paper we propose pathGCN, a novel approach to\nlearn the spatial operator from random paths on the graph. We analyze the\nconvergence of our method and its difference from existing GCNs. Furthermore,\nwe discuss several options of combining our learnt spatial operator with\npoint-wise convolutions. Our extensive experiments on numerous datasets suggest\nthat by properly learning both the spatial and point-wise convolutions,\nphenomena like over-smoothing can be inherently avoided, and new\nstate-of-the-art performance is achieved.\n","authors":["Moshe Eliasof","Eldad Haber","Eran Treister"],"pdf_url":"https://arxiv.org/pdf/2207.07408v1.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2207.07399v1","updated":"2022-07-15T11:03:25Z","published":"2022-07-15T11:03:25Z","title":"An Approach for Link Prediction in Directed Complex Networks based on\n  Asymmetric Similarity-Popularity","summary":"  Complex networks are graphs representing real-life systems that exhibit\nunique characteristics not found in purely regular or completely random graphs.\nThe study of such systems is vital but challenging due to the complexity of the\nunderlying processes. This task has nevertheless been made easier in recent\ndecades thanks to the availability of large amounts of networked data. Link\nprediction in complex networks aims to estimate the likelihood that a link\nbetween two nodes is missing from the network. Links can be missing due to\nimperfections in data collection or simply because they are yet to appear.\nDiscovering new relationships between entities in networked data has attracted\nresearchers' attention in various domains such as sociology, computer science,\nphysics, and biology. Most existing research focuses on link prediction in\nundirected complex networks. However, not all real-life systems can be\nfaithfully represented as undirected networks. This simplifying assumption is\noften made when using link prediction algorithms but inevitably leads to loss\nof information about relations among nodes and degradation in prediction\nperformance. This paper introduces a link prediction method designed explicitly\nfor directed networks. It is based on the similarity-popularity paradigm, which\nhas recently proven successful in undirected networks. The presented algorithms\nhandle the asymmetry in node relationships by modeling it as asymmetry in\nsimilarity and popularity. Given the observed network topology, the algorithms\napproximate the hidden similarities as shortest path distances using edge\nweights that capture and factor out the links' asymmetry and nodes' popularity.\nThe proposed approach is evaluated on real-life networks, and the experimental\nresults demonstrate its effectiveness in predicting missing links across a\nbroad spectrum of networked data types and sizes.\n","authors":["Hafida Benhidour","Lama Almeshkhas","Said Kerrache"],"pdf_url":"https://arxiv.org/pdf/2207.07399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2003.07311v7","updated":"2022-07-15T10:39:38Z","published":"2020-03-16T16:27:49Z","title":"clDice -- A Novel Topology-Preserving Loss Function for Tubular\n  Structure Segmentation","summary":"  Accurate segmentation of tubular, network-like structures, such as vessels,\nneurons, or roads, is relevant to many fields of research. For such structures,\nthe topology is their most important characteristic; particularly preserving\nconnectedness: in the case of vascular networks, missing a connected vessel\nentirely alters the blood-flow dynamics. We introduce a novel similarity\nmeasure termed centerlineDice (short clDice), which is calculated on the\nintersection of the segmentation masks and their (morphological) skeleta. We\ntheoretically prove that clDice guarantees topology preservation up to homotopy\nequivalence for binary 2D and 3D segmentation. Extending this, we propose a\ncomputationally efficient, differentiable loss function (soft-clDice) for\ntraining arbitrary neural segmentation networks. We benchmark the soft-clDice\nloss on five public datasets, including vessels, roads and neurons (2D and 3D).\nTraining on soft-clDice leads to segmentation with more accurate connectivity\ninformation, higher graph similarity, and better volumetric scores.\n","authors":["Suprosanna Shit","Johannes C. Paetzold","Anjany Sekuboyina","Ivan Ezhov","Alexander Unger","Andrey Zhylka","Josien P. W. Pluim","Ulrich Bauer","Bjoern H. Menze"],"pdf_url":"https://arxiv.org/pdf/2003.07311v7.pdf","comment":"* The authors Suprosanna Shit and Johannes C. Paetzold contributed\n  equally to the work"},{"id":"http://arxiv.org/abs/2201.00378v3","updated":"2022-07-15T10:36:02Z","published":"2022-01-02T16:57:35Z","title":"Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring\n  Platforms","summary":"  Air pollution monitoring platforms play a very important role in preventing\nand mitigating the effects of pollution. Recent advances in the field of graph\nsignal processing have made it possible to describe and analyze air pollution\nmonitoring networks using graphs. One of the main applications is the\nreconstruction of the measured signal in a graph using a subset of sensors.\nReconstructing the signal using information from sensor neighbors can help\nimprove the quality of network data, examples are filling in missing data with\ncorrelated neighboring nodes, or correcting a drifting sensor with neighboring\nsensors that are more accurate. This paper compares the use of various types of\ngraph signal reconstruction methods applied to real data sets of Spanish air\npollution reference stations. The methods considered are Laplacian\ninterpolation, graph signal processing low-pass based graph signal\nreconstruction, and kernel-based graph signal reconstruction, and are compared\non actual air pollution data sets measuring O3, NO2, and PM10. The ability of\nthe methods to reconstruct the signal of a pollutant is shown, as well as the\ncomputational cost of this reconstruction. The results indicate the superiority\nof methods based on kernel-based graph signal reconstruction, as well as the\ndifficulties of the methods to scale in an air pollution monitoring network\nwith a large number of low-cost sensors. However, we show that scalability can\nbe overcome with simple methods, such as partitioning the network using a\nclustering algorithm.\n","authors":["Pau Ferrer-Cid","Jose M. Barcelo-Ordinas","Jorge Garcia-Vidal"],"pdf_url":"https://arxiv.org/pdf/2201.00378v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2207.03574v2","updated":"2022-07-15T10:19:22Z","published":"2022-06-18T04:14:38Z","title":"Demystifying the Adversarial Robustness of Random Transformation\n  Defenses","summary":"  Neural networks' lack of robustness against attacks raises concerns in\nsecurity-sensitive settings such as autonomous vehicles. While many\ncountermeasures may look promising, only a few withstand rigorous evaluation.\nDefenses using random transformations (RT) have shown impressive results,\nparticularly BaRT (Raff et al., 2019) on ImageNet. However, this type of\ndefense has not been rigorously evaluated, leaving its robustness properties\npoorly understood. Their stochastic properties make evaluation more challenging\nand render many proposed attacks on deterministic models inapplicable. First,\nwe show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation\nis ineffective and likely overestimates its robustness. We then attempt to\nconstruct the strongest possible RT defense through the informed selection of\ntransformations and Bayesian optimization for tuning their parameters.\nFurthermore, we create the strongest possible attack to evaluate our RT\ndefense. Our new attack vastly outperforms the baseline, reducing the accuracy\nby 83% compared to the 19% reduction by the commonly used EoT attack\n($4.3\\times$ improvement). Our result indicates that the RT defense on the\nImagenette dataset (a ten-class subset of ImageNet) is not robust against\nadversarial examples. Extending the study further, we use our new attack to\nadversarially train RT defense (called AdvRT), resulting in a large robustness\ngain. Code is available at\nhttps://github.com/wagner-group/demystify-random-transform.\n","authors":["Chawin Sitawarin","Zachary Golan-Strieb","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2207.03574v2.pdf","comment":"ICML 2022 (short presentation), AAAI 2022 AdvML Workshop (best paper,\n  oral presentation)"},{"id":"http://arxiv.org/abs/2206.12747v3","updated":"2022-07-15T10:14:45Z","published":"2022-06-25T22:48:27Z","title":"HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network","summary":"  Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in\nthe worst scenario, they may lead to adverse drug reactions (ADRs). Predicting\nall DDIs is a challenging and critical problem. Most existing computational\nmodels integrate drug-centric information from different sources and leverage\nthem as features in machine learning classifiers to predict DDIs. However,\nthese models have a high chance of failure, especially for the new drugs when\nall the information is not available. This paper proposes a novel Hypergraph\nNeural Network (HyGNN) model based on only the SMILES string of drugs,\navailable for any drug, for the DDI prediction problem. To capture the drug\nsimilarities, we create a hypergraph from drugs' chemical substructures\nextracted from the SMILES strings. Then, we develop HyGNN consisting of a novel\nattention-based hypergraph edge encoder to get the representation of drugs as\nhyperedges and a decoder to predict the interactions between drug pairs.\nFurthermore, we conduct extensive experiments to evaluate our model and compare\nit with several state-of-the-art methods. Experimental results demonstrate that\nour proposed HyGNN model effectively predicts DDIs and impressively outperforms\nthe baselines with a maximum ROC-AUC and PR-AUC of 97.9% and 98.1%,\nrespectively.\n","authors":["Khaled Mohammed Saifuddin","Briana Bumgardner","Farhan Tanvir","Esra Akbas"],"pdf_url":"https://arxiv.org/pdf/2206.12747v3.pdf","comment":"Some new experiments have been added. One more dataset has been\n  considered. Theoretical part has been updated too"},{"id":"http://arxiv.org/abs/2202.07364v2","updated":"2022-07-15T09:48:16Z","published":"2022-02-15T12:45:42Z","title":"Zero-Shot Assistance in Novel Decision Problems","summary":"  We consider the problem of creating assistants that can help agents - often\nhumans - solve novel sequential decision problems, assuming the agent is not\nable to specify the reward function explicitly to the assistant. Instead of\naiming to automate, and act in place of the agent as in current approaches, we\ngive the assistant an advisory role and keep the agent in the loop as the main\ndecision maker. The difficulty is that we must account for potential biases\ninduced by limitations or constraints of the agent which may cause it to\nseemingly irrationally reject advice. To do this we introduce a novel\nformalization of assistance that models these biases, allowing the assistant to\ninfer and adapt to them. We then introduce a new method for planning the\nassistant's advice which can scale to large decision making problems. Finally,\nwe show experimentally that our approach adapts to these agent biases, and\nresults in higher cumulative reward for the agent than automation-based\nalternatives.\n","authors":["Sebastiaan De Peuter","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2202.07364v2.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2207.07362v1","updated":"2022-07-15T09:21:09Z","published":"2022-07-15T09:21:09Z","title":"Error analysis for deep neural network approximations of parametric\n  hyperbolic conservation laws","summary":"  We derive rigorous bounds on the error resulting from the approximation of\nthe solution of parametric hyperbolic scalar conservation laws with ReLU neural\nnetworks. We show that the approximation error can be made as small as desired\nwith ReLU neural networks that overcome the curse of dimensionality. In\naddition, we provide an explicit upper bound on the generalization error in\nterms of the training error, number of training samples and the neural network\nsize. The theoretical results are illustrated by numerical experiments.\n","authors":["Tim De Ryck","Siddhartha Mishra"],"pdf_url":"https://arxiv.org/pdf/2207.07362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.05297v2","updated":"2022-07-15T09:11:18Z","published":"2022-07-12T04:12:10Z","title":"Efficient and Privacy Preserving Group Signature for Federated Learning","summary":"  Federated Learning (FL) is a Machine Learning (ML) technique that aims to\nreduce the threats to user data privacy. Training is done using the raw data on\nthe users' device, called clients, and only the training results, called\ngradients, are sent to the server to be aggregated and generate an updated\nmodel. However, we cannot assume that the server can be trusted with private\ninformation, such as metadata related to the owner or source of the data. So,\nhiding the client information from the server helps reduce privacy-related\nattacks. Therefore, the privacy of the client's identity, along with the\nprivacy of the client's data, is necessary to make such attacks more difficult.\nThis paper proposes an efficient and privacy-preserving protocol for FL based\non group signature. A new group signature for federated learning, called GSFL,\nis designed to not only protect the privacy of the client's data and identity\nbut also significantly reduce the computation and communication costs\nconsidering the iterative process of federated learning. We show that GSFL\noutperforms existing approaches in terms of computation, communication, and\nsignaling costs. Also, we show that the proposed protocol can handle various\nsecurity attacks in the federated learning environment.\n","authors":["Sneha Kanchan","Jae Won Jang","Jun Yong Yoon","Bong Jun Choi"],"pdf_url":"https://arxiv.org/pdf/2207.05297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07347v1","updated":"2022-07-15T08:48:40Z","published":"2022-07-15T08:48:40Z","title":"Feasibility of Inconspicuous GAN-generated Adversarial Patches against\n  Object Detection","summary":"  Standard approaches for adversarial patch generation lead to noisy\nconspicuous patterns, which are easily recognizable by humans. Recent research\nhas proposed several approaches to generate naturalistic patches using\ngenerative adversarial networks (GANs), yet only a few of them were evaluated\non the object detection use case. Moreover, the state of the art mostly focuses\non suppressing a single large bounding box in input by overlapping it with the\npatch directly. Suppressing objects near the patch is a different, more complex\ntask. In this work, we have evaluated the existing approaches to generate\ninconspicuous patches. We have adapted methods, originally developed for\ndifferent computer vision tasks, to the object detection use case with YOLOv3\nand the COCO dataset. We have evaluated two approaches to generate naturalistic\npatches: by incorporating patch generation into the GAN training process and by\nusing the pretrained GAN. For both cases, we have assessed a trade-off between\nperformance and naturalistic patch appearance. Our experiments have shown, that\nusing a pre-trained GAN helps to gain realistic-looking patches while\npreserving the performance similar to conventional adversarial patches.\n","authors":["Svetlana Pavlitskaya","Bianca-Marina Codău","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2207.07347v1.pdf","comment":"Accepted for publication at the IJCAI 2022 AISafety workshop"},{"id":"http://arxiv.org/abs/2207.05518v2","updated":"2022-07-15T08:47:30Z","published":"2022-07-12T13:22:29Z","title":"Tracking Objects as Pixel-wise Distributions","summary":"  Multi-object tracking (MOT) requires detecting and associating objects\nthrough frames. Unlike tracking via detected bounding boxes or tracking objects\nas points, we propose tracking objects as pixel-wise distributions. We\ninstantiate this idea on a transformer-based architecture, P3AFormer, with\npixel-wise propagation, prediction, and association. P3AFormer propagates\npixel-wise features guided by flow information to pass messages between frames.\nFurthermore, P3AFormer adopts a meta-architecture to produce multi-scale object\nfeature maps. During inference, a pixel-wise association procedure is proposed\nto recover object connections through frames based on the pixel-wise\nprediction. P3AFormer yields 81.2\\% in terms of MOTA on the MOT17 benchmark --\nthe first among all transformer networks to reach 80\\% MOTA in literature.\nP3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.\n","authors":["Zelin Zhao","Ze Wu","Yueqing Zhuang","Boxun Li","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2207.05518v2.pdf","comment":"Accepted in ECCV22 as an oral presentation paper. The code&project\n  page is at\n  https://github.com/dvlab-research/ECCV22-P3AFormer-Tracking-Objects-as-Pixel-wise-Distributions"},{"id":"http://arxiv.org/abs/2207.07338v1","updated":"2022-07-15T08:30:33Z","published":"2022-07-15T08:30:33Z","title":"Context-sensitive neocortical neurons transform the effectiveness and\n  efficiency of neural information processing","summary":"  There is ample neurobiological evidence that context-sensitive neocortical\nneurons use their apical inputs as context to amplify the transmission of\ncoherent feedforward (FF) inputs. However, it has not been demonstrated until\nnow how this known mechanism can provide useful neural computation. Here we\nshow for the first time that the processing and learning capabilities of this\nform of neural information processing are well-matched to the abilities of\nmammalian neocortex. Specifically, we show that a network composed of such\nlocal processors restricts the transmission of conflicting information to\nhigher levels and greatly reduces the amount of activity required to process\nlarge amounts of heterogeneous real-world data e.g., when processing\naudiovisual speech, these local processors use seen lip movements to\nselectively amplify FF transmission of the auditory information that those\nmovements generate and vice versa. As this mechanism is shown to be far more\neffective and efficient than the best available forms of deep neural nets, it\noffers a step-change in understanding the brain's mysterious energy-saving\nmechanism and inspires advances in designing enhanced forms of biologically\nplausible machine learning algorithms.\n","authors":["Ahsan Adeel","Mario Franco","Mohsin Raza","Khubaib Ahmed"],"pdf_url":"https://arxiv.org/pdf/2207.07338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1801.06720v4","updated":"2022-07-15T07:52:04Z","published":"2018-01-20T20:02:00Z","title":"Optimal Rates for Spectral Algorithms with Least-Squares Regression over\n  Hilbert Spaces","summary":"  In this paper, we study regression problems over a separable Hilbert space\nwith the square loss, covering non-parametric regression over a reproducing\nkernel Hilbert space. We investigate a class of spectral/regularized\nalgorithms, including ridge regression, principal component regression, and\ngradient methods. We prove optimal, high-probability convergence results in\nterms of variants of norms for the studied algorithms, considering a capacity\nassumption on the hypothesis space and a general source condition on the target\nfunction. Consequently, we obtain almost sure convergence results with optimal\nrates. Our results improve and generalize previous results, filling a\ntheoretical gap for the non-attainable cases.\n","authors":["Junhong Lin","Alessandro Rudi","Lorenzo Rosasco","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/1801.06720v4.pdf","comment":"Updating acknowledgments; Journal version"},{"id":"http://arxiv.org/abs/2103.10689v3","updated":"2022-07-15T07:36:31Z","published":"2021-03-19T08:40:30Z","title":"Interpretable Deep Learning: Interpretation, Interpretability,\n  Trustworthiness, and Beyond","summary":"  Deep neural networks have been well-known for their superb handling of\nvarious machine learning and artificial intelligence tasks. However, due to\ntheir over-parameterized black-box nature, it is often difficult to understand\nthe prediction results of deep models. In recent years, many interpretation\ntools have been proposed to explain or reveal how deep models make decisions.\nIn this paper, we review this line of research and try to make a comprehensive\nsurvey. Specifically, we first introduce and clarify two basic concepts --\ninterpretations and interpretability -- that people usually get confused about.\nTo address the research efforts in interpretations, we elaborate the designs of\na number of interpretation algorithms, from different perspectives, by\nproposing a new taxonomy. Then, to understand the interpretation results, we\nalso survey the performance metrics for evaluating interpretation algorithms.\nFurther, we summarize the current works in evaluating models' interpretability\nusing \"trustworthy\" interpretation algorithms. Finally, we review and discuss\nthe connections between deep models' interpretations and other factors, such as\nadversarial robustness and learning from interpretations, and we introduce\nseveral open-source libraries for interpretation algorithms and evaluation\napproaches.\n","authors":["Xuhong Li","Haoyi Xiong","Xingjian Li","Xuanyu Wu","Xiao Zhang","Ji Liu","Jiang Bian","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2103.10689v3.pdf","comment":null},{"id":"http://arxiv.org/abs/1811.01760v2","updated":"2022-07-15T07:25:47Z","published":"2018-11-05T14:50:58Z","title":"Kernel Conjugate Gradient Methods with Random Projections","summary":"  We propose and study kernel conjugate gradient methods (KCGM) with random\nprojections for least-squares regression over a separable Hilbert space.\nConsidering two types of random projections generated by randomized sketches\nand Nystr\\\"{o}m subsampling, we prove optimal statistical results with respect\nto variants of norms for the algorithms under a suitable stopping rule.\nParticularly, our results show that if the projection dimension is proportional\nto the effective dimension of the problem, KCGM with randomized sketches can\ngeneralize optimally, while achieving a computational advantage. As a\ncorollary, we derive optimal rates for classic KCGM in the well-conditioned\nregimes for the case that the target function may not be in the hypothesis\nspace.\n","authors":["Junhong Lin","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/1811.01760v2.pdf","comment":"Updating acknowledgments; Accepted version for Applied and\n  Computational Harmonic Analysis"},{"id":"http://arxiv.org/abs/2206.11349v2","updated":"2022-07-15T07:15:31Z","published":"2022-05-31T08:43:07Z","title":"Prompt Injection: Parameterization of Fixed Inputs","summary":"  Recent works have shown that attaching prompts to the input is effective at\nconditioning Language Models (LM) to perform specific tasks. However, prompts\nare always included in the input text during inference, thus incurring\nsubstantial computational and memory overhead. Also, there is currently no\nstraightforward method of utilizing prompts that are longer than the maximum\ninput length of the LMs without incurring additional costs during inference. We\npropose Prompt Injection (PI), a novel formulation of injecting the prompt into\nthe parameters of an LM to be an efficient alternative to attaching fixed\nprompts to the input. We show that in scenarios with long fixed prompts, PI can\nbe up to 280 times more efficient in terms of total FLOPs than previous\napproaches. We further explore methodologies for PI and show promising results\nin persona-dependent conversation, semantic parsing, and zero-shot learning\nwith task instructions. Through these explorations, we show that PI can be a\npromising direction for conditioning language models, especially in scenarios\nwith long and fixed prompts.\n","authors":["Eunbi Choi","Yongrae Jo","Joel Jang","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2206.11349v2.pdf","comment":"PING results in Table 2 updated (bug fixed)"},{"id":"http://arxiv.org/abs/2207.07315v1","updated":"2022-07-15T07:15:16Z","published":"2022-07-15T07:15:16Z","title":"Pattern Analysis of Money Flow in the Bitcoin Blockchain","summary":"  Bitcoin is the first and highest valued cryptocurrency that stores\ntransactions in a publicly distributed ledger called the blockchain.\nUnderstanding the activity and behavior of Bitcoin actors is a crucial research\ntopic as they are pseudonymous in the transaction network. In this article, we\npropose a method based on taint analysis to extract taint flows --dynamic\nnetworks representing the sequence of Bitcoins transferred from an initial\nsource to other actors until dissolution. Then, we apply graph embedding\nmethods to characterize taint flows. We evaluate our embedding method with\ntaint flows from top mining pools and show that it can classify mining pools\nwith high accuracy. We also found that taint flows from the same period show\nhigh similarity. Our work proves that tracing the money flows can be a\npromising approach to classifying source actors and characterizing different\nmoney flow patterns\n","authors":["Natkamon Tovanich","Rémy Cazabet"],"pdf_url":"https://arxiv.org/pdf/2207.07315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06257v3","updated":"2022-07-15T07:12:56Z","published":"2022-02-13T08:40:47Z","title":"Fine-Grained Population Mobility Data-Based Community-Level COVID-19\n  Prediction Model","summary":"  Predicting the number of infections in the anti-epidemic process is extremely\nbeneficial to the government in developing anti-epidemic strategies, especially\nin fine-grained geographic units. Previous works focus on low spatial\nresolution prediction, e.g., county-level, and preprocess data to the same\ngeographic level, which loses some useful information. In this paper, we\npropose a fine-grained population mobility data-based model (FGC-COVID)\nutilizing data of two geographic levels for community-level COVID-19\nprediction. We use the population mobility data between Census Block Groups\n(CBGs), which is a finer-grained geographic level than community, to build the\ngraph and capture the dependencies between CBGs using graph neural networks\n(GNNs). To mine as finer-grained patterns as possible for prediction, a spatial\nweighted aggregation module is introduced to aggregate the embeddings of CBGs\nto community level based on their geographic affiliation and spatial\nautocorrelation. Extensive experiments on 300 days LA city COVID-19 data\nindicate our model outperforms existing forecasting models on community-level\nCOVID-19 prediction.\n","authors":["Pengyue Jia","Ling Chen","Dandan Lyu"],"pdf_url":"https://arxiv.org/pdf/2202.06257v3.pdf","comment":"Accepted by Cybernetics and Systems"},{"id":"http://arxiv.org/abs/2107.12636v4","updated":"2022-07-15T07:01:47Z","published":"2021-07-27T07:17:12Z","title":"Exploring Sequence Feature Alignment for Domain Adaptive Detection\n  Transformers","summary":"  Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.\n","authors":["Wen Wang","Yang Cao","Jing Zhang","Fengxiang He","Zheng-Jun Zha","Yonggang Wen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2107.12636v4.pdf","comment":"Fix a typo in Eq. 13"},{"id":"http://arxiv.org/abs/2207.06414v2","updated":"2022-07-15T06:38:06Z","published":"2022-07-13T09:15:26Z","title":"Modeling Long-term Dependencies and Short-term Correlations in Patient\n  Journey Data with Temporal Attention Networks for Health Prediction","summary":"  Building models for health prediction based on Electronic Health Records\n(EHR) has become an active research area. EHR patient journey data consists of\npatient time-ordered clinical events/visits from patients. Most existing\nstudies focus on modeling long-term dependencies between visits, without\nexplicitly taking short-term correlations between consecutive visits into\naccount, where irregular time intervals, incorporated as auxiliary information,\nare fed into health prediction models to capture latent progressive patterns of\npatient journeys. We present a novel deep neural network with four modules to\ntake into account the contributions of various variables for health prediction:\ni) the Stacked Attention module strengthens the deep semantics in clinical\nevents within each patient journey and generates visit embeddings, ii) the\nShort-Term Temporal Attention module models short-term correlations between\nconsecutive visit embeddings while capturing the impact of time intervals\nwithin those visit embeddings, iii) the Long-Term Temporal Attention module\nmodels long-term dependencies between visit embeddings while capturing the\nimpact of time intervals within those visit embeddings, iv) and finally, the\nCoupled Attention module adaptively aggregates the outputs of Short-Term\nTemporal Attention and Long-Term Temporal Attention modules to make health\npredictions. Experimental results on MIMIC-III demonstrate superior predictive\naccuracy of our model compared to existing state-of-the-art methods, as well as\nthe interpretability and robustness of this approach. Furthermore, we found\nthat modeling short-term correlations contributes to local priors generation,\nleading to improved predictive modeling of patient journeys.\n","authors":["Yuxi Liu","Zhenhao Zhang","Antonio Jimeno Yepes","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2207.06414v2.pdf","comment":"10 pages, 4 figures, accepted at ACM BCB 2022"},{"id":"http://arxiv.org/abs/2207.07308v1","updated":"2022-07-15T06:21:35Z","published":"2022-07-15T06:21:35Z","title":"Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet\n  Text","summary":"  The wide use of social media and digital technologies facilitates sharing\nvarious news and information about events and activities. Despite sharing\npositive information misleading and false information is also spreading on\nsocial media. There have been efforts in identifying such misleading\ninformation both manually by human experts and automatic tools. Manual effort\ndoes not scale well due to the high volume of information, containing factual\nclaims, are appearing online. Therefore, automatically identifying check-worthy\nclaims can be very useful for human experts. In this study, we describe our\nparticipation in Subtask-1A: Check-worthiness of tweets (English, Dutch and\nSpanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing\nsteps and applied different models to identify whether a given text is worthy\nof fact checking or not. We use the oversampling technique to balance the\ndataset and applied SVM and Random Forest (RF) with TF-IDF representations. We\nalso used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models\nfor the experiments. We used BERT-m for the official submissions and our\nsystems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,\nrespectively. In further experiments, our evaluation shows that transformer\nmodels (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and\nEnglish languages where a different scenario is observed for Spanish.\n","authors":["Prerona Tarannum","Firoj Alam","Md. Arid Hasan","Sheak Rashed Haider Noori"],"pdf_url":"https://arxiv.org/pdf/2207.07308v1.pdf","comment":"Accepted in CLEF 2022"},{"id":"http://arxiv.org/abs/2207.07307v1","updated":"2022-07-15T06:18:00Z","published":"2022-07-15T06:18:00Z","title":"MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with\n  Unknown Number of Sound Sources","summary":"  Recent neural network based Direction of Arrival (DoA) estimation algorithms\nhave performed well on unknown number of sound sources scenarios. These\nalgorithms are usually achieved by mapping the multi-channel audio input to the\nsingle output (i.e. overall spatial pseudo-spectrum (SPS) of all sources), that\nis called MISO. However, such MISO algorithms strongly depend on empirical\nthreshold setting and the angle assumption that the angles between the sound\nsources are greater than a fixed angle. To address these limitations, we\npropose a novel multi-channel input and multiple outputs DoA network called\nMIMO-DoAnet. Unlike the general MISO algorithms, MIMO-DoAnet predicts the SPS\ncoding of each sound source with the help of the informative spatial covariance\nmatrix. By doing so, the threshold task of detecting the number of sound\nsources becomes an easier task of detecting whether there is a sound source in\neach output, and the serious interaction between sound sources disappears\nduring inference stage. Experimental results show that MIMO-DoAnet achieves\nrelative 18.6% and absolute 13.3%, relative 34.4% and absolute 20.2% F1 score\nimprovement compared with the MISO baseline system in 3, 4 sources scenes. The\nresults also demonstrate MIMO-DoAnet alleviates the threshold setting problem\nand solves the angle assumption problem effectively.\n","authors":["Haoran Yin","Meng Ge","Yanjie Fu","Gaoyan Zhang","Longbiao Wang","Lei Zhang","Lin Qiu","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2207.07307v1.pdf","comment":"Accepted by Interspeech 2022"},{"id":"http://arxiv.org/abs/2207.07303v1","updated":"2022-07-15T06:07:11Z","published":"2022-07-15T06:07:11Z","title":"Towards Better Dermoscopic Image Feature Representation Learning for\n  Melanoma Classification","summary":"  Deep learning-based melanoma classification with dermoscopic images has\nrecently shown great potential in automatic early-stage melanoma diagnosis.\nHowever, limited by the significant data imbalance and obvious extraneous\nartifacts, i.e., the hair and ruler markings, discriminative feature extraction\nfrom dermoscopic images is very challenging. In this study, we seek to resolve\nthese problems respectively towards better representation learning for lesion\nfeatures. Specifically, a GAN-based data augmentation (GDA) strategy is adapted\nto generate synthetic melanoma-positive images, in conjunction with the\nproposed implicit hair denoising (IHD) strategy. Wherein the hair-related\nrepresentations are implicitly disentangled via an auxiliary classifier network\nand reversely sent to the melanoma-feature extraction backbone for better\nmelanoma-specific representation learning. Furthermore, to train the IHD\nmodule, the hair noises are additionally labeled on the ISIC2020 dataset,\nmaking it the first large-scale dermoscopic dataset with annotation of\nhair-like artifacts. Extensive experiments demonstrate the superiority of the\nproposed framework as well as the effectiveness of each component. The improved\ndataset publicly avaliable at https://github.com/kirtsy/DermoscopicDataset.\n","authors":["ChengHui Yu","MingKang Tang","ShengGe Yang","MingQing Wang","Zhe Xu","JiangPeng Yan","HanMo Chen","Yu Yang","Xiao-Jun Zeng","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2207.07303v1.pdf","comment":"ICONIP 2021 conference"},{"id":"http://arxiv.org/abs/2207.07296v1","updated":"2022-07-15T05:14:27Z","published":"2022-07-15T05:14:27Z","title":"Direction-Aware Adaptive Online Neural Speech Enhancement with an\n  Augmented Reality Headset in Real Noisy Conversational Environments","summary":"  This paper describes the practical response- and performance-aware\ndevelopment of online speech enhancement for an augmented reality (AR) headset\nthat helps a user understand conversations made in real noisy echoic\nenvironments (e.g., cocktail party). One may use a state-of-the-art blind\nsource separation method called fast multichannel nonnegative matrix\nfactorization (FastMNMF) that works well in various environments thanks to its\nunsupervised nature. Its heavy computational cost, however, prevents its\napplication to real-time processing. In contrast, a supervised beamforming\nmethod that uses a deep neural network (DNN) for estimating spatial information\nof speech and noise readily fits real-time processing, but suffers from drastic\nperformance degradation in mismatched conditions. Given such complementary\ncharacteristics, we propose a dual-process robust online speech enhancement\nmethod based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF\n(back end) is performed in a mini-batch style and the noisy and enhanced speech\npairs are used together with the original parallel training data for updating\nthe direction-aware DNN (front end) with backpropagation at a\ncomputationally-allowable interval. This method is used with a blind\ndereverberation method called weighted prediction error (WPE) for transcribing\nthe noisy reverberant speech of a speaker, which can be detected from video or\nselected by a user's hand gesture or eye gaze, in a streaming manner and\nspatially showing the transcriptions with an AR technique. Our experiment\nshowed that the word error rate was improved by more than 10 points with the\nrun-time adaptation using only twelve minutes of observation.\n","authors":["Kouhei Sekiguchi","Aditya Arie Nugraha","Yicheng Du","Yoshiaki Bando","Mathieu Fontaine","Kazuyoshi Yoshii"],"pdf_url":"https://arxiv.org/pdf/2207.07296v1.pdf","comment":"IEEE/RSJ IROS 2022"},{"id":"http://arxiv.org/abs/2205.03906v2","updated":"2022-07-15T05:14:23Z","published":"2022-05-08T16:16:44Z","title":"Dynamic categories, dynamic operads: From deep learning to prediction\n  markets","summary":"  Natural organized systems adapt to internal and external pressures and this\nseems to happens all the way down. Wanting to think clearly about this idea\nmotivates our paper, and so the idea is elaborated extensively in the\nintroduction, which should be broadly accessible to a\nphilosophically-interested audience.\n  In the remaining sections, we turn to more compressed category theory. We\ndefine the monoidal double category $\\mathbf{Org}$ of dynamic organizations, we\nprovide definitions of $\\mathbf{Org}$-enriched, or \"dynamic\", categorical\nstructures -- e.g. dynamic categories, operads, and monoidal categories -- and\nwe show how they instantiate the motivating philosophical ideas. We give two\nexamples of dynamic categorical structures: prediction markets as a dynamic\noperad and deep learning as a dynamic monoidal category.\n","authors":["Brandon Shapiro","David I. Spivak"],"pdf_url":"https://arxiv.org/pdf/2205.03906v2.pdf","comment":"14 pages + two appendices"},{"id":"http://arxiv.org/abs/2207.07287v1","updated":"2022-07-15T04:33:10Z","published":"2022-07-15T04:33:10Z","title":"Riemannian Natural Gradient Methods","summary":"  This paper studies large-scale optimization problems on Riemannian manifolds\nwhose objective function is a finite sum of negative log-probability losses.\nSuch problems arise in various machine learning and signal processing\napplications. By introducing the notion of Fisher information matrix in the\nmanifold setting, we propose a novel Riemannian natural gradient method, which\ncan be viewed as a natural extension of the natural gradient method from the\nEuclidean setting to the manifold setting. We establish the almost-sure global\nconvergence of our proposed method under standard assumptions. Moreover, we\nshow that if the loss function satisfies certain convexity and smoothness\nconditions and the input-output map satisfies a Riemannian Jacobian stability\ncondition, then our proposed method enjoys a local linear -- or, under the\nLipschitz continuity of the Riemannian Jacobian of the input-output map, even\nquadratic -- rate of convergence. We then prove that the Riemannian Jacobian\nstability condition will be satisfied by a two-layer fully connected neural\nnetwork with batch normalization with high probability, provided that the width\nof the network is sufficiently large. This demonstrates the practical relevance\nof our convergence rate result. Numerical experiments on applications arising\nfrom machine learning demonstrate the advantages of the proposed method over\nstate-of-the-art ones.\n","authors":["Jiang Hu","Ruicheng Ao","Anthony Man-Cho So","Minghan Yang","Zaiwen Wen"],"pdf_url":"https://arxiv.org/pdf/2207.07287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.10450v2","updated":"2022-07-15T04:22:44Z","published":"2021-07-22T04:17:46Z","title":"Learning Sparse Fixed-Structure Gaussian Bayesian Networks","summary":"  Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation\nmodels) are widely used to model causal interactions among continuous\nvariables. In this work, we study the problem of learning a fixed-structure\nGaussian Bayesian network up to a bounded error in total variation distance. We\nanalyze the commonly used node-wise least squares regression (LeastSquares) and\nprove that it has a near-optimal sample complexity. We also study a couple of\nnew algorithms for the problem:\n  - BatchAvgLeastSquares takes the average of several batches of least squares\nsolutions at each node, so that one can interpolate between the batch size and\nthe number of batches. We show that BatchAvgLeastSquares also has near-optimal\nsample complexity.\n  - CauchyEst takes the median of solutions to several batches of linear\nsystems at each node. We show that the algorithm specialized to polytrees,\nCauchyEstTree, has near-optimal sample complexity.\n  Experimentally, we show that for uncontaminated, realizable data, the\nLeastSquares algorithm performs best, but in the presence of contamination or\nDAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares\nrespectively perform better.\n","authors":["Arnab Bhattacharyya","Davin Choo","Rishikesh Gajjala","Sutanu Gayen","Yuhao Wang"],"pdf_url":"https://arxiv.org/pdf/2107.10450v2.pdf","comment":"30 pages, 11 figures"},{"id":"http://arxiv.org/abs/2207.07273v1","updated":"2022-07-15T03:43:35Z","published":"2022-07-15T03:43:35Z","title":"Direction-Aware Joint Adaptation of Neural Speech Enhancement and\n  Recognition in Real Multiparty Conversational Environments","summary":"  This paper describes noisy speech recognition for an augmented reality\nheadset that helps verbal communication within real multiparty conversational\nenvironments. A major approach that has actively been studied in simulated\nenvironments is to sequentially perform speech enhancement and automatic speech\nrecognition (ASR) based on deep neural networks (DNNs) trained in a supervised\nmanner. In our task, however, such a pretrained system fails to work due to the\nmismatch between the training and test conditions and the head movements of the\nuser. To enhance only the utterances of a target speaker, we use beamforming\nbased on a DNN-based speech mask estimator that can adaptively extract the\nspeech components corresponding to a head-relative particular direction. We\npropose a semi-supervised adaptation method that jointly updates the mask\nestimator and the ASR model at run-time using clean speech signals with\nground-truth transcriptions and noisy speech signals with highly-confident\nestimated transcriptions. Comparative experiments using the state-of-the-art\ndistant speech recognition system show that the proposed method significantly\nimproves the ASR performance.\n","authors":["Yicheng Du","Aditya Arie Nugraha","Kouhei Sekiguchi","Yoshiaki Bando","Mathieu Fontaine","Kazuyoshi Yoshii"],"pdf_url":"https://arxiv.org/pdf/2207.07273v1.pdf","comment":"INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2207.07271v1","updated":"2022-07-15T03:37:59Z","published":"2022-07-15T03:37:59Z","title":"Set-based value operators for non-stationary Markovian environments","summary":"  This paper analyzes finite state Markov Decision Processes (MDPs) with\nuncertain parameters in compact sets and re-examines results from robust MDP\nvia set-based fixed point theory. We generalize the Bellman and policy\nevaluation operators to operators that contract on the space of value functions\nand denote them as \\emph{value operators}. We generalize these value operators\nto act on the space of value function sets and denote them as \\emph{set-based\nvalue operators}. We prove that these set-based value operators are\ncontractions in the space of compact value function sets. Leveraging insights\nfrom set theory, we generalize the rectangularity condition for the Bellman\noperator from classic robust MDP literature to a \\emph{containment condition}\nfor a generic value operator, which is weaker and can be applied to a larger\nset of parameter-uncertain MDPs and contractive operators in dynamic\nprogramming and reinforcement learning. We prove that both the rectangularity\ncondition and the containment condition sufficiently ensure that the set-based\nvalue operator's fixed point set contains its own supremum and infimum\nelements. For convex and compact sets of uncertain MDP parameters, we show\nequivalence between the classic robust value function and the supremum of the\nfixed point set of the set-based Bellman operator. Under dynamically changing\nMDP parameters in compact sets, we prove a set convergence result for value\niteration, which otherwise may not converge to a single value function.\n","authors":["Sarah H. Q. Li","Assalé Adjé","Pierre-Loïc Garoche","Behçet Açıkmeşe"],"pdf_url":"https://arxiv.org/pdf/2207.07271v1.pdf","comment":"14 pages, 11 figures, 1 table"},{"id":"http://arxiv.org/abs/2207.07267v1","updated":"2022-07-15T03:16:43Z","published":"2022-07-15T03:16:43Z","title":"ScaleNet: Searching for the Model to Scale","summary":"  Recently, community has paid increasing attention on model scaling and\ncontributed to developing a model family with a wide spectrum of scales.\nCurrent methods either simply resort to a one-shot NAS manner to construct a\nnon-structural and non-scalable model family or rely on a manual yet fixed\nscaling strategy to scale an unnecessarily best base model. In this paper, we\nbridge both two components and propose ScaleNet to jointly search base model\nand scaling strategy so that the scaled large model can have more promising\nperformance. Concretely, we design a super-supernet to embody models with\ndifferent spectrum of sizes (e.g., FLOPs). Then, the scaling strategy can be\nlearned interactively with the base model via a Markov chain-based evolution\nalgorithm and generalized to develop even larger models. To obtain a decent\nsuper-supernet, we design a hierarchical sampling strategy to enhance its\ntraining sufficiency and alleviate the disturbance. Experimental results show\nour scaled networks enjoy significant performance superiority on various FLOPs,\nbut with at least 2.53x reduction on search cost. Codes are available at\nhttps://github.com/luminolx/ScaleNet.\n","authors":["Jiyang Xie","Xiu Su","Shan You","Zhanyu Ma","Fei Wang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2207.07267v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2003.09795v5","updated":"2022-07-15T03:07:22Z","published":"2020-03-22T03:32:09Z","title":"Optimal No-regret Learning in Repeated First-price Auctions","summary":"  We study online learning in repeated first-price auctions with censored\nfeedback, where a bidder, only observing the winning bid at the end of each\nauction, learns to adaptively bid in order to maximize her cumulative payoff.\nTo achieve this goal, the bidder faces a challenging dilemma: if she wins the\nbid--the only way to achieve positive payoffs--then she is not able to observe\nthe highest bid of the other bidders, which we assume is iid drawn from an\nunknown distribution. This dilemma, despite being reminiscent of the\nexploration-exploitation trade-off in contextual bandits, cannot directly be\naddressed by the existing UCB or Thompson sampling algorithms.\n  In this paper, by exploiting the structural properties of first-price\nauctions, we develop the first learning algorithm that achieves\n$O(\\sqrt{T}\\log^{2.5} T)$ regret bound, which is minimax optimal up to $\\log$\nfactors, when the bidder's private values are stochastically generated. We do\nso by providing an algorithm on a general class of problems, called the\npartially ordered contextual bandits, which combine the graph feedback across\nactions, the cross learning across contexts, and a partial order over the\ncontexts. We establish both strengths and weaknesses of this framework, by\nshowing a curious separation that a regret nearly independent of the\naction/context sizes is possible under stochastic contexts, but is impossible\nunder adversarial contexts. Despite the limitation of this general framework,\nwe further exploit the structure of first-price auctions and develop a learning\nalgorithm that operates sample-efficiently (and computationally efficiently) in\nthe presence of adversarially generated private values. We establish an\n$O(\\sqrt{T}\\log^3 T)$ regret bound for this algorithm, hence providing a\ncomplete characterization of optimal learning guarantees for first-price\nauctions.\n","authors":["Yanjun Han","Zhengyuan Zhou","Tsachy Weissman"],"pdf_url":"https://arxiv.org/pdf/2003.09795v5.pdf","comment":"This version (v5) has significantly revised the previous version by\n  adding a new Section 5, and multiple new results to Section 3"},{"id":"http://arxiv.org/abs/2207.04203v2","updated":"2022-07-15T03:01:11Z","published":"2022-07-09T06:25:01Z","title":"Learning to Separate Voices by Spatial Regions","summary":"  We consider the problem of audio voice separation for binaural applications,\nsuch as earphones and hearing aids. While today's neural networks perform\nremarkably well (separating $4+$ sources with 2 microphones) they assume a\nknown or fixed maximum number of sources, K. Moreover, today's models are\ntrained in a supervised manner, using training data synthesized from generic\nsources, environments, and human head shapes.\n  This paper intends to relax both these constraints at the expense of a slight\nalteration in the problem definition. We observe that, when a received mixture\ncontains too many sources, it is still helpful to separate them by region,\ni.e., isolating signal mixtures from each conical sector around the user's\nhead. This requires learning the fine-grained spatial properties of each\nregion, including the signal distortions imposed by a person's head. We propose\na two-stage self-supervised framework in which overheard voices from earphones\nare pre-processed to extract relatively clean personalized signals, which are\nthen used to train a region-wise separation model. Results show promising\nperformance, underscoring the importance of personalization over a generic\nsupervised approach. (audio samples available at our project website:\nhttps://uiuc-earable-computing.github.io/binaural/. We believe this result\ncould help real-world applications in selective hearing, noise cancellation,\nand audio augmented reality.\n","authors":["Zhongweiyang Xu","Romit Roy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2207.04203v2.pdf","comment":"Accepted to ICML 2022. For associated audio samples, see\n  https://uiuc-earable-computing.github.io/binaural"},{"id":"http://arxiv.org/abs/2207.06529v2","updated":"2022-07-15T02:39:02Z","published":"2022-07-13T21:57:44Z","title":"Estimating Classification Confidence Using Kernel Densities","summary":"  This paper investigates the post-hoc calibration of confidence for\n\"exploratory\" machine learning classification problems. The difficulty in these\nproblems stems from the continuing desire to push the boundaries of which\ncategories have enough examples to generalize from when curating datasets, and\nconfusion regarding the validity of those categories. We argue that for such\nproblems the \"one-versus-all\" approach (top-label calibration) must be used\nrather than the \"calibrate-the-full-response-matrix\" approach advocated\nelsewhere in the literature. We introduce and test four new algorithms designed\nto handle the idiosyncrasies of category-specific confidence estimation. Chief\namong these methods is the use of kernel density ratios for confidence\ncalibration including a novel, bulletproof algorithm for choosing the\nbandwidth. We test our claims and explore the limits of calibration on a\nbioinformatics application (PhANNs) as well as the classic MNIST benchmark.\nFinally, our analysis argues that post-hoc calibration should always be\nperformed, should be based only on the test dataset, and should be\nsanity-checked visually.\n","authors":["Peter Salamon","David Salamon","V. Adrian Cantu","Michelle An","Tyler Perry","Robert A. Edwards","Anca M. Segall"],"pdf_url":"https://arxiv.org/pdf/2207.06529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07260v1","updated":"2022-07-15T02:35:41Z","published":"2022-07-15T02:35:41Z","title":"Accelerated Probabilistic Marching Cubes by Deep Learning for\n  Time-Varying Scalar Ensembles","summary":"  Visualizing the uncertainty of ensemble simulations is challenging due to the\nlarge size and multivariate and temporal features of ensemble data sets. One\npopular approach to studying the uncertainty of ensembles is analyzing the\npositional uncertainty of the level sets. Probabilistic marching cubes is a\ntechnique that performs Monte Carlo sampling of multivariate Gaussian noise\ndistributions for positional uncertainty visualization of level sets. However,\nthe technique suffers from high computational time, making interactive\nvisualization and analysis impossible to achieve. This paper introduces a\ndeep-learning-based approach to learning the level-set uncertainty for\ntwo-dimensional ensemble data with a multivariate Gaussian noise assumption. We\ntrain the model using the first few time steps from time-varying ensemble data\nin our workflow. We demonstrate that our trained model accurately infers\nuncertainty in level sets for new time steps and is up to 170X faster than that\nof the original probabilistic model with serial computation and 10X faster than\nthat of the original parallel computation.\n","authors":["Mengjiao Han","Tushar M. Athawale","David Pugmire","Chris R. Johnson"],"pdf_url":"https://arxiv.org/pdf/2207.07260v1.pdf","comment":"5 pages, IEEE Vis 2022 Short Paper"},{"id":"http://arxiv.org/abs/2207.01616v2","updated":"2022-07-15T02:22:49Z","published":"2022-07-04T17:58:39Z","title":"Breaking Feedback Loops in Recommender Systems with Causal Inference","summary":"  Recommender systems play a key role in shaping modern web ecosystems. These\nsystems alternate between (1) making recommendations (2) collecting user\nresponses to these recommendations, and (3) retraining the recommendation\nalgorithm based on this feedback. During this process the recommender system\ninfluences the user behavioral data that is subsequently used to update it,\nthus creating a feedback loop. Recent work has shown that feedback loops may\ncompromise recommendation quality and homogenize user behavior, raising ethical\nand performance concerns when deploying recommender systems. To address these\nissues, we propose the Causal Adjustment for Feedback Loops (CAFL), an\nalgorithm that provably breaks feedback loops using causal inference and can be\napplied to any recommendation algorithm that optimizes a training loss. Our\nmain observation is that a recommender system does not suffer from feedback\nloops if it reasons about causal quantities, namely the intervention\ndistributions of recommendations on user ratings. Moreover, we can calculate\nthis intervention distribution from observational data by adjusting for the\nrecommender system's predictions of user preferences. Using simulated\nenvironments, we demonstrate that CAFL improves recommendation quality when\ncompared to prior correction methods.\n","authors":["Karl Krauth","Yixin Wang","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2207.01616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07256v1","updated":"2022-07-15T02:16:09Z","published":"2022-07-15T02:16:09Z","title":"Improving Task-free Continual Learning by Distributionally Robust Memory\n  Evolution","summary":"  Task-free continual learning (CL) aims to learn a non-stationary data stream\nwithout explicit task definitions and not forget previous knowledge. The widely\nadopted memory replay approach could gradually become less effective for long\ndata streams, as the model may memorize the stored examples and overfit the\nmemory buffer. Second, existing methods overlook the high uncertainty in the\nmemory data distribution since there is a big gap between the memory data\ndistribution and the distribution of all the previous data examples. To address\nthese problems, for the first time, we propose a principled memory evolution\nframework to dynamically evolve the memory data distribution by making the\nmemory buffer gradually harder to be memorized with distributionally robust\noptimization (DRO). We then derive a family of methods to evolve the memory\nbuffer data in the continuous probability measure space with Wasserstein\ngradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory\ndata distribution, thus guarantees the model performance and learns\nsignificantly more robust features than existing memory-replay-based methods.\nExtensive experiments on existing benchmarks demonstrate the effectiveness of\nthe proposed methods for alleviating forgetting. As a by-product of the\nproposed framework, our method is more robust to adversarial examples than\nexisting task-free CL methods.\n","authors":["Zhenyi Wang","Li Shen","Le Fang","Qiuling Suo","Tiehang Duan","Mingchen Gao"],"pdf_url":"https://arxiv.org/pdf/2207.07256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07255v1","updated":"2022-07-15T02:08:41Z","published":"2022-07-15T02:08:41Z","title":"Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights","summary":"  Investigating cooperativity of interlocutors is central in studying\npragmatics of dialogue. Models of conversation that only assume cooperative\nagents fail to explain the dynamics of strategic conversations. Thus, we\ninvestigate the ability of agents to identify non-cooperative interlocutors\nwhile completing a concurrent visual-dialogue task. Within this novel setting,\nwe study the optimality of communication strategies for achieving this\nmulti-task objective. We use the tools of learning theory to develop a\ntheoretical model for identifying non-cooperative interlocutors and apply this\ntheory to analyze different communication strategies. We also introduce a\ncorpus of non-cooperative conversations about images in the GuessWhat?! dataset\nproposed by De Vries et al. (2017). We use reinforcement learning to implement\nmultiple communication strategies in this context and find empirical results\nvalidate our theory.\n","authors":["Anthony Sicilia","Tristan Maidment","Pat Healy","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2207.07255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04690v3","updated":"2022-07-15T02:04:52Z","published":"2022-07-11T08:12:02Z","title":"Dynamic Budget Throttling in Repeated Second-Price Auctions","summary":"  Throttling is one of the most popular budget control methods in today's\nonline advertising markets. When a budget-constrained advertiser employs\nthrottling, she can choose whether or not to participate in an auction after\nthe advertising platform recommends a bid. This paper focuses on the dynamic\nbudget throttling process in repeated second-price auctions from a theoretical\nview. An essential feature of the underlying problem is that the advertiser\ndoes not know the distribution of the highest competing bid upon entering the\nmarket. To model the difficulty of eliminating such uncertainty, we consider\ntwo different information structures. The advertiser could obtain the highest\ncompeting bid in each round with full-information feedback. Meanwhile, with\npartial information feedback, the advertiser could only have access to the\nhighest competing bid in the auctions she participates in. We propose the\nOGD-CB algorithm, which involves simultaneous distribution learning and revenue\noptimization. In both settings, we demonstrate that this algorithm guarantees\nan $O(\\sqrt{T\\log T})$ regret with probability $1 - O(1/T)$ relative to the\nfluid adaptive throttling benchmark. By proving a lower bound of\n$\\Omega(\\sqrt{T})$ on the minimal regret for even the hindsight optimum, we\nestablish the near optimality of our algorithm. Finally, we compare the fluid\noptimum of throttling to that of pacing, another widely adopted budget control\nmethod. The numerical relationship of these benchmarks sheds new light on the\nunderstanding of different online algorithms for revenue maximization under\nbudget constraints.\n","authors":["Zhaohua Chen","Chang Wang","Qian Wang","Yuqi Pan","Zhuming Shi","Chuyue Tang","Zheng Cai","Yukun Ren","Zhihua Zhu","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2207.04690v3.pdf","comment":"29 pages, 1 table"},{"id":"http://arxiv.org/abs/2207.07250v1","updated":"2022-07-15T01:41:53Z","published":"2022-07-15T01:41:53Z","title":"On the Super-exponential Quantum Speedup of Equivariant Quantum Machine\n  Learning Algorithms with SU($d$) Symmetry","summary":"  We introduce a framework of the equivariant convolutional algorithms which is\ntailored for a number of machine-learning tasks on physical systems with\narbitrary SU($d$) symmetries. It allows us to enhance a natural model of\nquantum computation--permutational quantum computing (PQC) [Quantum Inf.\nComput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While\nPQC was shown to be effectively classically simulatable, we exhibit a problem\nwhich can be efficiently solved on PQC+ machine, whereas the best known\nclassical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence\nagainst PQC+ being classically simulatable. We further discuss practical\nquantum machine learning algorithms which can be carried out in the paradigm of\nPQC+.\n","authors":["Han Zheng","Zimu Li","Junyu Liu","Sergii Strelchuk","Risi Kondor"],"pdf_url":"https://arxiv.org/pdf/2207.07250v1.pdf","comment":"A shorter version established based on arXiv:2112.07611, presented in\n  TQC 2022"},{"id":"http://arxiv.org/abs/2112.06189v2","updated":"2022-07-15T01:25:23Z","published":"2021-12-12T09:16:00Z","title":"An original model for multi-target learning of logical rules for\n  knowledge graph reasoning","summary":"  Large-scale knowledge graphs provide structured representations of human\nknowledge. However, as it is impossible to collect all knowledge, knowledge\ngraphs are usually incomplete. Reasoning based on existing facts paves a way to\ndiscover missing facts. In this paper, we study the problem of learning logical\nrules for reasoning on knowledge graphs for completing missing factual\ntriplets. Learning logical rules equips a model with strong interpretability as\nwell as the ability to generalize to similar tasks. We propose a model able to\nfully use training data which also considers multi-target scenarios. In\naddition, considering the deficiency in evaluating the performance of models\nand the quality of mined rules, we further propose two novel indicators to help\nwith the problem. Experimental results empirically demonstrate that our model\noutperforms state-of-the-art methods on five benchmark datasets. The results\nalso prove the effectiveness of the indicators.\n","authors":["Yuliang Wei","Haotian Li","Guodong Xin","Yao Wang","Bailing Wang"],"pdf_url":"https://arxiv.org/pdf/2112.06189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07243v1","updated":"2022-07-15T00:35:59Z","published":"2022-07-15T00:35:59Z","title":"LineCap: Line Charts for Data Visualization Captioning Models","summary":"  Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.\n","authors":["Anita Mahinpei","Zona Kostic","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2207.07243v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.02643v2","updated":"2022-07-15T15:33:25Z","published":"2022-07-06T13:06:31Z","title":"Effective and Efficient Training for Sequential Recommendation using\n  Recency Sampling","summary":"  Many modern sequential recommender systems use deep neural networks, which\ncan effectively estimate the relevance of items but require a lot of time to\ntrain. Slow training increases expenses, hinders product development timescales\nand prevents the model from being regularly updated to adapt to changing user\npreferences. Training such sequential models involves appropriately sampling\npast user interactions to create a realistic training objective. The existing\ntraining objectives have limitations. For instance, next item prediction never\nuses the beginning of the sequence as a learning target, thereby potentially\ndiscarding valuable data. On the other hand, the item masking used by BERT4Rec\nis only weakly related to the goal of the sequential recommendation; therefore,\nit requires much more time to obtain an effective model. Hence, we propose a\nnovel Recency-based Sampling of Sequences training objective that addresses\nboth limitations. We apply our method to various recent and state-of-the-art\nmodel architectures - such as GRU4Rec, Caser, and SASRec. We show that the\nmodels enhanced with our method can achieve performances exceeding or very\nclose to stateof-the-art BERT4Rec, but with much less training time.\n","authors":["Aleksandr Petrov","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2207.02643v2.pdf","comment":"This full research paper is accepted at 16th ACM Conference on\n  Recommender Systems (ACM RecSys)"},{"id":"http://arxiv.org/abs/2207.07483v1","updated":"2022-07-15T14:09:04Z","published":"2022-07-15T14:09:04Z","title":"A Systematic Review and Replicability Study of BERT4Rec for Sequential\n  Recommendation","summary":"  BERT4Rec is an effective model for sequential recommendation based on the\nTransformer architecture. In the original publication, BERT4Rec claimed\nsuperiority over other available sequential recommendation approaches (e.g.\nSASRec), and it is now frequently being used as a state-of-the art baseline for\nsequential recommendations. However, not all subsequent publications confirmed\nthis result and proposed other models that were shown to outperform BERT4Rec in\neffectiveness. In this paper we systematically review all publications that\ncompare BERT4Rec with another popular Transformer-based model, namely SASRec,\nand show that BERT4Rec results are not consistent within these publications. To\nunderstand the reasons behind this inconsistency, we analyse the available\nimplementations of BERT4Rec and show that we fail to reproduce results of the\noriginal BERT4Rec publication when using their default configuration\nparameters. However, we are able to replicate the reported results with the\noriginal code if training for a much longer amount of time (up to 30x) compared\nto the default configuration. We also propose our own implementation of\nBERT4Rec based on the Hugging Face Transformers library, which we demonstrate\nreplicates the originally reported results on 3 out 4 datasets, while requiring\nup to 95% less training time to converge. Overall, from our systematic review\nand detailed experiments, we conclude that BERT4Rec does indeed exhibit\nstate-of-the-art effectiveness for sequential recommendation, but only when\ntrained for a sufficient amount of time. Additionally, we show that our\nimplementation can further benefit from adapting other Transformer\narchitectures that are available in the Hugging Face Transformers library (e.g.\nusing disentangled attention, as provided by DeBERTa, or larger hidden layer\nsize cf. ALBERT).\n","authors":["Aleksandr Petrov","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2207.07483v1.pdf","comment":"This paper is accepted at the Reproducibility track of the ACM RecSys\n  '22 conference"},{"id":"http://arxiv.org/abs/2201.13317v2","updated":"2022-07-15T08:44:11Z","published":"2022-01-28T11:58:24Z","title":"Hyper-Class Representation of Data","summary":"  Data representation is usually a natural form with their attribute values. On\nthis basis, data processing is an attribute-centered calculation. However,\nthere are three limitations in the attribute-centered calculation, saying,\ninflexible calculation, preference computation, and unsatisfactory output. To\nattempt the issues, a new data representation, named as hyper-classes\nrepresentation, is proposed for improving recommendation. First, the cross\nentropy, KL divergence and JS divergence of features in data are defined. And\nthen, the hyper-classes in data can be discovered with these three parameters.\nFinally, a kind of recommendation algorithm is used to evaluate the proposed\nhyper-class representation of data, and shows that the hyper-class\nrepresentation is able to provide truly useful reference information for\nrecommendation systems and makes recommendations much better than existing\nalgorithms, i.e., this approach is efficient and promising.\n","authors":["Shichao Zhang","Jiaye Li","Wenzhen Zhang","Yongsong Qin"],"pdf_url":"https://arxiv.org/pdf/2201.13317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07331v1","updated":"2022-07-15T08:03:37Z","published":"2022-07-15T08:03:37Z","title":"Modeling Multi-interest News Sequence for News Recommendation","summary":"  A session-based news recommender system recommends the next news to a user by\nmodeling the potential interests embedded in a sequence of news read/clicked by\nher/him in a session. Generally, a user's interests are diverse, namely there\nare multiple interests corresponding to different types of news, e.g., news of\ndistinct topics, within a session. %Modeling such multiple interests is\ncritical for precise news recommendation. However, most of existing methods\ntypically overlook such important characteristic and thus fail to distinguish\nand model the potential multiple interests of a user, impeding accurate\nrecommendation of the next piece of news. Therefore, this paper proposes\nmulti-interest news sequence (MINS) model for news recommendation. In MINS, a\nnews encoder based on self-attention is devised on learn an informative\nembedding for each piece of news, and then a novel parallel interest network is\ndevised to extract the potential multiple interests embedded in the news\nsequence in preparation for the subsequent next-news recommendations. The\nexperimental results on a real-world dataset demonstrate that our model can\nachieve better performance than the state-of-the-art compared models.\n","authors":["Rongyao Wang","Wenpeng Lu","Xueping Peng"],"pdf_url":"https://arxiv.org/pdf/2207.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07330v1","updated":"2022-07-15T07:58:49Z","published":"2022-07-15T07:58:49Z","title":"A Comparison of Source Distribution and Result Overlap in Web Search\n  Engines","summary":"  When it comes to search engines, users generally prefer Google. Our study\naims to find the differences between the results found in Google compared to\nother search engines. We compared the top 10 results from Google, Bing,\nDuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from\nGermany and the US. Google displays more unique domains in the top results than\nits competitors. Wikipedia and news websites are the most popular sources\noverall. With some top sources dominating search results, the distribution of\ndomains is also consistent across all search engines. The overlap between\nGoogle and Bing is always under 32%, while Metager has a higher overlap with\nBing than DuckDuckGo, going up to 78%. This study shows that the use of another\nsearch engine, especially in addition to Google, provides a wider variety in\nsources and might lead the user to find new perspectives.\n","authors":["Nurce Yagci","Sebastian Sünkler","Helena Häußler","Dirk Lewandowski"],"pdf_url":"https://arxiv.org/pdf/2207.07330v1.pdf","comment":"Submitted to the 85th Annual Meeting of the Association for\n  Information Science & Technology and will be published in the conference\n  proceedings"},{"id":"http://arxiv.org/abs/2201.05333v2","updated":"2022-07-15T04:21:27Z","published":"2022-01-14T08:21:28Z","title":"Attention over Self-attention:Intention-aware Re-ranking with Dynamic\n  Transformer Encoders for Recommendation","summary":"  Re-ranking models refine item recommendation lists generated by the prior\nglobal ranking model, which have demonstrated their effectiveness in improving\nthe recommendation quality. However, most existing re-ranking solutions only\nlearn from implicit feedback with a shared prediction model, which regrettably\nignore inter-item relationships under diverse user intentions. In this paper,\nwe propose a novel Intention-aware Re-ranking Model with Dynamic Transformer\nEncoder (RAISE), aiming to perform user-specific prediction for each individual\nuser based on her intentions. Specifically, we first propose to mine latent\nuser intentions from text reviews with an intention discovering module (IDM).\nBy differentiating the importance of review information with a co-attention\nnetwork, the latent user intention can be explicitly modeled for each user-item\npair. We then introduce a dynamic transformer encoder (DTE) to capture\nuser-specific inter-item relationships among item candidates by seamlessly\naccommodating the learned latent user intentions via IDM. As such, one can not\nonly achieve more personalized recommendations but also obtain corresponding\nexplanations by constructing RAISE upon existing recommendation engines.\nEmpirical study on four public datasets shows the superiority of our proposed\nRAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by\nPrecision@5, MAP@5, and NDCG@5 respectively.\n","authors":["Zhuoyi Lin","Sheng Zang","Rundong Wang","Zhu Sun","J. Senthilnath","Chi Xu","Chee-Keong Kwoh"],"pdf_url":"https://arxiv.org/pdf/2201.05333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01616v2","updated":"2022-07-15T02:22:49Z","published":"2022-07-04T17:58:39Z","title":"Breaking Feedback Loops in Recommender Systems with Causal Inference","summary":"  Recommender systems play a key role in shaping modern web ecosystems. These\nsystems alternate between (1) making recommendations (2) collecting user\nresponses to these recommendations, and (3) retraining the recommendation\nalgorithm based on this feedback. During this process the recommender system\ninfluences the user behavioral data that is subsequently used to update it,\nthus creating a feedback loop. Recent work has shown that feedback loops may\ncompromise recommendation quality and homogenize user behavior, raising ethical\nand performance concerns when deploying recommender systems. To address these\nissues, we propose the Causal Adjustment for Feedback Loops (CAFL), an\nalgorithm that provably breaks feedback loops using causal inference and can be\napplied to any recommendation algorithm that optimizes a training loss. Our\nmain observation is that a recommender system does not suffer from feedback\nloops if it reasons about causal quantities, namely the intervention\ndistributions of recommendations on user ratings. Moreover, we can calculate\nthis intervention distribution from observational data by adjusting for the\nrecommender system's predictions of user preferences. Using simulated\nenvironments, we demonstrate that CAFL improves recommendation quality when\ncompared to prior correction methods.\n","authors":["Karl Krauth","Yixin Wang","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2207.01616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07769v1","updated":"2022-07-15T22:04:27Z","published":"2022-07-15T22:04:27Z","title":"Anomalous behaviour in loss-gradient based interpretability methods","summary":"  Loss-gradients are used to interpret the decision making process of deep\nlearning models. In this work, we evaluate loss-gradient based attribution\nmethods by occluding parts of the input and comparing the performance of the\noccluded input to the original input. We observe that the occluded input has\nbetter performance than the original across the test dataset under certain\nconditions. Similar behaviour is observed in sound and image recognition tasks.\nWe explore different loss-gradient attribution methods, occlusion levels and\nreplacement values to explain the phenomenon of performance improvement under\nocclusion.\n","authors":["Vinod Subramanian","Siddharth Gururani","Emmanouil Benetos","Mark Sandler"],"pdf_url":"https://arxiv.org/pdf/2207.07769v1.pdf","comment":"Accepted at ICLR RobustML workshop 2021"},{"id":"http://arxiv.org/abs/2207.07706v1","updated":"2022-07-15T19:04:43Z","published":"2022-07-15T19:04:43Z","title":"Probing Semantic Grounding in Language Models of Code with\n  Representational Similarity Analysis","summary":"  Representational Similarity Analysis is a method from cognitive neuroscience,\nwhich helps in comparing representations from two different sources of data. In\nthis paper, we propose using Representational Similarity Analysis to probe the\nsemantic grounding in language models of code. We probe representations from\nthe CodeBERT model for semantic grounding by using the data from the IBM\nCodeNet dataset. Through our experiments, we show that current pre-training\nmethods do not induce semantic grounding in language models of code, and\ninstead focus on optimizing form-based patterns. We also show that even a\nlittle amount of fine-tuning on semantically relevant tasks increases the\nsemantic grounding in CodeBERT significantly. Our ablations with the input\nmodality to the CodeBERT model show that using bimodal inputs (code and natural\nlanguage) over unimodal inputs (only code) gives better semantic grounding and\nsample efficiency during semantic fine-tuning. Finally, our experiments with\nsemantic perturbations in code reveal that CodeBERT is able to robustly\ndistinguish between semantically correct and incorrect code.\n","authors":["Shounak Naik","Rajaswa Patil","Swati Agarwal","Veeky Baths"],"pdf_url":"https://arxiv.org/pdf/2207.07706v1.pdf","comment":"Under review at ADMA 2022"}],"Multimedia":[{"id":"http://arxiv.org/abs/2108.09717v8","updated":"2022-07-15T17:27:20Z","published":"2021-08-22T13:21:58Z","title":"EKTVQA: Generalized use of External Knowledge to empower Scene Text in\n  Text-VQA","summary":"  The open-ended question answering task of Text-VQA often requires reading and\nreasoning about rarely seen or completely unseen scene-text content of an\nimage. We address this zero-shot nature of the problem by proposing the\ngeneralized use of external knowledge to augment our understanding of the scene\ntext. We design a framework to extract, validate, and reason with knowledge\nusing a standard multimodal transformer for vision language understanding\ntasks. Through empirical evidence and qualitative results, we demonstrate how\nexternal knowledge can highlight instance-only cues and thus help deal with\ntraining data bias, improve answer entity type correctness, and detect\nmultiword named entities. We generate results comparable to the\nstate-of-the-art on three publicly available datasets, under the constraints of\nsimilar upstream OCR systems and training data.\n","authors":["Arka Ujjal Dey","Ernest Valveny","Gaurav Harit"],"pdf_url":"https://arxiv.org/pdf/2108.09717v8.pdf","comment":"Accepted at IEEE Access"},{"id":"http://arxiv.org/abs/2207.07520v1","updated":"2022-07-15T15:09:07Z","published":"2022-07-15T15:09:07Z","title":"Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual\n  Reality with Redirected Walking","summary":"  Full-immersive multiuser Virtual Reality (VR) envisions supporting\nunconstrained mobility of the users in the virtual worlds, while at the same\ntime constraining their physical movements inside VR setups through redirected\nwalking. For enabling delivery of high data rate video content in real-time,\nthe supporting wireless networks will leverage highly directional communication\nlinks that will \"track\" the users for maintaining the Line-of-Sight (LoS)\nconnectivity. Recurrent Neural Networks (RNNs) and in particular Long\nShort-Term Memory (LSTM) networks have historically presented themselves as a\nsuitable candidate for near-term movement trajectory prediction for natural\nhuman mobility, and have also recently been shown as applicable in predicting\nVR users' mobility under the constraints of redirected walking. In this work,\nwe extend these initial findings by showing that Gated Recurrent Unit (GRU)\nnetworks, another candidate from the RNN family, generally outperform the\ntraditionally utilized LSTMs. Second, we show that context from a virtual world\ncan enhance the accuracy of the prediction if used as an additional input\nfeature in comparison to the more traditional utilization of solely the\nhistorical physical movements of the VR users. Finally, we show that the\nprediction system trained on a static number of coexisting VR users be scaled\nto a multi-user system without significant accuracy degradation.\n","authors":["Filip Lemic","Jakob Struye","Jeroen Famaey"],"pdf_url":"https://arxiv.org/pdf/2207.07520v1.pdf","comment":"7 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2207.07394v1","updated":"2022-07-15T10:48:41Z","published":"2022-07-15T10:48:41Z","title":"FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud\n  Video Streaming","summary":"  Point cloud video transmission is challenging due to high encoding/decoding\ncomplexity, high video bitrate, and low latency requirement. Consequently,\nconventional adaptive streaming methodologies often find themselves\nunsatisfactory to meet the requirements in threefold: 1) current algorithms\nreuse existing quality of experience (QoE) definitions while overlooking the\nunique features of point cloud video thus failing to provide optimal user\nexperience, 2) most deep learning approaches require long-span data collections\nto learn sufficiently varied network conditions and result in long training\nperiod and capacity occupation, 3) cloud training approaches pose privacy risks\ncaused by leakage of user reported service usage and networking conditions.\n  To overcome the limitations, we present FRAS, the first federated\nreinforcement learning framework, to the best of our knowledge, for adaptive\npoint cloud video streaming. We define a new QoE model which takes the unique\nfeatures of point cloud video into account. Each client uses reinforcement\nlearning (RL) to train encoding rate selection with the objective of optimizing\nthe user's QoE under multiple constraints. Then, a federated learning framework\nis integrated with the RL algorithm to enhance training performance with\nprivacy preservation. Extensive simulations using real point cloud videos and\nnetwork traces reveal the superiority of the proposed scheme over baseline\nschemes. We also implement a prototype that demonstrates the performance of\nFRAS via real-world tests.\n","authors":["Yu Gao","Pengyuan Zhou","Zhi Liu","Bo Han","Hui Pan"],"pdf_url":"https://arxiv.org/pdf/2207.07394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07386v1","updated":"2022-07-15T10:24:26Z","published":"2022-07-15T10:24:26Z","title":"ChoreoGraph: Music-conditioned Automatic Dance Choreography over a Style\n  and Tempo Consistent Dynamic Graph","summary":"  To generate dance that temporally and aesthetically matches the music is a\nchallenging problem, as the following factors need to be considered. First, the\naesthetic styles and messages conveyed by the motion and music should be\nconsistent. Second, the beats of the generated motion should be locally aligned\nto the musical features. And finally, basic choreomusical rules should be\nobserved, and the motion generated should be diverse. To address these\nchallenges, we propose ChoreoGraph, which choreographs high-quality dance\nmotion for a given piece of music over a Dynamic Graph. A data-driven learning\nstrategy is proposed to evaluate the aesthetic style and rhythmic connections\nbetween music and motion in a progressively learned cross-modality embedding\nspace. The motion sequences will be beats-aligned based on the music segments\nand then incorporated as nodes of a Dynamic Motion Graph. Compatibility factors\nsuch as the style and tempo consistency, motion context connection, action\ncompleteness, and transition smoothness are comprehensively evaluated to\ndetermine the node transition in the graph. We demonstrate that our\nrepertoire-based framework can generate motions with aesthetic consistency and\nrobustly extensible in diversity. Both quantitative and qualitative experiment\nresults show that our proposed model outperforms other baseline models.\n","authors":["Ho Yin Au","Jie Chen","Junkun Jiang","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2207.07386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04521v2","updated":"2022-07-15T08:25:41Z","published":"2022-07-10T19:45:24Z","title":"Information-Theoretic Bounds for Steganography in Multimedia","summary":"  Steganography in multimedia aims to embed secret data into an innocent\nlooking multimedia cover object. This embedding introduces some distortion to\nthe cover object and produces a corresponding stego object. The embedding\ndistortion is measured by a cost function that determines the detection\nprobability of the existence of the embedded secret data. A cost function\nrelated to the maximum embedding rate is typically employed to evaluate a\nsteganographic system. In addition, the distribution of multimedia sources\nfollows the Gibbs distribution which is a complex statistical model that\nrestricts analysis. Thus, previous multimedia steganographic approaches either\nassume a relaxed distribution or presume a proposition on the maximum embedding\nrate and then try to prove it is correct. Conversely, this paper introduces an\nanalytic approach to determining the maximum embedding rate in multimedia cover\nobjects through a constrained optimization problem concerning the relationship\nbetween the maximum embedding rate and the probability of detection by any\nsteganographic detector. The KL-divergence between the distributions for the\ncover and stego objects is used as the cost function as it upper bounds the\nperformance of the optimal steganographic detector. An equivalence between the\nGibbs and correlated-multivariate-quantized-Gaussian distributions is\nestablished to solve this optimization problem. The solution provides an\nanalytic form for the maximum embedding rate in terms of the WrightOmega\nfunction. Moreover, it is proven that the maximum embedding rate is in\nagreement with the commonly used Square Root Law (SRL) for steganography, but\nthe solution presented here is more accurate. Finally, the theoretical results\nobtained are verified experimentally.\n","authors":["Hassan Y. El Arsh","Amr Abdelaziz","Ahmed Elliethy","Hussein A. Aly","T. Aaron Gulliver"],"pdf_url":"https://arxiv.org/pdf/2207.04521v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2111.04960"},{"id":"http://arxiv.org/abs/2203.04904v3","updated":"2022-07-15T04:16:21Z","published":"2022-03-09T17:26:53Z","title":"Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning","summary":"  Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n","authors":["Zhenhailong Wang","Hang Yu","Manling Li","Han Zhao","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2203.04904v3.pdf","comment":"4 pages, 4 figures, under review"},{"id":"http://arxiv.org/abs/2207.07278v1","updated":"2022-07-15T03:58:04Z","published":"2022-07-15T03:58:04Z","title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified\n  Learning Scheme and Dynamic Range Minimization","summary":"  With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.\n","authors":["Mengyin Liu","Chao Zhu","Hongyu Gao","Weibo Gu","Hongfa Wang","Wei Liu","Xu-cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2207.07278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04203v2","updated":"2022-07-15T03:01:11Z","published":"2022-07-09T06:25:01Z","title":"Learning to Separate Voices by Spatial Regions","summary":"  We consider the problem of audio voice separation for binaural applications,\nsuch as earphones and hearing aids. While today's neural networks perform\nremarkably well (separating $4+$ sources with 2 microphones) they assume a\nknown or fixed maximum number of sources, K. Moreover, today's models are\ntrained in a supervised manner, using training data synthesized from generic\nsources, environments, and human head shapes.\n  This paper intends to relax both these constraints at the expense of a slight\nalteration in the problem definition. We observe that, when a received mixture\ncontains too many sources, it is still helpful to separate them by region,\ni.e., isolating signal mixtures from each conical sector around the user's\nhead. This requires learning the fine-grained spatial properties of each\nregion, including the signal distortions imposed by a person's head. We propose\na two-stage self-supervised framework in which overheard voices from earphones\nare pre-processed to extract relatively clean personalized signals, which are\nthen used to train a region-wise separation model. Results show promising\nperformance, underscoring the importance of personalization over a generic\nsupervised approach. (audio samples available at our project website:\nhttps://uiuc-earable-computing.github.io/binaural/. We believe this result\ncould help real-world applications in selective hearing, noise cancellation,\nand audio augmented reality.\n","authors":["Zhongweiyang Xu","Romit Roy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2207.04203v2.pdf","comment":"Accepted to ICML 2022. For associated audio samples, see\n  https://uiuc-earable-computing.github.io/binaural"},{"id":"http://arxiv.org/abs/2207.08689v1","updated":"2022-07-15T02:09:17Z","published":"2022-07-15T02:09:17Z","title":"Quality Assessment of Image Super-Resolution: Balancing Deterministic\n  and Statistical Fidelity","summary":"  There has been a growing interest in developing image super-resolution (SR)\nalgorithms that convert low-resolution (LR) to higher resolution images, but\nautomatically evaluating the visual quality of super-resolved images remains a\nchallenging problem. Here we look at the problem of SR image quality assessment\n(SR IQA) in a two-dimensional (2D) space of deterministic fidelity (DF) versus\nstatistical fidelity (SF). This allows us to better understand the advantages\nand disadvantages of existing SR algorithms, which produce images at different\nclusters in the 2D space of (DF, SF). Specifically, we observe an interesting\ntrend from more traditional SR algorithms that are typically inclined to\noptimize for DF while losing SF, to more recent generative adversarial network\n(GAN) based approaches that by contrast exhibit strong advantages in achieving\nhigh SF but sometimes appear weak at maintaining DF. Furthermore, we propose an\nuncertainty weighting scheme based on content-dependent sharpness and texture\nassessment that merges the two fidelity measures into an overall quality\nprediction named the Super Resolution Image Fidelity (SRIF) index, which\ndemonstrates superior performance against state-of-the-art IQA models when\ntested on subject-rated datasets.\n","authors":["Wei Zhou","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2207.08689v1.pdf","comment":"Accepted by ACMMM2022 https://github.com/weizhou-geek/SRIF"}]},"2022-07-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2112.08670v2","updated":"2022-07-18T17:48:03Z","published":"2021-12-16T07:10:02Z","title":"Amortized Noisy Channel Neural Machine Translation","summary":"  Noisy channel models have been especially effective in neural machine\ntranslation (NMT). However, recent approaches like \"beam search and rerank\"\n(BSR) incur significant computation overhead during inference, making\nreal-world application infeasible. We aim to study if it is possible to build\nan amortized noisy channel NMT model such that when we do greedy decoding\nduring inference, the translation accuracy matches that of BSR in terms of\nreward (based on the source-to-target log probability and the target-to-source\nlog probability) and quality (based on BLEU and BLEURT). We attempt three\napproaches to train the new model: knowledge distillation, one-step-deviation\nimitation learning, and Q learning. The first approach obtains the noisy\nchannel signal from a pseudo-corpus, and the latter two approaches aim to\noptimize toward a noisy-channel MT reward directly. For all three approaches,\nthe generated translations fail to achieve rewards comparable to BSR, but the\ntranslation quality approximated by BLEU and BLEURT is similar to the quality\nof BSR-produced translations. Additionally, all three approaches speed up\ninference by 1-2 orders of magnitude.\n","authors":["Richard Yuanzhe Pang","He He","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2112.08670v2.pdf","comment":"INLG 2022"},{"id":"http://arxiv.org/abs/2207.08739v1","updated":"2022-07-18T16:30:50Z","published":"2022-07-18T16:30:50Z","title":"Rethinking Data Augmentation for Robust Visual Question Answering","summary":"  Data Augmentation (DA) -- generating extra training samples beyond original\ntraining set -- has been widely-used in today's unbiased VQA models to mitigate\nthe language biases. Current mainstream DA strategies are synthetic-based\nmethods, which synthesize new samples by either editing some visual\nregions/words, or re-generating them from scratch. However, these synthetic\nsamples are always unnatural and error-prone. To avoid this issue, a recent DA\nwork composes new augmented samples by randomly pairing pristine images and\nother human-written questions. Unfortunately, to guarantee augmented samples\nhave reasonable ground-truth answers, they manually design a set of heuristic\nrules for several question types, which extremely limits its generalization\nabilities. To this end, we propose a new Knowledge Distillation based Data\nAugmentation for VQA, dubbed KDDAug. Specifically, we first relax the\nrequirements of reasonable image-question pairs, which can be easily applied to\nany question types. Then, we design a knowledge distillation (KD) based answer\nassignment to generate pseudo answers for all composed image-question pairs,\nwhich are robust to both in-domain and out-of-distribution settings. Since\nKDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into\nany VQA architectures. Extensive ablation studies on multiple backbones and\nbenchmarks have demonstrated the effectiveness and generalization abilities of\nKDDAug.\n","authors":["Long Chen","Yuhang Zheng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2207.08739v1.pdf","comment":"Accepted to ECCV 2022; Codes: https://github.com/ItemZheng/KDDAug"},{"id":"http://arxiv.org/abs/2207.08635v1","updated":"2022-07-18T14:29:18Z","published":"2022-07-18T14:29:18Z","title":"GOAL: Towards Benchmarking Few-Shot Sports Game Summarization","summary":"  Sports game summarization aims to generate sports news based on real-time\ncommentaries. The task has attracted wide research attention but is still\nunder-explored probably due to the lack of corresponding English datasets.\nTherefore, in this paper, we release GOAL, the first English sports game\nsummarization dataset. Specifically, there are 103 commentary-news pairs in\nGOAL, where the average lengths of commentaries and news are 2724.9 and 476.3\nwords, respectively. Moreover, to support the research in the semi-supervised\nsetting, GOAL additionally provides 2,160 unlabeled commentary documents. Based\non our GOAL, we build and evaluate several baselines, including extractive and\nabstractive baselines. The experimental results show the challenges of this\ntask still remain. We hope our work could promote the research of sports game\nsummarization. The dataset has been released at\nhttps://github.com/krystalan/goal.\n","authors":["Jiaan Wang","Tingyi Zhang","Haoxiang Shi"],"pdf_url":"https://arxiv.org/pdf/2207.08635v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2207.08583v1","updated":"2022-07-18T13:20:22Z","published":"2022-07-18T13:20:22Z","title":"MAD for Robust Reinforcement Learning in Machine Translation","summary":"  We introduce a new distributed policy gradient algorithm and show that it\noutperforms existing reward-aware training procedures such as REINFORCE,\nminimum risk training (MRT) and proximal policy optimization (PPO) in terms of\ntraining stability and generalization performance when optimizing machine\ntranslation models. Our algorithm, which we call MAD (on account of using the\nmean absolute deviation in the importance weighting calculation), has\ndistributed data generators sampling multiple candidates per source sentence on\nworker nodes, while a central learner updates the policy. MAD depends crucially\non two variance reduction strategies: (1) a conditional reward normalization\nmethod that ensures each source sentence has both positive and negative reward\ntranslation examples and (2) a new robust importance weighting scheme that acts\nas a conditional entropy regularizer. Experiments on a variety of translation\ntasks show that policies learned using the MAD algorithm perform very well when\nusing both greedy decoding and beam search, and that the learned policies are\nsensitive to the specific reward used during training.\n","authors":["Domenic Donato","Lei Yu","Wang Ling","Chris Dyer"],"pdf_url":"https://arxiv.org/pdf/2207.08583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08562v1","updated":"2022-07-18T12:44:59Z","published":"2022-07-18T12:44:59Z","title":"DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link\n  Prediction and Entity Typing","summary":"  In the field of representation learning on knowledge graphs (KGs), a\nhyper-relational fact consists of a main triple and several auxiliary attribute\nvalue descriptions, which is considered to be more comprehensive and specific\nthan a triple-based fact. However, the existing hyper-relational KG embedding\nmethods in a single view are limited in application due to weakening the\nhierarchical structure representing the affiliation between entities. To break\nthis limitation, we propose a dual-view hyper-relational KG (DH-KG) structure\nwhich contains a hyper-relational instance view for entities and a\nhyper-relational ontology view for concepts abstracted hierarchically from\nentities to jointly model hyper-relational and hierarchical information. In\nthis paper, we first define link prediction and entity typing tasks on DH-KG\nand construct two DH-KG datasets, JW44K-6K extracted from Wikidata and HTDM\nbased on medical data. Furthermore, We propose a DH-KG embedding model DHGE,\nbased on GRAN encoder, HGNN, and joint learning. Experimental results show that\nDHGE outperforms baseline models on DH-KG. We also provide an example of the\napplication of this technology in the field of hypertension medication. Our\nmodel and datasets are publicly available.\n","authors":["Haoran Luo","Haihong E","Ling Tan","Xueyuan Lin","Gengxian Zhou","Jundi Li","Tianyu Yao","Kaiyang Wan"],"pdf_url":"https://arxiv.org/pdf/2207.08562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08557v1","updated":"2022-07-18T12:33:51Z","published":"2022-07-18T12:33:51Z","title":"AlexU-AIC at Arabic Hate Speech 2022: Contrast to Classify","summary":"  Online presence on social media platforms such as Facebook and Twitter has\nbecome a daily habit for internet users. Despite the vast amount of services\nthe platforms offer for their users, users suffer from cyber-bullying, which\nfurther leads to mental abuse and may escalate to cause physical harm to\nindividuals or targeted groups. In this paper, we present our submission to the\nArabic Hate Speech 2022 Shared Task Workshop (OSACT5 2022) using the associated\nArabic Twitter dataset. The shared task consists of 3 sub-tasks, sub-task A\nfocuses on detecting whether the tweet is offensive or not. Then, For offensive\nTweets, sub-task B focuses on detecting whether the tweet is hate speech or\nnot. Finally, For hate speech Tweets, sub-task C focuses on detecting the\nfine-grained type of hate speech among six different classes. Transformer\nmodels proved their efficiency in classification tasks, but with the problem of\nover-fitting when fine-tuned on a small or an imbalanced dataset. We overcome\nthis limitation by investigating multiple training paradigms such as\nContrastive learning and Multi-task learning along with Classification\nfine-tuning and an ensemble of our top 5 performers. Our proposed solution\nachieved 0.841, 0.817, and 0.476 macro F1-average in sub-tasks A, B, and C\nrespectively.\n","authors":["Ahmad Shapiro","Ayman Khalafallah","Marwan Torki"],"pdf_url":"https://arxiv.org/pdf/2207.08557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08522v1","updated":"2022-07-18T11:37:47Z","published":"2022-07-18T11:37:47Z","title":"Classifying COVID-19 vaccine narratives","summary":"  COVID-19 vaccine hesitancy is widespread, despite governments' information\ncampaigns and WHO efforts. One of the reasons behind this is vaccine\ndisinformation which widely spreads in social media. In particular, recent\nsurveys have established that vaccine disinformation is impacting negatively\ncitizen trust in COVID-19 vaccination. At the same time, fact-checkers are\nstruggling with detecting and tracking of vaccine disinformation, due to the\nlarge scale of social media. To assist fact-checkers in monitoring vaccine\nnarratives online, this paper studies a new vaccine narrative classification\ntask, which categorises COVID-19 vaccine claims into one of seven categories.\nFollowing a data augmentation approach, we first construct a novel dataset for\nthis new classification task, focusing on the minority classes. We also make\nuse of fact-checker annotated data. The paper also presents a neural vaccine\nnarrative classifier that achieves an accuracy of 84% under cross-validation.\nThe classifier is publicly available for researchers and journalists.\n","authors":["Yue Li","Carolina Scarton","Xingyi Song","Kalina Bontcheva"],"pdf_url":"https://arxiv.org/pdf/2207.08522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08478v1","updated":"2022-07-18T09:59:21Z","published":"2022-07-18T09:59:21Z","title":"Towards Automated Classification of Attackers' TTPs by combining NLP\n  with ML Techniques","summary":"  The increasingly sophisticated and growing number of threat actors along with\nthe sheer speed at which cyber attacks unfold, make timely identification of\nattacks imperative to an organisations' security. Consequently, persons\nresponsible for security employ a large variety of information sources\nconcerning emerging attacks, attackers' course of actions or indicators of\ncompromise. However, a vast amount of the needed security information is\navailable in unstructured textual form, which complicates the automated and\ntimely extraction of attackers' Tactics, Techniques and Procedures (TTPs). In\norder to address this problem we systematically evaluate and compare different\nNatural Language Processing (NLP) and machine learning techniques used for\nsecurity information extraction in research. Based on our investigations we\npropose a data processing pipeline that automatically classifies unstructured\ntext according to attackers' tactics and techniques derived from a knowledge\nbase of adversary tactics, techniques and procedures.\n","authors":["Clemens Sauerwein","Alexander Pfohl"],"pdf_url":"https://arxiv.org/pdf/2207.08478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.05188v2","updated":"2022-07-18T09:24:35Z","published":"2022-07-11T20:51:28Z","title":"Knowledge Graph Induction enabling Recommending and Trend Analysis: A\n  Corporate Research Community Use Case","summary":"  A research division plays an important role of driving innovation in an\norganization. Drawing insights, following trends, keeping abreast of new\nresearch, and formulating strategies are increasingly becoming more challenging\nfor both researchers and executives as the amount of information grows in both\nvelocity and volume. In this paper we present a use case of how a corporate\nresearch community, IBM Research, utilizes Semantic Web technologies to induce\na unified Knowledge Graph from both structured and textual data obtained by\nintegrating various applications used by the community related to research\nprojects, academic papers, datasets, achievements and recognition. In order to\nmake the Knowledge Graph more accessible to application developers, we\nidentified a set of common patterns for exploiting the induced knowledge and\nexposed them as APIs. Those patterns were born out of user research which\nidentified the most valuable use cases or user pain points to be alleviated. We\noutline two distinct scenarios: recommendation and analytics for business use.\nWe will discuss these scenarios in detail and provide an empirical evaluation\non entity recommendation specifically. The methodology used and the lessons\nlearned from this work can be applied to other organizations facing similar\nchallenges.\n","authors":["Nandana Mihindukulasooriya","Mike Sava","Gaetano Rossiello","Md Faisal Mahbub Chowdhury","Irene Yachbes","Aditya Gidh","Jillian Duckwitz","Kovit Nisar","Michael Santos","Alfio Gliozzo"],"pdf_url":"https://arxiv.org/pdf/2207.05188v2.pdf","comment":"Accepted at ISWC 2022"},{"id":"http://arxiv.org/abs/2207.02802v2","updated":"2022-07-18T09:13:26Z","published":"2022-07-06T16:45:25Z","title":"Rethinking the Value of Gazetteer in Chinese Named Entity Recognition","summary":"  Gazetteer is widely used in Chinese named entity recognition (NER) to enhance\nspan boundary detection and type classification. However, to further understand\nthe generalizability and effectiveness of gazetteers, the NLP community still\nlacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,\nwe first re-examine the effectiveness several common practices of the\ngazetteer-enhanced NER models and carry out a series of detailed analysis to\nevaluate the relationship between the model performance and the gazetteer\ncharacteristics, which can guide us to build a more suitable gazetteer. The\nfindings of this paper are as follows: (1) the gazetteer improves most of the\nsituations that the traditional NER model datasets are difficult to learn. (2)\nthe performance of model greatly benefits from the high-quality pre-trained\nlexeme embeddings. (3) a good gazetteer should cover more entities that can be\nmatched in both the training set and testing set.\n","authors":["Qianglong Chen","Xiangji Zeng","Jiangang Zhu","Yin Zhang","Bojia Lin","Yang Yang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2207.02802v2.pdf","comment":"Accepted by NLPCC 2022"},{"id":"http://arxiv.org/abs/2207.01940v3","updated":"2022-07-18T08:50:34Z","published":"2022-07-05T10:27:17Z","title":"MIA 2022 Shared Task Submission: Leveraging Entity Representations,\n  Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question\n  Answering","summary":"  We describe our two-stage system for the Multilingual Information Access\n(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The\nfirst stage consists of multilingual passage retrieval with a hybrid dense and\nsparse retrieval strategy. The second stage consists of a reader which outputs\nthe answer from the top passages returned by the first stage. We show the\nefficacy of using a multilingual language model with entity representations in\npretraining, sparse retrieval signals to help dense retrieval, and\nFusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA\nand 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we\nobtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of\n31.61. We improve over the official baseline by over 4 F1 points on both the\ndevelopment and test sets.\n","authors":["Zhucheng Tu","Sarguna Janani Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2207.01940v3.pdf","comment":"System description for the Multilingual Information Access 2022\n  Shared Task"},{"id":"http://arxiv.org/abs/2109.05729v4","updated":"2022-07-18T08:19:30Z","published":"2021-09-13T06:25:45Z","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language\n  Understanding and Generation","summary":"  In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed to utilize the shared knowledge\nbetween natural language understanding (NLU) and natural language generation\n(NLG) to boost the performance. CPT consists of three parts: a shared encoder,\nan understanding decoder, and a generation decoder. Two specific decoders with\na shared encoder are pre-trained with masked language modeling (MLM) and\ndenoising auto-encoding (DAE) tasks, respectively. With the partially shared\narchitecture and multi-task pre-training, CPT can (1) learn specific knowledge\nof both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that\nfully exploits the potential of the model. Moreover, the unbalanced Transformer\nsaves the computational and storage cost, which makes CPT competitive and\ngreatly accelerates the inference of text generation. Experimental results on a\nwide range of Chinese NLU and NLG tasks show the effectiveness of CPT.\n","authors":["Yunfan Shao","Zhichao Geng","Yitao Liu","Junqi Dai","Hang Yan","Fei Yang","Li Zhe","Hujun Bao","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2109.05729v4.pdf","comment":"Code is available at https://github.com/fastnlp/CPT"},{"id":"http://arxiv.org/abs/2207.04719v2","updated":"2022-07-18T08:01:59Z","published":"2022-07-11T08:59:43Z","title":"Identifying public values and spatial conflicts in urban planning","summary":"  Identifying the diverse and often competing values of citizens, and resolving\nthe consequent public value conflicts, are of significant importance for\ninclusive and integrated urban development. Scholars have highlighted that\nrelational, value-laden urban space gives rise to many diverse conflicts that\nvary both spatially and temporally. Although notions of public value conflicts\nhave been conceived in theory, there are very few empirical studies that\nidentify such values and their conflicts in urban space. Building on public\nvalue theory and using a case-study mixed-methods approach, this paper proposes\na new approach to empirically investigate public value conflicts in urban\nspace. Using unstructured participatory data of 4,528 citizen contributions\nfrom a Public Participation Geographic Information Systems in Hamburg, Germany,\nnatural language processing and spatial clustering techniques are used to\nidentify areas of potential value conflicts. Four expert workshops assess and\ninterpret these quantitative findings. Integrating both quantitative and\nqualitative results, 19 general public values and a total of 9 archetypical\nconflicts are identified. On the basis of these results, this paper proposes a\nnew conceptual tool of Public Value Spheres that extends the theoretical notion\nof public-value conflicts and helps to further account for the value-laden\nnature of urban space.\n","authors":["Rico H. Herzog","Juliana E. Gonçalves","Geertje Slingerland","Reinout Kleinhans","Holger Prang","Frances Brazier","Trivik Verma"],"pdf_url":"https://arxiv.org/pdf/2207.04719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.03967v3","updated":"2022-07-18T07:50:21Z","published":"2022-01-10T02:11:25Z","title":"Emotion Intensity and its Control for Emotional Voice Conversion","summary":"  Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n","authors":["Kun Zhou","Berrak Sisman","Rajib Rana","Björn W. Schuller","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2201.03967v3.pdf","comment":"Accepted by IEEE Transactions on Affective Computing"},{"id":"http://arxiv.org/abs/2207.08408v1","updated":"2022-07-18T07:07:22Z","published":"2022-07-18T07:07:22Z","title":"STT: Soft Template Tuning for Few-Shot Adaptation","summary":"  Prompt tuning has been an extremely effective tool to adapt a pre-trained\nmodel to downstream tasks. However, standard prompt-based methods mainly\nconsider the case of sufficient data of downstream tasks. It is still unclear\nwhether the advantage can be transferred to the few-shot regime, where only\nlimited data are available for each downstream task. Although some works have\ndemonstrated the potential of prompt-tuning under the few-shot setting, the\nmain stream methods via searching discrete prompts or tuning soft prompts with\nlimited data are still very challenging. Through extensive empirical studies,\nwe find that there is still a gap between prompt tuning and fully fine-tuning\nfor few-shot learning. To bridge the gap, we propose a new prompt-tuning\nframework, called Soft Template Tuning (STT). STT combines manual and auto\nprompts, and treats downstream classification tasks as a masked language\nmodeling task. Comprehensive evaluation on different settings suggests STT can\nclose the gap between fine-tuning and prompt-based methods without introducing\nadditional parameters. Significantly, it can even outperform the time- and\nresource-consuming fine-tuning method on sentiment classification tasks.\n","authors":["Ping Yu","Wei Wang","Chunyuan Li","Ruiyi Zhang","Zhanpeng Jin","Changyou Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08376v1","updated":"2022-07-18T04:31:07Z","published":"2022-07-18T04:31:07Z","title":"Human Brains Can't Detect Fake News: A Neuro-Cognitive Study of Textual\n  Disinformation Susceptibility","summary":"  The spread of digital disinformation (aka \"fake news\") is arguably one of the\nmost significant threats on the Internet which can cause individual and\nsocietal harm of large scales. The susceptibility to fake news attacks hinges\non whether Internet users perceive a fake news article/snippet to be legitimate\nafter reading it. In this paper, we attempt to garner an in-depth understanding\nof users' susceptibility to text-centric fake news attacks via a\nneuro-cognitive methodology. We investigate the neural underpinnings relevant\nto fake/real news through EEG. We run an experiment with human users to pursue\na thorough investigation of users' perception and cognitive processing of\nfake/real news. We analyze the neural activity associated with the fake/real\nnews detection task for different categories of news articles. Our results show\nthere may be no statistically significant or automatically inferable\ndifferences in the way the human brain processes the fake vs. real news, while\nmarked differences are observed when people are subject to (real/fake) news vs.\nresting state and even between some different categories of fake news. This\nneuro-cognitive finding may help to justify users' susceptibility to fake news\nattacks, as also confirmed from the behavioral analysis. In other words, the\nfake news articles may seem almost indistinguishable from the real news\narticles in both behavioral and neural domains. Our work serves to dissect the\nfundamental neural phenomena underlying fake news attacks and explains users'\nsusceptibility to these attacks through the limits of human biology. We believe\nthis could be a notable insight for the researchers and practitioners\nsuggesting the human detection of fake news might be ineffective, which may\nalso have an adverse impact on the design of automated detection approaches\nthat crucially rely upon human labeling of text articles for building training\nmodels\n","authors":["Cagri Arisoy","Anuradha Mandal","Nitesh Saxena"],"pdf_url":"https://arxiv.org/pdf/2207.08376v1.pdf","comment":"12 pages, 9 tables, 2 figures, published in PST2022"},{"id":"http://arxiv.org/abs/2110.05679v4","updated":"2022-07-18T01:42:10Z","published":"2021-10-12T01:45:27Z","title":"Large Language Models Can Be Strong Differentially Private Learners","summary":"  Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n","authors":["Xuechen Li","Florian Tramèr","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2110.05679v4.pdf","comment":"31 pages; ICLR 2022 camera ready with additional writing\n  clarification and no \\vspace!"},{"id":"http://arxiv.org/abs/2207.08988v1","updated":"2022-07-18T23:53:17Z","published":"2022-07-18T23:53:17Z","title":"Training Large-Vocabulary Neural Language Models by Private Federated\n  Learning for Resource-Constrained Devices","summary":"  Federated Learning (FL) is a technique to train models using data distributed\nacross devices. Differential Privacy (DP) provides a formal privacy guarantee\nfor sensitive data. Our goal is to train a large neural network language model\n(NNLM) on compute-constrained devices while preserving privacy using FL and DP.\nHowever, the DP-noise introduced to the model increases as the model size\ngrows, which often prevents convergence. We propose Partial Embedding Updates\n(PEU), a novel technique to decrease noise by decreasing payload size.\nFurthermore, we adopt Low Rank Adaptation (LoRA) and Noise Contrastive\nEstimation (NCE) to reduce the memory demands of large models on\ncompute-constrained devices. This combination of techniques makes it possible\nto train large-vocabulary language models while preserving accuracy and\nprivacy.\n","authors":["Mingbin Xu","Congzheng Song","Ye Tian","Neha Agrawal","Filip Granqvist","Rogier van Dalen","Xiao Zhang","Arturo Argueta","Shiyi Han","Yaqiao Deng","Leo Liu","Anmol Walia","Alex Jin"],"pdf_url":"https://arxiv.org/pdf/2207.08988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08982v1","updated":"2022-07-18T23:43:52Z","published":"2022-07-18T23:43:52Z","title":"Selection Bias Induced Spurious Correlations in Large Language Models","summary":"  In this work we show how large language models (LLMs) can learn statistical\ndependencies between otherwise unconditionally independent variables due to\ndataset selection bias. To demonstrate the effect, we developed a masked gender\ntask that can be applied to BERT-family models to reveal spurious correlations\nbetween predicted gender pronouns and a variety of seemingly gender-neutral\nvariables like date and location, on pre-trained (unmodified) BERT and RoBERTa\nlarge models. Finally, we provide an online demo, inviting readers to\nexperiment further.\n","authors":["Emily McMilin"],"pdf_url":"https://arxiv.org/pdf/2207.08982v1.pdf","comment":"8 pages, 5 figures, Published at the ICML 2022 Workshop on Spurious\n  Correlations, Invariance, and Stability"},{"id":"http://arxiv.org/abs/2207.08943v1","updated":"2022-07-18T21:05:39Z","published":"2022-07-18T21:05:39Z","title":"MRCLens: an MRC Dataset Bias Detection Toolkit","summary":"  Many recent neural models have shown remarkable empirical results in Machine\nReading Comprehension, but evidence suggests sometimes the models take\nadvantage of dataset biases to predict and fail to generalize on out-of-sample\ndata. While many other approaches have been proposed to address this issue from\nthe computation perspective such as new architectures or training procedures,\nwe believe a method that allows researchers to discover biases, and adjust the\ndata or the models in an earlier stage will be beneficial. Thus, we introduce\nMRCLens, a toolkit that detects whether biases exist before users train the\nfull model. For the convenience of introducing the toolkit, we also provide a\ncategorization of common biases in MRC.\n","authors":["Yifan Zhong","Haohan Wang","Eric P. Xing"],"pdf_url":"https://arxiv.org/pdf/2207.08943v1.pdf","comment":"dataperf workshop at IMCL"},{"id":"http://arxiv.org/abs/2207.08880v1","updated":"2022-07-18T18:47:18Z","published":"2022-07-18T18:47:18Z","title":"Deep Sequence Models for Text Classification Tasks","summary":"  The exponential growth of data generated on the Internet in the current\ninformation age is a driving force for the digital economy. Extraction of\ninformation is the major value in an accumulated big data. Big data dependency\non statistical analysis and hand-engineered rules machine learning algorithms\nare overwhelmed with vast complexities inherent in human languages. Natural\nLanguage Processing (NLP) is equipping machines to understand these human\ndiverse and complicated languages. Text Classification is an NLP task which\nautomatically identifies patterns based on predefined or undefined labeled\nsets. Common text classification application includes information retrieval,\nmodeling news topic, theme extraction, sentiment analysis, and spam detection.\nIn texts, some sequences of words depend on the previous or next word sequences\nto make full meaning; this is a challenging dependency task that requires the\nmachine to be able to store some previous important information to impact\nfuture meaning. Sequence models such as RNN, GRU, and LSTM is a breakthrough\nfor tasks with long-range dependencies. As such, we applied these models to\nBinary and Multi-class classification. Results generated were excellent with\nmost of the models performing within the range of 80% and 94%. However, this\nresult is not exhaustive as we believe there is room for improvement if\nmachines are to compete with humans.\n","authors":["Saheed Salahudeen Abdullahi","Sun Yiming","Shamsuddeen Hassan Muhammad","Abdulrasheed Mustapha","Ahmad Muhammad Aminu","Abdulkadir Abdullahi","Musa Bello","Saminu Mohammad Aliyu"],"pdf_url":"https://arxiv.org/pdf/2207.08880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08823v1","updated":"2022-07-18T16:24:34Z","published":"2022-07-18T16:24:34Z","title":"Using attention methods to predict judicial outcomes","summary":"  Legal Judgment Prediction is one of the most acclaimed fields for the\ncombined area of NLP, AI, and Law. By legal prediction we mean an intelligent\nsystems capable to predict specific judicial characteristics, such as judicial\noutcome, a judicial class, predict an specific case. In this research, we have\nused AI classifiers to predict judicial outcomes in the Brazilian legal system.\nFor this purpose, we developed a text crawler to extract data from the official\nBrazilian electronic legal systems. These texts formed a dataset of\nsecond-degree murder and active corruption cases. We applied different\nclassifiers, such as Support Vector Machines and Neural Networks, to predict\njudicial outcomes by analyzing textual features from the dataset. Our research\nshowed that Regression Trees, Gated Recurring Units and Hierarchical Attention\nNetworks presented higher metrics for different subsets. As a final goal, we\nexplored the weights of one of the algorithms, the Hierarchical Attention\nNetworks, to find a sample of the most important words used to absolve or\nconvict defendants.\n","authors":["Vithor Gomes Ferreira Bertalan","Evandro Eduardo Seron Ruiz"],"pdf_url":"https://arxiv.org/pdf/2207.08823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10644v1","updated":"2022-07-18T09:09:23Z","published":"2022-07-18T09:09:23Z","title":"CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net\n  for the Single-Corpus and Cross-Corpus Speech Emotion Recognition","summary":"  Speech Emotion Recognition (SER) has become a growing focus of research in\nhuman-computer interaction. An essential challenge in SER is to extract common\nattributes from different speakers or languages, especially when a specific\nsource corpus has to be trained to recognize the unknown data coming from\nanother speech corpus. To address this challenge, a Capsule Network (CapsNet)\nand Transfer Learning based Mixed Task Net (CTLMTNet) are proposed to deal with\nboth the singlecorpus and cross-corpus SER tasks simultaneously in this paper.\nFor the single-corpus task, the combination of Convolution-Pooling and\nAttention CapsNet module CPAC) is designed by embedding the self-attention\nmechanism to the CapsNet, guiding the module to focus on the important features\nthat can be fed into different capsules. The extracted high-level features by\nCPAC provide sufficient discriminative ability. Furthermore, to handle the\ncross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module\n(CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can\nlearn the domain-invariant emotion representations through extracting the\nstrong emotion commonness. Experiments including ablation studies and\nvisualizations on both singleand cross-corpus tasks using four well-known SER\ndatasets in different languages are conducted for performance evaluation and\ncomparison. The results indicate that in both tasks the CTL-MTNet showed better\nperformance in all cases compared to a number of state-of-the-art methods. The\nsource code and the supplementary materials are available at:\nhttps://github.com/MLDMXM2017/CTLMTNet\n","authors":["Xin-Cheng Wen","Jia-Xin Ye","Yan Luo","Yong Xu","Xuan-Ze Wang","Chang-Li Wu","Kun-Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2207.10644v1.pdf","comment":"this paper has been accepted by IJCAI 2022. Please cite it by:\n  Xin-Cheng Wen#, JiaXin Ye#, Yan Luo, Yong Xu, Xuan-Ze WANG, Chang-Li Wu,\n  Kun-Hong Liu*, CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed\n  Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition,\n  IJCAI 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2207.08803v1","updated":"2022-07-18T17:59:58Z","published":"2022-07-18T17:59:58Z","title":"Adversarial Pixel Restoration as a Pretext Task for Transferable\n  Perturbations","summary":"  Transferable adversarial attacks optimize adversaries from a pretrained\nsurrogate model and known label space to fool the unknown black-box models.\nTherefore, these attacks are restricted by the availability of an effective\nsurrogate model. In this work, we relax this assumption and propose Adversarial\nPixel Restoration as a self-supervised alternative to train an effective\nsurrogate model from scratch under the condition of no labels and few data\nsamples. Our training approach is based on a min-max objective which reduces\noverfitting via an adversarial objective and thus optimizes for a more\ngeneralizable surrogate model. Our proposed attack is complimentary to our\nadversarial pixel restoration and is independent of any task specific objective\nas it can be launched in a self-supervised manner. We successfully demonstrate\nthe adversarial transferability of our approach to Vision Transformers as well\nas Convolutional Neural Networks for the tasks of classification, object\ndetection, and video segmentation. Our codes & pre-trained surrogate models are\navailable at: https://github.com/HashmatShadab/APR\n","authors":["Hashmat Shadab Malik","Shahina K Kunhimon","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2207.08803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07115v2","updated":"2022-07-18T17:56:53Z","published":"2022-07-14T17:59:37Z","title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin\n  Memory Model","summary":"  We present XMem, a video object segmentation architecture for long videos\nwith unified feature memory stores inspired by the Atkinson-Shiffrin memory\nmodel. Prior work on video object segmentation typically only uses one type of\nfeature memory. For videos longer than a minute, a single feature memory model\ntightly links memory consumption and accuracy. In contrast, following the\nAtkinson-Shiffrin model, we develop an architecture that incorporates multiple\nindependent yet deeply-connected feature memory stores: a rapidly updated\nsensory memory, a high-resolution working memory, and a compact thus sustained\nlong-term memory. Crucially, we develop a memory potentiation algorithm that\nroutinely consolidates actively used working memory elements into the long-term\nmemory, which avoids memory explosion and minimizes performance decay for\nlong-term prediction. Combined with a new memory reading mechanism, XMem\ngreatly exceeds state-of-the-art performance on long-video datasets while being\non par with state-of-the-art methods (that do not work on long videos) on\nshort-video datasets. Code is available at https://hkchengrex.github.io/XMem\n","authors":["Ho Kei Cheng","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2207.07115v2.pdf","comment":"Accepted to ECCV 2022. Project page:\n  https://hkchengrex.github.io/XMem"},{"id":"http://arxiv.org/abs/2207.07039v2","updated":"2022-07-18T17:48:37Z","published":"2022-07-14T16:32:28Z","title":"Convolutional Bypasses Are Better Vision Transformer Adapters","summary":"  The pretrain-then-finetune paradigm has been widely adopted in computer\nvision. But as the size of Vision Transformer (ViT) grows exponentially, the\nfull finetuning becomes prohibitive in view of the heavier storage overhead.\nMotivated by parameter-efficient transfer learning (PETL) on language\ntransformers, recent studies attempt to insert lightweight adaptation modules\n(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune\nthese modules while the pretrained weights are frozen. However, these modules\nwere originally proposed to finetune language models. Although ported well to\nViT, their design lacks prior knowledge for visual tasks. In this paper, we\npropose to construct Convolutional Bypasses (Convpass) in ViT as adaptation\nmodules, introducing only a small amount (less than 0.5% of model parameters)\nof trainable parameters to adapt the large ViT. Different from other PETL\nmethods, Convpass benefits from the hard-coded inductive bias of convolutional\nlayers and thus is more suitable for visual tasks, especially in the low-data\nregime. Experimental results on VTAB-1k benchmark and few-shot learning\ndatasets demonstrate that Convpass outperforms current language-oriented\nadaptation modules, demonstrating the necessity to tailor vision-oriented\nadaptation modules for vision models.\n","authors":["Shibo Jie","Zhi-Hong Deng"],"pdf_url":"https://arxiv.org/pdf/2207.07039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08794v1","updated":"2022-07-18T17:47:39Z","published":"2022-07-18T17:47:39Z","title":"DeFlowSLAM: Self-Supervised Scene Motion Decomposition for Dynamic Dense\n  SLAM","summary":"  We present a novel dual-flow representation of scene motion that decomposes\nthe optical flow into a static flow field caused by the camera motion and\nanother dynamic flow field caused by the objects' movements in the scene. Based\non this representation, we present a dynamic SLAM, dubbed DeFlowSLAM, that\nexploits both static and dynamic pixels in the images to solve the camera\nposes, rather than simply using static background pixels as other dynamic SLAM\nsystems do. We propose a dynamic update module to train our DeFlowSLAM in a\nself-supervised manner, where a dense bundle adjustment layer takes in\nestimated static flow fields and the weights controlled by the dynamic mask and\noutputs the residual of the optimized static flow fields, camera poses, and\ninverse depths. The static and dynamic flow fields are estimated by warping the\ncurrent image to the neighboring images, and the optical flow can be obtained\nby summing the two fields. Extensive experiments demonstrate that DeFlowSLAM\ngeneralizes well to both static and dynamic scenes as it exhibits comparable\nperformance to the state-of-the-art DROID-SLAM in static and less dynamic\nscenes while significantly outperforming DROID-SLAM in highly dynamic\nenvironments. Code and data are available on the project webpage: \\urlstyle{tt}\n\\textcolor{url_color}{\\url{https://zju3dv.github.io/deflowslam/}}.\n","authors":["Weicai Ye","Xingyuan Yu","Xinyue Lan","Yuhang Ming","Jinyu Li","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.08794v1.pdf","comment":"Homepage: https://zju3dv.github.io/deflowslam"},{"id":"http://arxiv.org/abs/2203.16513v2","updated":"2022-07-18T17:44:15Z","published":"2022-03-30T17:50:21Z","title":"PromptDet: Towards Open-vocabulary Detection using Uncurated Images","summary":"  The goal of this work is to establish a scalable pipeline for expanding an\nobject detector towards novel/unseen categories, using zero manual annotations.\nTo achieve that, we make the following four contributions: (i) in pursuit of\ngeneralisation, we propose a two-stage open-vocabulary object detector, where\nthe class-agnostic object proposals are classified with a text encoder from\npre-trained visual-language model; (ii) To pair the visual latent space (of RPN\nbox proposals) with that of the pre-trained text encoder, we propose the idea\nof regional prompt learning to align the textual embedding space with regional\nvisual object features; (iii) To scale up the learning procedure towards\ndetecting a wider spectrum of objects, we exploit the available online resource\nvia a novel self-training framework, which allows to train the proposed\ndetector on a large corpus of noisy uncurated web images. Lastly, (iv) to\nevaluate our proposed detector, termed as PromptDet, we conduct extensive\nexperiments on the challenging LVIS and MS-COCO dataset. PromptDet shows\nsuperior performance over existing approaches with fewer additional training\nimages and zero manual annotations whatsoever. Project page with code:\nhttps://fcjian.github.io/promptdet.\n","authors":["Chengjian Feng","Yujie Zhong","Zequn Jie","Xiangxiang Chu","Haibing Ren","Xiaolin Wei","Weidi Xie","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2203.16513v2.pdf","comment":"ECCV2022"},{"id":"http://arxiv.org/abs/2108.11364v3","updated":"2022-07-18T17:43:29Z","published":"2021-08-25T17:37:19Z","title":"Blind Image Decomposition","summary":"  We propose and study a novel task named Blind Image Decomposition (BID),\nwhich requires separating a superimposed image into constituent underlying\nimages in a blind setting, that is, both the source components involved in\nmixing as well as the mixing mechanism are unknown. For example, rain may\nconsist of multiple components, such as rain streaks, raindrops, snow, and\nhaze. Rainy images can be treated as an arbitrary combination of these\ncomponents, some of them or all of them. How to decompose superimposed images,\nlike rainy images, into distinct source components is a crucial step toward\nreal-world vision systems. To facilitate research on this new task, we\nconstruct multiple benchmark datasets, including mixed image decomposition\nacross multiple domains, real-scenario deraining, and joint\nshadow/reflection/watermark removal. Moreover, we propose a simple yet general\nBlind Image Decomposition Network (BIDeN) to serve as a strong baseline for\nfuture work. Experimental results demonstrate the tenability of our benchmarks\nand the effectiveness of BIDeN.\n","authors":["Junlin Han","Weihao Li","Pengfei Fang","Chunyi Sun","Jie Hong","Mohammad Ali Armin","Lars Petersson","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2108.11364v3.pdf","comment":"ECCV 2022. Project page:\n  https://junlinhan.github.io/projects/BID.html. Code:\n  https://github.com/JunlinHan/BID"},{"id":"http://arxiv.org/abs/2207.06400v2","updated":"2022-07-18T17:41:15Z","published":"2022-07-13T17:58:33Z","title":"PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular\n  Images","summary":"  We present PyMAF-X, a regression-based approach to recovering a full-body\nparametric model from a single image. This task is very challenging since minor\nparametric deviation may lead to noticeable misalignment between the estimated\nmesh and the input image. Moreover, when integrating part-specific estimations\nto the full-body model, existing solutions tend to either degrade the alignment\nor produce unnatural wrist poses. To address these issues, we propose a\nPyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for\nwell-aligned human mesh recovery and extend it as PyMAF-X for the recovery of\nexpressive full-body models. The core idea of PyMAF is to leverage a feature\npyramid and rectify the predicted parameters explicitly based on the mesh-image\nalignment status. Specifically, given the currently predicted parameters,\nmesh-aligned evidence will be extracted from finer-resolution features\naccordingly and fed back for parameter rectification. To enhance the alignment\nperception, an auxiliary dense supervision is employed to provide mesh-image\ncorrespondence guidance while spatial alignment attention is introduced to\nenable the awareness of the global contexts for our network. When extending\nPyMAF for full-body mesh recovery, an adaptive integration strategy is proposed\nin PyMAF-X to produce natural wrist poses while maintaining the well-aligned\nperformance of the part-specific estimations. The efficacy of our approach is\nvalidated on several benchmark datasets for body-only and full-body mesh\nrecovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment\nand achieve new state-of-the-art results. The project page with code and video\nresults can be found at https://www.liuyebin.com/pymaf-x.\n","authors":["Hongwen Zhang","Yating Tian","Yuxiang Zhang","Mengcheng Li","Liang An","Zhenan Sun","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2207.06400v2.pdf","comment":"An eXpressive extension of PyMAF [arXiv:2103.16507], Supporting\n  SMPL-X, Project page: https://www.liuyebin.com/pymaf-x"},{"id":"http://arxiv.org/abs/2207.08782v1","updated":"2022-07-18T17:38:40Z","published":"2022-07-18T17:38:40Z","title":"Instance-Aware Observer Network for Out-of-Distribution Object\n  Segmentation","summary":"  Recent work on Observer Network has shown promising results on\nOut-Of-Distribution (OOD) detection for semantic segmentation. These methods\nhave difficulty in precisely locating the point of interest in the image, i.e,\nthe anomaly. This limitation is due to the difficulty of fine-grained\nprediction at the pixel level. To address this issue, we provide instance\nknowledge to the observer. We extend the approach of ObsNet by harnessing an\ninstance-wise mask prediction. We use an additional, class agnostic, object\ndetector to filter and aggregate observer predictions. Finally, we predict an\nunique anomaly score for each instance in the image. We show that our proposed\nmethod accurately disentangle in-distribution objects from Out-Of-Distribution\nobjects on three datasets.\n","authors":["Victor Besnier","Andrei Bursuc","David Picard","Alexandre Briot"],"pdf_url":"https://arxiv.org/pdf/2207.08782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.11430v5","updated":"2022-07-18T17:33:36Z","published":"2021-11-22T18:59:29Z","title":"Class-agnostic Object Detection with Multi-modal Transformer","summary":"  What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and novel objects. In this paper, we advocate\nthat existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. For the first time in literature, we\ndemonstrate that Multi-modal Vision Transformers (MViT) trained with aligned\nimage-text pairs can effectively bridge this gap. Our extensive experiments\nacross various domains and novel objects show the state-of-the-art performance\nof MViTs to localize generic objects in images. Based on the observation that\nexisting MViTs do not include multi-scale feature processing and usually\nrequire longer training schedules, we develop an efficient MViT architecture\nusing multi-scale deformable attention and late vision-language fusion. We show\nthe significance of MViT proposals in a diverse range of applications including\nopen-world object detection, salient and camouflage object detection,\nsupervised and self-supervised detection tasks. Further, MViTs can adaptively\ngenerate proposals given a specific language query and thus offer enhanced\ninteractability. Code: \\url{https://git.io/J1HPY}.\n","authors":["Muhammad Maaz","Hanoona Rasheed","Salman Khan","Fahad Shahbaz Khan","Rao Muhammad Anwer","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2111.11430v5.pdf","comment":"ECCV 2022 accepted"},{"id":"http://arxiv.org/abs/2111.10659v2","updated":"2022-07-18T17:24:18Z","published":"2021-11-20T19:00:51Z","title":"Are Vision Transformers Robust to Patch Perturbations?","summary":"  Recent advances in Vision Transformer (ViT) have demonstrated its impressive\nperformance in image classification, which makes it a promising alternative to\nConvolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image\nas a sequence of image patches. The patch-based input image representation\nmakes the following question interesting: How does ViT perform when individual\ninput image patches are perturbed with natural corruptions or adversarial\nperturbations, compared to CNNs? In this work, we study the robustness of ViT\nto patch-wise perturbations. Surprisingly, we find that ViTs are more robust to\nnaturally corrupted patches than CNNs, whereas they are more vulnerable to\nadversarial patches. Furthermore, we discover that the attention mechanism\ngreatly affects the robustness of vision transformers. Specifically, the\nattention module can help improve the robustness of ViT by effectively ignoring\nnatural corrupted patches. However, when ViTs are attacked by an adversary, the\nattention mechanism can be easily fooled to focus more on the adversarially\nperturbed patches and cause a mistake. Based on our analysis, we propose a\nsimple temperature-scaling based method to improve the robustness of ViT\nagainst adversarial patches. Extensive qualitative and quantitative experiments\nare performed to support our findings, understanding, and improvement of ViT\nrobustness to patch-wise perturbations across a set of transformer-based\narchitectures.\n","authors":["Jindong Gu","Volker Tresp","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2111.10659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01573v2","updated":"2022-07-18T17:03:48Z","published":"2022-07-04T16:51:56Z","title":"Embedding contrastive unsupervised features to cluster in- and\n  out-of-distribution noise in corrupted image datasets","summary":"  Using search engines for web image retrieval is a tempting alternative to\nmanual curation when creating an image dataset, but their main drawback remains\nthe proportion of incorrect (noisy) samples retrieved. These noisy samples have\nbeen evidenced by previous works to be a mixture of in-distribution (ID)\nsamples, assigned to the incorrect category but presenting similar visual\nsemantics to other classes in the dataset, and out-of-distribution (OOD)\nimages, which share no semantic correlation with any category from the dataset.\nThe latter are, in practice, the dominant type of noisy images retrieved. To\ntackle this noise duality, we propose a two stage algorithm starting with a\ndetection step where we use unsupervised contrastive feature learning to\nrepresent images in a feature space. We find that the alignment and uniformity\nprinciples of contrastive learning allow OOD samples to be linearly separated\nfrom ID samples on the unit hypersphere. We then spectrally embed the\nunsupervised representations using a fixed neighborhood size and apply an\noutlier sensitive clustering at the class level to detect the clean and OOD\nclusters as well as ID noisy outliers. We finally train a noise robust neural\nnetwork that corrects ID noise to the correct category and utilizes OOD samples\nin a guided contrastive objective, clustering them to improve low-level\nfeatures. Our algorithm improves the state-of-the-art results on synthetic\nnoise image datasets as well as real-world web-crawled data. Our work is fully\nreproducible github.com/PaulAlbert31/SNCF.\n","authors":["Paul Albert","Eric Arazo","Noel E. O'Connor","Kevin McGuinness"],"pdf_url":"https://arxiv.org/pdf/2207.01573v2.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2205.15781v2","updated":"2022-07-18T16:59:33Z","published":"2022-05-31T13:30:36Z","title":"Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation\n  Models","summary":"  Semantic image segmentation is addressed by training deep models. Since\nsupervised training draws to a curse of human-based image labeling, using\nsynthetic images with automatically generated ground truth together with\nunlabeled real-world images is a promising alternative. This implies to address\nan unsupervised domain adaptation (UDA) problem. In this paper, we proposed a\nnew co-training process for synth-to-real UDA of semantic segmentation models.\nFirst, we design a self-training procedure which provides two initial models.\nThen, we keep training these models in a collaborative manner for obtaining the\nfinal model. The overall process treats the deep models as black boxes and\ndrives their collaboration at the level of pseudo-labeled target images, i.e.,\nneither modifying loss functions is required, nor explicit feature alignment.\nWe test our proposal on standard synthetic and real-world datasets. Our\nco-training shows improvements of 15-20 percentage points of mIoU over\nbaselines, so establishing new state-of-the-art results.\n","authors":["Jose L. Gómez","Gabriel Villalonga","Antonio M. López"],"pdf_url":"https://arxiv.org/pdf/2205.15781v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08739v1","updated":"2022-07-18T16:30:50Z","published":"2022-07-18T16:30:50Z","title":"Rethinking Data Augmentation for Robust Visual Question Answering","summary":"  Data Augmentation (DA) -- generating extra training samples beyond original\ntraining set -- has been widely-used in today's unbiased VQA models to mitigate\nthe language biases. Current mainstream DA strategies are synthetic-based\nmethods, which synthesize new samples by either editing some visual\nregions/words, or re-generating them from scratch. However, these synthetic\nsamples are always unnatural and error-prone. To avoid this issue, a recent DA\nwork composes new augmented samples by randomly pairing pristine images and\nother human-written questions. Unfortunately, to guarantee augmented samples\nhave reasonable ground-truth answers, they manually design a set of heuristic\nrules for several question types, which extremely limits its generalization\nabilities. To this end, we propose a new Knowledge Distillation based Data\nAugmentation for VQA, dubbed KDDAug. Specifically, we first relax the\nrequirements of reasonable image-question pairs, which can be easily applied to\nany question types. Then, we design a knowledge distillation (KD) based answer\nassignment to generate pseudo answers for all composed image-question pairs,\nwhich are robust to both in-domain and out-of-distribution settings. Since\nKDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into\nany VQA architectures. Extensive ablation studies on multiple backbones and\nbenchmarks have demonstrated the effectiveness and generalization abilities of\nKDDAug.\n","authors":["Long Chen","Yuhang Zheng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2207.08739v1.pdf","comment":"Accepted to ECCV 2022; Codes: https://github.com/ItemZheng/KDDAug"},{"id":"http://arxiv.org/abs/2207.08736v1","updated":"2022-07-18T16:29:41Z","published":"2022-07-18T16:29:41Z","title":"Towards Diverse and Faithful One-shot Adaption of Generative Adversarial\n  Networks","summary":"  One-shot generative domain adaption aims to transfer a pre-trained generator\non one domain to a new domain using one reference image only. However, it\nremains very challenging for the adapted generator (i) to generate diverse\nimages inherited from the pre-trained generator while (ii) faithfully acquiring\nthe domain-specific attributes and styles of the reference image. In this\npaper, we present a novel one-shot generative domain adaption method, i.e.,\nDiFa, for diverse generation and faithful adaptation. For global-level\nadaptation, we leverage the difference between the CLIP embedding of reference\nimage and the mean embedding of source images to constrain the target\ngenerator. For local-level adaptation, we introduce an attentive style loss\nwhich aligns each intermediate token of adapted image with its corresponding\ntoken of the reference image. To facilitate diverse generation, selective\ncross-domain consistency is introduced to select and retain the domain-sharing\nattributes in the editing latent $\\mathcal{W}+$ space to inherit the diversity\nof pre-trained generator. Extensive experiments show that our method\noutperforms the state-of-the-arts both quantitatively and qualitatively,\nespecially for the cases of large domain gaps. Moreover, our DiFa can easily be\nextended to zero-shot generative domain adaption with appealing results. Code\nis available at https://github.com/1170300521/DiFa.\n","authors":["Yabo Zhang","Mingshuai Yao","Yuxiang Wei","Zhilong Ji","Jinfeng Bai","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2207.08736v1.pdf","comment":"Code is available at https://github.com/1170300521/DiFa"},{"id":"http://arxiv.org/abs/2207.08734v1","updated":"2022-07-18T16:28:00Z","published":"2022-07-18T16:28:00Z","title":"Temporal Lift Pooling for Continuous Sign Language Recognition","summary":"  Pooling methods are necessities for modern neural networks for increasing\nreceptive fields and lowering down computational costs. However, commonly used\nhand-crafted pooling approaches, e.g., max pooling and average pooling, may not\nwell preserve discriminative features. While many researchers have elaborately\ndesigned various pooling variants in spatial domain to handle these limitations\nwith much progress, the temporal aspect is rarely visited where directly\napplying hand-crafted methods or these specialized spatial variants may not be\noptimal. In this paper, we derive temporal lift pooling (TLP) from the Lifting\nScheme in signal processing to intelligently downsample features of different\ntemporal hierarchies. The Lifting Scheme factorizes input signals into various\nsub-bands with different frequency, which can be viewed as different temporal\nmovement patterns. Our TLP is a three-stage procedure, which performs signal\ndecomposition, component weighting and information fusion to generate a refined\ndownsized feature map. We select a typical temporal task with long sequences,\ni.e. continuous sign language recognition (CSLR), as our testbed to verify the\neffectiveness of TLP. Experiments on two large-scale datasets show TLP\noutperforms hand-crafted methods and specialized spatial variants by a large\nmargin (1.5%) with similar computational overhead. As a robust feature\nextractor, TLP exhibits great generalizability upon multiple backbones on\nvarious datasets and achieves new state-of-the-art results on two large-scale\nCSLR datasets. Visualizations further demonstrate the mechanism of TLP in\ncorrecting gloss borders. Code is released.\n","authors":["Lianyu Hu","Liqing Gao","Zekang Liu","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2207.08734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08699v1","updated":"2022-07-18T15:49:27Z","published":"2022-07-18T15:49:27Z","title":"Semantic Novelty Detection via Relational Reasoning","summary":"  Semantic novelty detection aims at discovering unknown categories in the test\ndata. This task is particularly relevant in safety-critical applications, such\nas autonomous driving or healthcare, where it is crucial to recognize unknown\nobjects at deployment time and issue a warning to the user accordingly. Despite\nthe impressive advancements of deep learning research, existing models still\nneed a finetuning stage on the known categories in order to recognize the\nunknown ones. This could be prohibitive when privacy rules limit data access,\nor in case of strict memory and computational constraints (e.g. edge\ncomputing). We claim that a tailored representation learning strategy may be\nthe right solution for effective and efficient semantic novelty detection.\nBesides extensively testing state-of-the-art approaches for this task, we\npropose a novel representation learning paradigm based on relational reasoning.\nIt focuses on learning how to measure semantic similarity rather than\nrecognizing known categories. Our experiments show that this knowledge is\ndirectly transferable to a wide range of scenarios, and it can be exploited as\na plug-and-play module to convert closed-set recognition models into reliable\nopen-set ones.\n","authors":["Francesco Cappio Borlino","Silvia Bucci","Tatiana Tommasi"],"pdf_url":"https://arxiv.org/pdf/2207.08699v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.05515v2","updated":"2022-07-18T15:33:15Z","published":"2022-07-12T13:17:38Z","title":"Compound Prototype Matching for Few-shot Action Recognition","summary":"  Few-shot action recognition aims to recognize novel action classes using only\na small number of labeled training samples. In this work, we propose a novel\napproach that first summarizes each video into compound prototypes consisting\nof a group of global prototypes and a group of focused prototypes, and then\ncompares video similarity based on the prototypes. Each global prototype is\nencouraged to summarize a specific aspect from the entire video, for example,\nthe start/evolution of the action. Since no clear annotation is provided for\nthe global prototypes, we use a group of focused prototypes to focus on certain\ntimestamps in the video. We compare video similarity by matching the compound\nprototypes between the support and query videos. The global prototypes are\ndirectly matched to compare videos from the same perspective, for example, to\ncompare whether two actions start similarly. For the focused prototypes, since\nactions have various temporal variations in the videos, we apply bipartite\nmatching to allow the comparison of actions with different temporal positions\nand shifts. Experiments demonstrate that our proposed method achieves\nstate-of-the-art results on multiple benchmarks.\n","authors":["Lijin Yang","Yifei Huang","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2207.05515v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08677v1","updated":"2022-07-18T15:12:33Z","published":"2022-07-18T15:12:33Z","title":"Label2Label: A Language Modeling Framework for Multi-Attribute Learning","summary":"  Objects are usually associated with multiple attributes, and these attributes\noften exhibit high correlations. Modeling complex relationships between\nattributes poses a great challenge for multi-attribute learning. This paper\nproposes a simple yet generic framework named Label2Label to exploit the\ncomplex attribute correlations. Label2Label is the first attempt for\nmulti-attribute prediction from the perspective of language modeling.\nSpecifically, it treats each attribute label as a \"word\" describing the sample.\nAs each sample is annotated with multiple attribute labels, these \"words\" will\nnaturally form an unordered but meaningful \"sentence\", which depicts the\nsemantic information of the corresponding sample. Inspired by the remarkable\nsuccess of pre-training language models in NLP, Label2Label introduces an\nimage-conditioned masked language model, which randomly masks some of the\n\"word\" tokens from the label \"sentence\" and aims to recover them based on the\nmasked \"sentence\" and the context conveyed by image features. Our intuition is\nthat the instance-wise attribute relations are well grasped if the neural net\ncan infer the missing attributes based on the context and the remaining\nattribute hints. Label2Label is conceptually simple and empirically powerful.\nWithout incorporating task-specific prior knowledge and highly specialized\nnetwork designs, our approach achieves state-of-the-art results on three\ndifferent multi-attribute learning tasks, compared to highly customized\ndomain-specific methods. Code is available at\nhttps://github.com/Li-Wanhua/Label2Label.\n","authors":["Wanhua Li","Zhexuan Cao","Jianjiang Feng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2207.08677v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08674v1","updated":"2022-07-18T15:11:18Z","published":"2022-07-18T15:11:18Z","title":"Boosting Video Super Resolution with Patch-Based Temporal Redundancy\n  Optimization","summary":"  The success of existing video super-resolution (VSR) algorithms stems mainly\nexploiting the temporal information from the neighboring frames. However, none\nof these methods have discussed the influence of the temporal redundancy in the\npatches with stationary objects and background and usually use all the\ninformation in the adjacent frames without any discrimination. In this paper,\nwe observe that the temporal redundancy will bring adverse effect to the\ninformation propagation,which limits the performance of the most existing VSR\nmethods. Motivated by this observation, we aim to improve existing VSR\nalgorithms by handling the temporal redundancy patches in an optimized manner.\nWe develop two simple yet effective plug and play methods to improve the\nperformance of existing local and non-local propagation-based VSR algorithms on\nwidely-used public videos. For more comprehensive evaluating the robustness and\nperformance of existing VSR algorithms, we also collect a new dataset which\ncontains a variety of public videos as testing set. Extensive evaluations show\nthat the proposed methods can significantly improve the performance of existing\nVSR methods on the collected videos from wild scenarios while maintain their\nperformance on existing commonly used datasets. The code is available at\nhttps://github.com/HYHsimon/Boosted-VSR.\n","authors":["Yuhao Huang","Hang Dong","Jinshan Pan","Chao Zhu","Yu Guo","Ding Liu","Lean Fu","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2207.08674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08664v1","updated":"2022-07-18T15:02:27Z","published":"2022-07-18T15:02:27Z","title":"Action-based Contrastive Learning for Trajectory Prediction","summary":"  Trajectory prediction is an essential task for successful human robot\ninteraction, such as in autonomous driving. In this work, we address the\nproblem of predicting future pedestrian trajectories in a first person view\nsetting with a moving camera. To that end, we propose a novel action-based\ncontrastive learning loss, that utilizes pedestrian action information to\nimprove the learned trajectory embeddings. The fundamental idea behind this new\nloss is that trajectories of pedestrians performing the same action should be\ncloser to each other in the feature space than the trajectories of pedestrians\nwith significantly different actions. In other words, we argue that behavioral\ninformation about pedestrian action influences their future trajectory.\nFurthermore, we introduce a novel sampling strategy for trajectories that is\nable to effectively increase negative and positive contrastive samples.\nAdditional synthetic trajectory samples are generated using a trained\nConditional Variational Autoencoder (CVAE), which is at the core of several\nmodels developed for trajectory prediction. Results show that our proposed\ncontrastive framework employs contextual information about pedestrian behavior,\ni.e. action, effectively, and it learns a better trajectory representation.\nThus, integrating the proposed contrastive framework within a trajectory\nprediction model improves its results and outperforms state-of-the-art methods\non three trajectory prediction benchmarks [31, 32, 26].\n","authors":["Marah Halawa","Olaf Hellwich","Pia Bideau"],"pdf_url":"https://arxiv.org/pdf/2207.08664v1.pdf","comment":"This paper will appear in the proceedings of The European Conference\n  on Computer Vision (ECCV 2022)"},{"id":"http://arxiv.org/abs/2207.08656v1","updated":"2022-07-18T14:54:57Z","published":"2022-07-18T14:54:57Z","title":"Towards High-Fidelity Single-view Holistic Reconstruction of Indoor\n  Scenes","summary":"  We present a new framework to reconstruct holistic 3D indoor scenes including\nboth room background and indoor objects from single-view images. Existing\nmethods can only produce 3D shapes of indoor objects with limited geometry\nquality because of the heavy occlusion of indoor scenes. To solve this, we\npropose an instance-aligned implicit function (InstPIFu) for detailed object\nreconstruction. Combining with instance-aligned attention module, our method is\nempowered to decouple mixed local features toward the occluded instances.\nAdditionally, unlike previous methods that simply represents the room\nbackground as a 3D bounding box, depth map or a set of planes, we recover the\nfine geometry of the background via implicit representation. Extensive\nexperiments on the e SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets\ndemonstrate that our method outperforms existing approaches in both background\nand foreground object reconstruction. Our code and model will be made publicly\navailable.\n","authors":["Haolin Liu","Yujian Zheng","Guanying Chen","Shuguang Cui","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2207.08656v1.pdf","comment":"ECCV 2022, project page: https://github.com/UncleMEDM/InstPIFu"},{"id":"http://arxiv.org/abs/2207.08653v1","updated":"2022-07-18T14:52:37Z","published":"2022-07-18T14:52:37Z","title":"Leveraging Action Affinity and Continuity for Semi-supervised Temporal\n  Action Segmentation","summary":"  We present a semi-supervised learning approach to the temporal action\nsegmentation task. The goal of the task is to temporally detect and segment\nactions in long, untrimmed procedural videos, where only a small set of videos\nare densely labelled, and a large collection of videos are unlabelled. To this\nend, we propose two novel loss functions for the unlabelled data: an action\naffinity loss and an action continuity loss. The action affinity loss guides\nthe unlabelled samples learning by imposing the action priors induced from the\nlabelled set. Action continuity loss enforces the temporal continuity of\nactions, which also provides frame-wise classification supervision. In\naddition, we propose an Adaptive Boundary Smoothing (ABS) approach to build\ncoarser action boundaries for more robust and reliable learning. The proposed\nloss functions and ABS were evaluated on three benchmarks. Results show that\nthey significantly improved action segmentation performance with a low amount\n(5% and 10%) of labelled data and achieved comparable results to full\nsupervision with 50% labelled data. Furthermore, ABS succeeded in boosting\nperformance when integrated into fully-supervised learning.\n","authors":["Guodong Ding","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2207.08653v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.08648v1","updated":"2022-07-18T14:50:20Z","published":"2022-07-18T14:50:20Z","title":"Interpolation, extrapolation, and local generalization in common neural\n  networks","summary":"  There has been a long history of works showing that neural networks have hard\ntime extrapolating beyond the training set. A recent study by Balestriero et\nal. (2021) challenges this view: defining interpolation as the state of\nbelonging to the convex hull of the training set, they show that the test set,\neither in input or neural space, cannot lie for the most part in this convex\nhull, due to the high dimensionality of the data, invoking the well known curse\nof dimensionality. Neural networks are then assumed to necessarily work in\nextrapolative mode. We here study the neural activities of the last hidden\nlayer of typical neural networks. Using an autoencoder to uncover the intrinsic\nspace underlying the neural activities, we show that this space is actually\nlow-dimensional, and that the better the model, the lower the dimensionality of\nthis intrinsic space. In this space, most samples of the test set actually lie\nin the convex hull of the training set: under the convex hull definition, the\nmodels thus happen to work in interpolation regime. Moreover, we show that\nbelonging to the convex hull does not seem to be the relevant criteria.\nDifferent measures of proximity to the training set are actually better related\nto performance accuracy. Thus, typical neural networks do seem to operate in\ninterpolation regime. Good generalization performances are linked to the\nability of a neural network to operate well in such a regime.\n","authors":["Laurent Bonnasse-Gahot"],"pdf_url":"https://arxiv.org/pdf/2207.08648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07078v2","updated":"2022-07-18T14:38:05Z","published":"2022-07-14T17:27:19Z","title":"Towards Grand Unification of Object Tracking","summary":"  We present a unified method, termed Unicorn, that can simultaneously solve\nfour tracking problems (SOT, MOT, VOS, MOTS) with a single network using the\nsame model parameters. Due to the fragmented definitions of the object tracking\nproblem itself, most existing trackers are developed to address a single or\npart of tasks and overspecialize on the characteristics of specific tasks. By\ncontrast, Unicorn provides a unified solution, adopting the same input,\nbackbone, embedding, and head across all tracking tasks. For the first time, we\naccomplish the great unification of the tracking network architecture and\nlearning paradigm. Unicorn performs on-par or better than its task-specific\ncounterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,\nBDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will\nserve as a solid step towards the general vision model. Code is available at\nhttps://github.com/MasterBin-IIAU/Unicorn.\n","authors":["Bin Yan","Yi Jiang","Peize Sun","Dong Wang","Zehuan Yuan","Ping Luo","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2207.07078v2.pdf","comment":"ECCV2022 Oral"},{"id":"http://arxiv.org/abs/2207.08631v1","updated":"2022-07-18T14:24:46Z","published":"2022-07-18T14:24:46Z","title":"Latent Partition Implicit with Surface Codes for 3D Representation","summary":"  Deep implicit functions have shown remarkable shape modeling ability in\nvarious 3D computer vision tasks. One drawback is that it is hard for them to\nrepresent a 3D shape as multiple parts. Current solutions learn various\nprimitives and blend the primitives directly in the spatial space, which still\nstruggle to approximate the 3D shape accurately. To resolve this problem, we\nintroduce a novel implicit representation to represent a single 3D shape as a\nset of parts in the latent space, towards both highly accurate and plausibly\ninterpretable shape modeling. Our insight here is that both the part learning\nand the part blending can be conducted much easier in the latent space than in\nthe spatial space. We name our method Latent Partition Implicit (LPI), because\nof its ability of casting the global shape modeling into multiple local part\nmodeling, which partitions the global shape unity. LPI represents a shape as\nSigned Distance Functions (SDFs) using surface codes. Each surface code is a\nlatent code representing a part whose center is on the surface, which enables\nus to flexibly employ intrinsic attributes of shapes or additional surface\nproperties. Eventually, LPI can reconstruct both the shape and the parts on the\nshape, both of which are plausible meshes. LPI is a multi-level representation,\nwhich can partition a shape into different numbers of parts after training. LPI\ncan be learned without ground truth signed distances, point normals or any\nsupervision for part partition. LPI outperforms the latest methods under the\nwidely used benchmarks in terms of reconstruction accuracy and modeling\ninterpretability. Our code, data and models are available at\nhttps://github.com/chenchao15/LPI.\n","authors":["Chao Chen","Yu-Shen Liu","Zhihong Han"],"pdf_url":"https://arxiv.org/pdf/2207.08631v1.pdf","comment":"20pages,14figures"},{"id":"http://arxiv.org/abs/2207.08630v1","updated":"2022-07-18T14:23:38Z","published":"2022-07-18T14:23:38Z","title":"FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity\n  in Data-Efficient GANs","summary":"  Data-Efficient GANs (DE-GANs), which aim to learn generative models with a\nlimited amount of training data, encounter several challenges for generating\nhigh-quality samples. Since data augmentation strategies have largely\nalleviated the training instability, how to further improve the generative\nperformance of DE-GANs becomes a hotspot. Recently, contrastive learning has\nshown the great potential of increasing the synthesis quality of DE-GANs, yet\nrelated principles are not well explored. In this paper, we revisit and compare\ndifferent contrastive learning strategies in DE-GANs, and identify (i) the\ncurrent bottleneck of generative performance is the discontinuity of latent\nspace; (ii) compared to other contrastive learning strategies,\nInstance-perturbation works towards latent space continuity, which brings the\nmajor improvement to DE-GANs. Based on these observations, we propose FakeCLR,\nwhich only applies contrastive learning on perturbed fake samples, and devises\nthree related training techniques: Noise-related Latent Augmentation,\nDiversity-aware Queue, and Forgetting Factor of Queue. Our experimental results\nmanifest the new state of the arts on both few-shot generation and limited-data\ngeneration. On multiple datasets, FakeCLR acquires more than 15% FID\nimprovement compared to existing DE-GANs. Code is available at\nhttps://github.com/iceli1007/FakeCLR.\n","authors":["Ziqiang Li","Chaoyue Wang","Heliang Zheng","Jing Zhang","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2207.08630v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.07372v2","updated":"2022-07-18T14:20:24Z","published":"2022-07-15T09:38:56Z","title":"3D Instances as 1D Kernels","summary":"  We introduce a 3D instance representation, termed instance kernels, where\ninstances are represented by one-dimensional vectors that encode the semantic,\npositional, and shape information of 3D instances. We show that instance\nkernels enable easy mask inference by simply scanning kernels over the entire\nscenes, avoiding the heavy reliance on proposals or heuristic clustering\nalgorithms in standard 3D instance segmentation pipelines. The idea of instance\nkernel is inspired by recent success of dynamic convolutions in 2D/3D instance\nsegmentation. However, we find it non-trivial to represent 3D instances due to\nthe disordered and unstructured nature of point cloud data, e.g., poor instance\nlocalization can significantly degrade instance representation. To remedy this,\nwe construct a novel 3D instance encoding paradigm. First, potential instance\ncentroids are localized as candidates. Then, a candidate merging scheme is\ndevised to simultaneously aggregate duplicated candidates and collect context\naround the merged centroids to form the instance kernels. Once instance kernels\nare available, instance masks can be reconstructed via dynamic convolutions\nwhose weights are conditioned on instance kernels. The whole pipeline is\ninstantiated with a dynamic kernel network (DKNet). Results show that DKNet\noutperforms the state of the arts on both ScanNetV2 and S3DIS datasets with\nbetter instance localization. Code is available:\nhttps://github.com/W1zheng/DKNet.\n","authors":["Yizheng Wu","Min Shi","Shuaiyuan Du","Hao Lu","Zhiguo Cao","Weicai Zhong"],"pdf_url":"https://arxiv.org/pdf/2207.07372v2.pdf","comment":"Appearing in ECCV, 2022"},{"id":"http://arxiv.org/abs/2203.09836v2","updated":"2022-07-18T14:19:43Z","published":"2022-03-18T10:20:21Z","title":"Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation","summary":"  Most recent 6D object pose estimation methods, including unsupervised ones,\nrequire many real training images. Unfortunately, for some applications, such\nas those in space or deep under water, acquiring real images, even unannotated,\nis virtually impossible. In this paper, we propose a method that can be trained\nsolely on synthetic images, or optionally using a few additional real ones.\nGiven a rough pose estimate obtained from a first network, it uses a second\nnetwork to predict a dense 2D correspondence field between the image rendered\nusing the rough pose and the real image and infers the required pose\ncorrection. This approach is much less sensitive to the domain shift between\nsynthetic and real images than state-of-the-art methods. It performs on par\nwith methods that require annotated real images for training when not using\nany, and outperforms them considerably when using as few as twenty real images.\n","authors":["Yinlin Hu","Pascal Fua","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2203.09836v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08625v1","updated":"2022-07-18T14:18:13Z","published":"2022-07-18T14:18:13Z","title":"Unifying Event Detection and Captioning as Sequence Generation via\n  Pre-Training","summary":"  Dense video captioning aims to generate corresponding text descriptions for a\nseries of events in the untrimmed video, which can be divided into two\nsub-tasks, event detection and event captioning. Unlike previous works that\ntackle the two sub-tasks separately, recent works have focused on enhancing the\ninter-task association between the two sub-tasks. However, designing inter-task\ninteractions for event detection and captioning is not trivial due to the large\ndifferences in their task specific solutions. Besides, previous event detection\nmethods normally ignore temporal dependencies between events, leading to event\nredundancy or inconsistency problems. To tackle above the two defects, in this\npaper, we define event detection as a sequence generation task and propose a\nunified pre-training and fine-tuning framework to naturally enhance the\ninter-task association between event detection and captioning. Since the model\npredicts each event with previous events as context, the inter-dependency\nbetween events is fully exploited and thus our model can detect more diverse\nand consistent events in the video. Experiments on the ActivityNet dataset show\nthat our model outperforms the state-of-the-art methods, and can be further\nboosted when pre-trained on extra large-scale video-text data. Code is\navailable at \\url{https://github.com/QiQAng/UEDVC}.\n","authors":["Qi Zhang","Yuqing Song","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2207.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.05892v3","updated":"2022-07-18T14:15:11Z","published":"2021-04-13T01:53:04Z","title":"CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge\n  Distillation","summary":"  As segmentation labels are scarce, extensive researches have been conducted\nto train segmentation networks with domain adaptation, semi-supervised or\nself-supervised learning techniques to utilize abundant unlabeled dataset.\nHowever, these approaches appear different from each other, so it is not clear\nhow these approaches can be combined for better performance. Inspired by recent\nmulti-domain image translation approaches, here we propose a novel segmentation\nframework using adaptive instance normalization (AdaIN), so that a single\ngenerator is trained to perform both domain adaptation and semi-supervised\nsegmentation tasks via knowledge distillation by simply changing task-specific\nAdaIN codes. Specifically, our framework is designed to deal with difficult\nsituations in chest X-ray radiograph (CXR) segmentation, where labels are only\navailable for normal data, but trained model should be applied to both normal\nand abnormal data. The proposed network demonstrates great generalizability\nunder domain shift and achieves the state-of-the-art performance for abnormal\nCXR segmentation.\n","authors":["Yujin Oh","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2104.05892v3.pdf","comment":"ECCV 2022 camera ready"},{"id":"http://arxiv.org/abs/2111.05759v2","updated":"2022-07-18T14:11:51Z","published":"2021-11-10T16:04:49Z","title":"Multimodal Transformer with Variable-length Memory for\n  Vision-and-Language Navigation","summary":"  Vision-and-Language Navigation (VLN) is a task that an agent is required to\nfollow a language instruction to navigate to the goal position, which relies on\nthe ongoing interactions with the environment during moving. Recent\nTransformer-based VLN methods have made great progress benefiting from the\ndirect connections between visual observations and the language instruction via\nthe multimodal cross-attention mechanism. However, these methods usually\nrepresent temporal context as a fixed-length vector by using an LSTM decoder or\nusing manually designed hidden states to build a recurrent Transformer.\nConsidering a single fixed-length vector is often insufficient to capture\nlong-term temporal context, in this paper, we introduce Multimodal Transformer\nwith Variable-length Memory (MTVM) for visually-grounded natural language\nnavigation by modelling the temporal context explicitly. Specifically, MTVM\nenables the agent to keep track of the navigation trajectory by directly\nstoring previous activations in a memory bank. To further boost the\nperformance, we propose a memory-aware consistency loss to help learn a better\njoint representation of temporal context with random masked instructions. We\nevaluate MTVM on popular R2R and CVDN datasets, and our model improves Success\nRate on R2R unseen validation and test set by 2% each, and reduce Goal Process\nby 1.6m on CVDN test set.\n","authors":["Chuang Lin","Yi Jiang","Jianfei Cai","Lizhen Qu","Gholamreza Haffari","Zehuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2111.05759v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08619v1","updated":"2022-07-18T14:05:25Z","published":"2022-07-18T14:05:25Z","title":"CACTUSS: Common Anatomical CT-US Space for US examinations","summary":"  Abdominal aortic aneurysm (AAA) is a vascular disease in which a section of\nthe aorta enlarges, weakening its walls and potentially rupturing the vessel.\nAbdominal ultrasound has been utilized for diagnostics, but due to its limited\nimage quality and operator dependency, CT scans are usually required for\nmonitoring and treatment planning. Recently, abdominal CT datasets have been\nsuccessfully utilized to train deep neural networks for automatic aorta\nsegmentation. Knowledge gathered from this solved task could therefore be\nleveraged to improve US segmentation for AAA diagnosis and monitoring. To this\nend, we propose CACTUSS: a common anatomical CT-US space, which acts as a\nvirtual bridge between CT and US modalities to enable automatic AAA screening\nsonography. CACTUSS makes use of publicly available labelled data to learn to\nsegment based on an intermediary representation that inherits properties from\nboth US and CT. We train a segmentation network in this new representation and\nemploy an additional image-to-image translation network which enables our model\nto perform on real B-mode images. Quantitative comparisons against fully\nsupervised methods demonstrate the capabilities of CACTUSS in terms of Dice\nScore and diagnostic metrics, showing that our method also meets the clinical\nrequirements for AAA scanning and diagnosis.\n","authors":["Yordanka Velikova","Walter Simson","Mehrdad Salehi","Mohammad Farid Azampour","Philipp Paprottka","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2207.08619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05469v2","updated":"2022-07-18T13:59:42Z","published":"2022-03-10T16:46:05Z","title":"Prediction-Guided Distillation for Dense Object Detection","summary":"  Real-world object detection models should be cheap and accurate. Knowledge\ndistillation (KD) can boost the accuracy of a small, cheap detection model by\nleveraging useful information from a larger teacher model. However, a key\nchallenge is identifying the most informative features produced by the teacher\nfor distillation. In this work, we show that only a very small fraction of\nfeatures within a ground-truth bounding box are responsible for a teacher's\nhigh detection performance. Based on this, we propose Prediction-Guided\nDistillation (PGD), which focuses distillation on these key predictive regions\nof the teacher and yields considerable gains in performance over many existing\nKD baselines. In addition, we propose an adaptive weighting scheme over the key\nregions to smooth out their influence and achieve even better performance. Our\nproposed approach outperforms current state-of-the-art KD baselines on a\nvariety of advanced one-stage detection architectures. Specifically, on the\nCOCO dataset, our method achieves between +3.1% and +4.6% AP improvement using\nResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On\nthe CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,\nalso using these backbones. Our code is available at\nhttps://github.com/ChenhongyiYang/PGD.\n","authors":["Chenhongyi Yang","Mateusz Ochal","Amos Storkey","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2203.05469v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08609v1","updated":"2022-07-18T13:55:48Z","published":"2022-07-18T13:55:48Z","title":"ExAgt: Expert-guided Augmentation for Representation Learning of Traffic\n  Scenarios","summary":"  Representation learning in recent years has been addressed with\nself-supervised learning methods. The input data is augmented into two\ndistorted views and an encoder learns the representations that are invariant to\ndistortions -- cross-view prediction. Augmentation is one of the key components\nin cross-view self-supervised learning frameworks to learn visual\nrepresentations. This paper presents ExAgt, a novel method to include expert\nknowledge for augmenting traffic scenarios, to improve the learnt\nrepresentations without any human annotation. The expert-guided augmentations\nare generated in an automated fashion based on the infrastructure, the\ninteractions between the EGO and the traffic participants and an ideal sensor\nmodel. The ExAgt method is applied in two state-of-the-art cross-view\nprediction methods and the representations learnt are tested in downstream\ntasks like classification and clustering. Results show that the ExAgt method\nimproves representation learning compared to using only standard augmentations\nand it provides a better representation space stability. The code is available\nat \\url{https://github.com/lab176344/ExAgt}.\n","authors":["Lakshman Balasubramanian","Jonas Wurst","Robin Egolf","Michael Botsch","Wolfgang Utschick","Ke Deng"],"pdf_url":"https://arxiv.org/pdf/2207.08609v1.pdf","comment":"Accepted as a conference paper in ITSC 2022, Macau, China"},{"id":"http://arxiv.org/abs/2206.13076v3","updated":"2022-07-18T13:52:37Z","published":"2022-06-27T06:37:02Z","title":"SearchMorph:Multi-scale Correlation Iterative Network for Deformable\n  Registration","summary":"  Deformable image registration can obtain dynamic information about images,\nwhich is of great significance in medical image analysis. The unsupervised deep\nlearning registration method can quickly achieve high registration accuracy\nwithout labels. However, these methods generally suffer from uncorrelated\nfeatures, poor ability to register large deformations and details, and\nunnatural deformation fields. To address the issues above, we propose an\nunsupervised multi-scale correlation iterative registration network\n(SearchMorph). In the proposed network, we introduce a correlation layer to\nstrengthen the relevance between features and construct a correlation pyramid\nto provide multi-scale relevance information for the network. We also design a\ndeformation field iterator, which improves the ability of the model to register\ndetails and large deformations through the search module and GRU while ensuring\nthat the deformation field is realistic. We use single-temporal brain MR images\nand multi-temporal echocardiographic sequences to evaluate the model's ability\nto register large deformations and details. The experimental results\ndemonstrate that the method in this paper achieves the highest registration\naccuracy and the lowest folding point ratio using a short elapsed time to\nstate-of-the-art.\n","authors":["Xiao Fan","Shuxin Zhuang","Zhemin Zhuang","Ye Yuan","Shunmin Qiu","Alex Noel Joseph Raj","Yibiao Rong"],"pdf_url":"https://arxiv.org/pdf/2206.13076v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08605v1","updated":"2022-07-18T13:49:27Z","published":"2022-07-18T13:49:27Z","title":"Class-incremental Novel Class Discovery","summary":"  We study the new task of class-incremental Novel Class Discovery\n(class-iNCD), which refers to the problem of discovering novel categories in an\nunlabelled data set by leveraging a pre-trained model that has been trained on\na labelled data set containing disjoint yet related categories. Apart from\ndiscovering novel classes, we also aim at preserving the ability of the model\nto recognize previously seen base categories. Inspired by rehearsal-based\nincremental learning methods, in this paper we propose a novel approach for\nclass-iNCD which prevents forgetting of past information about the base classes\nby jointly exploiting base class feature prototypes and feature-level knowledge\ndistillation. We also propose a self-training clustering strategy that\nsimultaneously clusters novel categories and trains a joint classifier for both\nthe base and novel classes. This makes our method able to operate in a\nclass-incremental setting. Our experiments, conducted on three common\nbenchmarks, demonstrate that our method significantly outperforms\nstate-of-the-art approaches. Code is available at\nhttps://github.com/OatmealLiu/class-iNCD\n","authors":["Subhankar Roy","Mingxuan Liu","Zhun Zhong","Nicu Sebe","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2207.08605v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08601v1","updated":"2022-07-18T13:46:47Z","published":"2022-07-18T13:46:47Z","title":"Geometry-Aware Reference Synthesis for Multi-View Image Super-Resolution","summary":"  Recent multi-view multimedia applications struggle between high-resolution\n(HR) visual experience and storage or bandwidth constraints. Therefore, this\npaper proposes a Multi-View Image Super-Resolution (MVISR) task. It aims to\nincrease the resolution of multi-view images captured from the same scene. One\nsolution is to apply image or video super-resolution (SR) methods to\nreconstruct HR results from the low-resolution (LR) input view. However, these\nmethods cannot handle large-angle transformations between views and leverage\ninformation in all multi-view images. To address these problems, we propose the\nMVSRnet, which uses geometry information to extract sharp details from all LR\nmulti-view to support the SR of the LR input view. Specifically, the proposed\nGeometry-Aware Reference Synthesis module in MVSRnet uses geometry information\nand all multi-view LR images to synthesize pixel-aligned HR reference images.\nThen, the proposed Dynamic High-Frequency Search network fully exploits the\nhigh-frequency textural details in reference images for SR. Extensive\nexperiments on several benchmarks show that our method significantly improves\nover the state-of-the-art approaches.\n","authors":["Ri Cheng","Yuqi Sun","Bo Yan","Weimin Tan","Chenxi Ma"],"pdf_url":"https://arxiv.org/pdf/2207.08601v1.pdf","comment":"16 pages, 10 figures, ACM MULTIMEDIA 2022"},{"id":"http://arxiv.org/abs/2207.05315v2","updated":"2022-07-18T13:42:54Z","published":"2022-07-12T04:53:24Z","title":"CANF-VC: Conditional Augmented Normalizing Flows for Video Compression","summary":"  This paper presents an end-to-end learning-based video compression system,\ntermed CANF-VC, based on conditional augmented normalizing flows (CANF). Most\nlearned video compression systems adopt the same hybrid-based coding\narchitecture as the traditional codecs. Recent research on conditional coding\nhas shown the sub-optimality of the hybrid-based coding and opens up\nopportunities for deep generative models to take a key role in creating new\ncoding frameworks. CANF-VC represents a new attempt that leverages the\nconditional ANF to learn a video generative model for conditional inter-frame\ncoding. We choose ANF because it is a special type of generative model, which\nincludes variational autoencoder as a special case and is able to achieve\nbetter expressiveness. CANF-VC also extends the idea of conditional coding to\nmotion coding, forming a purely conditional coding framework. Extensive\nexperimental results on commonly used datasets confirm the superiority of\nCANF-VC to the state-of-the-art methods. The source code of CANF-VC is\navailable at https://github.com/NYCU-MAPL/CANF-VC.\n","authors":["Yung-Han Ho","Chih-Peng Chang","Peng-Yu Chen","Alessandro Gnutti","Wen-Hsiao Peng"],"pdf_url":"https://arxiv.org/pdf/2207.05315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08592v1","updated":"2022-07-18T13:32:20Z","published":"2022-07-18T13:32:20Z","title":"Symmetrized Robust Procrustes: Constant-Factor Approximation and Exact\n  Recovery","summary":"  The classical $\\textit{Procrustes}$ problem is to find a rigid motion\n(orthogonal transformation and translation) that best aligns two given\npoint-sets in the least-squares sense. The $\\textit{Robust Procrustes}$ problem\nis an important variant, in which a power-1 objective is used instead of least\nsquares to improve robustness to outliers. While the optimal solution of the\nleast-squares problem can be easily computed in closed form, dating back to\nSch\\\"onemann (1966), no such solution is known for the power-1 problem. In this\npaper we propose a novel convex relaxation for the Robust Procrustes problem.\nOur relaxation enjoys several theoretical and practical advantages:\nTheoretically, we prove that our method provides a $\\sqrt{2}$-factor\napproximation to the Robust Procrustes problem, and that, under appropriate\nassumptions, it exactly recovers the true rigid motion from point\ncorrespondences contaminated by outliers. In practice, we find in numerical\nexperiments on both synthetic and real robust Procrustes problems, that our\nmethod performs similarly to the standard Iteratively Reweighted Least Squares\n(IRLS). However the convexity of our algorithm allows incorporating additional\nconvex penalties, which are not readily amenable to IRLS. This turns out to be\na substantial advantage, leading to improved results in high-dimensional\nproblems, including non-rigid shape alignment and semi-supervised interlingual\nword translation.\n","authors":["Tal Amir","Shahar Kovalsky","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2207.08592v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2207.08591v1","updated":"2022-07-18T13:31:26Z","published":"2022-07-18T13:31:26Z","title":"The Brain-Inspired Decoder for Natural Visual Image Reconstruction","summary":"  Decoding images from brain activity has been a challenge. Owing to the\ndevelopment of deep learning, there are available tools to solve this problem.\nThe decoded image, which aims to map neural spike trains to low-level visual\nfeatures and high-level semantic information space. Recently, there are a few\nstudies of decoding from spike trains, however, these studies pay less\nattention to the foundations of neuroscience and there are few studies that\nmerged receptive field into visual image reconstruction. In this paper, we\npropose a deep learning neural network architecture with biological properties\nto reconstruct visual image from spike trains. As far as we know, we\nimplemented a method that integrated receptive field property matrix into loss\nfunction at the first time. Our model is an end-to-end decoder from neural\nspike trains to images. We not only merged Gabor filter into auto-encoder which\nused to generate images but also proposed a loss function with receptive field\nproperties. We evaluated our decoder on two datasets which contain macaque\nprimary visual cortex neural spikes and salamander retina ganglion cells (RGCs)\nspikes. Our results show that our method can effectively combine receptive\nfield features to reconstruct images, providing a new approach to visual\nreconstruction based on neural information.\n","authors":["Wenyi Li","Shengjie Zheng","Yufan Liao","Rongqi Hong","Weiliang Chen","Chenggnag He","Xiaojian Li"],"pdf_url":"https://arxiv.org/pdf/2207.08591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08581v1","updated":"2022-07-18T13:18:34Z","published":"2022-07-18T13:18:34Z","title":"Study of the performance and scalablity of federated learning for\n  medical imaging with intermittent clients","summary":"  Federated learning is a data decentralization privacy-preserving technique\nused to perform machine or deep learning in a secure way. In this paper we\npresent theoretical aspects about federated learning, such as the presentation\nof an aggregation operator, different types of federated learning, and issues\nto be taken into account in relation to the distribution of data from the\nclients, together with the exhaustive analysis of a use case where the number\nof clients varies. Specifically, a use case of medical image analysis is\nproposed, using chest X-ray images obtained from an open data repository. In\naddition to the advantages related to privacy, improvements in predictions (in\nterms of accuracy and area under the curve) and reduction of execution times\nwill be studied with respect to the classical case (the centralized approach).\nDifferent clients will be simulated from the training data, selected in an\nunbalanced manner, i.e., they do not all have the same number of data. The\nresults of considering three or ten clients are exposed and compared between\nthem and against the centralized case. Two approaches to follow will be\nanalyzed in the case of intermittent clients, as in a real scenario some\nclients may leave the training, and some new ones may enter the training. The\nevolution of the results for the test set in terms of accuracy, area under the\ncurve and execution time is shown as the number of clients into which the\noriginal data is divided increases. Finally, improvements and future work in\nthe field are proposed.\n","authors":["Judith Sáinz-Pardo Díaz","Álvaro López García"],"pdf_url":"https://arxiv.org/pdf/2207.08581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09686v3","updated":"2022-07-18T12:55:00Z","published":"2021-12-17T18:57:54Z","title":"Efficient Visual Tracking with Exemplar Transformers","summary":"  The design of more complex and powerful neural network models has\nsignificantly advanced the state-of-the-art in visual object tracking. These\nadvances can be attributed to deeper networks, or the introduction of new\nbuilding blocks, such as transformers. However, in the pursuit of increased\ntracking performance, runtime is often hindered. Furthermore, efficient\ntracking architectures have received surprisingly little attention. In this\npaper, we introduce the Exemplar Transformer, a transformer module utilizing a\nsingle instance level attention layer for realtime visual object tracking.\nE.T.Track, our visual tracker that incorporates Exemplar Transformer modules,\nruns at 47 FPS on a CPU. This is up to 8x faster than other transformer-based\nmodels. When compared to lightweight trackers that can operate in realtime on\nstandard CPUs, E.T.Track consistently outperforms all other methods on the\nLaSOT, OTB-100, NFS, TrackingNet, and VOT-ST2020 datasets. The code will be\nmade publicly available upon publication.\n","authors":["Philippe Blatter","Menelaos Kanakis","Martin Danelljan","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2112.09686v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08569v1","updated":"2022-07-18T12:53:53Z","published":"2022-07-18T12:53:53Z","title":"Multi-manifold Attention for Vision Transformers","summary":"  Vision Transformer are very popular nowadays due to their state-of-the-art\nperformance in several computer vision tasks, such as image classification and\naction recognition. Although the performance of Vision Transformers have been\ngreatly improved by employing Convolutional Neural Networks, hierarchical\nstructures and compact forms, there is limited research on ways to utilize\nadditional data representations to refine the attention map derived from the\nmulti-head attention of a Transformer network. This work proposes a novel\nattention mechanism, called multi-manifold attention, that can substitute any\nstandard attention mechanism in a Transformer-based network. The proposed\nattention models the input space in three distinct manifolds, namely Euclidean,\nSymmetric Positive Definite and Grassmann, with different statistical and\ngeometrical properties, guiding the network to take into consideration a rich\nset of information that describe the appearance, color and texture of an image,\nfor the computation of a highly descriptive attention map. In this way, a\nVision Transformer with the proposed attention is guided to become more\nattentive towards discriminative features, leading to improved classification\nresults, as shown by the experimental results on several well-known image\nclassification datasets.\n","authors":["Dimitrios Konstantinidis","Ilias Papastratis","Kosmas Dimitropoulos","Petros Daras"],"pdf_url":"https://arxiv.org/pdf/2207.08569v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2202.07570v3","updated":"2022-07-18T12:47:39Z","published":"2022-02-15T16:55:09Z","title":"ScoreNet: Learning Non-Uniform Attention and Augmentation for\n  Transformer-Based Histopathological Image Classification","summary":"  Progress in digital pathology is hindered by high-resolution images and the\nprohibitive cost of exhaustive localized annotations. The commonly used\nparadigm to categorize pathology images is patch-based processing, which often\nincorporates multiple instance learning (MIL) to aggregate local patch-level\nrepresentations yielding image-level prediction. Nonetheless, diagnostically\nrelevant regions may only take a small fraction of the whole tissue, and\ncurrent MIL-based approaches often process images uniformly, discarding the\ninter-patches interactions. To alleviate these issues, we propose ScoreNet, a\nnew efficient transformer that exploits a differentiable recommendation stage\nto extract discriminative image regions and dedicate computational resources\naccordingly. The proposed transformer leverages the local and global attention\nof a few dynamically recommended high-resolution regions at an efficient\ncomputational cost. We further introduce a novel mixing data-augmentation,\nnamely ScoreMix, by leveraging the image's semantic distribution to guide the\ndata mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly\nsimple and mitigates the pitfalls of previous augmentations, which assume a\nuniform semantic distribution and risk mislabeling the samples. Thorough\nexperiments and ablation studies on three breast cancer histology datasets of\nHaematoxylin & Eosin (H&E) have validated the superiority of our approach over\nprior arts, including transformer-based models on tumour regions-of-interest\n(TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation\ndemonstrates better generalization capabilities and achieves new\nstate-of-the-art (SOTA) results with only 50% of the data compared to other\nmixing augmentation variants. Finally, ScoreNet yields high efficacy and\noutperforms SOTA efficient transformers, namely TransPath and SwinTransformer.\n","authors":["Thomas Stegmüller","Behzad Bozorgtabar","Antoine Spahr","Jean-Philippe Thiran"],"pdf_url":"https://arxiv.org/pdf/2202.07570v3.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2207.08560v1","updated":"2022-07-18T12:40:48Z","published":"2022-07-18T12:40:48Z","title":"Latency-Aware Collaborative Perception","summary":"  Collaborative perception has recently shown great potential to improve\nperception capabilities over single-agent perception. Existing collaborative\nperception methods usually consider an ideal communication environment.\nHowever, in practice, the communication system inevitably suffers from latency\nissues, causing potential performance degradation and high risks in\nsafety-critical applications, such as autonomous driving. To mitigate the\neffect caused by the inevitable communication latency, from a machine learning\nperspective, we present the first latency-aware collaborative perception\nsystem, which actively adopts asynchronous perceptual features from multiple\nagents to the same timestamp, promoting the robustness and effectiveness of\ncollaboration. To achieve such a feature-level synchronization, we propose a\nnovel latency compensation module, calledSyncNet, which leverages\nfeature-attention symbiotic estimation and time modulation techniques.\nExperimental results show that our method outperforms the state-of-the-art\ncollaborative perception method by 15.6% on the latest collaborative perception\ndataset V2X-SIM.\n","authors":["Zixing Lei","Shunli Ren","Yue Hu","Wenjun Zhang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08560v1.pdf","comment":"14 pages, 11 figures, Accepted by European conference on computer\n  vision, 2022"},{"id":"http://arxiv.org/abs/2202.12613v3","updated":"2022-07-18T12:27:59Z","published":"2022-02-25T11:03:31Z","title":"LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View\n  Cameras with Negative Plane","summary":"  Visual-inertial-odometry has attracted extensive attention in the field of\nautonomous driving and robotics. The size of Field of View (FoV) plays an\nimportant role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a\nlarge FoV enables to perceive a wide range of surrounding scene elements and\nfeatures. However, when the field of the camera reaches the negative half\nplane, one cannot simply use [u,v,1]^T to represent the image feature points\nanymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for\ncameras with extremely large FoV. We leverage a three-dimensional vector with\nunit length to represent feature points, and design a series of algorithms to\novercome this challenge. To address the scarcity of panoramic visual odometry\ndatasets with ground-truth location and pose, we present the PALVIO dataset,\ncollected with a Panoramic Annular Lens (PAL) system with an entire FoV of\n360{\\deg}x(40{\\deg}-120{\\deg}) and an IMU sensor. With a comprehensive variety\nof experiments, the proposed LF-VIO is verified on both the established PALVIO\nbenchmark and a public fisheye camera dataset with a FoV of\n360{\\deg}x(0{\\deg}-93.5{\\deg}). LF-VIO outperforms state-of-the-art\nvisual-inertial-odometry methods. Our dataset and code are made publicly\navailable at https://github.com/flysoaryun/LF-VIO\n","authors":["Ze Wang","Kailun Yang","Hao Shi","Peng Li","Fei Gao","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2202.12613v3.pdf","comment":"Accepted to IROS 2022. Dataset and code are publicly available at\n  https://github.com/flysoaryun/LF-VIO"},{"id":"http://arxiv.org/abs/2110.15163v6","updated":"2022-07-18T12:23:51Z","published":"2021-10-28T14:39:35Z","title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes","summary":"  Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n","authors":["Axel Durbet","Pascal Lafourcade","Denis Migdal","Kevin Thiry-Atighehchi","Paul-Marie Grollemund"],"pdf_url":"https://arxiv.org/pdf/2110.15163v6.pdf","comment":"arXiv admin note: text overlap with arXiv:1910.01389 by other authors"},{"id":"http://arxiv.org/abs/2204.04236v2","updated":"2022-07-18T12:17:00Z","published":"2022-04-08T18:03:39Z","title":"ChildCI Framework: Analysis of Motor and Cognitive Development in\n  Children-Computer Interaction for Age Detection","summary":"  This article presents a comprehensive analysis of the different tests\nproposed in the recent ChildCI framework, proving its potential for generating\na better understanding of children's neuromotor and cognitive development along\ntime, as well as their possible application in other research areas such as\ne-Health and e-Learning. In particular, we propose a set of over 100 global\nfeatures related to motor and cognitive aspects of the children interaction\nwith mobile devices, some of them collected and adapted from the literature.\nFurthermore, we analyse the robustness and discriminative power of the proposed\nfeature set including experimental results for the task of children age group\ndetection based on their motor and cognitive behaviors. Two different scenarios\nare considered in this study: i) single-test scenario, and ii) multiple-test\nscenario. Results over 93% accuracy are achieved using the publicly available\nChildCIdb_v1 database (over 400 children from 18 months to 8 years old),\nproving the high correlation of children's age with the way they interact with\nmobile devices.\n","authors":["Juan Carlos Ruiz-Garcia","Ruben Tolosana","Ruben Vera-Rodriguez","Julian Fierrez","Jaime Herreros-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2204.04236v2.pdf","comment":"11 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2207.08549v1","updated":"2022-07-18T12:12:42Z","published":"2022-07-18T12:12:42Z","title":"Dense Cross-Query-and-Support Attention Weighted Mask Aggregation for\n  Few-Shot Segmentation","summary":"  Research into Few-shot Semantic Segmentation (FSS) has attracted great\nattention, with the goal to segment target objects in a query image given only\na few annotated support images of the target class. A key to this challenging\ntask is to fully utilize the information in the support images by exploiting\nfine-grained correlations between the query and support images. However, most\nexisting approaches either compressed the support information into a few\nclass-wise prototypes, or used partial support information (e.g., only\nforeground) at the pixel level, causing non-negligible information loss. In\nthis paper, we propose Dense pixel-wise Cross-query-and-support Attention\nweighted Mask Aggregation (DCAMA), where both foreground and background support\ninformation are fully exploited via multi-level pixel-wise correlations between\npaired query and support features. Implemented with the scaled dot-product\nattention in the Transformer architecture, DCAMA treats every query pixel as a\ntoken, computes its similarities with all support pixels, and predicts its\nsegmentation label as an additive aggregation of all the support pixels' labels\n-- weighted by the similarities. Based on the unique formulation of DCAMA, we\nfurther propose efficient and effective one-pass inference for n-shot\nsegmentation, where pixels of all support images are collected for the mask\naggregation at once. Experiments show that our DCAMA significantly advances the\nstate of the art on standard FSS benchmarks of PASCAL-5i, COCO-20i, and\nFSS-1000, e.g., with 3.1%, 9.7%, and 3.6% absolute improvements in 1-shot mIoU\nover previous best records. Ablative studies also verify the design DCAMA.\n","authors":["Xinyu Shi","Dong Wei","Yu Zhang","Donghuan Lu","Munan Ning","Jiashun Chen","Kai Ma","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2207.08549v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08547v1","updated":"2022-07-18T12:12:21Z","published":"2022-07-18T12:12:21Z","title":"Few-shot Fine-grained Image Classification via Multi-Frequency\n  Neighborhood and Double-cross Modulation","summary":"  Traditional fine-grained image classification typically relies on large-scale\ntraining samples with annotated ground-truth. However, some sub-categories may\nhave few available samples in real-world applications. In this paper, we\npropose a novel few-shot fine-grained image classification network (FicNet)\nusing multi-frequency Neighborhood (MFN) and double-cross modulation (DCM).\nModule MFN is adopted to capture the information in spatial domain and\nfrequency domain. Then, the self-similarity and multi-frequency components are\nextracted to produce multi-frequency structural representation. DCM employs\nbi-crisscross component and double 3D cross-attention components to modulate\nthe embedding process by considering global context information and subtle\nrelationship between categories, respectively. The comprehensive experiments on\nthree fine-grained benchmark datasets for two few-shot tasks verify that FicNet\nhas excellent performance compared to the state-of-the-art methods. Especially,\nthe experiments on two datasets, \"Caltech-UCSD Birds\" and \"Stanford Cars\", can\nobtain classification accuracy 93.17\\% and 95.36\\%, respectively. They are even\nhigher than that the general fine-grained image classification methods can\nachieve.\n","authors":["Hegui Zhu","Zhan Gao","Jiayi Wang","Yange Zhou","Chengqing Li"],"pdf_url":"https://arxiv.org/pdf/2207.08547v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2207.08536v1","updated":"2022-07-18T11:59:10Z","published":"2022-07-18T11:59:10Z","title":"UniFormer: Unified Multi-view Fusion Transformer for Spatial-Temporal\n  Representation in Bird's-Eye-View","summary":"  Bird's eye view (BEV) representation is a new perception formulation for\nautonomous driving, which is based on spatial fusion. Further, temporal fusion\nis also introduced in BEV representation and gains great success. In this work,\nwe propose a new method that unifies both spatial and temporal fusion and\nmerges them into a unified mathematical formulation. The unified fusion could\nnot only provide a new perspective on BEV fusion but also brings new\ncapabilities. With the proposed unified spatial-temporal fusion, our method\ncould support long-range fusion, which is hard to achieve in conventional BEV\nmethods. Moreover, the BEV fusion in our work is temporal-adaptive, and the\nweights of temporal fusion are learnable. In contrast, conventional methods\nmainly use fixed and equal weights for temporal fusion. Besides, the proposed\nunified fusion could avoid information lost in conventional BEV fusion methods\nand make full use of features. Extensive experiments and ablation studies on\nthe NuScenes dataset show the effectiveness of the proposed method and our\nmethod gains the state-of-the-art performance in the map segmentation task.\n","authors":["Zequn Qin","Jingyu Chen","Chao Chen","Xiaozhi Chen","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2207.08536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12924v2","updated":"2022-07-18T11:58:27Z","published":"2021-11-25T05:52:30Z","title":"Joint stereo 3D object detection and implicit surface reconstruction","summary":"  We present the first learning-based framework for category-level 3D object\ndetection and implicit shape estimation based on a pair of stereo RGB images in\nthe wild. Previous stereo 3D object detection approaches cannot describe the\ncomplete shape details of the detected objects and often fails for the small\nobjects. In contrast, we propose a new progressive approach that can (1)\nperform precise localization as well as provide a complete and\nresolution-agnostic shape description for the detected objects and (2) produce\nsignificantly more accurate orientation predictions for the tiny instances.\nThis approach features a new instance-level network that explicitly models the\nunseen surface hallucination problem using point-based representations and uses\na new geometric representation for orientation refinement. Extensive\nexperiments show that our approach achieves state-of-the-art performance using\nvarious metrics on the KITTI benchmark. Code and pre-trained models will be\navailable at this https URL.\n","authors":["Shichao Li","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2111.12924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08531v1","updated":"2022-07-18T11:49:18Z","published":"2022-07-18T11:49:18Z","title":"DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection","summary":"  Monocular 3D detection has drawn much attention from the community due to its\nlow cost and setup simplicity. It takes an RGB image as input and predicts 3D\nboxes in the 3D space. The most challenging sub-task lies in the instance depth\nestimation. Previous works usually use a direct estimation method. However, in\nthis paper we point out that the instance depth on the RGB image is\nnon-intuitive. It is coupled by visual depth clues and instance attribute\nclues, making it hard to be directly learned in the network. Therefore, we\npropose to reformulate the instance depth to the combination of the instance\nvisual surface depth (visual depth) and the instance attribute depth (attribute\ndepth). The visual depth is related to objects' appearances and positions on\nthe image. By contrast, the attribute depth relies on objects' inherent\nattributes, which are invariant to the object affine transformation on the\nimage. Correspondingly, we decouple the 3D location uncertainty into visual\ndepth uncertainty and attribute depth uncertainty. By combining different types\nof depths and associated uncertainties, we can obtain the final instance depth.\nFurthermore, data augmentation in monocular 3D detection is usually limited due\nto the physical nature, hindering the boost of performance. Based on the\nproposed instance depth disentanglement strategy, we can alleviate this\nproblem. Evaluated on KITTI, our method achieves new state-of-the-art results,\nand extensive ablation studies validate the effectiveness of each component in\nour method. The codes are released at https://github.com/SPengLiang/DID-M3D.\n","authors":["Liang Peng","Xiaopei Wu","Zheng Yang","Haifeng Liu","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2207.08531v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08525v1","updated":"2022-07-18T11:42:51Z","published":"2022-07-18T11:42:51Z","title":"Angular Gap: Reducing the Uncertainty of Image Difficulty through Model\n  Calibration","summary":"  Curriculum learning needs example difficulty to proceed from easy to hard.\nHowever, the credibility of image difficulty is rarely investigated, which can\nseriously affect the effectiveness of curricula. In this work, we propose\nAngular Gap, a measure of difficulty based on the difference in angular\ndistance between feature embeddings and class-weight embeddings built by\nhyperspherical learning. To ascertain difficulty estimation, we introduce\nclass-wise model calibration, as a post-training technique, to the learnt\nhyperbolic space. This bridges the gap between probabilistic model calibration\nand angular distance estimation of hyperspherical learning. We show the\nsuperiority of our calibrated Angular Gap over recent difficulty metrics on\nCIFAR10-H and ImageNetV2. We further propose Angular Gap based curriculum\nlearning for unsupervised domain adaptation that can translate from learning\neasy samples to mining hard samples. We combine this curriculum with a\nstate-of-the-art self-training method, Cycle Self Training (CST). The proposed\nCurricular CST learns robust representations and outperforms recent baselines\non Office31 and VisDA 2017.\n","authors":["Bohua Peng","Mobarakol Islam","Mei Tu"],"pdf_url":"https://arxiv.org/pdf/2207.08525v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2207.07253v2","updated":"2022-07-18T11:38:34Z","published":"2022-07-15T01:59:14Z","title":"Decoupling Recognition from Detection: Single Shot Self-Reliant Scene\n  Text Spotter","summary":"  Typical text spotters follow the two-stage spotting strategy: detect the\nprecise boundary for a text instance first and then perform text recognition\nwithin the located text region. While such strategy has achieved substantial\nprogress, there are two underlying limitations. 1) The performance of text\nrecognition depends heavily on the precision of text detection, resulting in\nthe potential error propagation from detection to recognition. 2) The RoI\ncropping which bridges the detection and recognition brings noise from\nbackground and leads to information loss when pooling or interpolating from\nfeature maps. In this work we propose the single shot Self-Reliant Scene Text\nSpotter (SRSTS), which circumvents these limitations by decoupling recognition\nfrom detection. Specifically, we conduct text detection and recognition in\nparallel and bridge them by the shared positive anchor point. Consequently, our\nmethod is able to recognize the text instances correctly even though the\nprecise text boundaries are challenging to detect. Additionally, our method\nreduces the annotation cost for text detection substantially. Extensive\nexperiments on regular-shaped benchmark and arbitrary-shaped benchmark\ndemonstrate that our SRSTS compares favorably to previous state-of-the-art\nspotters in terms of both accuracy and efficiency.\n","authors":["Jingjing Wu","Pengyuan Lyu","Guangming Lu","Chengquan Zhang","Kun Yao","Wenjie Pei"],"pdf_url":"https://arxiv.org/pdf/2207.07253v2.pdf","comment":"To be appeared in the Proceedings of the ACM International Conference\n  on Multimedia (ACM MM), 2022"},{"id":"http://arxiv.org/abs/2207.08518v1","updated":"2022-07-18T11:30:06Z","published":"2022-07-18T11:30:06Z","title":"HiFormer: Hierarchical Multi-scale Representations Using Transformers\n  for Medical Image Segmentation","summary":"  Convolutional neural networks (CNNs) have been the consensus for medical\nimage segmentation tasks. However, they suffer from the limitation in modeling\nlong-range dependencies and spatial correlations due to the nature of\nconvolution operation. Although transformers were first developed to address\nthis issue, they fail to capture low-level features. In contrast, it is\ndemonstrated that both local and global features are crucial for dense\nprediction, such as segmenting in challenging contexts. In this paper, we\npropose HiFormer, a novel method that efficiently bridges a CNN and a\ntransformer for medical image segmentation. Specifically, we design two\nmulti-scale feature representations using the seminal Swin Transformer module\nand a CNN-based encoder. To secure a fine fusion of global and local features\nobtained from the two aforementioned representations, we propose a Double-Level\nFusion (DLF) module in the skip connection of the encoder-decoder structure.\nExtensive experiments on various medical image segmentation datasets\ndemonstrate the effectiveness of HiFormer over other CNN-based,\ntransformer-based, and hybrid methods in terms of computational complexity, and\nquantitative and qualitative results. Our code is publicly available at:\nhttps://github.com/amirhossein-kz/HiFormer\n","authors":["Moein Heidari","Amirhossein Kazerouni","Milad Soltany","Reza Azad","Ehsan Khodapanah Aghdam","Julien Cohen-Adad","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2207.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.07414v3","updated":"2022-07-18T11:17:26Z","published":"2021-12-14T14:07:16Z","title":"Marine Bubble Flow Quantification Using Wide-Baseline Stereo\n  Photogrammetry","summary":"  Reliable quantification of natural and anthropogenic gas release (e.g.\\\nCO$_2$, methane) from the seafloor into the water column, and potentially to\nthe atmosphere, is a challenging task. While ship-based echo sounders such as\nsingle beam and multibeam systems allow detection of free gas, bubbles, in the\nwater even from a great distance, exact quantification utilizing the\nhydroacoustic data requires additional parameters such as rise speed and bubble\nsize distribution. Optical methods are complementary in the sense that they can\nprovide high temporal and spatial resolution of single bubbles or bubble\nstreams from close distance. In this contribution we introduce a complete\ninstrument and evaluation method for optical bubble stream characterization\ntargeted at flows of up to 100ml/min and bubbles with a few millimeters radius.\nThe dedicated instrument employs a high-speed deep sea capable stereo camera\nsystem that can record terabytes of bubble imagery when deployed at a seep site\nfor later automated analysis. Bubble characteristics can be obtained for short\nsequences, then relocating the instrument to other locations, or in autonomous\nmode of definable intervals up to several days, in order to capture bubble flow\nvariations due to e.g. tide dependent pressure changes or reservoir depletion.\nBeside reporting the steps to make bubble characterization robust and\nautonomous, we carefully evaluate the reachable accuracy to be in the range of\n1-2\\% of the bubble radius and propose a novel auto-calibration procedure that,\ndue to the lack of point correspondences, uses only the silhouettes of bubbles.\nThe system has been operated successfully in 1000m water depth at the Cascadia\nmargin offshore Oregon to assess methane fluxes from various seep locations.\nBesides sample results we also report failure cases and lessons learnt during\ndeployment and method development.\n","authors":["Mengkun She","Tim Weiß","Yifan Song","Peter Urban","Jens Greinert","Kevin Köser"],"pdf_url":"https://arxiv.org/pdf/2112.07414v3.pdf","comment":"56 pages, 26 figures"},{"id":"http://arxiv.org/abs/2111.11976v2","updated":"2022-07-18T10:54:54Z","published":"2021-11-23T16:10:06Z","title":"KTNet: Knowledge Transfer for Unpaired 3D Shape Completion","summary":"  Unpaired 3D object completion aims to predict a complete 3D shape from an\nincomplete input without knowing the correspondence between the complete and\nincomplete shapes. In this paper, we propose the novel KTNet to solve this task\nfrom the new perspective of knowledge transfer. KTNet elaborates a\nteacher-assistant-student network to establish multiple knowledge transfer\nprocesses. Specifically, the teacher network takes complete shape as input and\nlearns the knowledge of complete shape. The student network takes the\nincomplete one as input and restores the corresponding complete shape. And the\nassistant modules not only help to transfer the knowledge of complete shape\nfrom the teacher to the student, but also judge the learning effect of the\nstudent network. As a result, KTNet makes use of a more comprehensive\nunderstanding to establish the geometric correspondence between complete and\nincomplete shapes in a perspective of knowledge transfer, which enables more\ndetailed geometric inference for generating high-quality complete shapes. We\nconduct comprehensive experiments on several datasets, and the results show\nthat our method outperforms previous methods of unpaired point cloud completion\nby a large margin.\n","authors":["Zhen Cao","Wenxiao Zhang","Xin Wen","Zhen Dong","Yu-shen Liu","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2111.11976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16832v2","updated":"2022-07-18T10:42:39Z","published":"2022-03-31T06:36:07Z","title":"Point Scene Understanding via Disentangled Instance Mesh Reconstruction","summary":"  Semantic scene reconstruction from point cloud is an essential and\nchallenging task for 3D scene understanding. This task requires not only to\nrecognize each instance in the scene, but also to recover their geometries\nbased on the partial observed point cloud. Existing methods usually attempt to\ndirectly predict occupancy values of the complete object based on incomplete\npoint cloud proposals from a detection-based backbone. However, this framework\nalways fails to reconstruct high fidelity mesh due to the obstruction of\nvarious detected false positive object proposals and the ambiguity of\nincomplete point observations for learning occupancy values of complete\nobjects. To circumvent the hurdle, we propose a Disentangled Instance Mesh\nReconstruction (DIMR) framework for effective point scene understanding. A\nsegmentation-based backbone is applied to reduce false positive object\nproposals, which further benefits our exploration on the relationship between\nrecognition and reconstruction. Based on the accurate proposals, we leverage a\nmesh-aware latent code space to disentangle the processes of shape completion\nand mesh generation, relieving the ambiguity caused by the incomplete point\nobservations. Furthermore, with access to the CAD model pool at test time, our\nmodel can also be used to improve the reconstruction quality by performing mesh\nretrieval without extra training. We thoroughly evaluate the reconstructed mesh\nquality with multiple metrics, and demonstrate the superiority of our method on\nthe challenging ScanNet dataset. Code is available at\n\\url{https://github.com/ashawkey/dimr}.\n","authors":["Jiaxiang Tang","Xiaokang Chen","Jingbo Wang","Gang Zeng"],"pdf_url":"https://arxiv.org/pdf/2203.16832v2.pdf","comment":"ECCV 2022 Camera-ready version"},{"id":"http://arxiv.org/abs/2111.13353v3","updated":"2022-07-18T10:29:13Z","published":"2021-11-26T08:10:33Z","title":"Contrastive Vicinal Space for Unsupervised Domain Adaptation","summary":"  Recent unsupervised domain adaptation methods have utilized vicinal space\nbetween the source and target domains. However, the equilibrium collapse of\nlabels, a problem where the source labels are dominant over the target labels\nin the predictions of vicinal instances, has never been addressed. In this\npaper, we propose an instance-wise minimax strategy that minimizes the entropy\nof high uncertainty instances in the vicinal space to tackle the stated\nproblem. We divide the vicinal space into two subspaces through the solution of\nthe minimax problem: contrastive space and consensus space. In the contrastive\nspace, inter-domain discrepancy is mitigated by constraining instances to have\ncontrastive views and labels, and the consensus space reduces the confusion\nbetween intra-domain categories. The effectiveness of our method is\ndemonstrated on public benchmarks, including Office-31, Office-Home, and\nVisDA-C, achieving state-of-the-art performances. We further show that our\nmethod outperforms the current state-of-the-art methods on PACS, which\nindicates that our instance-wise approach works well for multi-source domain\nadaptation as well. Code is available at https://github.com/NaJaeMin92/CoVi.\n","authors":["Jaemin Na","Dongyoon Han","Hyung Jin Chang","Wonjun Hwang"],"pdf_url":"https://arxiv.org/pdf/2111.13353v3.pdf","comment":"10 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2207.08494v1","updated":"2022-07-18T10:20:23Z","published":"2022-07-18T10:20:23Z","title":"Rethinking Alignment in Video Super-Resolution Transformers","summary":"  The alignment of adjacent frames is considered an essential operation in\nvideo super-resolution (VSR). Advanced VSR models, including the latest VSR\nTransformers, are generally equipped with well-designed alignment modules.\nHowever, the progress of the self-attention mechanism may violate this common\nsense. In this paper, we rethink the role of alignment in VSR Transformers and\nmake several counter-intuitive observations. Our experiments show that: (i) VSR\nTransformers can directly utilize multi-frame information from unaligned\nvideos, and (ii) existing alignment methods are sometimes harmful to VSR\nTransformers. These observations indicate that we can further improve the\nperformance of VSR Transformers simply by removing the alignment module and\nadopting a larger attention window. Nevertheless, such designs will\ndramatically increase the computational burden, and cannot deal with large\nmotions. Therefore, we propose a new and efficient alignment method called\npatch alignment, which aligns image patches instead of pixels. VSR Transformers\nequipped with patch alignment could demonstrate state-of-the-art performance on\nmultiple benchmarks. Our work provides valuable insights on how multi-frame\ninformation is used in VSR and how to select alignment methods for different\nnetworks/datasets. Codes and models will be released at\nhttps://github.com/XPixelGroup/RethinkVSRAlignment.\n","authors":["Shuwei Shi","Jinjin Gu","Liangbin Xie","Xintao Wang","Yujiu Yang","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2207.08494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08489v1","updated":"2022-07-18T10:15:04Z","published":"2022-07-18T10:15:04Z","title":"Neural Distributed Image Compression with Cross-Attention Feature\n  Alignment","summary":"  We propose a novel deep neural network (DNN) architecture for compressing an\nimage when a correlated image is available as side information only at the\ndecoder side, a special case of the well-known and heavily studied distributed\nsource coding (DSC) problem. In particular, we consider a pair of stereo\nimages, which have overlapping fields of view, captured by a synchronized and\ncalibrated pair of cameras; and therefore, are highly correlated. We assume\nthat one image of the pair is to be compressed and transmitted, while the other\nimage is available only at the decoder. In the proposed architecture, the\nencoder maps the input image to a latent space using a DNN, quantizes the\nlatent representation, and compresses it losslessly using entropy coding. The\nproposed decoder extracts useful information common between the images solely\nfrom the available side information, as well as a latent representation of the\nside information. Then, the latent representations of the two images, one\nreceived from the encoder, the other extracted locally, along with the locally\ngenerated common information, are fed to the respective decoders of the two\nimages. We employ a cross-attention module (CAM) to align the feature maps\nobtained in the intermediate layers of the respective decoders of the two\nimages, thus allowing better utilization of the side information. We train and\ndemonstrate the effectiveness of the proposed algorithm on various realistic\nsetups, such as KITTI and Cityscape datasets of stereo image pairs. Our results\nshow that the proposed architecture is capable of exploiting the decoder-only\nside information in a more efficient manner as it outperforms previous works.\nWe also show that the proposed method is able to provide significant gains even\nin the case of uncalibrated and unsynchronized camera array use cases.\n","authors":["Nitish Mital","Ezgi Ozyilkan","Ali Garjani","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2207.08489v1.pdf","comment":"16 pages, 15 figures"},{"id":"http://arxiv.org/abs/2207.08485v1","updated":"2022-07-18T10:10:14Z","published":"2022-07-18T10:10:14Z","title":"Hierarchical Feature Alignment Network for Unsupervised Video Object\n  Segmentation","summary":"  Optical flow is an easily conceived and precious cue for advancing\nunsupervised video object segmentation (UVOS). Most of the previous methods\ndirectly extract and fuse the motion and appearance features for segmenting\ntarget objects in the UVOS setting. However, optical flow is intrinsically an\ninstantaneous velocity of all pixels among consecutive frames, thus making the\nmotion features not aligned well with the primary objects among the\ncorresponding frames. To solve the above challenge, we propose a concise,\npractical, and efficient architecture for appearance and motion feature\nalignment, dubbed hierarchical feature alignment network (HFAN). Specifically,\nthe key merits in HFAN are the sequential Feature AlignMent (FAM) module and\nthe Feature AdaptaTion (FAT) module, which are leveraged for processing the\nappearance and motion features hierarchically. FAM is capable of aligning both\nappearance and motion features with the primary object semantic\nrepresentations, respectively. Further, FAT is explicitly designed for the\nadaptive fusion of appearance and motion features to achieve a desirable\ntrade-off between cross-modal features. Extensive experiments demonstrate the\neffectiveness of the proposed HFAN, which reaches a new state-of-the-art\nperformance on DAVIS-16, achieving 88.7 $\\mathcal{J}\\&\\mathcal{F}$ Mean, i.e.,\na relative improvement of 3.5% over the best published result.\n","authors":["Gensheng Pei","Yazhou Yao","Guo-Sen Xie","Fumin Shen","Zhenmin Tang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2207.08485v1.pdf","comment":"Accepted by ECCV-2022"},{"id":"http://arxiv.org/abs/2206.00415v3","updated":"2022-07-18T10:05:50Z","published":"2022-06-01T11:33:33Z","title":"Learning Invariant Visual Representations for Compositional Zero-Shot\n  Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nusing knowledge learned from seen attribute-object compositions in the training\nset. Previous works mainly project an image and a composition into a common\nembedding space to measure their compatibility score. However, both attributes\nand objects share the visual representations learned above, leading the model\nto exploit spurious correlations and bias towards seen pairs. Instead, we\nreconsider CZSL as an out-of-distribution generalization problem. If an object\nis treated as a domain, we can learn object-invariant features to recognize the\nattributes attached to any object reliably. Similarly, attribute-invariant\nfeatures can also be learned when recognizing the objects with attributes as\ndomains. Specifically, we propose an invariant feature learning framework to\nalign different domains at the representation and gradient levels to capture\nthe intrinsic characteristics associated with the tasks. Experiments on two\nCZSL benchmarks demonstrate that the proposed method significantly outperforms\nthe previous state-of-the-art.\n","authors":["Tian Zhang","Kongming Liang","Ruoyi Du","Xian Sun","Zhanyu Ma","Jun Guo"],"pdf_url":"https://arxiv.org/pdf/2206.00415v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08467v1","updated":"2022-07-18T09:36:44Z","published":"2022-07-18T09:36:44Z","title":"Segmenting white matter hyperintensities on isotropic three-dimensional\n  Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison\n  of Deep learning tools on a Norwegian national imaging database","summary":"  Introduction Automated segmentation of white matter hyperintensities (WMHs)\nis an essential step in neuroimaging analysis of Magnetic Resonance Imaging\n(MRI). Fluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast\nthat is particularly useful to visualize and quantify WMHs, a hallmark of\ncerebral small vessel disease and Alzheimer's disease (AD). Clinical MRI\nprotocols migrate to a three-dimensional (3D) FLAIR-weighted acquisition to\nenable high spatial resolution in all three voxel dimensions. The current study\ndetails the deployment of deep learning tools to enable automated WMH\nsegmentation and characterization from 3D FLAIR-weighted images acquired as\npart of a national AD imaging initiative.\n  Materials and methods Among 642 participants (283 male, mean age: (65.18 +/-\n9.33) years) from the DDI study, two in-house networks were trained and\nvalidated across five national collection sites. Three models were tested on a\nheld-out subset of the internal data from the 642 participants and an external\ndataset with 29 cases from an international collaborator. These test sets were\nevaluated independently. Five established WMH performance metrics were used for\ncomparison against ground truth human-in-the-loop segmentation.\n  Results Of the three networks tested, the 3D nnU-Net had the best performance\nwith an average dice similarity coefficient score of 0.78 +/- 0.10, performing\nbetter than both the in-house developed 2.5D model and the SOTA Deep Bayesian\nnetwork.\n  Conclusion With the increasing use of 3D FLAIR-weighted images in MRI\nprotocols, our results suggest that WMH segmentation models can be trained on\n3D data and yield WMH segmentation performance that is comparable to or better\nthan state-of-the-art without the need for including T1-weighted image series.\n","authors":["Martin Soria Roevang","Per Selnes","Bradley John MacIntosh","Inge Rasmus Groote","Lene Paalhaugen","Carole Sudre","Tormod Fladby","Atle Bjoernerud"],"pdf_url":"https://arxiv.org/pdf/2207.08467v1.pdf","comment":"14 Pages, 7 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2207.08461v1","updated":"2022-07-18T09:29:10Z","published":"2022-07-18T09:29:10Z","title":"Multi-dimension Geospatial feature learning for urban region function\n  recognition","summary":"  Urban region function recognition plays a vital character in monitoring and\nmanaging the limited urban areas. Since urban functions are complex and full of\nsocial-economic properties, simply using remote sensing~(RS) images equipped\nwith physical and optical information cannot completely solve the\nclassification task. On the other hand, with the development of mobile\ncommunication and the internet, the acquisition of geospatial big data~(GBD)\nbecomes possible. In this paper, we propose a Multi-dimension Feature Learning\nModel~(MDFL) using high-dimensional GBD data in conjunction with RS images for\nurban region function recognition. When extracting multi-dimension features,\nour model considers the user-related information modeled by their activity, as\nwell as the region-based information abstracted from the region graph.\nFurthermore, we propose a decision fusion network that integrates the decisions\nfrom several neural networks and machine learning classifiers, and the final\ndecision is made considering both the visual cue from the RS images and the\nsocial information from the GBD data. Through quantitative evaluation, we\ndemonstrate that our model achieves overall accuracy at 92.75, outperforming\nthe state-of-the-art by 10 percent.\n","authors":["Wenjia Xu","Jiuniu Wang","Yirong Wu"],"pdf_url":"https://arxiv.org/pdf/2207.08461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07997v2","updated":"2022-07-18T09:25:33Z","published":"2022-03-15T15:29:08Z","title":"Inverted Pyramid Multi-task Transformer for Dense Scene Understanding","summary":"  Multi-task dense scene understanding is a thriving research domain that\nrequires simultaneous perception and reasoning on a series of correlated tasks\nwith pixel-wise prediction. Most existing works encounter a severe limitation\nof modeling in the locality due to heavy utilization of convolution operations,\nwhile learning interactions and inference in a global spatial-position and\nmulti-task context is critical for this problem. In this paper, we propose a\nnovel end-to-end Inverted Pyramid multi-task Transformer (InvPT) to perform\nsimultaneous modeling of spatial positions and multiple tasks in a unified\nframework. To the best of our knowledge, this is the first work that explores\ndesigning a transformer structure for multi-task dense prediction for scene\nunderstanding. Besides, it is widely demonstrated that a higher spatial\nresolution is remarkably beneficial for dense predictions, while it is very\nchallenging for existing transformers to go deeper with higher resolutions due\nto huge complexity to large spatial size. InvPT presents an efficient\nUP-Transformer block to learn multi-task feature interaction at gradually\nincreased resolutions, which also incorporates effective self-attention message\npassing and multi-scale feature aggregation to produce task-specific prediction\nat a high resolution. Our method achieves superior multi-task performance on\nNYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms\nprevious state-of-the-arts. The code is available at\nhttps://github.com/prismformore/InvPT\n","authors":["Hanrong Ye","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2203.07997v2.pdf","comment":"To appear in ECCV 2022 Conference. Code is available at\n  https://github.com/prismformore/InvPT"},{"id":"http://arxiv.org/abs/2204.13322v2","updated":"2022-07-18T09:22:00Z","published":"2022-04-28T07:43:52Z","title":"Two Decades of Colorization and Decolorization for Images and Videos","summary":"  Colorization is a computer-aided process, which aims to give color to a gray\nimage or video. It can be used to enhance black-and-white images, including\nblack-and-white photos, old-fashioned films, and scientific imaging results. On\nthe contrary, decolorization is to convert a color image or video into a\ngrayscale one. A grayscale image or video refers to an image or video with only\nbrightness information without color information. It is the basis of some\ndownstream image processing applications such as pattern recognition, image\nsegmentation, and image enhancement. Different from image decolorization, video\ndecolorization should not only consider the image contrast preservation in each\nvideo frame, but also respect the temporal and spatial consistency between\nvideo frames. Researchers were devoted to develop decolorization methods by\nbalancing spatial-temporal consistency and algorithm efficiency. With the\nprevalance of the digital cameras and mobile phones, image and video\ncolorization and decolorization have been paid more and more attention by\nresearchers. This paper gives an overview of the progress of image and video\ncolorization and decolorization methods in the last two decades.\n","authors":["Shiguang Liu"],"pdf_url":"https://arxiv.org/pdf/2204.13322v2.pdf","comment":"12 pages, 19 figures"},{"id":"http://arxiv.org/abs/2207.08455v1","updated":"2022-07-18T09:20:04Z","published":"2022-07-18T09:20:04Z","title":"Open-world Semantic Segmentation via Contrasting and Clustering\n  Vision-Language Embedding","summary":"  To bridge the gap between supervised semantic segmentation and real-world\napplications that acquires one model to recognize arbitrary new concepts,\nrecent zero-shot segmentation attracts a lot of attention by exploring the\nrelationships between unseen and seen object categories, yet requiring large\namounts of densely-annotated data with diverse base classes. In this paper, we\npropose a new open-world semantic segmentation pipeline that makes the first\nattempt to learn to segment semantic objects of various open-world categories\nwithout any efforts on dense annotations, by purely exploiting the\nimage-caption data that naturally exist on the Internet. Our method,\nVision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a\ntext encoder to generate visual and text embeddings for the image-caption data,\nwith two core components that endow its segmentation ability: First, the image\nencoder is jointly trained with a vision-based contrasting and a cross-modal\ncontrasting, which encourage the visual embeddings to preserve both\nfine-grained semantics and high-level category information that are crucial for\nthe segmentation task. Furthermore, an online clustering head is devised over\nthe image encoder, which allows to dynamically segment the visual embeddings\ninto distinct semantic groups such that they can be classified by comparing\nwith various text embeddings to complete our segmentation pipeline. Experiments\nshow that without using any data with dense annotations, our method can\ndirectly segment objects of arbitrary categories, outperforming zero-shot\nsegmentation methods that require data labeling on three benchmark datasets.\n","authors":["Quande Liu","Youpeng Wen","Jianhua Han","Chunjing Xu","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2207.08455v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2112.01335v2","updated":"2022-07-18T09:17:11Z","published":"2021-12-02T15:35:10Z","title":"Semantic-Sparse Colorization Network for Deep Exemplar-based\n  Colorization","summary":"  Exemplar-based colorization approaches rely on reference image to provide\nplausible colors for target gray-scale image. The key and difficulty of\nexemplar-based colorization is to establish an accurate correspondence between\nthese two images. Previous approaches have attempted to construct such a\ncorrespondence but are faced with two obstacles. First, using luminance\nchannels for the calculation of correspondence is inaccurate. Second, the dense\ncorrespondence they built introduces wrong matching results and increases the\ncomputation burden. To address these two problems, we propose Semantic-Sparse\nColorization Network (SSCN) to transfer both the global image style and\ndetailed semantic-related colors to the gray-scale image in a coarse-to-fine\nmanner. Our network can perfectly balance the global and local colors while\nalleviating the ambiguous matching problem. Experiments show that our method\noutperforms existing methods in both quantitative and qualitative evaluation\nand achieves state-of-the-art performance.\n","authors":["Yunpeng Bai","Chao Dong","Zenghao Chai","Andong Wang","Zhengzhuo Xu","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2112.01335v2.pdf","comment":"Accepted by ECCV2022; 14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2207.05342v2","updated":"2022-07-18T09:17:10Z","published":"2022-07-12T06:51:32Z","title":"Video Graph Transformer for Video Question Answering","summary":"  This paper proposes a Video Graph Transformer (VGT) model for Video Quetion\nAnswering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic\ngraph transformer module which encodes video by explicitly capturing the visual\nobjects, their relations, and dynamics for complex spatio-temporal reasoning;\nand 2) it exploits disentangled video and text Transformers for relevance\ncomparison between the video and text to perform QA, instead of entangled\ncross-modal Transformer for answer classification. Vision-text communication is\ndone by additional cross-modal interaction modules. With more reasonable video\nencoding and QA solution, we show that VGT can achieve much better performances\non VideoQA tasks that challenge dynamic relation reasoning than prior arts in\nthe pretraining-free scenario. Its performances even surpass those models that\nare pretrained with millions of external data. We further show that VGT can\nalso benefit a lot from self-supervised cross-modal pretraining, yet with\norders of magnitude smaller data. These results clearly demonstrate the\neffectiveness and superiority of VGT, and reveal its potential for more\ndata-efficient pretraining. With comprehensive analyses and some heuristic\nobservations, we hope that VGT can promote VQA research beyond coarse\nrecognition/description towards fine-grained relation reasoning in realistic\nvideos. Our code is available at https://github.com/sail-sg/VGT.\n","authors":["Junbin Xiao","Pan Zhou","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2207.05342v2.pdf","comment":"ECCV'22"},{"id":"http://arxiv.org/abs/2204.13339v2","updated":"2022-07-18T09:11:52Z","published":"2022-04-28T08:20:54Z","title":"An Overview of Color Transfer and Style Transfer for Images and Videos","summary":"  Image or video appearance features (e.g., color, texture, tone, illumination,\nand so on) reflect one's visual perception and direct impression of an image or\nvideo. Given a source image (video) and a target image (video), the image\n(video) color transfer technique aims to process the color of the source image\nor video (note that the source image or video is also referred to the reference\nimage or video in some literature) to make it look like that of the target\nimage or video, i.e., transferring the appearance of the target image or video\nto that of the source image or video, which can thereby change one's perception\nof the source image or video. As an extension of color transfer, style transfer\nrefers to rendering the content of a target image or video in the style of an\nartist with either a style sample or a set of images through a style transfer\nmodel. As an emerging field, the study of style transfer has attracted the\nattention of a large number of researchers. After decades of development, it\nhas become a highly interdisciplinary research with a variety of artistic\nexpression styles can be achieved. This paper provides an overview of color\ntransfer and style transfer methods over the past years.\n","authors":["Shiguang Liu"],"pdf_url":"https://arxiv.org/pdf/2204.13339v2.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2204.06718v9","updated":"2022-07-18T09:06:09Z","published":"2022-04-14T03:08:40Z","title":"Learning Convolutional Neural Networks in the Frequency Domain","summary":"  Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n","authors":["Hengyue Pan","Yixin Chen","Xin Niu","Wenbo Zhou","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2204.06718v9.pdf","comment":"Submitted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2207.07316v2","updated":"2022-07-18T08:58:22Z","published":"2022-07-15T07:15:36Z","title":"Privacy-Preserving Face Recognition with Learnable Privacy Budgets in\n  Frequency Domain","summary":"  Face recognition technology has been used in many fields due to its high\nrecognition accuracy, including the face unlocking of mobile devices, community\naccess control systems, and city surveillance. As the current high accuracy is\nguaranteed by very deep network structures, facial images often need to be\ntransmitted to third-party servers with high computational power for inference.\nHowever, facial images visually reveal the user's identity information. In this\nprocess, both untrusted service providers and malicious users can significantly\nincrease the risk of a personal privacy breach. Current privacy-preserving\napproaches to face recognition are often accompanied by many side effects, such\nas a significant increase in inference time or a noticeable decrease in\nrecognition accuracy. This paper proposes a privacy-preserving face recognition\nmethod using differential privacy in the frequency domain. Due to the\nutilization of differential privacy, it offers a guarantee of privacy in\ntheory. Meanwhile, the loss of accuracy is very slight. This method first\nconverts the original image to the frequency domain and removes the direct\ncomponent termed DC. Then a privacy budget allocation method can be learned\nbased on the loss of the back-end face recognition network within the\ndifferential privacy framework. Finally, it adds the corresponding noise to the\nfrequency domain features. Our method performs very well with several classical\nface recognition test sets according to the extensive experiments.\n","authors":["Jiazhen Ji","Huan Wang","Yuge Huang","Jiaxiang Wu","Xingkun Xu","Shouhong Ding","ShengChuan Zhang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2207.07316v2.pdf","comment":"ECCV 2022; Code is available at\n  https://github.com/Tencent/TFace/tree/master/recognition/tasks/dctdp"},{"id":"http://arxiv.org/abs/2107.14204v2","updated":"2022-07-18T08:54:04Z","published":"2021-07-29T17:42:12Z","title":"Personalized Trajectory Prediction via Distribution Discrimination","summary":"  Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.\n","authors":["Guangyi Chen","Junlong Li","Nuoxing Zhou","Liangliang Ren","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2107.14204v2.pdf","comment":"Accepted to ICCV 2021. Code: https://github.com/CHENGY12/DisDis"},{"id":"http://arxiv.org/abs/2207.08445v1","updated":"2022-07-18T08:53:17Z","published":"2022-07-18T08:53:17Z","title":"Automatic universal taxonomies for multi-domain semantic segmentation","summary":"  Training semantic segmentation models on multiple datasets has sparked a lot\nof recent interest in the computer vision community. This interest has been\nmotivated by expensive annotations and a desire to achieve proficiency across\nmultiple visual domains. However, established datasets have mutually\nincompatible labels which disrupt principled inference in the wild. We address\nthis issue by automatic construction of universal taxonomies through iterative\ndataset integration. Our method detects subset-superset relationships between\ndataset-specific labels, and supports learning of sub-class logits by treating\nsuper-classes as partial labels. We present experiments on collections of\nstandard datasets and demonstrate competitive generalization performance with\nrespect to previous work.\n","authors":["Petra Bevandić","Siniša Šegvić"],"pdf_url":"https://arxiv.org/pdf/2207.08445v1.pdf","comment":"8 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2207.06124v2","updated":"2022-07-18T08:50:44Z","published":"2022-07-13T11:12:03Z","title":"DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation","summary":"  One key challenge of exemplar-guided image generation lies in establishing\nfine-grained correspondences between input and guided images. Prior approaches,\ndespite the promising results, have relied on either estimating dense attention\nto compute per-point matching, which is limited to only coarse scales due to\nthe quadratic memory cost, or fixing the number of correspondences to achieve\nlinear complexity, which lacks flexibility. In this paper, we propose a dynamic\nsparse attention based Transformer model, termed Dynamic Sparse Transformer\n(DynaST), to achieve fine-level matching with favorable efficiency. The heart\nof our approach is a novel dynamic-attention unit, dedicated to covering the\nvariation on the optimal number of tokens one position should focus on.\nSpecifically, DynaST leverages the multi-layer nature of Transformer structure,\nand performs the dynamic attention scheme in a cascaded manner to refine\nmatching results and synthesize visually-pleasing outputs. In addition, we\nintroduce a unified training objective for DynaST, making it a versatile\nreference-based image translation framework for both supervised and\nunsupervised scenarios. Extensive experiments on three applications,\npose-guided person image generation, edge-based face synthesis, and undistorted\nimage style transfer, demonstrate that DynaST achieves superior performance in\nlocal details, outperforming the state of the art while reducing the\ncomputational cost significantly. Our code is available at\nhttps://github.com/Huage001/DynaST\n","authors":["Songhua Liu","Jingwen Ye","Sucheng Ren","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2207.06124v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08439v1","updated":"2022-07-18T08:45:54Z","published":"2022-07-18T08:45:54Z","title":"Revisiting PatchMatch Multi-View Stereo for Urban 3D Reconstruction","summary":"  In this paper, a complete pipeline for image-based 3D reconstruction of urban\nscenarios is proposed, based on PatchMatch Multi-View Stereo (MVS). Input\nimages are firstly fed into an off-the-shelf visual SLAM system to extract\ncamera poses and sparse keypoints, which are used to initialize PatchMatch\noptimization. Then, pixelwise depths and normals are iteratively computed in a\nmulti-scale framework with a novel depth-normal consistency loss term and a\nglobal refinement algorithm to balance the inherently local nature of\nPatchMatch. Finally, a large-scale point cloud is generated by back-projecting\nmulti-view consistent estimates in 3D. The proposed approach is carefully\nevaluated against both classical MVS algorithms and monocular depth networks on\nthe KITTI dataset, showing state of the art performances.\n","authors":["Marco Orsingher","Paolo Zani","Paolo Medici","Massimo Bertozzi"],"pdf_url":"https://arxiv.org/pdf/2207.08439v1.pdf","comment":"Poster presentation at IEEE Intelligent Vehicles Symposium (IV 2022,\n  https://iv2022.com/)"},{"id":"http://arxiv.org/abs/2207.08434v1","updated":"2022-07-18T08:33:52Z","published":"2022-07-18T08:33:52Z","title":"Efficient View Clustering and Selection for City-Scale 3D Reconstruction","summary":"  Image datasets have been steadily growing in size, harming the feasibility\nand efficiency of large-scale 3D reconstruction methods. In this paper, a novel\napproach for scaling Multi-View Stereo (MVS) algorithms up to arbitrarily large\ncollections of images is proposed. Specifically, the problem of reconstructing\nthe 3D model of an entire city is targeted, starting from a set of videos\nacquired by a moving vehicle equipped with several high-resolution cameras.\nInitially, the presented method exploits an approximately uniform distribution\nof poses and geometry and builds a set of overlapping clusters. Then, an\nInteger Linear Programming (ILP) problem is formulated for each cluster to\nselect an optimal subset of views that guarantees both visibility and\nmatchability. Finally, local point clouds for each cluster are separately\ncomputed and merged. Since clustering is independent from pairwise visibility\ninformation, the proposed algorithm runs faster than existing literature and\nallows for a massive parallelization. Extensive testing on urban data are\ndiscussed to show the effectiveness and the scalability of this approach.\n","authors":["Marco Orsingher","Paolo Zani","Paolo Medici","Massimo Bertozzi"],"pdf_url":"https://arxiv.org/pdf/2207.08434v1.pdf","comment":"Oral presentation at ICIAP 2021 (https://www.iciap2021.org/)"},{"id":"http://arxiv.org/abs/2202.06484v4","updated":"2022-07-18T08:33:16Z","published":"2022-02-14T05:17:38Z","title":"D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic\n  Segmentation","summary":"  In the field of domain adaptation, a trade-off exists between the model\nperformance and the number of target domain annotations. Active learning,\nmaximizing model performance with few informative labeled data, comes in handy\nfor such a scenario. In this work, we present D2ADA, a general active domain\nadaptation framework for semantic segmentation. To adapt the model to the\ntarget domain with minimum queried labels, we propose acquiring labels of the\nsamples with high probability density in the target domain yet with low\nprobability density in the source domain, complementary to the existing source\ndomain labeled data. To further facilitate labeling efficiency, we design a\ndynamic scheduling policy to adjust the labeling budgets between domain\nexploration and model uncertainty over time. Extensive experiments show that\nour method outperforms existing active learning and domain adaptation baselines\non two benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than\n5% target domain annotations, our method reaches comparable results with that\nof full supervision. Our code is publicly available at\nhttps://github.com/tsunghan-wu/D2ADA.\n","authors":["Tsung-Han Wu","Yi-Syuan Liou","Shao-Ji Yuan","Hsin-Ying Lee","Tung-I Chen","Kuan-Chih Huang","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2202.06484v4.pdf","comment":"Accepted by ECCV 2022. The code is available at\n  https://github.com/tsunghan-wu/D2ADA"},{"id":"http://arxiv.org/abs/2207.08427v1","updated":"2022-07-18T08:22:18Z","published":"2022-07-18T08:22:18Z","title":"Adaptive Assignment for Geometry Aware Local Feature Matching","summary":"  Local image feature matching, aiming to identify and correspond similar\nregions from image pairs, is an essential concept in computer vision. Most\nexisting image matching approaches follow a one-to-one assignment principle and\nemploy mutual nearest neighbor to guarantee unique correspondence between local\nfeatures across images. However, images from different conditions may hold\nlarge-scale variations or viewpoint diversification so that one-to-one\nassignment may cause ambiguous or missing representations in dense matching. In\nthis paper, we introduce AdaMatcher, a novel detector-free local feature\nmatching method, which first correlates dense features by a lightweight feature\ninteraction module and estimates co-visible area of the paired images, then\nperforms a patch-level many-to-one assignment to predict match proposals, and\nfinally refines them based on a one-to-one refinement module. Extensive\nexperiments show that AdaMatcher outperforms solid baselines and achieves\nstate-of-the-art results on many downstream tasks. Additionally, the\nmany-to-one assignment and one-to-one refinement module can be used as a\nrefinement network for other matching methods, such as SuperGlue, to boost\ntheir performance further. Code will be available upon publication.\n","authors":["Dihe Huang","Ying Chen","Shang Xu","Yong Liu","Wenlong Wu","Yikang Ding","Chengjie Wang","Fan Tang"],"pdf_url":"https://arxiv.org/pdf/2207.08427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.02811v2","updated":"2022-07-18T08:00:22Z","published":"2022-04-06T13:23:02Z","title":"BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy\n  for Source-free Domain Adaptation","summary":"  Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto the unlabeled target domain without accessing the well-labeled source data,\nwhich is a much more practical setting due to the data privacy, security, and\ntransmission issues. To make up for the absence of source data, most existing\nmethods introduced feature prototype based pseudo-labeling strategies to\nrealize self-training model adaptation. However, feature prototypes are\nobtained by instance-level predictions based feature clustering, which is\ncategory-biased and tends to result in noisy labels since the visual domain\ngaps between source and target are usually different between categories. In\naddition, we found that a monocentric feature prototype may be ineffective to\nrepresent each category and introduce negative transfer, especially for those\nhard-transfer data. To address these issues, we propose a general\nclass-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task.\nSpecifically, for each target category, we first introduce a global inter-class\nbalanced sampling strategy to aggregate potential representative target\nsamples. Then, we design an intra-class multicentric clustering strategy to\nachieve more robust and representative prototypes generation. In contrast to\nexisting strategies that update the pseudo label at a fixed training period, we\nfurther introduce a dynamic pseudo labeling strategy to incorporate network\nupdate information during model adaptation. Extensive experiments show that the\nproposed model-agnostic BMD strategy significantly improves representative SFDA\nmethods to yield new state-of-the-art results. The code is available at\nhttps://github.com/ispc-lab/BMD.\n","authors":["Sanqing Qu","Guang Chen","Jing Zhang","Zhijun Li","Wei He","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2204.02811v2.pdf","comment":"Camera-ready version of ECCV 2022. Code is available at\n  https://github.com/ispc-lab/BMD"},{"id":"http://arxiv.org/abs/2207.08417v1","updated":"2022-07-18T07:54:17Z","published":"2022-07-18T07:54:17Z","title":"Real-time End-to-End Video Text Spotter with Contrastive Representation\n  Learning","summary":"  Video text spotting(VTS) is the task that requires simultaneously detecting,\ntracking and recognizing text in the video. Existing video text spotting\nmethods typically develop sophisticated pipelines and multiple models, which is\nnot friend for real-time applications. Here we propose a real-time end-to-end\nvideo text spotter with Contrastive Representation learning (CoText). Our\ncontributions are three-fold: 1) CoText simultaneously address the three tasks\n(e.g., text detection, tracking, recognition) in a real-time end-to-end\ntrainable framework. 2) With contrastive learning, CoText models long-range\ndependencies and learning temporal information across multiple frames. 3) A\nsimple, lightweight architecture is designed for effective and accurate\nperformance, including GPU-parallel detection post-processing, CTC-based\nrecognition head with Masked RoI. Extensive experiments show the superiority of\nour method. Especially, CoText achieves an video text spotting IDF1 of 72.0% at\n41.0 FPS on ICDAR2015video, with 10.5% and 32.0 FPS improvement the previous\nbest method. The code can be found at github.com/weijiawu/CoText.\n","authors":["Wejia Wu","Zhuang Li","Jiahong Li","Chunhua Shen","Hong Zhou","Size Li","Zhongyuan Wang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2207.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08412v1","updated":"2022-07-18T07:21:56Z","published":"2022-07-18T07:21:56Z","title":"Multi-head Cascaded Swin Transformers with Attention to k-space Sampling\n  Pattern for Accelerated MRI Reconstruction","summary":"  Global correlations are widely seen in human anatomical structures due to\nsimilarity across tissues and bones. These correlations are reflected in\nmagnetic resonance imaging (MRI) scans as a result of close-range proton\ndensity and T1/T2 parameter. Furthermore, to achieve accelerated MRI, k-space\ndata are undersampled which causes global aliasing artifacts. Convolutional\nneural network (CNN) models are widely utilized for accelerated MRI\nreconstruction, but those models are limited in capturing global correlations\ndue to the intrinsic locality of the convolution operation. The\nself-attention-based transformer models are capable of capturing global\ncorrelations among image features, however, the current contributions of\ntransformer models for MRI reconstruction are minute. The existing\ncontributions mostly provide CNN-transformer hybrid solutions and rarely\nleverage the physics of MRI. In this paper, we propose a physics-based\nstand-alone (convolution free) transformer model titled, the Multi-head\nCascaded Swin Transformers (McSTRA) for accelerated MRI reconstruction. McSTRA\ncombines several interconnected MRI physics-related concepts with the\ntransformer networks: it exploits global MR features via the shifted window\nself-attention mechanism; it extracts MR features belonging to different\nspectral components separately using a multi-head setup; it iterates between\nintermediate de-aliasing and k-space correction via a cascaded network with\ndata consistency in k-space and intermediate loss computations; furthermore, we\npropose a novel positional embedding generation mechanism to guide\nself-attention utilizing the point spread function corresponding to the\nundersampling mask. Our model significantly outperforms state-of-the-art MRI\nreconstruction methods both visually and quantitatively while depicting\nimproved resolution and removal of aliasing artifacts.\n","authors":["Mevan Ekanayake","Kamlesh Pawar","Mehrtash Harandi","Gary Egan","Zhaolin Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13214v2","updated":"2022-07-18T07:14:03Z","published":"2022-03-24T17:10:26Z","title":"A Perturbation-Constrained Adversarial Attack for Evaluating the\n  Robustness of Optical Flow","summary":"  Recent optical flow methods are almost exclusively judged in terms of\naccuracy, while their robustness is often neglected. Although adversarial\nattacks offer a useful tool to perform such an analysis, current attacks on\noptical flow methods focus on real-world attacking scenarios rather than a\nworst case robustness assessment. Hence, in this work, we propose a novel\nadversarial attack - the Perturbation-Constrained Flow Attack (PCFA) - that\nemphasizes destructivity over applicability as a real-world attack. PCFA is a\nglobal attack that optimizes adversarial perturbations to shift the predicted\nflow towards a specified target flow, while keeping the L2 norm of the\nperturbation below a chosen bound. Our experiments demonstrate PCFA's\napplicability in white- and black-box settings, and show it finds stronger\nadversarial samples than previous attacks. Based on these strong samples, we\nprovide the first joint ranking of optical flow methods considering both\nprediction quality and adversarial robustness, which reveals state-of-the-art\nmethods to be particularly vulnerable. Code is available at\nhttps://github.com/cv-stuttgart/PCFA.\n","authors":["Jenny Schmalfuss","Philipp Scholze","Andrés Bruhn"],"pdf_url":"https://arxiv.org/pdf/2203.13214v2.pdf","comment":"Accepted at the European Conference on Computer Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2012.13073v3","updated":"2022-07-18T07:10:58Z","published":"2020-12-24T02:30:18Z","title":"Task-Adaptive Negative Envision for Few-Shot Open-Set Recognition","summary":"  We study the problem of few-shot open-set recognition (FSOR), which learns a\nrecognition system capable of both fast adaptation to new classes with limited\nlabeled examples and rejection of unknown negative samples. Traditional\nlarge-scale open-set methods have been shown ineffective for FSOR problem due\nto data limitation. Current FSOR methods typically calibrate few-shot\nclosed-set classifiers to be sensitive to negative samples so that they can be\nrejected via thresholding. However, threshold tuning is a challenging process\nas different FSOR tasks may require different rejection powers. In this paper,\nwe instead propose task-adaptive negative class envision for FSOR to integrate\nthreshold tuning into the learning process. Specifically, we augment the\nfew-shot closed-set classifier with additional negative prototypes generated\nfrom few-shot examples. By incorporating few-shot class correlations in the\nnegative generation process, we are able to learn dynamic rejection boundaries\nfor FSOR tasks. Besides, we extend our method to generalized few-shot open-set\nrecognition (GFSOR), which requires classification on both many-shot and\nfew-shot classes as well as rejection of negative samples. Extensive\nexperiments on public benchmarks validate our methods on both problems.\n","authors":["Shiyuan Huang","Jiawei Ma","Guangxing Han","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2012.13073v3.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2207.08409v1","updated":"2022-07-18T07:08:29Z","published":"2022-07-18T07:08:29Z","title":"TokenMix: Rethinking Image Mixing for Data Augmentation in Vision\n  Transformers","summary":"  CutMix is a popular augmentation technique commonly used for training modern\nconvolutional and transformer vision networks. It was originally designed to\nencourage Convolution Neural Networks (CNNs) to focus more on an image's global\ncontext instead of local information, which greatly improves the performance of\nCNNs. However, we found it to have limited benefits for transformer-based\narchitectures that naturally have a global receptive field. In this paper, we\npropose a novel data augmentation technique TokenMix to improve the performance\nof vision transformers. TokenMix mixes two images at token level via\npartitioning the mixing region into multiple separated parts. Besides, we show\nthat the mixed learning target in CutMix, a linear combination of a pair of the\nground truth labels, might be inaccurate and sometimes counter-intuitive. To\nobtain a more suitable target, we propose to assign the target score according\nto the content-based neural activation maps of the two images from a\npre-trained teacher model, which does not need to have high performance. With\nplenty of experiments on various vision transformer architectures, we show that\nour proposed TokenMix helps vision transformers focus on the foreground area to\ninfer the classes and enhances their robustness to occlusion, with consistent\nperformance gains. Notably, we improve DeiT-T/S/B with +1% ImageNet top-1\naccuracy. Besides, TokenMix enjoys longer training, which achieves 81.2% top-1\naccuracy on ImageNet with DeiT-S trained for 400 epochs. Code is available at\nhttps://github.com/Sense-X/TokenMix.\n","authors":["Jihao Liu","Boxiao Liu","Hang Zhou","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2207.08409v1.pdf","comment":"ECCV 2022; Code: https://github.com/Sense-X/TokenMix"},{"id":"http://arxiv.org/abs/2207.04808v3","updated":"2022-07-18T07:08:15Z","published":"2022-07-11T12:09:41Z","title":"CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer","summary":"  In this paper, we aim to devise a universally versatile style transfer method\ncapable of performing artistic, photo-realistic, and video style transfer\njointly, without seeing videos during training. Previous single-frame methods\nassume a strong constraint on the whole image to maintain temporal consistency,\nwhich could be violated in many cases. Instead, we make a mild and reasonable\nassumption that global inconsistency is dominated by local inconsistencies and\ndevise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local\npatches. CCPL can preserve the coherence of the content source during style\ntransfer without degrading stylization. Moreover, it owns a neighbor-regulating\nmechanism, resulting in a vast reduction of local distortions and considerable\nvisual quality improvement. Aside from its superior performance on versatile\nstyle transfer, it can be easily extended to other tasks, such as\nimage-to-image translation. Besides, to better fuse content and style features,\nwe propose Simple Covariance Transformation (SCT) to effectively align\nsecond-order statistics of the content feature with the style feature.\nExperiments demonstrate the effectiveness of the resulting model for versatile\nstyle transfer, when armed with CCPL.\n","authors":["Zijie Wu","Zhen Zhu","Junping Du","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2207.04808v3.pdf","comment":"Accepted by ECCV2022 as an oral paper; code url:\n  https://github.com/JarrentWu1031/CCPL; Video demo:\n  https://youtu.be/scZuJCXhL14"},{"id":"http://arxiv.org/abs/2207.06823v2","updated":"2022-07-18T06:52:21Z","published":"2022-07-14T11:27:02Z","title":"DEXTER: An end-to-end system to extract table contents from electronic\n  medical health documents","summary":"  In this paper, we propose DEXTER, an end to end system to extract information\nfrom tables present in medical health documents, such as electronic health\nrecords (EHR) and explanation of benefits (EOB). DEXTER consists of four\nsub-system stages: i) table detection ii) table type classification iii) cell\ndetection; and iv) cell content extraction. We propose a two-stage transfer\nlearning-based approach using CDeC-Net architecture along with Non-Maximal\nsuppression for table detection. We design a conventional computer vision-based\napproach for table type classification and cell detection using parameterized\nkernels based on image size for detecting rows and columns. Finally, we extract\nthe text from the detected cells using pre-existing OCR engine Tessaract. To\nevaluate our system, we manually annotated a sample of the real-world medical\ndataset (referred to as Meddata) consisting of wide variations of documents (in\nterms of appearance) covering different table structures, such as bordered,\npartially bordered, borderless, or coloured tables. We experimentally show that\nDEXTER outperforms the commercially available Amazon Textract and Microsoft\nAzure Form Recognizer systems on the annotated real-world medical dataset\n","authors":["Nandhinee PR","Harinath Krishnamoorthy","Koushik Srivatsan","Anil Goyal","Sudarsun Santhiappan"],"pdf_url":"https://arxiv.org/pdf/2207.06823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.04533v5","updated":"2022-07-18T06:41:58Z","published":"2022-02-09T15:57:21Z","title":"NIMBLE: A Non-rigid Hand Model with Bones and Muscles","summary":"  Emerging Metaverse applications demand reliable, accurate, and photorealistic\nreproductions of human hands to perform sophisticated operations as if in the\nphysical world. While real human hand represents one of the most intricate\ncoordination between bones, muscle, tendon, and skin, state-of-the-art\ntechniques unanimously focus on modeling only the skeleton of the hand. In this\npaper, we present NIMBLE, a novel parametric hand model that includes the\nmissing key components, bringing 3D hand model to a new level of realism. We\nfirst annotate muscles, bones and skins on the recent Magnetic Resonance\nImaging hand (MRI-Hand) dataset and then register a volumetric template hand\nonto individual poses and subjects within the dataset. NIMBLE consists of 20\nbones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin\nmesh. Via iterative shape registration and parameter learning, it further\nproduces shape blend shapes, pose blend shapes, and a joint regressor. We\ndemonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.\nBy enforcing the inner bones and muscles to match anatomic and kinematic rules,\nNIMBLE can animate 3D hands to new poses at unprecedented realism. To model the\nappearance of skin, we further construct a photometric HandStage to acquire\nhigh-quality textures and normal maps to model wrinkles and palm print.\nFinally, NIMBLE also benefits learning-based hand pose and shape estimation by\neither synthesizing rich data or acting directly as a differentiable layer in\nthe inference network.\n","authors":["Yuwei Li","Longwen Zhang","Zesong Qiu","Yingwenqi Jiang","Nianyi Li","Yuexin Ma","Yuyao Zhang","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2202.04533v5.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2207.08403v1","updated":"2022-07-18T06:27:24Z","published":"2022-07-18T06:27:24Z","title":"MPIB: An MPI-Based Bokeh Rendering Framework for Realistic Partial\n  Occlusion Effects","summary":"  Partial occlusion effects are a phenomenon that blurry objects near a camera\nare semi-transparent, resulting in partial appearance of occluded background.\nHowever, it is challenging for existing bokeh rendering methods to simulate\nrealistic partial occlusion effects due to the missing information of the\noccluded area in an all-in-focus image. Inspired by the learnable 3D scene\nrepresentation, Multiplane Image (MPI), we attempt to address the partial\nocclusion by introducing a novel MPI-based high-resolution bokeh rendering\nframework, termed MPIB. To this end, we first present an analysis on how to\napply the MPI representation to bokeh rendering. Based on this analysis, we\npropose an MPI representation module combined with a background inpainting\nmodule to implement high-resolution scene representation. This representation\ncan then be reused to render various bokeh effects according to the controlling\nparameters. To train and test our model, we also design a ray-tracing-based\nbokeh generator for data generation. Extensive experiments on synthesized and\nreal-world images validate the effectiveness and flexibility of this framework.\n","authors":["Juewen Peng","Jianming Zhang","Xianrui Luo","Hao Lu","Ke Xian","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2207.08403v1.pdf","comment":"Accepted by ECCV 2022; Project: https://juewenpeng.github.io/MPIB/"},{"id":"http://arxiv.org/abs/2112.09331v3","updated":"2022-07-18T06:05:07Z","published":"2021-12-17T05:40:28Z","title":"Contrastive Vision-Language Pre-training with Limited Resources","summary":"  Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have\nrevealed the potential of aligning multi-modal representations with contrastive\nlearning. However, these works require a tremendous amount of data and\ncomputational resources (e.g., billion-level web data and hundreds of GPUs),\nwhich prevent researchers with limited resources from reproduction and further\nexploration. To this end, we propose a stack of novel methods, which\nsignificantly cut down the heavy resource dependency and allow us to conduct\ndual-encoder multi-modal representation alignment with limited resources.\nBesides, we provide a reproducible baseline of competitive results, namely\nZeroVL, with only 14M publicly accessible academic datasets and 8 V100 GPUs.\nAdditionally, we collect 100M web data for pre-training, and achieve comparable\nor superior results than state-of-the-art methods, further proving the\neffectiveness of our methods on large-scale data. We hope that this work will\nprovide useful data points and experience for future research in contrastive\nvision-language pre-training. Code is available at\nhttps://github.com/zerovl/ZeroVL.\n","authors":["Quan Cui","Boyan Zhou","Yu Guo","Weidong Yin","Hao Wu","Osamu Yoshie","Yubo Chen"],"pdf_url":"https://arxiv.org/pdf/2112.09331v3.pdf","comment":"Accepted to ECCV2022"},{"id":"http://arxiv.org/abs/2207.08393v1","updated":"2022-07-18T06:01:29Z","published":"2022-07-18T06:01:29Z","title":"GLEAM: Greedy Learning for Large-Scale Accelerated MRI Reconstruction","summary":"  Unrolled neural networks have recently achieved state-of-the-art accelerated\nMRI reconstruction. These networks unroll iterative optimization algorithms by\nalternating between physics-based consistency and neural-network based\nregularization. However, they require several iterations of a large neural\nnetwork to handle high-dimensional imaging tasks such as 3D MRI. This limits\ntraditional training algorithms based on backpropagation due to prohibitively\nlarge memory and compute requirements for calculating gradients and storing\nintermediate activations. To address this challenge, we propose Greedy LEarning\nfor Accelerated MRI (GLEAM) reconstruction, an efficient training strategy for\nhigh-dimensional imaging settings. GLEAM splits the end-to-end network into\ndecoupled network modules. Each module is optimized in a greedy manner with\ndecoupled gradient updates, reducing the memory footprint during training. We\nshow that the decoupled gradient updates can be performed in parallel on\nmultiple graphical processing units (GPUs) to further reduce training time. We\npresent experiments with 2D and 3D datasets including multi-coil knee, brain,\nand dynamic cardiac cine MRI. We observe that: i) GLEAM generalizes as well as\nstate-of-the-art memory-efficient baselines such as gradient checkpointing and\ninvertible networks with the same memory footprint, but with 1.3x faster\ntraining; ii) for the same memory footprint, GLEAM yields 1.1dB PSNR gain in 2D\nand 1.8 dB in 3D over end-to-end baselines.\n","authors":["Batu Ozturkler","Arda Sahiner","Tolga Ergen","Arjun D Desai","Christopher M Sandino","Shreyas Vasanawala","John M Pauly","Morteza Mardani","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2207.08393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08387v1","updated":"2022-07-18T05:38:37Z","published":"2022-07-18T05:38:37Z","title":"A Semantic-aware Attention and Visual Shielding Network for\n  Cloth-changing Person Re-identification","summary":"  Cloth-changing person reidentification (ReID) is a newly emerging research\ntopic that aims to retrieve pedestrians whose clothes are changed. Since the\nhuman appearance with different clothes exhibits large variations, it is very\ndifficult for existing approaches to extract discriminative and robust feature\nrepresentations. Current works mainly focus on body shape or contour sketches,\nbut the human semantic information and the potential consistency of pedestrian\nfeatures before and after changing clothes are not fully explored or are\nignored. To solve these issues, in this work, a novel semantic-aware attention\nand visual shielding network for cloth-changing person ReID (abbreviated as\nSAVS) is proposed where the key idea is to shield clues related to the\nappearance of clothes and only focus on visual semantic information that is not\nsensitive to view/posture changes. Specifically, a visual semantic encoder is\nfirst employed to locate the human body and clothing regions based on human\nsemantic segmentation information. Then, a human semantic attention module\n(HSA) is proposed to highlight the human semantic information and reweight the\nvisual feature map. In addition, a visual clothes shielding module (VCS) is\nalso designed to extract a more robust feature representation for the\ncloth-changing task by covering the clothing regions and focusing the model on\nthe visual semantic information unrelated to the clothes. Most importantly,\nthese two modules are jointly explored in an end-to-end unified framework.\nExtensive experiments demonstrate that the proposed method can significantly\noutperform state-of-the-art methods, and more robust features can be extracted\nfor cloth-changing persons. Compared with FSAM (published in CVPR 2021), this\nmethod can achieve improvements of 32.7% (16.5%) and 14.9% (-) on the LTCC and\nPRCC datasets in terms of mAP (rank-1), respectively.\n","authors":["Zan Gao","Hongwei Wei","Weili Guan","Jie Nie","Meng Wang","Shenyong Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08387v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2108.04527"},{"id":"http://arxiv.org/abs/2207.08386v1","updated":"2022-07-18T05:30:45Z","published":"2022-07-18T05:30:45Z","title":"Entity-enhanced Adaptive Reconstruction Network for Weakly Supervised\n  Referring Expression Grounding","summary":"  Weakly supervised Referring Expression Grounding (REG) aims to ground a\nparticular target in an image described by a language expression while lacking\nthe correspondence between target and expression. Two main problems exist in\nweakly supervised REG. First, the lack of region-level annotations introduces\nambiguities between proposals and queries. Second, most previous weakly\nsupervised REG methods ignore the discriminative location and context of the\nreferent, causing difficulties in distinguishing the target from other\nsame-category objects. To address the above challenges, we design an\nentity-enhanced adaptive reconstruction network (EARN). Specifically, EARN\nincludes three modules: entity enhancement, adaptive grounding, and\ncollaborative reconstruction. In entity enhancement, we calculate semantic\nsimilarity as supervision to select the candidate proposals. Adaptive grounding\ncalculates the ranking score of candidate proposals upon subject, location and\ncontext with hierarchical attention. Collaborative reconstruction measures the\nranking result from three perspectives: adaptive reconstruction, language\nreconstruction and attribute classification. The adaptive mechanism helps to\nalleviate the variance of different referring expressions. Experiments on five\ndatasets show EARN outperforms existing state-of-the-art methods. Qualitative\nresults demonstrate that the proposed EARN can better handle the situation\nwhere multiple objects of a particular category are situated together.\n","authors":["Xuejing Liu","Liang Li","Shuhui Wang","Zheng-Jun Zha","Zechao Li","Qi Tian","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2207.08386v1.pdf","comment":"17 pages, 10 figures, accepted by TPAMI. arXiv admin note: text\n  overlap with arXiv:1908.10568"},{"id":"http://arxiv.org/abs/2205.11283v4","updated":"2022-07-18T05:18:36Z","published":"2022-05-23T13:10:10Z","title":"SelfReformer: Self-Refined Network with Transformer for Salient Object\n  Detection","summary":"  The global and local contexts significantly contribute to the integrity of\npredictions in Salient Object Detection (SOD). Unfortunately, existing methods\nstill struggle to generate complete predictions with fine details. There are\ntwo major problems in conventional approaches: first, for global context,\nhigh-level CNN-based encoder features cannot effectively catch long-range\ndependencies, resulting in incomplete predictions. Second, downsampling the\nground truth to fit the size of predictions will introduce inaccuracy as the\nground truth details are lost during interpolation or pooling. Thus, in this\nwork, we developed a Transformer-based network and framed a supervised task for\na branch to learn the global context information explicitly. Besides, we adopt\nPixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the\nsize of ground truth instead of the reverse. Thus details in the ground truth\nare untouched. In addition, we developed a two-stage Context Refinement Module\n(CRM) to fuse global context and automatically locate and refine the local\ndetails in the predictions. The proposed network can guide and correct itself\nbased on the global and local context generated, thus is named, Self-Refined\nTransformer (SelfReformer). Extensive experiments and evaluation results on\nfive benchmark datasets demonstrate the outstanding performance of the network,\nand we achieved the state-of-the-art.\n","authors":["Yi Ke Yun","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2205.11283v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08380v1","updated":"2022-07-18T05:14:24Z","published":"2022-07-18T05:14:24Z","title":"Visual Representations of Physiological Signals for Fake Video Detection","summary":"  Realistic fake videos are a potential tool for spreading harmful\nmisinformation given our increasing online presence and information intake.\nThis paper presents a multimodal learning-based method for detection of real\nand fake videos. The method combines information from three modalities - audio,\nvideo, and physiology. We investigate two strategies for combining the video\nand physiology modalities, either by augmenting the video with information from\nthe physiology or by novelly learning the fusion of those two modalities with a\nproposed Graph Convolutional Network architecture. Both strategies for\ncombining the two modalities rely on a novel method for generation of visual\nrepresentations of physiological signals. The detection of real and fake videos\nis then based on the dissimilarity between the audio and modified video\nmodalities. The proposed method is evaluated on two benchmark datasets and the\nresults show significant increase in detection performance compared to previous\nmethods.\n","authors":["Kalin Stefanov","Bhawna Paliwal","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2207.08380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09205v2","updated":"2022-07-18T05:08:16Z","published":"2021-12-16T21:22:17Z","title":"AFDetV2: Rethinking the Necessity of the Second Stage for Object\n  Detection from Point Clouds","summary":"  There have been two streams in the 3D detection from point clouds:\nsingle-stage methods and two-stage methods. While the former is more\ncomputationally efficient, the latter usually provides better detection\naccuracy. By carefully examining the two-stage approaches, we have found that\nif appropriately designed, the first stage can produce accurate box regression.\nIn this scenario, the second stage mainly rescores the boxes such that the\nboxes with better localization get selected. From this observation, we have\ndevised a single-stage anchor-free network that can fulfill these requirements.\nThis network, named AFDetV2, extends the previous work by incorporating a\nself-calibrated convolution block in the backbone, a keypoint auxiliary\nsupervision, and an IoU prediction branch in the multi-task head. As a result,\nthe detection accuracy is drastically boosted in the single-stage. To evaluate\nour approach, we have conducted extensive experiments on the Waymo Open Dataset\nand the nuScenes Dataset. We have observed that our AFDetV2 achieves the\nstate-of-the-art results on these two datasets, superior to all the prior arts,\nincluding both the single-stage and the two-stage 3D detectors. AFDetV2 won the\n1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge\n2021. In addition, a variant of our model AFDetV2-Base was entitled the \"Most\nEfficient Model\" by the Challenge Sponsor, showing a superior computational\nefficiency. To demonstrate the generality of this single-stage method, we have\nalso applied it to the first stage of the two-stage networks. Without\nexception, the results show that with the strengthened backbone and the\nrescoring approach, the second stage refinement is no longer needed.\n","authors":["Yihan Hu","Zhuangzhuang Ding","Runzhou Ge","Wenxin Shao","Li Huang","Kun Li","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2112.09205v2.pdf","comment":"AAAI 2022; 1st Place Solution for the Real-time 3D Detection and the\n  Most Efficient Model of the Waymo Open Dataset Challenges 2021\n  (http://cvpr2021.wad.vision/)"},{"id":"http://arxiv.org/abs/2202.11094v5","updated":"2022-07-18T05:04:01Z","published":"2022-02-22T18:56:04Z","title":"GroupViT: Semantic Segmentation Emerges from Text Supervision","summary":"  Grouping and recognition are important components of visual scene\nunderstanding, e.g., for object detection and semantic segmentation. With\nend-to-end deep learning systems, grouping of image regions usually happens\nimplicitly via top-down supervision from pixel-level recognition labels.\nInstead, in this paper, we propose to bring back the grouping mechanism into\ndeep networks, which allows semantic segments to emerge automatically with only\ntext supervision. We propose a hierarchical Grouping Vision Transformer\n(GroupViT), which goes beyond the regular grid structure representation and\nlearns to group image regions into progressively larger arbitrary-shaped\nsegments. We train GroupViT jointly with a text encoder on a large-scale\nimage-text dataset via contrastive losses. With only text supervision and\nwithout any pixel-level annotations, GroupViT learns to group together semantic\nregions and successfully transfers to the task of semantic segmentation in a\nzero-shot manner, i.e., without any further fine-tuning. It achieves a\nzero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on\nPASCAL Context datasets, and performs competitively to state-of-the-art\ntransfer-learning methods requiring greater levels of supervision. We\nopen-source our code at https://github.com/NVlabs/GroupViT .\n","authors":["Jiarui Xu","Shalini De Mello","Sifei Liu","Wonmin Byeon","Thomas Breuel","Jan Kautz","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2202.11094v5.pdf","comment":"CVPR 2022. Project page and code: https://jerryxu.net/GroupViT"},{"id":"http://arxiv.org/abs/2207.04693v2","updated":"2022-07-18T04:58:01Z","published":"2022-07-11T08:15:29Z","title":"Exploring Contextual Relationships for Cervical Abnormal Cell Detection","summary":"  Cervical abnormal cell detection is a challenging task as the morphological\ndiscrepancies between abnormal and normal cells are usually subtle. To\ndetermine whether a cervical cell is normal or abnormal, cytopathologists\nalways take surrounding cells as references to identify its abnormality. To\nmimic these behaviors, we propose to explore contextual relationships to boost\nthe performance of cervical abnormal cell detection. Specifically, both\ncontextual relationships between cells and cell-to-global images are exploited\nto enhance features of each region of interest (RoI) proposals. Accordingly,\ntwo modules, dubbed as RoI-relationship attention module (RRAM) and global RoI\nattention module (GRAM), are developed and their combination strategies are\nalso investigated. We establish a strong baseline by using Double-Head Faster\nR-CNN with feature pyramid network (FPN) and integrate our RRAM and GRAM into\nit to validate the effectiveness of the proposed modules. Experiments conducted\non a large cervical cell detection dataset reveal that the introduction of RRAM\nand GRAM both achieves better average precision (AP) than the baseline methods.\nMoreover, when cascading RRAM and GRAM, our method outperforms the\nstate-of-the-art (SOTA) methods. Furthermore, we also show the proposed feature\nenhancing scheme can facilitate both image-level and smear-level\nclassification. The code and trained models are publicly available at\nhttps://github.com/CVIU-CSU/CR4CACD.\n","authors":["Yixiong Liang","Shuo Feng","Qing Liu","Hulin Kuang","Jianfeng Liu","Liyan Liao","Yun Du","Jianxin Wang"],"pdf_url":"https://arxiv.org/pdf/2207.04693v2.pdf","comment":"10 pages, 14 tables, and 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2207.08799v1","updated":"2022-07-18T17:55:05Z","published":"2022-07-18T17:55:05Z","title":"Hidden Progress in Deep Learning: SGD Learns Parities Near the\n  Computational Limit","summary":"  There is mounting empirical evidence of emergent phenomena in the\ncapabilities of deep learning methods as we scale up datasets, model sizes, and\ntraining times. While there are some accounts of how these resources modulate\nstatistical capacity, far less is known about their effect on the computational\nproblem of model training. This work conducts such an exploration through the\nlens of learning $k$-sparse parities of $n$ bits, a canonical family of\nproblems which pose theoretical computational barriers. In this setting, we\nfind that neural networks exhibit surprising phase transitions when scaling up\ndataset size and running time. In particular, we demonstrate empirically that\nwith standard training, a variety of architectures learn sparse parities with\n$n^{O(k)}$ examples, with loss (and error) curves abruptly dropping after\n$n^{O(k)}$ iterations. These positive results nearly match known SQ lower\nbounds, even without an explicit sparsity-promoting prior. We elucidate the\nmechanisms of these phenomena with a theoretical analysis: we find that the\nphase transition in performance is not due to SGD \"stumbling in the dark\" until\nit finds the hidden set of features (a natural algorithm which also runs in\n$n^{O(k)}$ time); instead, we show that SGD gradually amplifies a Fourier gap\nin the population gradient.\n","authors":["Boaz Barak","Benjamin L. Edelman","Surbhi Goel","Sham Kakade","Eran Malach","Cyril Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.08799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.08670v2","updated":"2022-07-18T17:48:03Z","published":"2021-12-16T07:10:02Z","title":"Amortized Noisy Channel Neural Machine Translation","summary":"  Noisy channel models have been especially effective in neural machine\ntranslation (NMT). However, recent approaches like \"beam search and rerank\"\n(BSR) incur significant computation overhead during inference, making\nreal-world application infeasible. We aim to study if it is possible to build\nan amortized noisy channel NMT model such that when we do greedy decoding\nduring inference, the translation accuracy matches that of BSR in terms of\nreward (based on the source-to-target log probability and the target-to-source\nlog probability) and quality (based on BLEU and BLEURT). We attempt three\napproaches to train the new model: knowledge distillation, one-step-deviation\nimitation learning, and Q learning. The first approach obtains the noisy\nchannel signal from a pseudo-corpus, and the latter two approaches aim to\noptimize toward a noisy-channel MT reward directly. For all three approaches,\nthe generated translations fail to achieve rewards comparable to BSR, but the\ntranslation quality approximated by BLEU and BLEURT is similar to the quality\nof BSR-produced translations. Additionally, all three approaches speed up\ninference by 1-2 orders of magnitude.\n","authors":["Richard Yuanzhe Pang","He He","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2112.08670v2.pdf","comment":"INLG 2022"},{"id":"http://arxiv.org/abs/2207.08779v1","updated":"2022-07-18T17:36:54Z","published":"2022-07-18T17:36:54Z","title":"Simplifying Clustering with Graph Neural Networks","summary":"  The objective functions used in spectral clustering are usually composed of\ntwo terms: i) a term that minimizes the local quadratic variation of the\ncluster assignments on the graph and; ii) a term that balances the clustering\npartition and helps avoiding degenerate solutions. This paper shows that a\ngraph neural network, equipped with suitable message passing layers, can\ngenerate good cluster assignments by optimizing only a balancing term. Results\non attributed graph datasets show the effectiveness of the proposed approach in\nterms of clustering performance and computation time.\n","authors":["Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2207.08779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08770v1","updated":"2022-07-18T17:22:32Z","published":"2022-07-18T17:22:32Z","title":"Package for Fast ABC-Boost","summary":"  This report presents the open-source package which implements the series of\nour boosting works in the past years. In particular, the package includes\nmainly three lines of techniques, among which the following two are already the\nstandard implementations in popular boosted tree platforms:\n  (i) The histogram-based (feature-binning) approach makes the tree\nimplementation convenient and efficient. In Li et al (2007), a simple\nfixed-length adaptive binning algorithm was developed. In this report, we\ndemonstrate that such a simple algorithm is still surprisingly effective\ncompared to more sophisticated variants in popular tree platforms.\n  (ii) The explicit gain formula in Li (20010) for tree splitting based on\nsecond-order derivatives of the loss function typically improves, often\nconsiderably, over the first-order methods. Although the gain formula in Li\n(2010) was derived for logistic regression loss, it is a generic formula for\nloss functions with second-derivatives. For example, the open-source package\nalso includes $L_p$ regression for $p\\geq 1$.\n  The main contribution of this package is the ABC-Boost (adaptive base class\nboosting) for multi-class classification. The initial work in Li (2008) derived\na new set of derivatives of the classical multi-class logistic regression by\nspecifying a \"base class\". The accuracy can be substantially improved if the\nbase class is chosen properly. The major technical challenge is to design a\nsearch strategy to select the base class. The prior published works implemented\nan exhaustive search procedure to find the base class which is computationally\ntoo expensive. Recently, a new report (Li and Zhao, 20022) presents a unified\nframework of \"Fast ABC-Boost\" which allows users to efficiently choose the\nproper search space for the base class.\n  The package provides interfaces for linux, windows, mac, matlab, R, python.\n","authors":["Ping Li","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.08770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08768v1","updated":"2022-07-18T17:21:01Z","published":"2022-07-18T17:21:01Z","title":"Rank-based Decomposable Losses in Machine Learning: A Survey","summary":"  Recent works have revealed an essential paradigm in designing loss functions\nthat differentiate individual losses vs. aggregate losses. The individual loss\nmeasures the quality of the model on a sample, while the aggregate loss\ncombines individual losses/scores over each training sample. Both have a common\nprocedure that aggregates a set of individual values to a single numerical\nvalue. The ranking order reflects the most fundamental relation among\nindividual values in designing losses. In addition, decomposability, in which a\nloss can be decomposed into an ensemble of individual terms, becomes a\nsignificant property of organizing losses/scores. This survey provides a\nsystematic and comprehensive review of rank-based decomposable losses in\nmachine learning. Specifically, we provide a new taxonomy of loss functions\nthat follows the perspectives of aggregate loss and individual loss. We\nidentify the aggregator to form such losses, which are examples of set\nfunctions. We organize the rank-based decomposable losses into eight\ncategories. Following these categories, we review the literature on rank-based\naggregate losses and rank-based individual losses. We describe general formulas\nfor these losses and connect them with existing research topics. We also\nsuggest future research directions spanning unexplored, remaining, and emerging\nissues in rank-based decomposable losses.\n","authors":["Shu Hu","Xin Wang","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2207.08768v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2207.08766v1","updated":"2022-07-18T17:13:32Z","published":"2022-07-18T17:13:32Z","title":"Word Play for Playing Othello (Reverses)","summary":"  Language models like OpenAI's Generative Pre-Trained Transformers (GPT-2/3)\ncapture the long-term correlations needed to generate text in a variety of\ndomains (such as language translators) and recently in gameplay (chess, Go, and\ncheckers). The present research applies both the larger (GPT-3) and smaller\n(GPT-2) language models to explore the complex strategies for the game of\nOthello (or Reverses). Given the game rules for rapid reversals of fortune, the\nlanguage model not only represents a candidate predictor of the next move based\non previous game moves but also avoids sparse rewards in gameplay. The language\nmodel automatically captures or emulates championship-level strategies. The\nfine-tuned GPT-2 model generates Othello games ranging from 13-71% completion,\nwhile the larger GPT-3 model reaches 41% of a complete game. Like previous work\nwith chess and Go, these language models offer a novel way to generate\nplausible game archives, particularly for comparing opening moves across a\nlarger sample than humanly possible to explore. A primary contribution of these\nmodels magnifies (by two-fold) the previous record for player archives (120,000\nhuman games over 45 years from 1977-2022), thus supplying the research\ncommunity with more diverse and original strategies for sampling with other\nreinforcement learning techniques.\n","authors":["Samantha E. Miller Noever","David Noever"],"pdf_url":"https://arxiv.org/pdf/2207.08766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.09667v2","updated":"2022-07-18T16:42:12Z","published":"2022-02-19T20:00:44Z","title":"Doubly Robust Distributionally Robust Off-Policy Evaluation and Learning","summary":"  Off-policy evaluation and learning (OPE/L) use offline observational data to\nmake better decisions, which is crucial in applications where online\nexperimentation is limited. However, depending entirely on logged data, OPE/L\nis sensitive to environment distribution shifts -- discrepancies between the\ndata-generating environment and that where policies are deployed.\n\\citet{si2020distributional} proposed distributionally robust OPE/L (DROPE/L)\nto address this, but the proposal relies on inverse-propensity weighting, whose\nestimation error and regret will deteriorate if propensities are\nnonparametrically estimated and whose variance is suboptimal even if not. For\nstandard, non-robust, OPE/L, this is solved by doubly robust (DR) methods, but\nthey do not naturally extend to the more complex DROPE/L, which involves a\nworst-case expectation. In this paper, we propose the first DR algorithms for\nDROPE/L with KL-divergence uncertainty sets. For evaluation, we propose\nLocalized Doubly Robust DROPE (LDR$^2$OPE) and show that it achieves\nsemiparametric efficiency under weak product rates conditions. Thanks to a\nlocalization technique, LDR$^2$OPE only requires fitting a small number of\nregressions, just like DR methods for standard OPE. For learning, we propose\nContinuum Doubly Robust DROPL (CDR$^2$OPL) and show that, under a product rate\ncondition involving a continuum of regressions, it enjoys a fast regret rate of\n$\\mathcal{O}\\left(N^{-1/2}\\right)$ even when unknown propensities are\nnonparametrically estimated. We empirically validate our algorithms in\nsimulations and further extend our results to general $f$-divergence\nuncertainty sets.\n","authors":["Nathan Kallus","Xiaojie Mao","Kaiwen Wang","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2202.09667v2.pdf","comment":"Short Talk at ICML 2022"},{"id":"http://arxiv.org/abs/2207.08745v1","updated":"2022-07-18T16:39:56Z","published":"2022-07-18T16:39:56Z","title":"Amplitude Scintillation Forecasting Using Bagged Trees","summary":"  Electron density irregularities present within the ionosphere induce\nsignificant fluctuations in global navigation satellite system (GNSS) signals.\nFluctuations in signal power are referred to as amplitude scintillation and can\nbe monitored through the S4 index. Forecasting the severity of amplitude\nscintillation based on historical S4 index data is beneficial when real-time\ndata is unavailable. In this work, we study the possibility of using historical\ndata from a single GPS scintillation monitoring receiver to train a machine\nlearning (ML) model to forecast the severity of amplitude scintillation, either\nweak, moderate, or severe, with respect to temporal and spatial parameters. Six\ndifferent ML models were evaluated and the bagged trees model was the most\naccurate among them, achieving a forecasting accuracy of $81\\%$ using a\nbalanced dataset, and $97\\%$ using an imbalanced dataset.\n","authors":["Abdollah Masoud Darya","Aisha Abdulla Al-Owais","Muhammad Mubasshir Shaikh","Ilias Fernini"],"pdf_url":"https://arxiv.org/pdf/2207.08745v1.pdf","comment":"This paper was presented at IGARSS 2022, Kuala Lumpur, Malaysia"},{"id":"http://arxiv.org/abs/2008.11092v3","updated":"2022-07-18T16:36:20Z","published":"2020-08-25T15:10:57Z","title":"Looking Deeper into Tabular LIME","summary":"  In this paper, we present a thorough theoretical analysis of the default\nimplementation of LIME in the case of tabular data. We prove that in the large\nsample limit, the interpretable coefficients provided by Tabular LIME can be\ncomputed in an explicit way as a function of the algorithm parameters and some\nexpectation computations related to the black-box model. When the function to\nexplain has some nice algebraic structure (linear, multiplicative, or sparsely\ndepending on a subset of the coordinates), our analysis provides interesting\ninsights into the explanations provided by LIME. These can be applied to a\nrange of machine learning models including Gaussian kernels or CART random\nforests. As an example, for linear functions we show that LIME has the\ndesirable property to provide explanations that are proportional to the\ncoefficients of the function to explain and to ignore coordinates that are not\nused by the function to explain. For partition-based regressors, on the other\nside, we show that LIME produces undesired artifacts that may provide\nmisleading explanations.\n","authors":["Damien Garreau","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2008.11092v3.pdf","comment":"69 pages, 20 figures"},{"id":"http://arxiv.org/abs/2207.08735v1","updated":"2022-07-18T16:28:01Z","published":"2022-07-18T16:28:01Z","title":"An Information-Theoretic Analysis of Bayesian Reinforcement Learning","summary":"  Building on the framework introduced by Xu and Raginksy [1] for supervised\nlearning problems, we study the best achievable performance for model-based\nBayesian reinforcement learning problems. With this purpose, we define minimum\nBayesian regret (MBR) as the difference between the maximum expected cumulative\nreward obtainable either by learning from the collected data or by knowing the\nenvironment and its dynamics. We specialize this definition to reinforcement\nlearning problems modeled as Markov decision processes (MDPs) whose kernel\nparameters are unknown to the agent and whose uncertainty is expressed by a\nprior distribution. One method for deriving upper bounds on the MBR is\npresented and specific bounds based on the relative entropy and the Wasserstein\ndistance are given. We then focus on two particular cases of MDPs, the\nmulti-armed bandit problem (MAB) and the online optimization with partial\nfeedback problem. For the latter problem, we show that our bounds can recover\nfrom below the current information-theoretic bounds by Russo and Van Roy [2].\n","authors":["Amaury Gouverneur","Borja Rodríguez-Gálvez","Tobias J. Oechtering","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2207.08735v1.pdf","comment":"10 pages: 6 of the main text, 1 of references, and 3 of appendices"},{"id":"http://arxiv.org/abs/2207.08730v1","updated":"2022-07-18T16:23:26Z","published":"2022-07-18T16:23:26Z","title":"On stabilizing reinforcement learning without Lyapunov functions","summary":"  Reinforcement learning remains one of the major directions of the\ncontemporary development of control engineering and machine learning. Nice\nintuition, flexible settings, ease of application are among the many perks of\nthis methodology. From the standpoint of machine learning, the main strength of\na reinforcement learning agent is its ability to ``capture\" (learn) the optimal\nbehavior in the given environment. Typically, the agent is built on neural\nnetworks and it is their approximation abilities that give rise to the above\nbelief. From the standpoint of control engineering, however, reinforcement\nlearning has serious deficiencies. The most significant one is the lack of\nstability guarantee of the agent-environment closed loop. A great deal of\nresearch was and is being made towards stabilizing reinforcement learning.\nSpeaking of stability, the celebrated Lyapunov theory is the de facto tool. It\nis thus no wonder that so many techniques of stabilizing reinforcement learning\nrely on the Lyapunov theory in one way or another. In control theory, there is\nan intricate connection between a stabilizing controller and a Lyapunov\nfunction. Employing such a pair seems thus quite attractive to design\nstabilizing reinforcement learning. However, computation of a Lyapunov function\nis generally a cumbersome process. In this note, we show how to construct a\nstabilizing reinforcement learning agent that does not employ such a function\nat all. We only assume that a Lyapunov function exists, which is a natural\nthing to do if the given system (read: environment) is stabilizable, but we do\nnot need to compute one.\n","authors":["Pavel Osinenko","Grigory Yaremenko","Georgiy Malaniya"],"pdf_url":"https://arxiv.org/pdf/2207.08730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.11678v3","updated":"2022-07-18T15:54:39Z","published":"2021-09-21T09:34:14Z","title":"Improved optimization strategies for deep Multi-Task Networks","summary":"  In Multi-Task Learning (MTL), it is a common practice to train multi-task\nnetworks by optimizing an objective function, which is a weighted average of\nthe task-specific objective functions. Although the computational advantages of\nthis strategy are clear, the complexity of the resulting loss landscape has not\nbeen studied in the literature. Arguably, its optimization may be more\ndifficult than a separate optimization of the constituting task-specific\nobjectives. In this work, we investigate the benefits of such an alternative,\nby alternating independent gradient descent steps on the different\ntask-specific objective functions and we formulate a novel way to combine this\napproach with state-of-the-art optimizers. As the separation of task-specific\nobjectives comes at the cost of increased computational time, we propose a\nrandom task grouping as a trade-off between better optimization and\ncomputational efficiency. Experimental results over three well-known visual MTL\ndatasets show better overall absolute performance on losses and standard\nmetrics compared to an averaged objective function and other state-of-the-art\nMTL methods. In particular, our method shows the most benefits when dealing\nwith tasks of different nature and it enables a wider exploration of the shared\nparameter space. We also show that our random grouping strategy allows to\ntrade-off between these benefits and computational efficiency.\n","authors":["Lucas Pascal","Pietro Michiardi","Xavier Bost","Benoit Huet","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2109.11678v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08675v1","updated":"2022-07-18T15:11:43Z","published":"2022-07-18T15:11:43Z","title":"Learning differentiable solvers for systems with hard constraints","summary":"  We introduce a practical method to enforce linear partial differential\nequation (PDE) constraints for functions defined by neural networks (NNs), up\nto a desired tolerance. By combining methods in differentiable physics and\napplications of the implicit function theorem to NN models, we develop a\ndifferentiable PDE-constrained NN layer. During training, our model learns a\nfamily of functions, each of which defines a mapping from PDE parameters to PDE\nsolutions. At inference time, the model finds an optimal linear combination of\nthe functions in the learned family by solving a PDE-constrained optimization\nproblem. Our method provides continuous solutions over the domain of interest\nthat exactly satisfy desired physical constraints. Our results show that\nincorporating hard constraints directly into the NN architecture achieves much\nlower test error, compared to training on an unconstrained objective.\n","authors":["Geoffrey Négiar","Michael W. Mahoney","Aditi S. Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2207.08675v1.pdf","comment":"10 pages + references"},{"id":"http://arxiv.org/abs/2207.08673v1","updated":"2022-07-18T15:10:58Z","published":"2022-07-18T15:10:58Z","title":"Back to the Manifold: Recovering from Out-of-Distribution States","summary":"  Learning from previously collected datasets of expert data offers the promise\nof acquiring robotic policies without unsafe and costly online explorations.\nHowever, a major challenge is a distributional shift between the states in the\ntraining dataset and the ones visited by the learned policy at the test time.\nWhile prior works mainly studied the distribution shift caused by the policy\nduring the offline training, the problem of recovering from out-of-distribution\nstates at the deployment time is not very well studied yet. We alleviate the\ndistributional shift at the deployment time by introducing a recovery policy\nthat brings the agent back to the training manifold whenever it steps out of\nthe in-distribution states, e.g., due to an external perturbation. The recovery\npolicy relies on an approximation of the training data density and a learned\nequivariant mapping that maps visual observations into a latent space in which\ntranslations correspond to the robot actions. We demonstrate the effectiveness\nof the proposed method through several manipulation experiments on a real\nrobotic platform. Our results show that the recovery policy enables the agent\nto complete tasks while the behavioral cloning alone fails because of the\ndistributional shift problem.\n","authors":["Alfredo Reichlin","Giovanni Luca Marchetti","Hang Yin","Ali Ghadirzadeh","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2207.08673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08667v1","updated":"2022-07-18T15:06:30Z","published":"2022-07-18T15:06:30Z","title":"pGMM Kernel Regression and Comparisons with Boosted Trees","summary":"  In this work, we demonstrate the advantage of the pGMM (``powered generalized\nmin-max'') kernel in the context of (ridge) regression. In recent prior\nstudies, the pGMM kernel has been extensively evaluated for classification\ntasks, for logistic regression, support vector machines, as well as deep neural\nnetworks. In this paper, we provide an experimental study on ridge regression,\nto compare the pGMM kernel regression with the ordinary ridge linear regression\nas well as the RBF kernel ridge regression. Perhaps surprisingly, even without\na tuning parameter (i.e., $p=1$ for the power parameter of the pGMM kernel),\nthe pGMM kernel already performs well. Furthermore, by tuning the parameter\n$p$, this (deceptively simple) pGMM kernel even performs quite comparably to\nboosted trees.\n  Boosting and boosted trees are very popular in machine learning practice. For\nregression tasks, typically, practitioners use $L_2$ boost, i.e., for\nminimizing the $L_2$ loss. Sometimes for the purpose of robustness, the $L_1$\nboost might be a choice. In this study, we implement $L_p$ boost for $p\\geq 1$\nand include it in the package of ``Fast ABC-Boost''. Perhaps also surprisingly,\nthe best performance (in terms of $L_2$ regression loss) is often attained at\n$p>2$, in some cases at $p\\gg 2$. This phenomenon has already been demonstrated\nby Li et al (UAI 2010) in the context of k-nearest neighbor classification\nusing $L_p$ distances. In summary, the implementation of $L_p$ boost provides\npractitioners the additional flexibility of tuning boosting algorithms for\npotentially achieving better accuracy in regression applications.\n","authors":["Ping Li","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.08667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.01893v3","updated":"2022-07-18T14:54:14Z","published":"2020-06-02T19:19:49Z","title":"Unsupervised Discretization by Two-dimensional MDL-based Histogram","summary":"  Unsupervised discretization is a crucial step in many knowledge discovery\ntasks. The state-of-the-art method for one-dimensional data infers locally\nadaptive histograms using the minimum description length (MDL) principle, but\nthe multi-dimensional case is far less studied: current methods consider the\ndimensions one at a time (if not independently), which result in\ndiscretizations based on rectangular cells of adaptive size. Unfortunately,\nthis approach is unable to adequately characterize dependencies among\ndimensions and/or results in discretizations consisting of more cells (or bins)\nthan is desirable.\n  To address this problem, we propose an expressive model class that allows for\nfar more flexible partitions of two-dimensional data. We extend the state of\nthe art for the one-dimensional case to obtain a model selection problem based\non the normalized maximum likelihood, a form of refined MDL. As the flexibility\nof our model class comes at the cost of a vast search space, we introduce a\nheuristic algorithm, named PALM, which Partitions each dimension ALternately\nand then Merges neighboring regions, all using the MDL principle. Experiments\non synthetic data show that PALM 1) accurately reveals ground truth partitions\nthat are within the model class (i.e., the search space), given a large enough\nsample size; 2) approximates well a wide range of partitions outside the model\nclass; 3) converges, in contrast to the state-of-the-art multivariate\ndiscretization method IPD. Finally, we apply our algorithm to three spatial\ndatasets, and we demonstrate that, compared to kernel density estimation (KDE),\nour algorithm not only reveals more detailed density changes, but also fits\nunseen data better, as measured by the log-likelihood.\n","authors":["Lincen Yang","Mitra Baratchi","Matthijs van Leeuwen"],"pdf_url":"https://arxiv.org/pdf/2006.01893v3.pdf","comment":"revision submitted to springer machine learning journal"},{"id":"http://arxiv.org/abs/2207.08655v1","updated":"2022-07-18T14:53:50Z","published":"2022-07-18T14:53:50Z","title":"An Enhanced Graph Representation for Machine Learning Based Automatic\n  Intersection Management","summary":"  The improvement of traffic efficiency at urban intersections receives strong\nresearch interest in the field of automated intersection management. So far,\nmostly non-learning algorithms like reservation or optimization-based ones were\nproposed to solve the underlying multi-agent planning problem. At the same\ntime, automated driving functions for a single ego vehicle are increasingly\nimplemented using machine learning methods. In this work, we build upon a\npreviously presented graph-based scene representation and graph neural network\nto approach the problem using reinforcement learning. The scene representation\nis improved in key aspects by using edge features in addition to the existing\nnode features for the vehicles. This leads to an increased representation\nquality that is leveraged by an updated network architecture. The paper\nprovides an in-depth evaluation of the proposed method against baselines that\nare commonly used in automatic intersection management. Compared to a\ntraditional signalized intersection and an enhanced first-in-first-out scheme,\na significant reduction of induced delay is observed at varying traffic\ndensities. Finally, the generalization capability of the graph-based\nrepresentation is evaluated by testing the policy on intersection layouts not\nseen during training. The model generalizes virtually without restrictions to\nsmaller intersection layouts and within certain limits to larger ones.\n","authors":["Marvin Klimke","Jasper Gerigk","Benjamin Völz","Michael Buchholz"],"pdf_url":"https://arxiv.org/pdf/2207.08655v1.pdf","comment":"8 pages, 11 figures, to be published in IEEE 25th International\n  Conference on Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2207.08651v1","updated":"2022-07-18T14:51:24Z","published":"2022-07-18T14:51:24Z","title":"Boolean Decision Rules for Reinforcement Learning Policy Summarisation","summary":"  Explainability of Reinforcement Learning (RL) policies remains a challenging\nresearch problem, particularly when considering RL in a safety context.\nUnderstanding the decisions and intentions of an RL policy offer avenues to\nincorporate safety into the policy by limiting undesirable actions. We propose\nthe use of a Boolean Decision Rules model to create a post-hoc rule-based\nsummary of an agent's policy. We evaluate our proposed approach using a DQN\nagent trained on an implementation of a lava gridworld and show that, given a\nhand-crafted feature representation of this gridworld, simple generalised rules\ncan be created, giving a post-hoc explainable summary of the agent's policy. We\ndiscuss possible avenues to introduce safety into a RL agent's policy by using\nrules generated by this rule-based model as constraints imposed on the agent's\npolicy, as well as discuss how creating simple rule summaries of an agent's\npolicy may help in the debugging process of RL agents.\n","authors":["James McCarthy","Rahul Nair","Elizabeth Daly","Radu Marinescu","Ivana Dusparic"],"pdf_url":"https://arxiv.org/pdf/2207.08651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08650v1","updated":"2022-07-18T14:51:23Z","published":"2022-07-18T14:51:23Z","title":"Upper Limb Movement Recognition utilising EEG and EMG Signals for\n  Rehabilitative Robotics","summary":"  Upper limb movement classification, which maps input signals to the target\nactivities, is one of the crucial areas in the control of rehabilitative\nrobotics. Classifiers are trained for the rehabilitative system to comprehend\nthe desires of the patient whose upper limbs do not function properly.\nElectromyography (EMG) signals and Electroencephalography (EEG) signals are\nused widely for upper limb movement classification. By analysing the\nclassification results of the real-time EEG and EMG signals, the system can\nunderstand the intention of the user and predict the events that one would like\nto carry out. Accordingly, it will provide external help to the user to assist\none to perform the activities. However, not all users process effective EEG and\nEMG signals due to the noisy environment. The noise in the real-time data\ncollection process contaminates the effectiveness of the data. Moreover, not\nall patients process strong EMG signals due to muscle damage and neuromuscular\ndisorder. To address these issues, we would like to propose a novel\ndecision-level multisensor fusion technique. In short, the system will\nintegrate EEG signals with EMG signals, retrieve effective information from\nboth sources to understand and predict the desire of the user, and thus provide\nassistance. By testing out the proposed technique on a publicly available\nWAY-EEG-GAL dataset, which contains EEG and EMG signals that were recorded\nsimultaneously, we manage to conclude the feasibility and effectiveness of the\nnovel system.\n","authors":["Wang Zihao"],"pdf_url":"https://arxiv.org/pdf/2207.08650v1.pdf","comment":"28 pages, 16 figures, 2 tables, Undergraduate Research Project in\n  Computing"},{"id":"http://arxiv.org/abs/2011.04178v2","updated":"2022-07-18T14:50:23Z","published":"2020-11-09T04:07:45Z","title":"PRVNet: A Novel Partially-Regularized Variational Autoencoders for\n  Massive MIMO CSI Feedback","summary":"  In a multiple-input multiple-output frequency-division duplexing (MIMO-FDD)\nsystem, the user equipment (UE) sends the downlink channel state information\n(CSI) to the base station to report link status. Due to the complexity of MIMO\nsystems, the overhead incurred in sending this information negatively affects\nthe system bandwidth. Although this problem has been widely considered in the\nliterature, prior work generally assumes an ideal feedback channel. In this\npaper, we introduce PRVNet, a neural network architecture inspired by\nvariational autoencoders (VAE) to compress the CSI matrix before sending it\nback to the base station under noisy channel conditions. Moreover, we propose a\ncustomized loss function that best suits the special characteristics of the\nproblem being addressed. We also introduce an additional regularization\nhyperparameter for the learning objective, which is crucial for achieving\ncompetitive performance. In addition, we provide an efficient way to tune this\nhyperparameter using KL-annealing. Experimental results show the proposed model\noutperforms the benchmark models including two deep learning-based models in a\nnoise-free feedback channel assumption. In addition, the proposed model\nachieves an outstanding performance under different noise levels for additive\nwhite Gaussian noise feedback channels.\n","authors":["Mostafa Hussien","Kim Khoa Nguyen","Mohamed Cheriet"],"pdf_url":"https://arxiv.org/pdf/2011.04178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08648v1","updated":"2022-07-18T14:50:20Z","published":"2022-07-18T14:50:20Z","title":"Interpolation, extrapolation, and local generalization in common neural\n  networks","summary":"  There has been a long history of works showing that neural networks have hard\ntime extrapolating beyond the training set. A recent study by Balestriero et\nal. (2021) challenges this view: defining interpolation as the state of\nbelonging to the convex hull of the training set, they show that the test set,\neither in input or neural space, cannot lie for the most part in this convex\nhull, due to the high dimensionality of the data, invoking the well known curse\nof dimensionality. Neural networks are then assumed to necessarily work in\nextrapolative mode. We here study the neural activities of the last hidden\nlayer of typical neural networks. Using an autoencoder to uncover the intrinsic\nspace underlying the neural activities, we show that this space is actually\nlow-dimensional, and that the better the model, the lower the dimensionality of\nthis intrinsic space. In this space, most samples of the test set actually lie\nin the convex hull of the training set: under the convex hull definition, the\nmodels thus happen to work in interpolation regime. Moreover, we show that\nbelonging to the convex hull does not seem to be the relevant criteria.\nDifferent measures of proximity to the training set are actually better related\nto performance accuracy. Thus, typical neural networks do seem to operate in\ninterpolation regime. Good generalization performances are linked to the\nability of a neural network to operate well in such a regime.\n","authors":["Laurent Bonnasse-Gahot"],"pdf_url":"https://arxiv.org/pdf/2207.08648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08645v1","updated":"2022-07-18T14:45:55Z","published":"2022-07-18T14:45:55Z","title":"Active Exploration for Inverse Reinforcement Learning","summary":"  Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a\nreward function from expert demonstrations. Many IRL algorithms require a known\ntransition model and sometimes even a known expert policy, or they at least\nrequire access to a generative model. However, these assumptions are too strong\nfor many real-world applications, where the environment can be accessed only\nthrough sequential interaction. We propose a novel IRL algorithm: Active\nexploration for Inverse Reinforcement Learning (AceIRL), which actively\nexplores an unknown environment and expert policy to quickly learn the expert's\nreward function and identify a good policy. AceIRL uses previous observations\nto construct confidence intervals that capture plausible reward functions and\nfind exploration policies that focus on the most informative regions of the\nenvironment. AceIRL is the first approach to active IRL with sample-complexity\nbounds that does not require a generative model of the environment. AceIRL\nmatches the sample complexity of active IRL with a generative model in the\nworst case. Additionally, we establish a problem-dependent bound that relates\nthe sample complexity of AceIRL to the suboptimality gap of a given IRL\nproblem. We empirically evaluate AceIRL in simulations and find that it\nsignificantly outperforms more naive exploration strategies.\n","authors":["David Lindner","Andreas Krause","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2207.08645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03099v2","updated":"2022-07-18T14:42:47Z","published":"2022-02-07T12:18:28Z","title":"FL_PyTorch: optimization research simulator for federated learning","summary":"  Federated Learning (FL) has emerged as a promising technique for edge devices\nto collaboratively learn a shared machine learning model while keeping training\ndata locally on the device, thereby removing the need to store and access the\nfull data in the cloud. However, FL is difficult to implement, test and deploy\nin practice considering heterogeneity in common edge device settings, making it\nfundamentally hard for researchers to efficiently prototype and test their\noptimization algorithms. In this work, our aim is to alleviate this problem by\nintroducing FL_PyTorch : a suite of open-source software written in python that\nbuilds on top of one the most popular research Deep Learning (DL) framework\nPyTorch. We built FL_PyTorch as a research simulator for FL to enable fast\ndevelopment, prototyping and experimenting with new and existing FL\noptimization algorithms. Our system supports abstractions that provide\nresearchers with a sufficient level of flexibility to experiment with existing\nand novel approaches to advance the state-of-the-art. Furthermore, FL_PyTorch\nis a simple to use console system, allows to run several clients simultaneously\nusing local CPUs or GPU(s), and even remote compute devices without the need\nfor any distributed implementation provided by the user. FL_PyTorch also offers\na Graphical User Interface. For new methods, researchers only provide the\ncentralized implementation of their algorithm. To showcase the possibilities\nand usefulness of our system, we experiment with several well-known\nstate-of-the-art FL algorithms and a few of the most common FL datasets.\n","authors":["Konstantin Burlachenko","Samuel Horváth","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2202.03099v2.pdf","comment":"DistributedML '21: Proceedings of the 2nd ACM International Workshop\n  on Distributed Machine Learning"},{"id":"http://arxiv.org/abs/2207.08640v1","updated":"2022-07-18T14:38:11Z","published":"2022-07-18T14:38:11Z","title":"Lightweight Automated Feature Monitoring for Data Streams","summary":"  Monitoring the behavior of automated real-time stream processing systems has\nbecome one of the most relevant problems in real world applications. Such\nsystems have grown in complexity relying heavily on high dimensional input\ndata, and data hungry Machine Learning (ML) algorithms. We propose a flexible\nsystem, Feature Monitoring (FM), that detects data drifts in such data sets,\nwith a small and constant memory footprint and a small computational cost in\nstreaming applications. The method is based on a multi-variate statistical test\nand is data driven by design (full reference distributions are estimated from\nthe data). It monitors all features that are used by the system, while\nproviding an interpretable features ranking whenever an alarm occurs (to aid in\nroot cause analysis). The computational and memory lightness of the system\nresults from the use of Exponential Moving Histograms. In our experimental\nstudy, we analyze the system's behavior with its parameters and, more\nimportantly, show examples where it detects problems that are not directly\nrelated to a single feature. This illustrates how FM eliminates the need to add\ncustom signals to detect specific types of problems and that monitoring the\navailable space of features is often enough.\n","authors":["João Conde","Ricardo Moreira","João Torres","Pedro Cardoso","Hugo Ferreira","Marco O. P. Sampaio","João Tiago Ascensão","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2207.08640v1.pdf","comment":"10 pages, 5 figures. AutoML, KDD22, August 14-17, 2022, Washington,\n  DC, US"},{"id":"http://arxiv.org/abs/2207.03830v2","updated":"2022-07-18T14:37:52Z","published":"2022-07-08T11:33:53Z","title":"Safe reinforcement learning for multi-energy management systems with\n  known constraint functions","summary":"  Reinforcement learning (RL) is a promising optimal control technique for\nmulti-energy management systems. It does not require a model a priori -\nreducing the upfront and ongoing project-specific engineering effort and is\ncapable of learning better representations of the underlying system dynamics.\nHowever, vanilla RL does not provide constraint satisfaction guarantees -\nresulting in various potentially unsafe interactions within its safety-critical\nenvironment. In this paper, we present two novel safe RL methods, namely\nSafeFallback and GiveSafe, where the safety constraint formulation is decoupled\nfrom the RL formulation and which provides hard-constraint satisfaction\nguarantees both during training a (near) optimal policy (which involves\nexploratory and exploitative, i.e. greedy, steps) as well as during deployment\nof any policy (e.g. random agents or offline trained RL agents). In a simulated\nmulti-energy systems case study we have shown that both methods start with a\nsignificantly higher utility (i.e. useful policy) compared to a vanilla RL\nbenchmark (94,6% and 82,8% compared to 35,5%) and that the proposed\nSafeFallback method even can outperform the vanilla RL benchmark (102,9% to\n100%). We conclude that both methods are viably safety constraint handling\ntechniques applicable beyond RL, as demonstrated with random policies while\nstill providing hard-constraint guarantees. Finally, we propose directions for\nfuture work to i.a. improve the constraint functions itself as more data\nbecomes available.\n","authors":["Glenn Ceusters","Luis Ramirez Camargo","Rüdiger Franke","Ann Nowé","Maarten Messagie"],"pdf_url":"https://arxiv.org/pdf/2207.03830v2.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.08630v1","updated":"2022-07-18T14:23:38Z","published":"2022-07-18T14:23:38Z","title":"FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity\n  in Data-Efficient GANs","summary":"  Data-Efficient GANs (DE-GANs), which aim to learn generative models with a\nlimited amount of training data, encounter several challenges for generating\nhigh-quality samples. Since data augmentation strategies have largely\nalleviated the training instability, how to further improve the generative\nperformance of DE-GANs becomes a hotspot. Recently, contrastive learning has\nshown the great potential of increasing the synthesis quality of DE-GANs, yet\nrelated principles are not well explored. In this paper, we revisit and compare\ndifferent contrastive learning strategies in DE-GANs, and identify (i) the\ncurrent bottleneck of generative performance is the discontinuity of latent\nspace; (ii) compared to other contrastive learning strategies,\nInstance-perturbation works towards latent space continuity, which brings the\nmajor improvement to DE-GANs. Based on these observations, we propose FakeCLR,\nwhich only applies contrastive learning on perturbed fake samples, and devises\nthree related training techniques: Noise-related Latent Augmentation,\nDiversity-aware Queue, and Forgetting Factor of Queue. Our experimental results\nmanifest the new state of the arts on both few-shot generation and limited-data\ngeneration. On multiple datasets, FakeCLR acquires more than 15% FID\nimprovement compared to existing DE-GANs. Code is available at\nhttps://github.com/iceli1007/FakeCLR.\n","authors":["Ziqiang Li","Chaoyue Wang","Heliang Zheng","Jing Zhang","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2207.08630v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.08629v1","updated":"2022-07-18T14:23:31Z","published":"2022-07-18T14:23:31Z","title":"Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) tend to suffer from high computation costs due\nto the exponentially increasing scale of graph data and the number of model\nparameters, which restricts their utility in practical applications. To this\nend, some recent works focus on sparsifying GNNs with the lottery ticket\nhypothesis (LTH) to reduce inference costs while maintaining performance\nlevels. However, the LTH-based methods suffer from two major drawbacks: 1) they\nrequire exhaustive and iterative training of dense models, resulting in an\nextremely large training computation cost, and 2) they only trim graph\nstructures and model parameters but ignore the node feature dimension, where\nsignificant redundancy exists. To overcome the above limitations, we propose a\ncomprehensive graph gradual pruning framework termed CGP. This is achieved by\ndesigning a during-training graph pruning paradigm to dynamically prune GNNs\nwithin one training process. Unlike LTH-based methods, the proposed CGP\napproach requires no re-training, which significantly reduces the computation\ncosts. Furthermore, we design a co-sparsifying strategy to comprehensively trim\nall three core elements of GNNs: graph structures, node features, and model\nparameters. Meanwhile, aiming at refining the pruning operation, we introduce a\nregrowth process into our CGP framework, in order to re-establish the pruned\nbut important connections. The proposed CGP is evaluated by using a node\nclassification task across 6 GNN architectures, including shallow models (GCN\nand GAT), shallow-but-deep-propagation models (SGC and APPNP), and deep models\n(GCNII and ResGCN), on a total of 14 real-world graph datasets, including\nlarge-scale graph datasets from the challenging Open Graph Benchmark.\nExperiments reveal that our proposed strategy greatly improves both training\nand inference efficiency while matching or even exceeding the accuracy of\nexisting methods.\n","authors":["Chuang Liu","Xueqi Ma","Yinbing Zhan","Liang Ding","Dapeng Tao","Bo Du","Wenbin Hu","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2207.08629v1.pdf","comment":"29 pages, 27 figures, submitting to IEEE TNNLS"},{"id":"http://arxiv.org/abs/2104.05892v3","updated":"2022-07-18T14:15:11Z","published":"2021-04-13T01:53:04Z","title":"CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge\n  Distillation","summary":"  As segmentation labels are scarce, extensive researches have been conducted\nto train segmentation networks with domain adaptation, semi-supervised or\nself-supervised learning techniques to utilize abundant unlabeled dataset.\nHowever, these approaches appear different from each other, so it is not clear\nhow these approaches can be combined for better performance. Inspired by recent\nmulti-domain image translation approaches, here we propose a novel segmentation\nframework using adaptive instance normalization (AdaIN), so that a single\ngenerator is trained to perform both domain adaptation and semi-supervised\nsegmentation tasks via knowledge distillation by simply changing task-specific\nAdaIN codes. Specifically, our framework is designed to deal with difficult\nsituations in chest X-ray radiograph (CXR) segmentation, where labels are only\navailable for normal data, but trained model should be applied to both normal\nand abnormal data. The proposed network demonstrates great generalizability\nunder domain shift and achieves the state-of-the-art performance for abnormal\nCXR segmentation.\n","authors":["Yujin Oh","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2104.05892v3.pdf","comment":"ECCV 2022 camera ready"},{"id":"http://arxiv.org/abs/2111.04090v2","updated":"2022-07-18T14:12:07Z","published":"2021-11-07T13:45:35Z","title":"Learn-Morph-Infer: a new way of solving the inverse problem for brain\n  tumor modeling","summary":"  Current treatment planning of patients diagnosed with a brain tumor, such as\nglioma, could significantly benefit by accessing the spatial distribution of\ntumor cell concentration. Existing diagnostic modalities, e.g. magnetic\nresonance imaging (MRI), contrast sufficiently well areas of high cell density.\nIn gliomas, however, they do not portray areas of low cell concentration, which\ncan often serve as a source for the secondary appearance of the tumor after\ntreatment. To estimate tumor cell densities beyond the visible boundaries of\nthe lesion, numerical simulations of tumor growth could complement imaging\ninformation by providing estimates of full spatial distributions of tumor\ncells. Over recent years a corpus of literature on medical image-based tumor\nmodeling was published. It includes different mathematical formalisms\ndescribing the forward tumor growth model. Alongside, various parametric\ninference schemes were developed to perform an efficient tumor model\npersonalization, i.e. solving the inverse problem. However, the unifying\ndrawback of all existing approaches is the time complexity of the model\npersonalization which prohibits a potential integration of the modeling into\nclinical settings. In this work, we introduce a deep learning based methodology\nfor inferring the patient-specific spatial distribution of brain tumors from\nT1Gd and FLAIR MRI medical scans. Coined as Learn-Morph-Infer the method\nachieves real-time performance in the order of minutes on widely available\nhardware and the compute time is stable across tumor models of different\ncomplexity, such as reaction-diffusion and reaction-advection-diffusion models.\nWe believe the proposed inverse solution approach not only bridges the way for\nclinical translation of brain tumor personalization but can also be adopted to\nother scientific and engineering domains.\n","authors":["Ivan Ezhov","Kevin Scibilia","Katharina Franitza","Felix Steinbauer","Suprosanna Shit","Lucas Zimmer","Jana Lipkova","Florian Kofler","Johannes Paetzold","Luca Canalini","Diana Waldmannstetter","Martin Menten","Marie Metz","Benedikt Wiestler","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2111.04090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05469v2","updated":"2022-07-18T13:59:42Z","published":"2022-03-10T16:46:05Z","title":"Prediction-Guided Distillation for Dense Object Detection","summary":"  Real-world object detection models should be cheap and accurate. Knowledge\ndistillation (KD) can boost the accuracy of a small, cheap detection model by\nleveraging useful information from a larger teacher model. However, a key\nchallenge is identifying the most informative features produced by the teacher\nfor distillation. In this work, we show that only a very small fraction of\nfeatures within a ground-truth bounding box are responsible for a teacher's\nhigh detection performance. Based on this, we propose Prediction-Guided\nDistillation (PGD), which focuses distillation on these key predictive regions\nof the teacher and yields considerable gains in performance over many existing\nKD baselines. In addition, we propose an adaptive weighting scheme over the key\nregions to smooth out their influence and achieve even better performance. Our\nproposed approach outperforms current state-of-the-art KD baselines on a\nvariety of advanced one-stage detection architectures. Specifically, on the\nCOCO dataset, our method achieves between +3.1% and +4.6% AP improvement using\nResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On\nthe CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,\nalso using these backbones. Our code is available at\nhttps://github.com/ChenhongyiYang/PGD.\n","authors":["Chenhongyi Yang","Mateusz Ochal","Amos Storkey","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2203.05469v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08603v1","updated":"2022-07-18T13:47:20Z","published":"2022-07-18T13:47:20Z","title":"Abstraction between Structural Causal Models: A Review of Definitions\n  and Properties","summary":"  Structural causal models (SCMs) are a widespread formalism to deal with\ncausal systems. A recent direction of research has considered the problem of\nrelating formally SCMs at different levels of abstraction, by defining maps\nbetween SCMs and imposing a requirement of interventional consistency. This\npaper offers a review of the solutions proposed so far, focusing on the formal\nproperties of a map between SCMs, and highlighting the different layers\n(structural, distributional) at which these properties may be enforced. This\nallows us to distinguish families of abstractions that may or may not be\npermitted by choosing to guarantee certain properties instead of others. Such\nan understanding not only allows to distinguish among proposal for causal\nabstraction with more awareness, but it also allows to tailor the definition of\nabstraction with respect to the forms of abstraction relevant to specific\napplications.\n","authors":["Fabio Massimo Zennaro"],"pdf_url":"https://arxiv.org/pdf/2207.08603v1.pdf","comment":"6 pages, 6 pages appendix, 12 figures Submitted to Causal\n  Representation Learning workshop at the 38th Conference on Uncertainty in\n  Artificial Intelligence (UAI CRL 2022)"},{"id":"http://arxiv.org/abs/2207.05315v2","updated":"2022-07-18T13:42:54Z","published":"2022-07-12T04:53:24Z","title":"CANF-VC: Conditional Augmented Normalizing Flows for Video Compression","summary":"  This paper presents an end-to-end learning-based video compression system,\ntermed CANF-VC, based on conditional augmented normalizing flows (CANF). Most\nlearned video compression systems adopt the same hybrid-based coding\narchitecture as the traditional codecs. Recent research on conditional coding\nhas shown the sub-optimality of the hybrid-based coding and opens up\nopportunities for deep generative models to take a key role in creating new\ncoding frameworks. CANF-VC represents a new attempt that leverages the\nconditional ANF to learn a video generative model for conditional inter-frame\ncoding. We choose ANF because it is a special type of generative model, which\nincludes variational autoencoder as a special case and is able to achieve\nbetter expressiveness. CANF-VC also extends the idea of conditional coding to\nmotion coding, forming a purely conditional coding framework. Extensive\nexperimental results on commonly used datasets confirm the superiority of\nCANF-VC to the state-of-the-art methods. The source code of CANF-VC is\navailable at https://github.com/NYCU-MAPL/CANF-VC.\n","authors":["Yung-Han Ho","Chih-Peng Chang","Peng-Yu Chen","Alessandro Gnutti","Wen-Hsiao Peng"],"pdf_url":"https://arxiv.org/pdf/2207.05315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08597v1","updated":"2022-07-18T13:36:20Z","published":"2022-07-18T13:36:20Z","title":"FunQG: Molecular Representation Learning Via Quotient Graphs","summary":"  Learning expressive molecular representations is crucial to facilitate the\naccurate prediction of molecular properties. Despite the significant\nadvancement of graph neural networks (GNNs) in molecular representation\nlearning, they generally face limitations such as neighbors-explosion,\nunder-reaching, over-smoothing, and over-squashing. Also, GNNs usually have\nhigh computational complexity because of the large-scale number of parameters.\nTypically, such limitations emerge or increase when facing relatively\nlarge-size graphs or using a deeper GNN model architecture. An idea to overcome\nthese problems is to simplify a molecular graph into a small, rich, and\ninformative one, which is more efficient and less challenging to train GNNs. To\nthis end, we propose a novel molecular graph coarsening framework named FunQG\nutilizing Functional groups, as influential building blocks of a molecule to\ndetermine its properties, based on a graph-theoretic concept called Quotient\nGraph. By experiments, we show that the resulting informative graphs are much\nsmaller than the molecular graphs and thus are good candidates for training\nGNNs. We apply the FunQG on popular molecular property prediction benchmarks\nand then compare the performance of a GNN architecture on the obtained datasets\nwith several state-of-the-art baselines on the original datasets. By\nexperiments, this method significantly outperforms previous baselines on\nvarious datasets, besides its dramatic reduction in the number of parameters\nand low computational complexity. Therefore, the FunQG can be used as a simple,\ncost-effective, and robust method for solving the molecular representation\nlearning problem.\n","authors":["Hossein Hajiabolhassan","Zahra Taheri","Ali Hojatnia","Yavar Taheri Yeganeh"],"pdf_url":"https://arxiv.org/pdf/2207.08597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08592v1","updated":"2022-07-18T13:32:20Z","published":"2022-07-18T13:32:20Z","title":"Symmetrized Robust Procrustes: Constant-Factor Approximation and Exact\n  Recovery","summary":"  The classical $\\textit{Procrustes}$ problem is to find a rigid motion\n(orthogonal transformation and translation) that best aligns two given\npoint-sets in the least-squares sense. The $\\textit{Robust Procrustes}$ problem\nis an important variant, in which a power-1 objective is used instead of least\nsquares to improve robustness to outliers. While the optimal solution of the\nleast-squares problem can be easily computed in closed form, dating back to\nSch\\\"onemann (1966), no such solution is known for the power-1 problem. In this\npaper we propose a novel convex relaxation for the Robust Procrustes problem.\nOur relaxation enjoys several theoretical and practical advantages:\nTheoretically, we prove that our method provides a $\\sqrt{2}$-factor\napproximation to the Robust Procrustes problem, and that, under appropriate\nassumptions, it exactly recovers the true rigid motion from point\ncorrespondences contaminated by outliers. In practice, we find in numerical\nexperiments on both synthetic and real robust Procrustes problems, that our\nmethod performs similarly to the standard Iteratively Reweighted Least Squares\n(IRLS). However the convexity of our algorithm allows incorporating additional\nconvex penalties, which are not readily amenable to IRLS. This turns out to be\na substantial advantage, leading to improved results in high-dimensional\nproblems, including non-rigid shape alignment and semi-supervised interlingual\nword translation.\n","authors":["Tal Amir","Shahar Kovalsky","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2207.08592v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2109.00783v3","updated":"2022-07-18T13:20:13Z","published":"2021-09-02T08:45:53Z","title":"Computer Vision Self-supervised Learning Methods on Time Series","summary":"  Self-supervised learning (SSL) has had great success in both computer vision\nand natural language processing. These approaches often rely on cleverly\ncrafted loss functions and training setups to avoid feature collapse. In this\nstudy, the effectiveness of mainstream SSL frameworks from computer vision and\nsome SSL frameworks for time series are evaluated on the UCR, UEA and PTB-XL\ndatasets, and we show that computer vision SSL frameworks can be effective for\ntime series. In addition, we propose a new method that improves on the recently\nproposed VICReg method. Our method improves on a \\textit{covariance} term\nproposed in VICReg, and in addition we augment the head of the architecture by\nan IterNorm layer that accelerates the convergence of the model.\n","authors":["Daesoo Lee","Erlend Aune"],"pdf_url":"https://arxiv.org/pdf/2109.00783v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08581v1","updated":"2022-07-18T13:18:34Z","published":"2022-07-18T13:18:34Z","title":"Study of the performance and scalablity of federated learning for\n  medical imaging with intermittent clients","summary":"  Federated learning is a data decentralization privacy-preserving technique\nused to perform machine or deep learning in a secure way. In this paper we\npresent theoretical aspects about federated learning, such as the presentation\nof an aggregation operator, different types of federated learning, and issues\nto be taken into account in relation to the distribution of data from the\nclients, together with the exhaustive analysis of a use case where the number\nof clients varies. Specifically, a use case of medical image analysis is\nproposed, using chest X-ray images obtained from an open data repository. In\naddition to the advantages related to privacy, improvements in predictions (in\nterms of accuracy and area under the curve) and reduction of execution times\nwill be studied with respect to the classical case (the centralized approach).\nDifferent clients will be simulated from the training data, selected in an\nunbalanced manner, i.e., they do not all have the same number of data. The\nresults of considering three or ten clients are exposed and compared between\nthem and against the centralized case. Two approaches to follow will be\nanalyzed in the case of intermittent clients, as in a real scenario some\nclients may leave the training, and some new ones may enter the training. The\nevolution of the results for the test set in terms of accuracy, area under the\ncurve and execution time is shown as the number of clients into which the\noriginal data is divided increases. Finally, improvements and future work in\nthe field are proposed.\n","authors":["Judith Sáinz-Pardo Díaz","Álvaro López García"],"pdf_url":"https://arxiv.org/pdf/2207.08581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08574v1","updated":"2022-07-18T12:58:01Z","published":"2022-07-18T12:58:01Z","title":"ManiFeSt: Manifold-based Feature Selection for Small Data Sets","summary":"  In this paper, we present a new method for few-sample supervised feature\nselection (FS). Our method first learns the manifold of the feature space of\neach class using kernels capturing multi-feature associations. Then, based on\nRiemannian geometry, a composite kernel is computed, extracting the differences\nbetween the learned feature associations. Finally, a FS score based on spectral\nanalysis is proposed. Considering multi-feature associations makes our method\nmultivariate by design. This in turn allows for the extraction of the hidden\nmanifold underlying the features and avoids overfitting, facilitating\nfew-sample FS. We showcase the efficacy of our method on illustrative examples\nand several benchmarks, where our method demonstrates higher accuracy in\nselecting the informative features compared to competing methods. In addition,\nwe show that our FS leads to improved classification and better generalization\nwhen applied to test data.\n","authors":["David Cohen","Tal Shnitzer","Yuval Kluger","Ronen Talmon"],"pdf_url":"https://arxiv.org/pdf/2207.08574v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2207.08562v1","updated":"2022-07-18T12:44:59Z","published":"2022-07-18T12:44:59Z","title":"DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link\n  Prediction and Entity Typing","summary":"  In the field of representation learning on knowledge graphs (KGs), a\nhyper-relational fact consists of a main triple and several auxiliary attribute\nvalue descriptions, which is considered to be more comprehensive and specific\nthan a triple-based fact. However, the existing hyper-relational KG embedding\nmethods in a single view are limited in application due to weakening the\nhierarchical structure representing the affiliation between entities. To break\nthis limitation, we propose a dual-view hyper-relational KG (DH-KG) structure\nwhich contains a hyper-relational instance view for entities and a\nhyper-relational ontology view for concepts abstracted hierarchically from\nentities to jointly model hyper-relational and hierarchical information. In\nthis paper, we first define link prediction and entity typing tasks on DH-KG\nand construct two DH-KG datasets, JW44K-6K extracted from Wikidata and HTDM\nbased on medical data. Furthermore, We propose a DH-KG embedding model DHGE,\nbased on GRAN encoder, HGNN, and joint learning. Experimental results show that\nDHGE outperforms baseline models on DH-KG. We also provide an example of the\napplication of this technology in the field of hypertension medication. Our\nmodel and datasets are publicly available.\n","authors":["Haoran Luo","Haihong E","Ling Tan","Xueyuan Lin","Gengxian Zhou","Jundi Li","Tianyu Yao","Kaiyang Wan"],"pdf_url":"https://arxiv.org/pdf/2207.08562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08548v1","updated":"2022-07-18T12:12:24Z","published":"2022-07-18T12:12:24Z","title":"GATE: Gated Additive Tree Ensemble for Tabular Classification and\n  Regression","summary":"  We propose a novel high-performance, parameter and computationally efficient\ndeep learning architecture for tabular data, Gated Additive Tree\nEnsemble(GATE). GATE uses a gating mechanism, inspired from GRU, as a feature\nrepresentation learning unit with an in-built feature selection mechanism. We\ncombine it with an ensemble of differentiable, non-linear decision trees,\nre-weighted with simple self-attention to predict our desired output. We\ndemonstrate that GATE is a competitive alternative to SOTA approaches like\nGBDTs, NODE, FT Transformers, etc. by experiments on several public datasets\n(both classification and regression). The code will be uploaded as soon as the\npaper comes out of review.\n","authors":["Manu Joseph","Harsh Raj"],"pdf_url":"https://arxiv.org/pdf/2207.08548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08544v1","updated":"2022-07-18T12:10:27Z","published":"2022-07-18T12:10:27Z","title":"Hardware-agnostic Computation for Large-scale Knowledge Graph Embeddings","summary":"  Knowledge graph embedding research has mainly focused on learning continuous\nrepresentations of knowledge graphs towards the link prediction problem.\nRecently developed frameworks can be effectively applied in research related\napplications. Yet, these frameworks do not fulfill many requirements of\nreal-world applications. As the size of the knowledge graph grows, moving\ncomputation from a commodity computer to a cluster of computers in these\nframeworks becomes more challenging. Finding suitable hyperparameter settings\nw.r.t. time and computational budgets are left to practitioners. In addition,\nthe continual learning aspect in knowledge graph embedding frameworks is often\nignored, although continual learning plays an important role in many real-world\n(deep) learning-driven applications. Arguably, these limitations explain the\nlack of publicly available knowledge graph embedding models for large knowledge\ngraphs. We developed a framework based on the frameworks DASK, Pytorch\nLightning and Hugging Face to compute embeddings for large-scale knowledge\ngraphs in a hardware-agnostic manner, which is able to address real-world\nchallenges pertaining to the scale of real application. We provide an\nopen-source version of our framework along with a hub of pre-trained models\nhaving more than 11.4 B parameters.\n","authors":["Caglar Demir","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2207.08544v1.pdf","comment":"accepted in Software Impacts journal"},{"id":"http://arxiv.org/abs/2207.08540v1","updated":"2022-07-18T12:03:26Z","published":"2022-07-18T12:03:26Z","title":"Multi-block-Single-probe Variance Reduced Estimator for Coupled\n  Compositional Optimization","summary":"  Variance reduction techniques such as SPIDER/SARAH/STORM have been\nextensively studied to improve the convergence rates of stochastic non-convex\noptimization, which usually maintain and update a sequence of estimators for a\nsingle function across iterations. {\\it What if we need to track multiple\nfunctional mappings across iterations but only with access to stochastic\nsamples of $\\mathcal{O}(1)$ functional mappings at each iteration?} There is an\nimportant application in solving an emerging family of coupled compositional\noptimization problems in the form of $\\sum_{i=1}^m f_i(g_i(\\mathbf{w}))$, where\n$g_i$ is accessible through a stochastic oracle. The key issue is to track and\nestimate a sequence of $\\mathbf g(\\mathbf{w})=(g_1(\\mathbf{w}), \\ldots,\ng_m(\\mathbf{w}))$ across iterations, where $\\mathbf g(\\mathbf{w})$ has $m$\nblocks and it is only allowed to probe $\\mathcal{O}(1)$ blocks to attain their\nstochastic values and Jacobians. To improve the complexity for solving these\nproblems, we propose a novel stochastic method named Multi-block-Single-probe\nVariance Reduced (MSVR) estimator to track the sequence of $\\mathbf\ng(\\mathbf{w})$. It is inspired by STORM but introduces a customized error\ncorrection term to alleviate the noise not only in stochastic samples for the\nselected blocks but also in those blocks that are not sampled. With the help of\nthe MSVR estimator, we develop several algorithms for solving the\naforementioned compositional problems with improved complexities across a\nspectrum of settings with non-convex/convex/strongly convex objectives. Our\nresults improve upon prior ones in several aspects, including the order of\nsample complexities and dependence on the strong convexity parameter. Empirical\nstudies on multi-task deep AUC maximization demonstrate the better performance\nof using the new estimator.\n","authors":["Wei Jiang","Gang Li","Yibo Wang","Lijun Zhang","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2207.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.05041v2","updated":"2022-07-18T11:03:48Z","published":"2022-02-10T13:57:20Z","title":"On characterizations of learnability with computable learners","summary":"  We study computable PAC (CPAC) learning as introduced by Agarwal et al.\n(2020). First, we consider the main open question of finding characterizations\nof proper and improper CPAC learning. We give a characterization of a closely\nrelated notion of strong CPAC learning, and provide a negative answer to the\nCOLT open problem posed by Agarwal et al. (2021) whether all decidably\nrepresentable VC classes are improperly CPAC learnable. Second, we consider\nundecidability of (computable) PAC learnability. We give a simple general\nargument to exhibit such ndecidability, and initiate a study of the\narithmetical complexity of learnability. We briefly discuss the relation to the\nundecidability result of Ben-David et al. (2019), that motivated the work of\nAgarwal et al.\n","authors":["Tom F. Sterkenburg"],"pdf_url":"https://arxiv.org/pdf/2202.05041v2.pdf","comment":"Final version, as accepted for COLT 2022. Changes w.r.t. previous\n  arXiv version: In response to reviewer comments, major revision of the\n  discussion in Section 3.2, incl. reformulation of Question 2 (that in its\n  original form had an easy answer) and retraction of Definition 6 and\n  Proposition 2 (that lost their relevance in the revised discussion). Minor\n  textual revisions elsewhere"},{"id":"http://arxiv.org/abs/2207.08501v1","updated":"2022-07-18T10:44:02Z","published":"2022-07-18T10:44:02Z","title":"Explainable Deep Belief Network based Auto encoder using novel Extended\n  Garson Algorithm","summary":"  The most difficult task in machine learning is to interpret trained shallow\nneural networks. Deep neural networks (DNNs) provide impressive results on a\nlarger number of tasks, but it is generally still unclear how decisions are\nmade by such a trained deep neural network. Providing feature importance is the\nmost important and popular interpretation technique used in shallow and deep\nneural networks. In this paper, we develop an algorithm extending the idea of\nGarson Algorithm to explain Deep Belief Network based Auto-encoder (DBNA). It\nis used to determine the contribution of each input feature in the DBN. It can\nbe used for any kind of neural network with many hidden layers. The\neffectiveness of this method is tested on both classification and regression\ndatasets taken from literature. Important features identified by this method\nare compared against those obtained by Wald chi square (\\c{hi}2). For 2 out of\n4 classification datasets and 2 out of 5 regression datasets, our proposed\nmethodology resulted in the identification of better-quality features leading\nto statistically more significant results vis-\\`a-vis Wald \\c{hi}2.\n","authors":["Satyam Kumar","Vadlamani Ravi"],"pdf_url":"https://arxiv.org/pdf/2207.08501v1.pdf","comment":"37 pages, 28 figures, 21 tables"},{"id":"http://arxiv.org/abs/2110.07238v2","updated":"2022-07-18T10:36:11Z","published":"2021-10-14T09:07:42Z","title":"On the difficulty of learning chaotic dynamics with RNNs","summary":"  Recurrent neural networks (RNNs) are wide-spread machine learning tools for\nmodeling sequential and time series data. They are notoriously hard to train\nbecause their loss gradients backpropagated in time tend to saturate or diverge\nduring training. This is known as the exploding and vanishing gradient problem.\nPrevious solutions to this issue either built on rather complicated,\npurpose-engineered architectures with gated memory buffers, or - more recently\n- imposed constraints that ensure convergence to a fixed point or restrict (the\neigenspectrum of) the recurrence matrix. Such constraints, however, convey\nsevere limitations on the expressivity of the RNN. Essential intrinsic dynamics\nsuch as multistability or chaos are disabled. This is inherently at disaccord\nwith the chaotic nature of many, if not most, time series encountered in nature\nand society. It is particularly problematic in scientific applications where\none aims to reconstruct the underlying dynamical system. Here we offer a\ncomprehensive theoretical treatment of this problem by relating the loss\ngradients during RNN training to the Lyapunov spectrum of RNN-generated orbits.\nWe mathematically prove that RNNs producing stable equilibrium or cyclic\nbehavior have bounded gradients, whereas the gradients of RNNs with chaotic\ndynamics always diverge. Based on these analyses and insights we suggest ways\nof how to optimize the training process on chaotic data according to the\nsystem's Lyapunov spectrum, regardless of the employed RNN architecture.\n","authors":["Jonas M. Mikhaeil","Zahra Monfared","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2110.07238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08486v1","updated":"2022-07-18T10:10:45Z","published":"2022-07-18T10:10:45Z","title":"Detection of Poisoning Attacks with Anomaly Detection in Federated\n  Learning for Healthcare Applications: A Machine Learning Approach","summary":"  The application of Federated Learning (FL) is steadily increasing, especially\nin privacy-aware applications, such as healthcare. However, its applications\nhave been limited by security concerns due to various adversarial attacks, such\nas poisoning attacks (model and data poisoning). Such attacks attempt to poison\nthe local models and data to manipulate the global models in order to obtain\nundue benefits and malicious use. Traditional methods of data auditing to\nmitigate poisoning attacks find their limited applications in FL because the\nedge devices never share their raw data directly due to privacy concerns, and\nare globally distributed with no insight into their training data. Thereafter,\nit is challenging to develop appropriate strategies to address such attacks and\nminimize their impact on the global model in federated learning. In order to\naddress such challenges in FL, we proposed a novel framework to detect\npoisoning attacks using deep neural networks and support vector machines, in\nthe form of anomaly without acquiring any direct access or information about\nthe underlying training data of local edge devices. We illustrate and evaluate\nthe proposed framework using different state of art poisoning attacks for two\ndifferent healthcare applications: Electrocardiograph classification and human\nactivity recognition. Our experimental analysis shows that the proposed method\ncan efficiently detect poisoning attacks and can remove the identified poisoned\nupdated from the global aggregation. Thereafter can increase the performance of\nthe federated global.\n","authors":["Ali Raza","Shujun Li","Kim-Phuc Tran","Ludovic Koehl"],"pdf_url":"https://arxiv.org/pdf/2207.08486v1.pdf","comment":"We will updated this article soon"},{"id":"http://arxiv.org/abs/2207.08483v1","updated":"2022-07-18T10:07:13Z","published":"2022-07-18T10:07:13Z","title":"wPINNs: Weak Physics informed neural networks for approximating entropy\n  solutions of hyperbolic conservation laws","summary":"  Physics informed neural networks (PINNs) require regularity of solutions of\nthe underlying PDE to guarantee accurate approximation. Consequently, they may\nfail at approximating discontinuous solutions of PDEs such as nonlinear\nhyperbolic equations. To ameliorate this, we propose a novel variant of PINNs,\ntermed as weak PINNs (wPINNs) for accurate approximation of entropy solutions\nof scalar conservation laws. wPINNs are based on approximating the solution of\na min-max optimization problem for a residual, defined in terms of Kruzkhov\nentropies, to determine parameters for the neural networks approximating the\nentropy solution as well as test functions. We prove rigorous bounds on the\nerror incurred by wPINNs and illustrate their performance through numerical\nexperiments to demonstrate that wPINNs can approximate entropy solutions\naccurately.\n","authors":["Tim De Ryck","Siddhartha Mishra","Roberto Molinaro"],"pdf_url":"https://arxiv.org/pdf/2207.08483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08476v1","updated":"2022-07-18T09:57:43Z","published":"2022-07-18T09:57:43Z","title":"High-Order Conditional Mutual Information Maximization for dealing with\n  High-Order Dependencies in Feature Selection","summary":"  This paper presents a novel feature selection method based on the conditional\nmutual information (CMI). The proposed High Order Conditional Mutual\nInformation Maximization (HOCMIM) incorporates high order dependencies into the\nfeature selection procedure and has a straightforward interpretation due to its\nbottom-up derivation. The HOCMIM is derived from the CMI's chain expansion and\nexpressed as a maximization optimization problem. The maximization problem is\nsolved using a greedy search procedure, which speeds up the entire feature\nselection process. The experiments are run on a set of benchmark datasets (20\nin total). The HOCMIM is compared with eighteen state-of-the-art feature\nselection algorithms, from the results of two supervised learning classifiers\n(Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best\nresults in terms of accuracy and shows to be faster than high order feature\nselection counterparts.\n","authors":["Francisco Souza","Cristiano Premebida","Rui Araújo"],"pdf_url":"https://arxiv.org/pdf/2207.08476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04827v2","updated":"2022-07-18T09:48:46Z","published":"2022-07-11T12:51:27Z","title":"Multiple-Modality Associative Memory: a framework for Learning","summary":"  Drawing from memory the face of a friend you have not seen in years is a\ndifficult task. However, if you happen to cross paths, you would easily\nrecognize each other. The biological memory is equipped with an impressive\ncompression algorithm that can store the essential, and then infer the details\nto match perception. Willshaw's model of Associative memory is a likely\ncandidate for a computational model of this brain function, but its application\non real-world data is hindered by the so-called Sparse Coding Problem. Due to a\nrecently proposed sparse encoding prescription [31], which maps visual patterns\ninto binary feature maps, we were able to analyze the behavior of the Willshaw\nNetwork (WN) on real-world data and gain key insights into the strengths of the\nmodel. To further enhance the capabilities of the WN, we propose the\nMultiple-Modality architecture. In this new setting, the memory stores several\nmodalities (e.g., visual, or textual) simultaneously. After training, the model\ncan be used to infer missing modalities when just a subset is perceived, thus\nserving as a flexible framework for learning tasks. We evaluated the model on\nthe MNIST dataset. By storing both the images and labels as modalities, we were\nable to successfully perform pattern completion, classification, and generation\nwith a single model.\n","authors":["Rodrigo Simas","Luis Sa-Couto","Andreas Wichert"],"pdf_url":"https://arxiv.org/pdf/2207.04827v2.pdf","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2207.07563v2","updated":"2022-07-18T09:48:22Z","published":"2022-07-14T12:22:51Z","title":"QSAN: A Near-term Achievable Quantum Self-Attention Network","summary":"  Self-Attention Mechanism (SAM), an important component of machine learning,\nhas been relatively little investigated in the field of quantum machine\nlearning. Inspired by the Variational Quantum Algorithm (VQA) framework and\nSAM, Quantum Self-Attention Network (QSAN) that can be implemented on a\nnear-term quantum computer is proposed.Theoretically, Quantum Self-Attention\nMechanism (QSAM), a novel interpretation of SAM with linearization and\nlogicalization is defined, in which Quantum Logical Similarity (QLS) is\npresented firstly to impel a better execution of QSAM on quantum computers\nsince inner product operations are replaced with logical operations, and then a\nQLS-based density matrix named Quantum Bit Self-Attention Score Matrix (QBSASM)\nis deduced for representing the output distribution effectively. Moreover, QSAN\nis implemented based on the QSAM framework and its practical quantum circuit is\ndesigned with 5 modules. Finally, QSAN is tested on a quantum computer with a\nsmall sample of data. The experimental results show that QSAN can converge\nfaster in the quantum natural gradient descent framework and reassign weights\nto word vectors. The above illustrates that QSAN is able to provide attention\nwith quantum characteristics faster, laying the foundation for Quantum Natural\nLanguage Processing (QNLP).\n","authors":["Ren-xin Zhao","Jinjing Shi","Shichao Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.07563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13293v3","updated":"2022-07-18T09:46:09Z","published":"2022-04-28T05:34:50Z","title":"Model Selection, Adaptation, and Combination for Transfer Learning in\n  Wind and Photovoltaic Power Forecasts","summary":"  There is recent interest in using model hubs, a collection of pre-trained\nmodels, in computer vision tasks. To utilize the model hub, we first select a\nsource model and then adapt the model for the target to compensate for\ndifferences. While there is yet limited research on model selection and\nadaption for computer vision tasks, this holds even more for the field of\nrenewable power. At the same time, it is a crucial challenge to provide\nforecasts for the increasing demand for power forecasts based on weather\nfeatures from a numerical weather prediction. We close these gaps by conducting\nthe first thorough experiment for model selection and adaptation for transfer\nlearning in renewable power forecast, adopting recent results from the field of\ncomputer vision on 667 wind and photovoltaic parks. To the best of our\nknowledge, this makes it the most extensive study for transfer learning in\nrenewable power forecasts reducing the computational effort and improving the\nforecast error. Therefore, we adopt source models based on target data from\ndifferent seasons and limit the amount of training data. As an extension of the\ncurrent state of the art, we utilize a Bayesian linear regression for\nforecasting the response based on features extracted from a neural network.\nThis approach outperforms the baseline with only seven days of training data.\nWe further show how combining multiple models through ensembles can\nsignificantly improve the model selection and adaptation approach.\n","authors":["Jens Schreiber","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2204.13293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.03748v4","updated":"2022-07-18T09:43:28Z","published":"2021-09-08T16:11:31Z","title":"A robust approach for deep neural networks in presence of label noise:\n  relabelling and filtering instances during training","summary":"  Deep learning has outperformed other machine learning algorithms in a variety\nof tasks, and as a result, it is widely used. However, like other machine\nlearning algorithms, deep learning, and convolutional neural networks (CNNs) in\nparticular, perform worse when the data sets present label noise. Therefore, it\nis important to develop algorithms that help the training of deep networks and\ntheir generalization to noise-free test sets. In this paper, we propose a\nrobust training strategy against label noise, called RAFNI, that can be used\nwith any CNN. This algorithm filters and relabels instances of the training set\nbased on the predictions and their probabilities made by the backbone neural\nnetwork during the training process. That way, this algorithm improves the\ngeneralization ability of the CNN on its own. RAFNI consists of three\nmechanisms: two mechanisms that filter instances and one mechanism that\nrelabels instances. In addition, it does not suppose that the noise rate is\nknown nor does it need to be estimated. We evaluated our algorithm using\ndifferent data sets of several sizes and characteristics. We also compared it\nwith state-of-the-art models using the CIFAR10 and CIFAR100 benchmarks under\ndifferent types and rates of label noise and found that RAFNI achieves better\nresults in most cases.\n","authors":["Anabel Gómez-Ríos","Julián Luengo","Francisco Herrera"],"pdf_url":"https://arxiv.org/pdf/2109.03748v4.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.05466v3","updated":"2022-07-18T09:34:24Z","published":"2022-07-12T11:25:53Z","title":"A Benchmark dataset for predictive maintenance","summary":"  The paper describes the MetroPT data set, an outcome of a eXplainable\nPredictive Maintenance (XPM) project with an urban metro public transportation\nservice in Porto, Portugal. The data was collected in 2022 that aimed to\nevaluate machine learning methods for online anomaly detection and failure\nprediction. By capturing several analogic sensor signals (pressure,\ntemperature, current consumption), digital signals (control signals, discrete\nsignals), and GPS information (latitude, longitude, and speed), we provide a\ndataset that can be easily used to evaluate online machine learning methods.\nThis dataset contains some interesting characteristics and can be a good\nbenchmark for predictive maintenance models.\n","authors":["Bruno Veloso","João Gama","Rita P. Ribeiro","Pedro M. Pereira"],"pdf_url":"https://arxiv.org/pdf/2207.05466v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08804v3","updated":"2022-07-18T09:27:50Z","published":"2022-06-17T14:34:35Z","title":"Truly Unordered Probabilistic Rule Sets for Multi-class Classification","summary":"  Rule set learning has long been studied and has recently been frequently\nrevisited due to the need for interpretable models. Still, existing methods\nhave several shortcomings: 1) most recent methods require a binary feature\nmatrix as input, while learning rules directly from numeric variables is\nunderstudied; 2) existing methods impose orders among rules, either explicitly\nor implicitly, which harms interpretability; and 3) currently no method exists\nfor learning probabilistic rule sets for multi-class target variables (there is\nonly one for probabilistic rule lists).\n  We propose TURS, for Truly Unordered Rule Sets, which addresses these\nshortcomings. We first formalize the problem of learning truly unordered rule\nsets. To resolve conflicts caused by overlapping rules, i.e., instances covered\nby multiple rules, we propose a novel approach that exploits the probabilistic\nproperties of our rule sets. We next develop a two-phase heuristic algorithm\nthat learns rule sets by carefully growing rules. An important innovation is\nthat we use a surrogate score to take the global potential of the rule set into\naccount when learning a local rule.\n  Finally, we empirically demonstrate that, compared to non-probabilistic and\n(explicitly or implicitly) ordered state-of-the-art methods, our method learns\nrule sets that not only have better interpretability but also better predictive\nperformance.\n","authors":["Lincen Yang","Matthijs van Leeuwen"],"pdf_url":"https://arxiv.org/pdf/2206.08804v3.pdf","comment":"Camera ready version for ECMLPKDD 2022, with Supplementary Materials"},{"id":"http://arxiv.org/abs/2207.08457v1","updated":"2022-07-18T09:26:07Z","published":"2022-07-18T09:26:07Z","title":"A Meta-Reinforcement Learning Algorithm for Causal Discovery","summary":"  Causal discovery is a major task with the utmost importance for machine\nlearning since causal structures can enable models to go beyond pure\ncorrelation-based inference and significantly boost their performance. However,\nfinding causal structures from data poses a significant challenge both in\ncomputational effort and accuracy, let alone its impossibility without\ninterventions in general. In this paper, we develop a meta-reinforcement\nlearning algorithm that performs causal discovery by learning to perform\ninterventions such that it can construct an explicit causal graph. Apart from\nbeing useful for possible downstream applications, the estimated causal graph\nalso provides an explanation for the data-generating process. In this article,\nwe show that our algorithm estimates a good graph compared to the SOTA\napproaches, even in environments whose underlying causal structure is\npreviously unseen. Further, we make an ablation study that shows how learning\ninterventions contribute to the overall performance of our approach. We\nconclude that interventions indeed help boost the performance, efficiently\nyielding an accurate estimate of the causal structure of a possibly unseen\nenvironment.\n","authors":["Andreas Sauter","Erman Acar","Vincent François-Lavet"],"pdf_url":"https://arxiv.org/pdf/2207.08457v1.pdf","comment":"Accepted submission to CRL@UAI 22"},{"id":"http://arxiv.org/abs/2204.06718v9","updated":"2022-07-18T09:06:09Z","published":"2022-04-14T03:08:40Z","title":"Learning Convolutional Neural Networks in the Frequency Domain","summary":"  Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n","authors":["Hengyue Pan","Yixin Chen","Xin Niu","Wenbo Zhou","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2204.06718v9.pdf","comment":"Submitted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2107.00501v2","updated":"2022-07-18T08:56:42Z","published":"2021-07-01T14:45:01Z","title":"Secure Quantized Training for Deep Learning","summary":"  We implement training of neural networks in secure multi-party computation\n(MPC) using quantization commonly used in said setting. We are the first to\npresent an MNIST classifier purely trained in MPC that comes within 0.2 percent\nof the accuracy of the same convolutional neural network trained via plaintext\ncomputation. More concretely, we have trained a network with two convolutional\nand two dense layers to 99.2% accuracy in 3.5 hours (under one hour for 99%\naccuracy). We have also implemented AlexNet for CIFAR-10, which converges in a\nfew hours. We develop novel protocols for exponentiation and inverse square\nroot. Finally, we present experiments in a range of MPC security models for up\nto ten parties, both with honest and dishonest majority as well as semi-honest\nand malicious security.\n","authors":["Marcel Keller","Ke Sun"],"pdf_url":"https://arxiv.org/pdf/2107.00501v2.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2207.08435v1","updated":"2022-07-18T08:41:00Z","published":"2022-07-18T08:41:00Z","title":"Robust Simulation-Based Inference in Cosmology with Bayesian Neural\n  Networks","summary":"  Simulation-based inference (SBI) is rapidly establishing itself as a standard\nmachine learning technique for analyzing data in cosmological surveys. Despite\ncontinual improvements to the quality of density estimation by learned models,\napplications of such techniques to real data are entirely reliant on the\ngeneralization power of neural networks far outside the training distribution,\nwhich is mostly unconstrained. Due to the imperfections in scientist-created\nsimulations, and the large computational expense of generating all possible\nparameter combinations, SBI methods in cosmology are vulnerable to such\ngeneralization issues. Here, we discuss the effects of both issues, and show\nhow using a Bayesian neural network framework for training SBI can mitigate\nbiases, and result in more reliable inference outside the training set. We\nintroduce cosmoSWAG, the first application of Stochastic Weight Averaging to\ncosmology, and apply it to SBI trained for inference on the cosmic microwave\nbackground.\n","authors":["Pablo Lemos","Miles Cranmer","Muntazir Abidi","ChangHoon Hahn","Michael Eickenberg","Elena Massara","David Yallup","Shirley Ho"],"pdf_url":"https://arxiv.org/pdf/2207.08435v1.pdf","comment":"5 pages, 3 figures. Accepted at the ML4Astro Machine Learning for\n  Astrophysics Workshop at the Thirty-ninth International Conference on Machine\n  Learning (ICML 2022)"},{"id":"http://arxiv.org/abs/2202.06484v4","updated":"2022-07-18T08:33:16Z","published":"2022-02-14T05:17:38Z","title":"D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic\n  Segmentation","summary":"  In the field of domain adaptation, a trade-off exists between the model\nperformance and the number of target domain annotations. Active learning,\nmaximizing model performance with few informative labeled data, comes in handy\nfor such a scenario. In this work, we present D2ADA, a general active domain\nadaptation framework for semantic segmentation. To adapt the model to the\ntarget domain with minimum queried labels, we propose acquiring labels of the\nsamples with high probability density in the target domain yet with low\nprobability density in the source domain, complementary to the existing source\ndomain labeled data. To further facilitate labeling efficiency, we design a\ndynamic scheduling policy to adjust the labeling budgets between domain\nexploration and model uncertainty over time. Extensive experiments show that\nour method outperforms existing active learning and domain adaptation baselines\non two benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than\n5% target domain annotations, our method reaches comparable results with that\nof full supervision. Our code is publicly available at\nhttps://github.com/tsunghan-wu/D2ADA.\n","authors":["Tsung-Han Wu","Yi-Syuan Liou","Shao-Ji Yuan","Hsin-Ying Lee","Tung-I Chen","Kuan-Chih Huang","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2202.06484v4.pdf","comment":"Accepted by ECCV 2022. The code is available at\n  https://github.com/tsunghan-wu/D2ADA"},{"id":"http://arxiv.org/abs/2207.08426v1","updated":"2022-07-18T08:21:39Z","published":"2022-07-18T08:21:39Z","title":"Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum\n  Extensive Form Games","summary":"  The study of learning in games has thus far focused primarily on normal form\ngames. In contrast, our understanding of learning in extensive form games\n(EFGs) and particularly in EFGs with many agents lags far behind, despite them\nbeing closer in nature to many real world applications. We consider the natural\nclass of Network Zero-Sum Extensive Form Games, which combines the global\nzero-sum property of agent payoffs, the efficient representation of graphical\ngames as well the expressive power of EFGs. We examine the convergence\nproperties of Optimistic Gradient Ascent (OGA) in these games. We prove that\nthe time-average behavior of such online learning dynamics exhibits $O(1/T)$\nrate convergence to the set of Nash Equilibria. Moreover, we show that the\nday-to-day behavior also converges to Nash with rate $O(c^{-t})$ for some\ngame-dependent constant $c>0$.\n","authors":["Georgios Piliouras","Lillian Ratliff","Ryann Sim","Stratis Skoulakis"],"pdf_url":"https://arxiv.org/pdf/2207.08426v1.pdf","comment":"To appear in SAGT 2022"},{"id":"http://arxiv.org/abs/2201.03967v3","updated":"2022-07-18T07:50:21Z","published":"2022-01-10T02:11:25Z","title":"Emotion Intensity and its Control for Emotional Voice Conversion","summary":"  Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n","authors":["Kun Zhou","Berrak Sisman","Rajib Rana","Björn W. Schuller","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2201.03967v3.pdf","comment":"Accepted by IEEE Transactions on Affective Computing"},{"id":"http://arxiv.org/abs/2207.08414v1","updated":"2022-07-18T07:47:36Z","published":"2022-07-18T07:47:36Z","title":"Outlier Explanation via Sum-Product Networks","summary":"  Outlier explanation is the task of identifying a set of features that\ndistinguish a sample from normal data, which is important for downstream\n(human) decision-making. Existing methods are based on beam search in the space\nof feature subsets. They quickly becomes computationally expensive, as they\nrequire to run an outlier detection algorithm from scratch for each feature\nsubset. To alleviate this problem, we propose a novel outlier explanation\nalgorithm based on Sum-Product Networks (SPNs), a class of probabilistic\ncircuits. Our approach leverages the tractability of marginal inference in SPNs\nto compute outlier scores in feature subsets. By using SPNs, it becomes\nfeasible to perform backwards elimination instead of the usual forward beam\nsearch, which is less susceptible to missing relevant features in an\nexplanation, especially when the number of features is large. We empirically\nshow that our approach achieves state-of-the-art results for outlier\nexplanation, outperforming recent search-based as well as deep learning-based\nexplanation methods\n","authors":["Stefan Lüdtke","Christian Bartelt","Heiner Stuckenschmidt"],"pdf_url":"https://arxiv.org/pdf/2207.08414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.12792v2","updated":"2022-07-18T07:27:51Z","published":"2021-02-25T11:28:46Z","title":"Mixed Variable Bayesian Optimization with Frequency Modulated Kernels","summary":"  The sample efficiency of Bayesian optimization(BO) is often boosted by\nGaussian Process(GP) surrogate models. However, on mixed variable spaces,\nsurrogate models other than GPs are prevalent, mainly due to the lack of\nkernels which can model complex dependencies across different types of\nvariables. In this paper, we propose the frequency modulated (FM) kernel\nflexibly modeling dependencies among different types of variables, so that BO\ncan enjoy the further improved sample efficiency. The FM kernel uses distances\non continuous variables to modulate the graph Fourier spectrum derived from\ndiscrete variables. However, the frequency modulation does not always define a\nkernel with the similarity measure behavior which returns higher values for\npairs of more similar points. Therefore, we specify and prove conditions for FM\nkernels to be positive definite and to exhibit the similarity measure behavior.\nIn experiments, we demonstrate the improved sample efficiency of GP BO using FM\nkernels (BO-FM).On synthetic problems and hyperparameter optimization problems,\nBO-FM outperforms competitors consistently. Also, the importance of the\nfrequency modulation principle is empirically demonstrated on the same\nproblems. On joint optimization of neural architectures and SGD\nhyperparameters, BO-FM outperforms competitors including Regularized\nevolution(RE) and BOHB. Remarkably, BO-FM performs better even than RE and BOHB\nusing three times as many evaluations.\n","authors":["Changyong Oh","Efstratios Gavves","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2102.12792v2.pdf","comment":"Proceedings of the Thirty-Seventh Conference on Uncertainty in\n  Artificial Intelligence, PMLR 161:950-960, 2021"},{"id":"http://arxiv.org/abs/2207.08412v1","updated":"2022-07-18T07:21:56Z","published":"2022-07-18T07:21:56Z","title":"Multi-head Cascaded Swin Transformers with Attention to k-space Sampling\n  Pattern for Accelerated MRI Reconstruction","summary":"  Global correlations are widely seen in human anatomical structures due to\nsimilarity across tissues and bones. These correlations are reflected in\nmagnetic resonance imaging (MRI) scans as a result of close-range proton\ndensity and T1/T2 parameter. Furthermore, to achieve accelerated MRI, k-space\ndata are undersampled which causes global aliasing artifacts. Convolutional\nneural network (CNN) models are widely utilized for accelerated MRI\nreconstruction, but those models are limited in capturing global correlations\ndue to the intrinsic locality of the convolution operation. The\nself-attention-based transformer models are capable of capturing global\ncorrelations among image features, however, the current contributions of\ntransformer models for MRI reconstruction are minute. The existing\ncontributions mostly provide CNN-transformer hybrid solutions and rarely\nleverage the physics of MRI. In this paper, we propose a physics-based\nstand-alone (convolution free) transformer model titled, the Multi-head\nCascaded Swin Transformers (McSTRA) for accelerated MRI reconstruction. McSTRA\ncombines several interconnected MRI physics-related concepts with the\ntransformer networks: it exploits global MR features via the shifted window\nself-attention mechanism; it extracts MR features belonging to different\nspectral components separately using a multi-head setup; it iterates between\nintermediate de-aliasing and k-space correction via a cascaded network with\ndata consistency in k-space and intermediate loss computations; furthermore, we\npropose a novel positional embedding generation mechanism to guide\nself-attention utilizing the point spread function corresponding to the\nundersampling mask. Our model significantly outperforms state-of-the-art MRI\nreconstruction methods both visually and quantitatively while depicting\nimproved resolution and removal of aliasing artifacts.\n","authors":["Mevan Ekanayake","Kamlesh Pawar","Mehrtash Harandi","Gary Egan","Zhaolin Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08406v1","updated":"2022-07-18T06:40:46Z","published":"2022-07-18T06:40:46Z","title":"Kullback-Leibler and Renyi divergences in reproducing kernel Hilbert\n  space and Gaussian process settings","summary":"  In this work, we present formulations for regularized Kullback-Leibler and\nR\\'enyi divergences via the Alpha Log-Determinant (Log-Det) divergences between\npositive Hilbert-Schmidt operators on Hilbert spaces in two different settings,\nnamely (i) covariance operators and Gaussian measures defined on reproducing\nkernel Hilbert spaces (RKHS); and (ii) Gaussian processes with squared\nintegrable sample paths. For characteristic kernels, the first setting leads to\ndivergences between arbitrary Borel probability measures on a complete,\nseparable metric space. We show that the Alpha Log-Det divergences are\ncontinuous in the Hilbert-Schmidt norm, which enables us to apply laws of large\nnumbers for Hilbert space-valued random variables. As a consequence of this, we\nshow that, in both settings, the infinite-dimensional divergences can be\nconsistently and efficiently estimated from their finite-dimensional versions,\nusing finite-dimensional Gram matrices/Gaussian measures and finite sample\ndata, with {\\it dimension-independent} sample complexities in all cases. RKHS\nmethodology plays a central role in the theoretical analysis in both settings.\nThe mathematical formulation is illustrated by numerical experiments.\n","authors":["Minh Ha Quang"],"pdf_url":"https://arxiv.org/pdf/2207.08406v1.pdf","comment":"74 pages"},{"id":"http://arxiv.org/abs/2006.03906v2","updated":"2022-07-18T06:29:31Z","published":"2020-06-06T16:17:07Z","title":"Identifying Causal Structure in Dynamical Systems","summary":"  Mathematical models are fundamental building blocks in the design of\ndynamical control systems. As control systems are becoming increasingly complex\nand networked, approaches for obtaining such models based on first principles\nreach their limits. Data-driven methods provide an alternative. However,\nwithout structural knowledge, these methods are prone to finding spurious\ncorrelations in the training data, which can hamper generalization capabilities\nof the obtained models. This can significantly lower control and prediction\nperformance when the system is exposed to unknown situations. A preceding\ncausal identification can prevent this pitfall. In this paper, we propose a\nmethod that identifies the causal structure of control systems. We design\nexperiments based on the concept of controllability, which provides a\nsystematic way to compute input trajectories that steer the system to specific\nregions in its state space. We then analyze the resulting data leveraging\npowerful techniques from causal inference and extend them to control systems.\nFurther, we derive conditions that guarantee the discovery of the true causal\nstructure of the system. Experiments on a robot arm demonstrate reliable causal\nidentification from real-world data and enhanced generalization capabilities.\n","authors":["Dominik Baumann","Friedrich Solowjow","Karl H. Johansson","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2006.03906v2.pdf","comment":"Accepted final versions to appear in the Transactions on Machine\n  Learning Research"},{"id":"http://arxiv.org/abs/2112.10166v3","updated":"2022-07-18T06:25:54Z","published":"2021-12-19T15:11:37Z","title":"FedNI: Federated Graph Learning with Network Inpainting for\n  Population-Based Disease Prediction","summary":"  Graph Convolutional Neural Networks (GCNs) are widely used for graph\nanalysis. Specifically, in medical applications, GCNs can be used for disease\nprediction on a population graph, where graph nodes represent individuals and\nedges represent individual similarities. However, GCNs rely on a vast amount of\ndata, which is challenging to collect for a single medical institution. In\naddition, a critical challenge that most medical institutions continue to face\nis addressing disease prediction in isolation with incomplete data information.\nTo address these issues, Federated Learning (FL) allows isolated local\ninstitutions to collaboratively train a global model without data sharing. In\nthis work, we propose a framework, FedNI, to leverage network inpainting and\ninter-institutional data via FL. Specifically, we first federatively train\nmissing node and edge predictor using a graph generative adversarial network\n(GAN) to complete the missing information of local networks. Then we train a\nglobal GCN node classifier across institutions using a federated graph learning\nplatform. The novel design enables us to build more accurate machine learning\nmodels by leveraging federated learning and also graph learning approaches. We\ndemonstrate that our federated model outperforms local and baseline FL methods\nwith significant margins on two public neuroimaging datasets.\n","authors":["Liang Peng","Nan Wang","Nicha Dvornek","Xiaofeng Zhu","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2112.10166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08398v1","updated":"2022-07-18T06:17:06Z","published":"2022-07-18T06:17:06Z","title":"Bayesian Optimization for Macro Placement","summary":"  Macro placement is the problem of placing memory blocks on a chip canvas. It\ncan be formulated as a combinatorial optimization problem over sequence pairs,\na representation which describes the relative positions of macros. Solving this\nproblem is particularly challenging since the objective function is expensive\nto evaluate. In this paper, we develop a novel approach to macro placement\nusing Bayesian optimization (BO) over sequence pairs. BO is a machine learning\ntechnique that uses a probabilistic surrogate model and an acquisition function\nthat balances exploration and exploitation to efficiently optimize a black-box\nobjective function. BO is more sample-efficient than reinforcement learning and\ntherefore can be used with more realistic objectives. Additionally, the ability\nto learn from data and adapt the algorithm to the objective function makes BO\nan appealing alternative to other black-box optimization methods such as\nsimulated annealing, which relies on problem-dependent heuristics and\nparameter-tuning. We benchmark our algorithm on the fixed-outline macro\nplacement problem with the half-perimeter wire length objective and demonstrate\ncompetitive performance.\n","authors":["Changyong Oh","Roberto Bondesan","Dana Kianfar","Rehan Ahmed","Rishubh Khurana","Payal Agarwal","Romain Lepert","Mysore Sriram","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2207.08398v1.pdf","comment":"ICML2022 Workshop on Adaptive Experimental Design and Active Learning\n  in the Real World"},{"id":"http://arxiv.org/abs/2207.08391v1","updated":"2022-07-18T05:58:19Z","published":"2022-07-18T05:58:19Z","title":"Federated Learning for Non-IID Data via Client Variance Reduction and\n  Adaptive Server Update","summary":"  Federated learning (FL) is an emerging technique used to collaboratively\ntrain a global machine learning model while keeping the data localized on the\nuser devices. The main obstacle to FL's practical implementation is the\nNon-Independent and Identical (Non-IID) data distribution across users, which\nslows convergence and degrades performance. To tackle this fundamental issue,\nwe propose a method (ComFed) that enhances the whole training process on both\nthe client and server sides. The key idea of ComFed is to simultaneously\nutilize client-variance reduction techniques to facilitate server aggregation\nand global adaptive update techniques to accelerate learning. Our experiments\non the Cifar-10 classification task show that ComFed can improve\nstate-of-the-art algorithms dedicated to Non-IID data.\n","authors":["Hiep Nguyen","Lam Phan","Harikrishna Warrier","Yogesh Gupta"],"pdf_url":"https://arxiv.org/pdf/2207.08391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08389v1","updated":"2022-07-18T05:47:29Z","published":"2022-07-18T05:47:29Z","title":"MLGOPerf: An ML Guided Inliner to Optimize Performance","summary":"  For the past 25 years, we have witnessed an extensive application of Machine\nLearning to the Compiler space; the selection and the phase-ordering problem.\nHowever, limited works have been upstreamed into the state-of-the-art\ncompilers, i.e., LLVM, to seamlessly integrate the former into the optimization\npipeline of a compiler to be readily deployed by the user. MLGO was among the\nfirst of such projects and it only strives to reduce the code size of a binary\nwith an ML-based Inliner using Reinforcement Learning.\n  This paper presents MLGOPerf; the first end-to-end framework capable of\noptimizing performance using LLVM's ML-Inliner. It employs a secondary ML model\nto generate rewards used for training a retargeted Reinforcement learning\nagent, previously used as the primary model by MLGO. It does so by predicting\nthe post-inlining speedup of a function under analysis and it enables a fast\ntraining framework for the primary model which otherwise wouldn't be practical.\nThe experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with\nrespect to LLVM's optimization at O3 when trained for performance on SPEC\nCPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach\nprovides up to 26% increased opportunities to autotune code regions for our\nbenchmarks which can be translated into an additional 3.7% speedup value.\n","authors":["Amir H. Ashouri","Mostafa Elhoushi","Yuzhe Hua","Xiang Wang","Muhammad Asif Manzoor","Bryan Chan","Yaoqing Gao"],"pdf_url":"https://arxiv.org/pdf/2207.08389v1.pdf","comment":"Version 1: The short version of this work is accepted at ACM/IEEE\n  CASES 2022"},{"id":"http://arxiv.org/abs/2202.00628v2","updated":"2022-07-18T05:32:02Z","published":"2022-02-01T18:22:17Z","title":"Regret Minimization with Performative Feedback","summary":"  In performative prediction, the deployment of a predictive model triggers a\nshift in the data distribution. As these shifts are typically unknown ahead of\ntime, the learner needs to deploy a model to get feedback about the\ndistribution it induces. We study the problem of finding near-optimal models\nunder performativity while maintaining low regret. On the surface, this problem\nmight seem equivalent to a bandit problem. However, it exhibits a fundamentally\nricher feedback structure that we refer to as performative feedback: after\nevery deployment, the learner receives samples from the shifted distribution\nrather than only bandit feedback about the reward. Our main contribution is an\nalgorithm that achieves regret bounds scaling only with the complexity of the\ndistribution shifts and not that of the reward function. The algorithm only\nrelies on smoothness of the shifts and does not assume convexity. Moreover, its\nfinal iterate is guaranteed to be near-optimal. The key algorithmic idea is\ncareful exploration of the distribution shifts that informs a novel\nconstruction of confidence bounds on the risk of unexplored models. More\nbroadly, our work establishes a conceptual approach for leveraging tools from\nthe bandits literature for the purpose of regret minimization with performative\nfeedback.\n","authors":["Meena Jagadeesan","Tijana Zrnic","Celestine Mendler-Dünner"],"pdf_url":"https://arxiv.org/pdf/2202.00628v2.pdf","comment":"Appeared at ICML 2022"},{"id":"http://arxiv.org/abs/2205.11283v4","updated":"2022-07-18T05:18:36Z","published":"2022-05-23T13:10:10Z","title":"SelfReformer: Self-Refined Network with Transformer for Salient Object\n  Detection","summary":"  The global and local contexts significantly contribute to the integrity of\npredictions in Salient Object Detection (SOD). Unfortunately, existing methods\nstill struggle to generate complete predictions with fine details. There are\ntwo major problems in conventional approaches: first, for global context,\nhigh-level CNN-based encoder features cannot effectively catch long-range\ndependencies, resulting in incomplete predictions. Second, downsampling the\nground truth to fit the size of predictions will introduce inaccuracy as the\nground truth details are lost during interpolation or pooling. Thus, in this\nwork, we developed a Transformer-based network and framed a supervised task for\na branch to learn the global context information explicitly. Besides, we adopt\nPixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the\nsize of ground truth instead of the reverse. Thus details in the ground truth\nare untouched. In addition, we developed a two-stage Context Refinement Module\n(CRM) to fuse global context and automatically locate and refine the local\ndetails in the predictions. The proposed network can guide and correct itself\nbased on the global and local context generated, thus is named, Self-Refined\nTransformer (SelfReformer). Extensive experiments and evaluation results on\nfive benchmark datasets demonstrate the outstanding performance of the network,\nand we achieved the state-of-the-art.\n","authors":["Yi Ke Yun","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2205.11283v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.06903v5","updated":"2022-07-18T04:55:08Z","published":"2021-05-14T15:41:15Z","title":"Posterior Regularization on Bayesian Hierarchical Mixture Clustering","summary":"  Bayesian hierarchical mixture clustering (BHMC) improves on the traditional\nBayesian hierarchical clustering by, with regard to the parent-to-child\ndiffusion in the generative process, replacing the conventional\nGaussian-to-Gaussian (G2G) kernels with a Hierarchical Dirichlet Process\nMixture Model (HDPMM). However, the drawback of the BHMC lies in the\npossibility of obtaining trees with comparatively high nodal variance in the\nhigher levels (i.e., those closer to the root node). This can be interpreted as\nthat the separation between the nodes, particularly those in the higher levels,\nmight be weak. We attempt to overcome this drawback through a recent\ninferential framework named posterior regularization, which facilitates a\nsimple manner to impose extra constraints on a Bayesian model to address its\nweakness. To enhance the separation of clusters, we apply posterior\nregularization to impose max-margin constraints on the nodes at every level of\nthe hierarchy. In this paper, we illustrate the modeling detail of applying the\nPR on BHMC and show that this solution achieves the desired improvements over\nthe BHMC model.\n","authors":["Weipeng Huang","Tin Lok James Ng","Nishma Laitonjam","Neil J. Hurley"],"pdf_url":"https://arxiv.org/pdf/2105.06903v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.14550v3","updated":"2022-07-18T04:52:56Z","published":"2022-05-29T00:59:38Z","title":"Machine Learning for Microcontroller-Class Hardware -- A Review","summary":"  The advancements in machine learning opened a new opportunity to bring\nintelligence to the low-end Internet-of-Things nodes such as microcontrollers.\nConventional machine learning deployment has high memory and compute footprint\nhindering their direct deployment on ultra resource-constrained\nmicrocontrollers. This paper highlights the unique requirements of enabling\nonboard machine learning for microcontroller class devices. Researchers use a\nspecialized model development workflow for resource-limited applications to\nensure the compute and latency budget is within the device limits while still\nmaintaining the desired performance. We characterize a closed-loop widely\napplicable workflow of machine learning model development for microcontroller\nclass devices and show that several classes of applications adopt a specific\ninstance of it. We present both qualitative and numerical insights into\ndifferent stages of model development by showcasing several use cases. Finally,\nwe identify the open research challenges and unsolved questions demanding\ncareful considerations moving forward.\n","authors":["Swapnil Sayan Saha","Sandeep Singh Sandha","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2205.14550v3.pdf","comment":"Accepted for publication at IEEE Sensors Journal"},{"id":"http://arxiv.org/abs/2207.08377v1","updated":"2022-07-18T04:34:08Z","published":"2022-07-18T04:34:08Z","title":"Deep Manifold Learning with Graph Mining","summary":"  Admittedly, Graph Convolution Network (GCN) has achieved excellent results on\ngraph datasets such as social networks, citation networks, etc. However,\nsoftmax used as the decision layer in these frameworks is generally optimized\nwith thousands of iterations via gradient descent. Furthermore, due to ignoring\nthe inner distribution of the graph nodes, the decision layer might lead to an\nunsatisfactory performance in semi-supervised learning with less label support.\nTo address the referred issues, we propose a novel graph deep model with a\nnon-gradient decision layer for graph mining. Firstly, manifold learning is\nunified with label local-structure preservation to capture the topological\ninformation of the nodes. Moreover, owing to the non-gradient property,\nclosed-form solutions is achieved to be employed as the decision layer for GCN.\nParticularly, a joint optimization method is designed for this graph model,\nwhich extremely accelerates the convergence of the model. Finally, extensive\nexperiments show that the proposed model has achieved state-of-the-art\nperformance compared to the current models.\n","authors":["Xuelong Li","Ziheng Jiao","Hongyuan Zhang","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13972v2","updated":"2022-07-18T04:23:04Z","published":"2022-05-27T13:40:50Z","title":"Counterfactual Fairness with Partially Known Causal Graph","summary":"  Fair machine learning aims to avoid treating individuals or sub-populations\nunfavourably based on \\textit{sensitive attributes}, such as gender and race.\nThose methods in fair machine learning that are built on causal inference\nascertain discrimination and bias through causal effects. Though\ncausality-based fair learning is attracting increasing attention, current\nmethods assume the true causal graph is fully known. This paper proposes a\ngeneral method to achieve the notion of counterfactual fairness when the true\ncausal graph is unknown. To be able to select features that lead to\ncounterfactual fairness, we derive the conditions and algorithms to identify\nancestral relations between variables on a \\textit{Partially Directed Acyclic\nGraph (PDAG)}, specifically, a class of causal DAGs that can be learned from\nobservational data combined with domain knowledge. Interestingly, we find that\ncounterfactual fairness can be achieved as if the true causal graph were fully\nknown, when specific background knowledge is provided: the sensitive attributes\ndo not have ancestors in the causal graph. Results on both simulated and\nreal-world datasets demonstrate the effectiveness of our method.\n","authors":["Aoqi Zuo","Susan Wei","Tongliang Liu","Bo Han","Kun Zhang","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2205.13972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.00063v2","updated":"2022-07-18T04:21:21Z","published":"2021-02-26T21:59:29Z","title":"Local Clustering in Contextual Multi-Armed Bandits","summary":"  We study identifying user clusters in contextual multi-armed bandits (MAB).\nContextual MAB is an effective tool for many real applications, such as content\nrecommendation and online advertisement. In practice, user dependency plays an\nessential role in the user's actions, and thus the rewards. Clustering similar\nusers can improve the quality of reward estimation, which in turn leads to more\neffective content recommendation and targeted advertising. Different from\ntraditional clustering settings, we cluster users based on the unknown bandit\nparameters, which will be estimated incrementally. In particular, we define the\nproblem of cluster detection in contextual MAB, and propose a bandit algorithm,\nLOCB, embedded with local clustering procedure. And, we provide theoretical\nanalysis about LOCB in terms of the correctness and efficiency of clustering\nand its regret bound. Finally, we evaluate the proposed algorithm from various\naspects, which outperforms state-of-the-art baselines.\n","authors":["Yikun Ban","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2103.00063v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/1909.03354v6","updated":"2022-07-18T04:15:23Z","published":"2019-09-08T00:01:37Z","title":"Deep Weakly-Supervised Learning Methods for Classification and\n  Localization in Histology Images: A Survey","summary":"  Using deep learning models to diagnose cancer from histology data presents\nseveral challenges. Cancer grading and localization of regions of interest\n(ROIs) in these images normally relies on both image- and pixel-level labels,\nthe latter requiring a costly annotation process. Deep weakly-supervised object\nlocalization (WSOL) methods provide different strategies for low-cost training\nof deep learning models. Using only image-class annotations, these methods can\nbe trained to classify an image, and yield class activation maps (CAMs) for ROI\nlocalization. This paper provides a review of state-of-art DL methods for WSOL.\nWe propose a taxonomy where these methods are divided into bottom-up and\ntop-down methods according to the information flow in models. Although the\nlatter have seen limited progress, recent bottom-up methods are currently\ndriving much progress with deep WSOL methods. Early works focused on designing\ndifferent spatial pooling functions. However, these methods reached limited\nlocalization accuracy, and unveiled a major limitation -- the under-activation\nof CAMs which leads to high false negative localization. Subsequent works aimed\nto alleviate this issue and recover complete object. Representative methods\nfrom our taxonomy are evaluated and compared in terms of classification and\nlocalization accuracy on two challenging histology datasets. Overall, the\nresults indicate poor localization performance, particularly for generic\nmethods that were initially designed to process natural images. Methods\ndesigned to address the challenges of histology data yielded good results.\nHowever, all methods suffer from high false positive/negative localization.\nFour key challenges are identified for the application of deep WSOL methods in\nhistology -- under/over activation of CAMs, sensitivity to thresholding, and\nmodel selection.\n","authors":["Jérôme Rony","Soufiane Belharbi","Jose Dolz","Ismail Ben Ayed","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/1909.03354v6.pdf","comment":"31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2207.08374v1","updated":"2022-07-18T04:14:36Z","published":"2022-07-18T04:14:36Z","title":"Adversarial Contrastive Learning via Asymmetric InfoNCE","summary":"  Contrastive learning (CL) has recently been applied to adversarial learning\ntasks. Such practice considers adversarial samples as additional positive views\nof an instance, and by maximizing their agreements with each other, yields\nbetter adversarial robustness. However, this mechanism can be potentially\nflawed, since adversarial perturbations may cause instance-level identity\nconfusion, which can impede CL performance by pulling together different\ninstances with separate identities. To address this issue, we propose to treat\nadversarial samples unequally when contrasted, with an asymmetric InfoNCE\nobjective ($A-InfoNCE$) that allows discriminating considerations of\nadversarial samples. Specifically, adversaries are viewed as inferior positives\nthat induce weaker learning signals, or as hard negatives exhibiting higher\ncontrast to other negative samples. In the asymmetric fashion, the adverse\nimpacts of conflicting objectives between CL and adversarial learning can be\neffectively mitigated. Experiments show that our approach consistently\noutperforms existing Adversarial CL methods across different finetuning schemes\nwithout additional computational cost. The proposed A-InfoNCE is also a generic\nform that can be readily extended to other CL methods. Code is available at\nhttps://github.com/yqy2001/A-InfoNCE.\n","authors":["Qiying Yu","Jieming Lou","Xianyuan Zhan","Qizhang Li","Wangmeng Zuo","Yang Liu","Jingjing Liu"],"pdf_url":"https://arxiv.org/pdf/2207.08374v1.pdf","comment":"Accepted by ECCV 2022. Code is available at\n  https://github.com/yqy2001/A-InfoNCE"},{"id":"http://arxiv.org/abs/2207.08367v1","updated":"2022-07-18T03:54:38Z","published":"2022-07-18T03:54:38Z","title":"Protecting Global Properties of Datasets with Distribution Privacy\n  Mechanisms","summary":"  Alongside the rapid development of data collection and analysis techniques in\nrecent years, there is increasingly an emphasis on the need to address\ninformation leakage associated with such usage of data. To this end, much work\nin the privacy literature is devoted to the protection of individual users and\ncontributors of data. However, some situations instead require a different\nnotion of data confidentiality involving global properties aggregated over the\nrecords of a dataset. Such notions of information protection are particularly\napplicable for business and organization data, where global properties may\nreflect trade secrets, or demographic data, which can be harmful if mishandled.\nRecent work on property inference attacks furthermore shows how data analysis\nalgorithms can be susceptible to leaking these global properties of data,\nhighlighting the importance of developing mechanisms that can protect such\ninformation.\n  In this work, we demonstrate how a distribution privacy framework can be\napplied to formalize the problem of protecting global properties of datasets.\nGiven this framework, we investigate several mechanisms and their tradeoffs for\nproviding this notion of data confidentiality. We analyze the theoretical\nprotection guarantees offered by these mechanisms under various data\nassumptions, then implement and empirically evaluate these mechanisms for\nseveral data analysis tasks. The results of our experiments show that our\nmechanisms can indeed reduce the effectiveness of practical property inference\nattacks while providing utility substantially greater than a crude group\ndifferential privacy baseline. Our work thus provides groundwork for\ntheoretically supported mechanisms for protecting global properties of\ndatasets.\n","authors":["Michelle Chen","Olga Ohrimenko"],"pdf_url":"https://arxiv.org/pdf/2207.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1808.08469v4","updated":"2022-07-18T03:40:02Z","published":"2018-08-25T21:02:46Z","title":"Optimal Nonparametric Inference with Two-Scale Distributional Nearest\n  Neighbors","summary":"  The weighted nearest neighbors (WNN) estimator has been popularly used as a\nflexible and easy-to-implement nonparametric tool for mean regression\nestimation. The bagging technique is an elegant way to form WNN estimators with\nweights automatically generated to the nearest neighbors; we name the resulting\nestimator as the distributional nearest neighbors (DNN) for easy reference.\nYet, there is a lack of distributional results for such estimator, limiting its\napplication to statistical inference. Moreover, when the mean regression\nfunction has higher-order smoothness, DNN does not achieve the optimal\nnonparametric convergence rate, mainly because of the bias issue. In this work,\nwe provide an in-depth technical analysis of the DNN, based on which we suggest\na bias reduction approach for the DNN estimator by linearly combining two DNN\nestimators with different subsampling scales, resulting in the novel two-scale\nDNN (TDNN) estimator. The two-scale DNN estimator has an equivalent\nrepresentation of WNN with weights admitting explicit forms and some being\nnegative. We prove that, thanks to the use of negative weights, the two-scale\nDNN estimator enjoys the optimal nonparametric rate of convergence in\nestimating the regression function under the fourth-order smoothness condition.\nWe further go beyond estimation and establish that the DNN and two-scale DNN\nare both asymptotically normal as the subsampling scales and sample size\ndiverge to infinity. For the practical implementation, we also provide variance\nestimators and a distribution estimator using the jackknife and bootstrap\ntechniques for the two-scale DNN. These estimators can be exploited for\nconstructing valid confidence intervals for nonparametric inference of the\nregression function. The theoretical results and appealing finite-sample\nperformance of the suggested two-scale DNN method are illustrated with several\nnumerical examples.\n","authors":["Emre Demirkaya","Yingying Fan","Lan Gao","Jinchi Lv","Patrick Vossler","Jingbo Wang"],"pdf_url":"https://arxiv.org/pdf/1808.08469v4.pdf","comment":"99 pages, 2 figures, to appear in Journal of the American Statistical\n  Association"},{"id":"http://arxiv.org/abs/2204.06255v4","updated":"2022-07-18T03:30:13Z","published":"2022-04-13T08:53:41Z","title":"Neural Operator with Regularity Structure for Modeling Dynamics Driven\n  by SPDEs","summary":"  Stochastic partial differential equations (SPDEs) are significant tools for\nmodeling dynamics in many areas including atmospheric sciences and physics.\nNeural Operators, generations of neural networks with capability of learning\nmaps between infinite-dimensional spaces, are strong tools for solving\nparametric PDEs. However, they lack the ability to modeling SPDEs which usually\nhave poor regularity due to the driving noise. As the theory of regularity\nstructure has achieved great successes in analyzing SPDEs and provides the\nconcept model feature vectors that well-approximate SPDEs' solutions, we\npropose the Neural Operator with Regularity Structure (NORS) which incorporates\nthe feature vectors for modeling dynamics driven by SPDEs. We conduct\nexperiments on various of SPDEs including the dynamic Phi41 model and the 2d\nstochastic Navier-Stokes equation, and the results demonstrate that the NORS is\nresolution-invariant, efficient, and achieves one order of magnitude lower\nerror with a modest amount of data.\n","authors":["Peiyan Hu","Qi Meng","Bingguang Chen","Shiqi Gong","Yue Wang","Wei Chen","Rongchan Zhu","Zhi-Ming Ma","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2204.06255v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08363v1","updated":"2022-07-18T03:18:08Z","published":"2022-07-18T03:18:08Z","title":"Predictive Neural Speech Coding","summary":"  Neural audio/speech coding has shown its capability to deliver a high quality\nat much lower bitrates than traditional methods recently. However, existing\nneural audio/speech codecs employ either acoustic features or learned blind\nfeatures with a convolutional neural network for encoding, by which there are\nstill temporal redundancies inside encoded features. This paper introduces\nlatent-domain predictive coding into the VQ-VAE framework to fully remove such\nredundancies and proposes the TF-Codec for low-latency neural speech coding in\nan end-to-end way. Specifically, the extracted features are encoded conditioned\non a prediction from past quantized latent frames so that temporal correlations\nare further removed. What's more, we introduce a learnable compression on the\ntime-frequency input to adaptively adjust the attention paid on main\nfrequencies and details at different bitrates. A differentiable vector\nquantization scheme based on distance-to-soft mapping and Gumbel-Softmax is\nproposed to better model the latent distributions with rate constraint.\nSubjective results on multilingual speech datasets show that with a latency of\n40ms, the proposed TF-Codec at 1kbps can achieve a much better quality than\nOpus 9kbps and TF-Codec at 3kbps outperforms both EVS 9.6kbps and Opus 12kbps.\nNumerous studies are conducted to show the effectiveness of these techniques.\n","authors":["Xue Jiang","Xiulian Peng","Huaying Xue","Yuan Zhang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2207.08363v1.pdf","comment":"Submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE\n  PROCESSING (TASLP)"},{"id":"http://arxiv.org/abs/2107.09028v4","updated":"2022-07-18T02:47:51Z","published":"2021-07-19T17:18:10Z","title":"Structured Stochastic Gradient MCMC","summary":"  Stochastic gradient Markov Chain Monte Carlo (SGMCMC) is considered the gold\nstandard for Bayesian inference in large-scale models, such as Bayesian neural\nnetworks. Since practitioners face speed versus accuracy tradeoffs in these\nmodels, variational inference (VI) is often the preferable option.\nUnfortunately, VI makes strong assumptions on both the factorization and\nfunctional form of the posterior. In this work, we propose a new non-parametric\nvariational approximation that makes no assumptions about the approximate\nposterior's functional form and allows practitioners to specify the exact\ndependencies the algorithm should respect or break. The approach relies on a\nnew Langevin-type algorithm that operates on a modified energy function, where\nparts of the latent variables are averaged over samples from earlier iterations\nof the Markov chain. This way, statistical dependencies can be broken in a\ncontrolled way, allowing the chain to mix faster. This scheme can be further\nmodified in a \"dropout\" manner, leading to even more scalability. We test our\nscheme for ResNet-20 on CIFAR-10, SVHN, and FMNIST. In all cases, we find\nimprovements in convergence speed and/or final accuracy compared to SG-MCMC and\nVI.\n","authors":["Antonios Alexos","Alex Boyd","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2107.09028v4.pdf","comment":"paper accepted in ICML2022. Code can be found here\n  https://github.com/ajboyd2/pytorch_lvi"},{"id":"http://arxiv.org/abs/2207.08349v1","updated":"2022-07-18T02:18:20Z","published":"2022-07-18T02:18:20Z","title":"Retweet-BERT: Political Leaning Detection Using Language Features and\n  Information Diffusion on Social Networks","summary":"  Estimating the political leanings of social media users is a challenging and\never more pressing problem given the increase in social media consumption. We\nintroduce Retweet-BERT, a simple and scalable model to estimate the political\nleanings of Twitter users. Retweet-BERT leverages the retweet network structure\nand the language used in users' profile descriptions. Our assumptions stem from\npatterns of networks and linguistics homophily among people who share similar\nideologies. Retweet-BERT demonstrates competitive performance against other\nstate-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitter\ndatasets (a COVID-19 dataset and a 2020 United States presidential elections\ndataset). We also perform manual validation to validate the performance of\nRetweet-BERT on users not in the training data. Finally, in a case study of\nCOVID-19, we illustrate the presence of political echo chambers on Twitter and\nshow that it exists primarily among right-leaning users. Our code is\nopen-sourced and our data is publicly available.\n","authors":["Julie Jiang","Xiang Ren","Emilio Ferrara"],"pdf_url":"https://arxiv.org/pdf/2207.08349v1.pdf","comment":"11 pages, 3 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:2103.10979"},{"id":"http://arxiv.org/abs/2206.15478v2","updated":"2022-07-18T02:09:04Z","published":"2022-06-30T17:59:52Z","title":"On the Learning and Learnablity of Quasimetrics","summary":"  Our world is full of asymmetries. Gravity and wind can make reaching a place\neasier than coming back. Social artifacts such as genealogy charts and citation\ngraphs are inherently directed. In reinforcement learning and control, optimal\ngoal-reaching strategies are rarely reversible (symmetrical). Distance\nfunctions supported on these asymmetrical structures are called quasimetrics.\nDespite their common appearance, little research has been done on the learning\nof quasimetrics.\n  Our theoretical analysis reveals that a common class of learning algorithms,\nincluding unconstrained multilayer perceptrons (MLPs), provably fails to learn\na quasimetric consistent with training data. In contrast, our proposed Poisson\nQuasimetric Embedding (PQE) is the first quasimetric learning formulation that\nboth is learnable with gradient-based optimization and enjoys strong\nperformance guarantees. Experiments on random graphs, social graphs, and\noffline Q-learning demonstrate its effectiveness over many common baselines.\n","authors":["Tongzhou Wang","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2206.15478v2.pdf","comment":"Project page: https://ssnl.github.io/quasimetric/ Code:\n  https://github.com/SsnL/poisson_quasimetric_embedding"},{"id":"http://arxiv.org/abs/2207.08347v1","updated":"2022-07-18T02:02:22Z","published":"2022-07-18T02:02:22Z","title":"Private Convex Optimization in General Norms","summary":"  We propose a new framework for differentially private optimization of convex\nfunctions which are Lipschitz in an arbitrary norm $\\normx{\\cdot}$. Our\nalgorithms are based on a regularized exponential mechanism which samples from\nthe density $\\propto \\exp(-k(F+\\mu r))$ where $F$ is the empirical loss and $r$\nis a regularizer which is strongly convex with respect to $\\normx{\\cdot}$,\ngeneralizing a recent work of \\cite{GLL22} to non-Euclidean settings. We show\nthat this mechanism satisfies Gaussian differential privacy and solves both\nDP-ERM (empirical risk minimization) and DP-SCO (stochastic convex\noptimization), by using localization tools from convex geometry. Our framework\nis the first to apply to private convex optimization in general normed spaces,\nand directly recovers non-private SCO rates achieved by mirror descent, as the\nprivacy parameter $\\eps \\to \\infty$. As applications, for Lipschitz\noptimization in $\\ell_p$ norms for all $p \\in (1, 2)$, we obtain the first\noptimal privacy-utility tradeoffs; for $p = 1$, we improve tradeoffs obtained\nby the recent works \\cite{AsiFKT21, BassilyGN21} by at least a logarithmic\nfactor. Our $\\ell_p$ norm and Schatten-$p$ norm optimization frameworks are\ncomplemented with polynomial-time samplers whose query complexity we explicitly\nbound.\n","authors":["Sivakanth Gopi","Yin Tat Lee","Daogao Liu","Ruoqi Shen","Kevin Tian"],"pdf_url":"https://arxiv.org/pdf/2207.08347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.15477v3","updated":"2022-07-18T02:01:20Z","published":"2022-06-30T17:59:49Z","title":"Denoised MDPs: Learning World Models Better Than the World Itself","summary":"  The ability to separate signal from noise, and reason with clean\nabstractions, is critical to intelligence. With this ability, humans can\nefficiently perform real world tasks without considering all possible nuisance\nfactors.How can artificial agents do the same? What kind of information can\nagents safely discard as noises?\n  In this work, we categorize information out in the wild into four types based\non controllability and relation with reward, and formulate useful information\nas that which is both controllable and reward-relevant. This framework\nclarifies the kinds information removed by various prior work on representation\nlearning in reinforcement learning (RL), and leads to our proposed approach of\nlearning a Denoised MDP that explicitly factors out certain noise distractors.\nExtensive experiments on variants of DeepMind Control Suite and RoboDesk\ndemonstrate superior performance of our denoised world model over using raw\nobservations alone, and over prior works, across policy optimization control\ntasks as well as the non-control task of joint position regression.\n","authors":["Tongzhou Wang","Simon S. Du","Antonio Torralba","Phillip Isola","Amy Zhang","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2206.15477v3.pdf","comment":"Project page: https://ssnl.github.io/denoised_mdp/ Code:\n  https://github.com/facebookresearch/denoised_mdp"},{"id":"http://arxiv.org/abs/2110.05679v4","updated":"2022-07-18T01:42:10Z","published":"2021-10-12T01:45:27Z","title":"Large Language Models Can Be Strong Differentially Private Learners","summary":"  Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n","authors":["Xuechen Li","Florian Tramèr","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2110.05679v4.pdf","comment":"31 pages; ICLR 2022 camera ready with additional writing\n  clarification and no \\vspace!"},{"id":"http://arxiv.org/abs/2207.08342v1","updated":"2022-07-18T01:39:13Z","published":"2022-07-18T01:39:13Z","title":"A Few Expert Queries Suffices for Sample-Efficient RL with Resets and\n  Linear Value Approximation","summary":"  The current paper studies sample-efficient Reinforcement Learning (RL) in\nsettings where only the optimal value function is assumed to be\nlinearly-realizable. It has recently been understood that, even under this\nseemingly strong assumption and access to a generative model, worst-case sample\ncomplexities can be prohibitively (i.e., exponentially) large. We investigate\nthe setting where the learner additionally has access to interactive\ndemonstrations from an expert policy, and we present a statistically and\ncomputationally efficient algorithm (Delphi) for blending exploration with\nexpert queries. In particular, Delphi requires $\\tilde{\\mathcal{O}}(d)$ expert\nqueries and a $\\texttt{poly}(d,H,|\\mathcal{A}|,1/\\varepsilon)$ amount of\nexploratory samples to provably recover an $\\varepsilon$-suboptimal policy.\nCompared to pure RL approaches, this corresponds to an exponential improvement\nin sample complexity with surprisingly-little expert input. Compared to prior\nimitation learning (IL) approaches, our required number of expert\ndemonstrations is independent of $H$ and logarithmic in $1/\\varepsilon$,\nwhereas all prior work required at least linear factors of both in addition to\nthe same dependence on $d$. Towards establishing the minimal amount of expert\nqueries needed, we show that, in the same setting, any learner whose\nexploration budget is polynomially-bounded (in terms of $d,H,$ and\n$|\\mathcal{A}|$) will require at least $\\tilde\\Omega(\\sqrt{d})$ oracle calls to\nrecover a policy competing with the expert's value function. Under the weaker\nassumption that the expert's policy is linear, we show that the lower bound\nincreases to $\\tilde\\Omega(d)$.\n","authors":["Philip Amortila","Nan Jiang","Dhruv Madeka","Dean P. Foster"],"pdf_url":"https://arxiv.org/pdf/2207.08342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.08603v4","updated":"2022-07-18T01:36:24Z","published":"2022-02-17T11:34:20Z","title":"Cross-Silo Heterogeneous Model Federated Multitask Learning","summary":"  Federated learning (FL) is a machine learning technique that enables\nparticipants to collaboratively train high-quality models without exchanging\ntheir private data. Participants utilizing cross-silo FL (CS-FL) settings are\nindependent organizations with different task needs, and they are concerned not\nonly with data privacy but also with independently training their unique models\ndue to intellectual property considerations. Most existing FL methods are\nincapable of satisfying the above scenarios. In this paper, we propose a FL\nmethod based on the pseudolabeling of unlabeled data via a process such as\ncotraining. To the best of our knowledge, this is the first FL method that is\nsimultaneously compatible with heterogeneous tasks, heterogeneous models, and\nheterogeneous training algorithms. Experimental results show that the proposed\nmethod achieves better performance than competing ones. This is especially true\nfor non-independent and identically distributed (IID) settings and\nheterogeneous models, where the proposed method achieves a 35% performance\nimprovement.\n","authors":["Xingjian Cao","Zonghang Li","Gang Sun","Hongfang Yu","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2202.08603v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.11066v3","updated":"2022-07-18T01:31:32Z","published":"2021-05-24T02:21:34Z","title":"Policy Mirror Descent for Regularized Reinforcement Learning: A\n  Generalized Framework with Linear Convergence","summary":"  Policy optimization, which finds the desired policy by maximizing value\nfunctions via optimization techniques, lies at the heart of reinforcement\nlearning (RL). In addition to value maximization, other practical\nconsiderations arise as well, including the need of encouraging exploration,\nand that of ensuring certain structural properties of the learned policy due to\nsafety, resource and operational constraints. These can often be accounted for\nvia regularized RL, which augments the target value function with a\nstructure-promoting regularizer.\n  Focusing on discounted infinite-horizon Markov decision processes, we propose\na generalized policy mirror descent (GPMD) algorithm for solving regularized\nRL. As a generalization of policy mirror descent (arXiv:2102.00135), our\nalgorithm accommodates a general class of convex regularizers and promotes the\nuse of Bregman divergence in cognizant of the regularizer in use. We\ndemonstrate that our algorithm converges linearly to the global solution over\nan entire range of learning rates, in a dimension-free fashion, even when the\nregularizer lacks strong convexity and smoothness. In addition, this linear\nconvergence feature is provably stable in the face of inexact policy evaluation\nand imperfect policy updates. Numerical experiments are provided to corroborate\nthe appealing performance of GPMD.\n","authors":["Wenhao Zhan","Shicong Cen","Baihe Huang","Yuxin Chen","Jason D. Lee","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2105.11066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08336v1","updated":"2022-07-18T01:10:25Z","published":"2022-07-18T01:10:25Z","title":"On Fair Classification with Mostly Private Sensitive Attributes","summary":"  Machine learning models have demonstrated promising performance in many\nareas. However, the concerns that they can be biased against specific groups\nhinder their adoption in high-stake applications. Thus it is essential to\nensure fairness in machine learning models. Most of the previous efforts\nrequire access to sensitive attributes for mitigating bias. Nonetheless, it is\noften infeasible to obtain large scale of data with sensitive attributes due to\npeople's increasing awareness of privacy and the legal compliance. Therefore,\nan important research question is how to make fair predictions under privacy?\nIn this paper, we study a novel problem on fair classification in a\nsemi-private setting, where most of the sensitive attributes are private and\nonly a small amount of clean sensitive attributes are available. To this end,\nwe propose a novel framework FairSP that can first learn to correct the noisy\nsensitive attributes under privacy guarantee via exploiting the limited clean\nsensitive attributes. Then, it jointly models the corrected and clean data in\nan adversarial way for debiasing and prediction. Theoretical analysis shows\nthat the proposed model can ensure fairness when most of the sensitive\nattributes are private. Experimental results on real-world datasets demonstrate\nthe effectiveness of the proposed model for making fair predictions under\nprivacy and maintaining high accuracy.\n","authors":["Canyu Chen","Yueqing Liang","Xiongxiao Xu","Shangyu Xie","Yuan Hong","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2207.08336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.09159v4","updated":"2022-07-18T00:50:56Z","published":"2021-03-16T15:56:57Z","title":"Learning to Shape Rewards using a Game of Two Partners","summary":"  Reward shaping (RS) is a powerful method in reinforcement learning (RL) for\novercoming the problem of sparse or uninformative rewards. However, RS\ntypically relies on manually engineered shaping-reward functions whose\nconstruction is time-consuming and error-prone. It also requires domain\nknowledge which runs contrary to the goal of autonomous learning. We introduce\nReinforcement Learning Optimising Shaping Algorithm (ROSA), an automated RS\nframework in which the shaping-reward function is constructed in a novel Markov\ngame between two agents. A reward-shaping agent (Shaper) uses switching\ncontrols to determine which states to add shaping rewards and their optimal\nvalues while the other agent (Controller) learns the optimal policy for the\ntask using these shaped rewards. We prove that ROSA, which easily adopts\nexisting RL algorithms, learns to construct a shaping-reward function that is\ntailored to the task thus ensuring efficient convergence to high performance\npolicies. We demonstrate ROSA's congenial properties in three carefully\ndesigned experiments and show its superior performance against state-of-the-art\nRS algorithms in challenging sparse reward environments.\n","authors":["David Mguni","Taher Jafferjee","Jianhong Wang","Nicolas Perez-Nieves","Yaodong Yang","Tianpei Yang","Matthew Taylor","Wenbin Song","Feifei Tong","Hui Chen","Jiangcheng Zhu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2103.09159v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14099v2","updated":"2022-07-18T00:38:23Z","published":"2021-10-27T00:44:32Z","title":"Tight Concentrations and Confidence Sequences from the Regret of\n  Universal Portfolio","summary":"  A classic problem in statistics is the estimation of the expectation of\nrandom variables from samples. This gives rise to the tightly connected\nproblems of deriving concentration inequalities and confidence sequences, that\nis confidence intervals that hold uniformly over time. Jun and Orabona\n[COLT'19] have shown how to easily convert the regret guarantee of an online\nbetting algorithm into a time-uniform concentration inequality. In this paper,\nwe show that we can go even further: We show that the regret of universal\nportfolio algorithms give rise to new implicit time-uniform concentrations and\nstate-of-the-art empirically calculated confidence sequences. In particular,\nour numerically obtained confidence sequences can be never vacuous, even with a\nsingle sample, and satisfy the law of iterated logarithm.\n","authors":["Francesco Orabona","Kwang-Sung Jun"],"pdf_url":"https://arxiv.org/pdf/2110.14099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02193v2","updated":"2022-07-18T00:35:21Z","published":"2022-02-04T15:39:32Z","title":"Stochastic smoothing of the top-K calibrated hinge loss for deep\n  imbalanced classification","summary":"  In modern classification tasks, the number of labels is getting larger and\nlarger, as is the size of the datasets encountered in practice. As the number\nof classes increases, class ambiguity and class imbalance become more and more\nproblematic to achieve high top-1 accuracy. Meanwhile, Top-K metrics (metrics\nallowing K guesses) have become popular, especially for performance reporting.\nYet, proposing top-K losses tailored for deep learning remains a challenge,\nboth theoretically and practically. In this paper we introduce a stochastic\ntop-K hinge loss inspired by recent developments on top-K calibrated losses.\nOur proposal is based on the smoothing of the top-K operator building on the\nflexible \"perturbed optimizer\" framework. We show that our loss function\nperforms very well in the case of balanced datasets, while benefiting from a\nsignificantly lower computational time than the state-of-the-art top-K loss\nfunction. In addition, we propose a simple variant of our loss for the\nimbalanced case. Experiments on a heavy-tailed dataset show that our loss\nfunction significantly outperforms other baseline loss functions.\n","authors":["Camille Garcin","Maximilien Servajean","Alexis Joly","Joseph Salmon"],"pdf_url":"https://arxiv.org/pdf/2202.02193v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.08537v1","updated":"2022-07-18T11:59:20Z","published":"2022-07-18T11:59:20Z","title":"A General Framework for Pairwise Unbiased Learning to Rank","summary":"  Pairwise debiasing is one of the most effective strategies in reducing\nposition bias in learning-to-rank (LTR) models. However, limiting the scope of\nthis strategy, are the underlying assumptions required by many pairwise\ndebiasing approaches. In this paper, we develop an approach based on a\nminimalistic set of assumptions that can be applied to a much broader range of\nuser browsing patterns and arbitrary presentation layouts. We implement the\napproach as a simplified version of the Unbiased LambdaMART and demonstrate\nthat it retains the underlying unbiasedness property in a wider variety of\nsettings than the original algorithm. Finally, using simulations with \"golden\"\nrelevance labels, we will show that the simplified version compares favourably\nwith the original Unbiased LambdaMART when the examination of different\npositions in a ranked list is not assumed to be independent.\n","authors":["Alexey Kurennoy","John Coleman","Ian Harris","Alice Lynch","Oisin Mac Fhearai","Daphne Tsatsoulis"],"pdf_url":"https://arxiv.org/pdf/2207.08537v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2207.05188v2","updated":"2022-07-18T09:24:35Z","published":"2022-07-11T20:51:28Z","title":"Knowledge Graph Induction enabling Recommending and Trend Analysis: A\n  Corporate Research Community Use Case","summary":"  A research division plays an important role of driving innovation in an\norganization. Drawing insights, following trends, keeping abreast of new\nresearch, and formulating strategies are increasingly becoming more challenging\nfor both researchers and executives as the amount of information grows in both\nvelocity and volume. In this paper we present a use case of how a corporate\nresearch community, IBM Research, utilizes Semantic Web technologies to induce\na unified Knowledge Graph from both structured and textual data obtained by\nintegrating various applications used by the community related to research\nprojects, academic papers, datasets, achievements and recognition. In order to\nmake the Knowledge Graph more accessible to application developers, we\nidentified a set of common patterns for exploiting the induced knowledge and\nexposed them as APIs. Those patterns were born out of user research which\nidentified the most valuable use cases or user pain points to be alleviated. We\noutline two distinct scenarios: recommendation and analytics for business use.\nWe will discuss these scenarios in detail and provide an empirical evaluation\non entity recommendation specifically. The methodology used and the lessons\nlearned from this work can be applied to other organizations facing similar\nchallenges.\n","authors":["Nandana Mihindukulasooriya","Mike Sava","Gaetano Rossiello","Md Faisal Mahbub Chowdhury","Irene Yachbes","Aditya Gidh","Jillian Duckwitz","Kovit Nisar","Michael Santos","Alfio Gliozzo"],"pdf_url":"https://arxiv.org/pdf/2207.05188v2.pdf","comment":"Accepted at ISWC 2022"},{"id":"http://arxiv.org/abs/2207.08922v1","updated":"2022-07-18T20:20:12Z","published":"2022-07-18T20:20:12Z","title":"ir_metadata: An Extensible Metadata Schema for IR Experiments","summary":"  The information retrieval (IR) community has a strong tradition of making the\ncomputational artifacts and resources available for future reuse, allowing the\nvalidation of experimental results. Besides the actual test collections, the\nunderlying run files are often hosted in data archives as part of conferences\nlike TREC, CLEF, or NTCIR. Unfortunately, the run data itself does not provide\nmuch information about the underlying experiment. For instance, the single run\nfile is not of much use without the context of the shared task's website or the\nrun data archive. In other domains, like the social sciences, it is good\npractice to annotate research data with metadata. In this work, we introduce\nir_metadata - an extensible metadata schema for TREC run files based on the\nPRIMAD model. We propose to align the metadata annotations to PRIMAD, which\nconsiders components of computational experiments that can affect\nreproducibility. Furthermore, we outline important components and information\nthat should be reported in the metadata and give evidence from the literature.\nTo demonstrate the usefulness of these metadata annotations, we implement new\nfeatures in repro_eval that support the outlined metadata schema for the use\ncase of reproducibility studies. Additionally, we curate a dataset with run\nfiles derived from experiments with different instantiations of PRIMAD\ncomponents and annotate these with the corresponding metadata. In the\nexperiments, we cover reproducibility experiments that are identified by the\nmetadata and classified by PRIMAD. With this work, we enable IR researchers to\nannotate TREC run files and improve the reuse value of experimental artifacts\neven further.\n","authors":["Timo Breuer","Jüri Keller","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2207.08922v1.pdf","comment":"Resource paper"},{"id":"http://arxiv.org/abs/2207.08818v1","updated":"2022-07-18T13:06:21Z","published":"2022-07-18T13:06:21Z","title":"SeLoC-ML: Semantic Low-Code Engineering for Machine Learning\n  Applications in Industrial IoT","summary":"  Internet of Things (IoT) is transforming the industry by bridging the gap\nbetween Information Technology (IT) and Operational Technology (OT). Machines\nare being integrated with connected sensors and managed by intelligent\nanalytics applications, accelerating digital transformation and business\noperations. Bringing Machine Learning (ML) to industrial devices is an\nadvancement aiming to promote the convergence of IT and OT. However, developing\nan ML application in industrial IoT (IIoT) presents various challenges,\nincluding hardware heterogeneity, non-standardized representations of ML\nmodels, device and ML model compatibility issues, and slow application\ndevelopment. Successful deployment in this area requires a deep understanding\nof hardware, algorithms, software tools, and applications. Therefore, this\npaper presents a framework called Semantic Low-Code Engineering for ML\nApplications (SeLoC-ML), built on a low-code platform to support the rapid\ndevelopment of ML applications in IIoT by leveraging Semantic Web technologies.\nSeLoC-ML enables non-experts to easily model, discover, reuse, and matchmake ML\nmodels and devices at scale. The project code can be automatically generated\nfor deployment on hardware based on the matching results. Developers can\nbenefit from semantic application templates, called recipes, to fast prototype\nend-user applications. The evaluations confirm an engineering effort reduction\nby a factor of at least three compared to traditional approaches on an\nindustrial ML classification case study, showing the efficiency and usefulness\nof SeLoC-ML. We share the code and welcome any contributions.\n","authors":["Haoyu Ren","Kirill Dorofeev","Darko Anicic","Youssef Hammad","Roland Eckl","Thomas A. Runkler"],"pdf_url":"https://arxiv.org/pdf/2207.08818v1.pdf","comment":"Accepted by the 21st International Semantic Web Conference (ISWC2022)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.08739v1","updated":"2022-07-18T16:30:50Z","published":"2022-07-18T16:30:50Z","title":"Rethinking Data Augmentation for Robust Visual Question Answering","summary":"  Data Augmentation (DA) -- generating extra training samples beyond original\ntraining set -- has been widely-used in today's unbiased VQA models to mitigate\nthe language biases. Current mainstream DA strategies are synthetic-based\nmethods, which synthesize new samples by either editing some visual\nregions/words, or re-generating them from scratch. However, these synthetic\nsamples are always unnatural and error-prone. To avoid this issue, a recent DA\nwork composes new augmented samples by randomly pairing pristine images and\nother human-written questions. Unfortunately, to guarantee augmented samples\nhave reasonable ground-truth answers, they manually design a set of heuristic\nrules for several question types, which extremely limits its generalization\nabilities. To this end, we propose a new Knowledge Distillation based Data\nAugmentation for VQA, dubbed KDDAug. Specifically, we first relax the\nrequirements of reasonable image-question pairs, which can be easily applied to\nany question types. Then, we design a knowledge distillation (KD) based answer\nassignment to generate pseudo answers for all composed image-question pairs,\nwhich are robust to both in-domain and out-of-distribution settings. Since\nKDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into\nany VQA architectures. Extensive ablation studies on multiple backbones and\nbenchmarks have demonstrated the effectiveness and generalization abilities of\nKDDAug.\n","authors":["Long Chen","Yuhang Zheng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2207.08739v1.pdf","comment":"Accepted to ECCV 2022; Codes: https://github.com/ItemZheng/KDDAug"},{"id":"http://arxiv.org/abs/2207.08461v1","updated":"2022-07-18T09:29:10Z","published":"2022-07-18T09:29:10Z","title":"Multi-dimension Geospatial feature learning for urban region function\n  recognition","summary":"  Urban region function recognition plays a vital character in monitoring and\nmanaging the limited urban areas. Since urban functions are complex and full of\nsocial-economic properties, simply using remote sensing~(RS) images equipped\nwith physical and optical information cannot completely solve the\nclassification task. On the other hand, with the development of mobile\ncommunication and the internet, the acquisition of geospatial big data~(GBD)\nbecomes possible. In this paper, we propose a Multi-dimension Feature Learning\nModel~(MDFL) using high-dimensional GBD data in conjunction with RS images for\nurban region function recognition. When extracting multi-dimension features,\nour model considers the user-related information modeled by their activity, as\nwell as the region-based information abstracted from the region graph.\nFurthermore, we propose a decision fusion network that integrates the decisions\nfrom several neural networks and machine learning classifiers, and the final\ndecision is made considering both the visual cue from the RS images and the\nsocial information from the GBD data. Through quantitative evaluation, we\ndemonstrate that our model achieves overall accuracy at 92.75, outperforming\nthe state-of-the-art by 10 percent.\n","authors":["Wenjia Xu","Jiuniu Wang","Yirong Wu"],"pdf_url":"https://arxiv.org/pdf/2207.08461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09331v3","updated":"2022-07-18T06:05:07Z","published":"2021-12-17T05:40:28Z","title":"Contrastive Vision-Language Pre-training with Limited Resources","summary":"  Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have\nrevealed the potential of aligning multi-modal representations with contrastive\nlearning. However, these works require a tremendous amount of data and\ncomputational resources (e.g., billion-level web data and hundreds of GPUs),\nwhich prevent researchers with limited resources from reproduction and further\nexploration. To this end, we propose a stack of novel methods, which\nsignificantly cut down the heavy resource dependency and allow us to conduct\ndual-encoder multi-modal representation alignment with limited resources.\nBesides, we provide a reproducible baseline of competitive results, namely\nZeroVL, with only 14M publicly accessible academic datasets and 8 V100 GPUs.\nAdditionally, we collect 100M web data for pre-training, and achieve comparable\nor superior results than state-of-the-art methods, further proving the\neffectiveness of our methods on large-scale data. We hope that this work will\nprovide useful data points and experience for future research in contrastive\nvision-language pre-training. Code is available at\nhttps://github.com/zerovl/ZeroVL.\n","authors":["Quan Cui","Boyan Zhou","Yu Guo","Weidong Yin","Hao Wu","Osamu Yoshie","Yubo Chen"],"pdf_url":"https://arxiv.org/pdf/2112.09331v3.pdf","comment":"Accepted to ECCV2022"},{"id":"http://arxiv.org/abs/2207.07394v2","updated":"2022-07-18T05:14:28Z","published":"2022-07-15T10:48:41Z","title":"FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud\n  Video Streaming","summary":"  Point cloud video transmission is challenging due to high encoding/decoding\ncomplexity, high video bitrate, and low latency requirement. Consequently,\nconventional adaptive streaming methodologies often find themselves\nunsatisfactory to meet the requirements in threefold: 1) current algorithms\nreuse existing quality of experience (QoE) definitions while overlooking the\nunique features of point cloud video thus failing to provide optimal user\nexperience, 2) most deep learning approaches require long-span data collections\nto learn sufficiently varied network conditions and result in long training\nperiod and capacity occupation, 3) cloud training approaches pose privacy risks\ncaused by leakage of user reported service usage and networking conditions.\n  To overcome the limitations, we present FRAS, the first federated\nreinforcement learning framework, to the best of our knowledge, for adaptive\npoint cloud video streaming. We define a new QoE model which takes the unique\nfeatures of point cloud video into account. Each client uses reinforcement\nlearning (RL) to train encoding rate selection with the objective of optimizing\nthe user's QoE under multiple constraints. Then, a federated learning framework\nis integrated with the RL algorithm to enhance training performance with\nprivacy preservation. Extensive simulations using real point cloud videos and\nnetwork traces reveal the superiority of the proposed scheme over baseline\nschemes. We also implement a prototype that demonstrates the performance of\nFRAS via real-world tests.\n","authors":["Yu Gao","Pengyuan Zhou","Zhi Liu","Bo Han","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2207.07394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08346v1","updated":"2022-07-18T01:58:20Z","published":"2022-07-18T01:58:20Z","title":"Display of 3D Illuminations using Flying Light Specks","summary":"  This paper presents techniques to display 3D illuminations using Flying Light\nSpecks, FLSs. Each FLS is a miniature (hundreds of micrometers) sized drone\nwith one or more light sources to generate different colors and textures with\nadjustable brightness. It is network enabled with a processor and local\nstorage. Synchronized swarms of cooperating FLSs render illumination of virtual\nobjects in a pre-specified 3D volume, an FLS display. We present techniques to\ndisplay both static and motion illuminations. Our display techniques consider\nthe limited flight time of an FLS on a fully charged battery and the duration\nof time to charge the FLS battery. Moreover, our techniques assume failure of\nFLSs is the norm rather than an exception. We present a hardware and a software\narchitecture for an FLS-display along with a family of techniques to compute\nflight paths of FLSs for illuminations. With motion illuminations, one\ntechnique (ICF) minimizes the overall distance traveled by the FLSs\nsignificantly when compared with the other techniques.\n","authors":["Shahram Ghandeharizadeh"],"pdf_url":"https://arxiv.org/pdf/2207.08346v1.pdf","comment":"A shorter version in the Proceedings of the 30th ACM International\n  Conference on Multimedia (MM '22), October 10--14, 2022, Lisboa, Portugal,\n  DOI https://dl.acm.org/doi/10.1145/3503161.3548250, ISBN\n  978-1-4503-9203-7/22/10. See https://github.com/shahramg/FLS-Multimedia2022\n  for experimental software"},{"id":"http://arxiv.org/abs/2207.08338v1","updated":"2022-07-18T01:20:18Z","published":"2022-07-18T01:20:18Z","title":"MobileCodec: Neural Inter-frame Video Compression on Mobile Devices","summary":"  Realizing the potential of neural video codecs on mobile devices is a big\ntechnological challenge due to the computational complexity of deep networks\nand the power-constrained mobile hardware. We demonstrate practical feasibility\nby leveraging Qualcomm's technology and innovation, bridging the gap from\nneural network-based codec simulations running on wall-powered workstations, to\nreal-time operation on a mobile device powered by Snapdragon technology. We\nshow the first-ever inter-frame neural video decoder running on a commercial\nmobile phone, decoding high-definition videos in real-time while maintaining a\nlow bitrate and high visual quality.\n","authors":["Hoang Le","Liang Zhang","Amir Said","Guillaume Sautiere","Yang Yang","Pranav Shrestha","Fei Yin","Reza Pourreza","Auke Wiggers"],"pdf_url":"https://arxiv.org/pdf/2207.08338v1.pdf","comment":"ACM MMSys 2022"},{"id":"http://arxiv.org/abs/2203.03553v2","updated":"2022-07-18T00:35:56Z","published":"2022-03-07T17:53:26Z","title":"Compression of user generated content using denoised references","summary":"  Video shared over the internet is commonly referred to as user generated\ncontent (UGC). UGC video may have low quality due to various factors including\nprevious compression. UGC video is uploaded by users, and then it is re-encoded\nto be made available at various levels of quality. In a traditional video\ncoding pipeline the encoder parameters are optimized to minimize a\nrate-distortion criterion, but when the input signal has low quality, this\nresults in sub-optimal coding parameters optimized to preserve undesirable\nartifacts. In this paper we formulate the UGC compression problem as that of\ncompression of a noisy/corrupted source. The noisy source coding theorem\nreveals that an optimal UGC compression system is comprised of optimal\ndenoising of the UGC signal, followed by compression of the denoised signal.\nSince optimal denoising is unattainable and users may be against modification\nof their content, we propose encoding the UGC signal, and using denoised\nreferences only to compute distortion, so the encoding process can be guided\ntowards perceptually better solutions. We demonstrate the effectiveness of the\nproposed strategy for JPEG compression of UGC images and videos.\n","authors":["Eduardo Pavez","Enrique Perez","Xin Xiong","Antonio Ortega","Balu Adsumilli"],"pdf_url":"https://arxiv.org/pdf/2203.03553v2.pdf","comment":"5 pages, 6 figures, accepted at International Conference on Image\n  Processing (ICIP) 2022"}]},"2022-07-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2207.08305v1","updated":"2022-07-17T21:43:18Z","published":"2022-07-17T21:43:18Z","title":"Effectiveness of French Language Models on Abstractive Dialogue\n  Summarization Task","summary":"  Pre-trained language models have established the state-of-the-art on various\nnatural language processing tasks, including dialogue summarization, which\nallows the reader to quickly access key information from long conversations in\nmeetings, interviews or phone calls. However, such dialogues are still\ndifficult to handle with current models because the spontaneity of the language\ninvolves expressions that are rarely present in the corpora used for\npre-training the language models. Moreover, the vast majority of the work\naccomplished in this field has been focused on English. In this work, we\npresent a study on the summarization of spontaneous oral dialogues in French\nusing several language specific pre-trained models: BARThez, and BelGPT-2, as\nwell as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments\nwere performed on the DECODA (Call Center) dialogue corpus whose task is to\ngenerate abstractive synopses from call center conversations between a caller\nand one or several agents depending on the situation. Results show that the\nBARThez models offer the best performance far above the previous\nstate-of-the-art on DECODA. We further discuss the limits of such pre-trained\nmodels and the challenges that must be addressed for summarizing spontaneous\ndialogues.\n","authors":["Yongxin Zhou","François Portet","Fabien Ringeval"],"pdf_url":"https://arxiv.org/pdf/2207.08305v1.pdf","comment":"Yongxin Zhou, Fran\\c{c}ois Portet, Fabien Ringeval. Effectiveness of\n  French Language Models on Abstractive Dialogue Summarization Task. LREC 2022,\n  Marseille, France, 21-23 June 2022"},{"id":"http://arxiv.org/abs/2207.08292v1","updated":"2022-07-17T21:18:03Z","published":"2022-07-17T21:18:03Z","title":"A Spoken Drug Prescription Dataset in French for Spoken Language\n  Understanding","summary":"  Spoken medical dialogue systems are increasingly attracting interest to\nenhance access to healthcare services and improve quality and traceability of\npatient care. In this paper, we focus on medical drug prescriptions acquired on\nsmartphones through spoken dialogue. Such systems would facilitate the\ntraceability of care and would free clinicians' time. However, there is a lack\nof speech corpora to develop such systems since most of the related corpora are\nin text form and in English. To facilitate the research and development of\nspoken medical dialogue systems, we present, to the best of our knowledge, the\nfirst spoken medical drug prescriptions corpus, named PxSLU. It contains 4\nhours of transcribed and annotated dialogues of drug prescriptions in French\nacquired through an experiment with 55 participants experts and non-experts in\nprescriptions. We also present some experiments that demonstrate the interest\nof this corpus for the evaluation and development of medical dialogue systems.\n","authors":["Ali Can Kocabiyikoglu","François Portet","Prudence Gibert","Hervé Blanchon","Jean-Marc Babouchkine","Gaëtan Gavazzi"],"pdf_url":"https://arxiv.org/pdf/2207.08292v1.pdf","comment":"Ali Can Kocabiyikoglu,Fran\\c{c}ois Portet, Prudence Gibert, Herv\\'e\n  Blanchon, Jean-Marc Babouchkine, Ga\\\"etan Gavazzi. A Spoken Drug Prescription\n  Dataset in French for Spoken Language Understanding. LREC2022, Marseille,\n  France, 21-22-23 June 2022"},{"id":"http://arxiv.org/abs/2207.08286v1","updated":"2022-07-17T21:02:04Z","published":"2022-07-17T21:02:04Z","title":"An Overview of Distant Supervision for Relation Extraction with a Focus\n  on Denoising and Pre-training Methods","summary":"  Relation Extraction (RE) is a foundational task of natural language\nprocessing. RE seeks to transform raw, unstructured text into structured\nknowledge by identifying relational information between entity pairs found in\ntext. RE has numerous uses, such as knowledge graph completion, text\nsummarization, question-answering, and search querying. The history of RE\nmethods can be roughly organized into four phases: pattern-based RE,\nstatistical-based RE, neural-based RE, and large language model-based RE. This\nsurvey begins with an overview of a few exemplary works in the earlier phases\nof RE, highlighting limitations and shortcomings to contextualize progress.\nNext, we review popular benchmarks and critically examine metrics used to\nassess RE performance. We then discuss distant supervision, a paradigm that has\nshaped the development of modern RE methods. Lastly, we review recent RE works\nfocusing on denoising and pre-training methods.\n","authors":["William Hogan"],"pdf_url":"https://arxiv.org/pdf/2207.08286v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.08256v1","updated":"2022-07-17T18:42:37Z","published":"2022-07-17T18:42:37Z","title":"Representation Learning of Image Schema","summary":"  Image schema is a recurrent pattern of reasoning where one entity is mapped\ninto another. Image schema is similar to conceptual metaphor and is also\nrelated to metaphoric gesture. Our main goal is to generate metaphoric gestures\nfor an Embodied Conversational Agent.\n  We propose a technique to learn the vector representation of image schemas.\nAs far as we are aware of, this is the first work which addresses that problem.\nOur technique uses Ravenet et al's algorithm which we use to compute the image\nschemas from the text input and also BERT and SenseBERT which we use as the\nbase word embedding technique to calculate the final vector representation of\nthe image schema. Our representation learning technique works by clustering:\nword embedding vectors which belong to the same image schema should be\nrelatively closer to each other, and thus form a cluster.\n  With the image schemas representable as vectors, it also becomes possible to\nhave a notion that some image schemas are closer or more similar to each other\nthan to the others because the distance between the vectors is a proxy of the\ndissimilarity between the corresponding image schemas. Therefore, after\nobtaining the vector representation of the image schemas, we calculate the\ndistances between those vectors. Based on these, we create visualizations to\nillustrate the relative distances between the different image schemas.\n","authors":["Fajrian Yunus","Chloé Clavel","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2207.08256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08230v1","updated":"2022-07-17T17:12:16Z","published":"2022-07-17T17:12:16Z","title":"Troll Tweet Detection Using Contextualized Word Representations","summary":"  In recent years, many troll accounts have emerged to manipulate social media\nopinion. Detecting and eradicating trolling is a critical issue for\nsocial-networking platforms because businesses, abusers, and\nnation-state-sponsored troll farms use false and automated accounts. NLP\ntechniques are used to extract data from social networking text, such as\nTwitter tweets. In many text processing applications, word embedding\nrepresentation methods, such as BERT, have performed better than prior NLP\ntechniques, offering novel breaks to precisely comprehend and categorize\nsocial-networking information for various tasks. This paper implements and\ncompares nine deep learning-based troll tweet detection architectures, with\nthree models for each BERT, ELMo, and GloVe word embedding model. Precision,\nrecall, F1 score, AUC, and classification accuracy are used to evaluate each\narchitecture. From the experimental results, most architectures using BERT\nmodels improved troll tweet detection. A customized ELMo-based architecture\nwith a GRU classifier has the highest AUC for detecting troll messages. The\nproposed architectures can be used by various social-based systems to detect\ntroll messages in the future.\n","authors":["Seyhmus Yilmaz","Sultan Zavrak"],"pdf_url":"https://arxiv.org/pdf/2207.08230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08212v1","updated":"2022-07-17T16:07:38Z","published":"2022-07-17T16:07:38Z","title":"RT-KGD: Relation Transition Aware Knowledge-Grounded Dialogue Generation","summary":"  Grounding dialogue system with external knowledge is a promising way to\nimprove the quality of responses. Most existing works adopt knowledge graphs\n(KGs) as the external resources, paying attention to the contribution of\nentities in the last utterance of the dialogue for context understanding and\nresponse generation. Nevertheless, the correlations between knowledge implied\nin the multi-turn context and the transition regularities between relations in\nKGs are under-explored. To this end, we propose a Relation Transition aware\nKnowledge-Grounded Dialogue Generation model (RT-KGD). Specifically, inspired\nby the latent logic of human conversation, our model integrates dialogue-level\nrelation transition regularities with turn-level entity semantic information.\nIn this manner, the interaction between knowledge is considered to produce\nabundant clues for predicting the appropriate knowledge and generating coherent\nresponses. The experimental results on both automatic evaluation and manual\nevaluation indicate that our model outperforms state-of-the-art baselines.\n","authors":["Kexin Wang","Zhixu Li","Jiaan Wang","Jianfeng Qu","Ying He","An Liu","Lei Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.08212v1.pdf","comment":"ISWC 2022"},{"id":"http://arxiv.org/abs/2207.08184v1","updated":"2022-07-17T13:59:46Z","published":"2022-07-17T13:59:46Z","title":"Zero-Shot Temporal Action Detection via Vision-Language Prompting","summary":"  Existing temporal action detection (TAD) methods rely on large training data\nincluding segment-level annotations, limited to recognizing previously seen\nclasses alone during inference. Collecting and annotating a large training set\nfor each class of interest is costly and hence unscalable. Zero-shot TAD\n(ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize\nany unseen action classes. Meanwhile, ZS-TAD is also much more challenging with\nsignificantly less investigation. Inspired by the success of zero-shot image\nclassification aided by vision-language (ViL) models such as CLIP, we aim to\ntackle the more complex TAD task. An intuitive method is to integrate an\noff-the-shelf proposal detector with CLIP style classification. However, due to\nthe sequential localization (e.g, proposal generation) and classification\ndesign, it is prone to localization error propagation. To overcome this\nproblem, in this paper we propose a novel zero-Shot Temporal Action detection\nmodel via Vision-LanguagE prompting (STALE). Such a novel design effectively\neliminates the dependence between localization and classification by breaking\nthe route for error propagation in-between. We further introduce an interaction\nmechanism between classification and localization for improved optimization.\nExtensive experiments on standard ZS-TAD video benchmarks show that our STALE\nsignificantly outperforms state-of-the-art alternatives. Besides, our model\nalso yields superior results on supervised TAD over recent strong competitors.\nThe PyTorch implementation of STALE is available at\nhttps://github.com/sauradip/STALE.\n","authors":["Sauradip Nag","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2207.08184v1.pdf","comment":"ECCV 2022; Code available at https://github.com/sauradip/STALE"},{"id":"http://arxiv.org/abs/2207.08179v1","updated":"2022-07-17T13:51:56Z","published":"2022-07-17T13:51:56Z","title":"End-to-End Spoken Language Understanding: Performance analyses of a\n  voice command task in a low resource setting","summary":"  Spoken Language Understanding (SLU) is a core task in most human-machine\ninteraction systems. With the emergence of smart homes, smart phones and smart\nspeakers, SLU has become a key technology for the industry. In a classical SLU\napproach, an Automatic Speech Recognition (ASR) module transcribes the speech\nsignal into a textual representation from which a Natural Language\nUnderstanding (NLU) module extracts semantic information. Recently End-to-End\nSLU (E2E SLU) based on Deep Neural Networks has gained momentum since it\nbenefits from the joint optimization of the ASR and the NLU parts, hence\nlimiting the cascade of error effect of the pipeline architecture. However,\nlittle is known about the actual linguistic properties used by E2E models to\npredict concepts and intents from speech input. In this paper, we present a\nstudy identifying the signal features and other linguistic properties used by\nan E2E model to perform the SLU task. The study is carried out in the\napplication domain of a smart home that has to handle non-English (here French)\nvoice commands. The results show that a good E2E SLU performance does not\nalways require a perfect ASR capability. Furthermore, the results show the\nsuperior capabilities of the E2E model in handling background noise and\nsyntactic variation compared to the pipeline model. Finally, a finer-grained\nanalysis suggests that the E2E model uses the pitch information of the input\nsignal to identify voice command concepts. The results and methodology outlined\nin this paper provide a springboard for further analyses of E2E models in\nspeech processing.\n","authors":["Thierry Desot","François Portet","Michel Vacher"],"pdf_url":"https://arxiv.org/pdf/2207.08179v1.pdf","comment":"Thierry Desot, Fran\\c{c}ois Portet, Michel Vacher, End-to-End Spoken\n  Language Understanding: Performance analyses of a voice command task in a low\n  resource setting, Computer Speech & Language, Volume 75, 2022"},{"id":"http://arxiv.org/abs/2201.11732v2","updated":"2022-07-17T13:01:43Z","published":"2022-01-27T18:53:22Z","title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and\n  Languages","summary":"  Reliable evaluation benchmarks designed for replicability and\ncomprehensiveness have driven progress in machine learning. Due to the lack of\na multilingual benchmark, however, vision-and-language research has mostly\nfocused on English language tasks. To fill this gap, we introduce the\nImage-Grounded Language Understanding Evaluation benchmark. IGLUE brings\ntogether - by both aggregating pre-existing datasets and creating new ones -\nvisual question answering, cross-modal retrieval, grounded reasoning, and\ngrounded entailment tasks across 20 diverse languages. Our benchmark enables\nthe evaluation of multilingual multimodal models for transfer learning, not\nonly in a zero-shot setting, but also in newly defined few-shot learning\nsetups. Based on the evaluation of the available state-of-the-art models, we\nfind that translate-test transfer is superior to zero-shot transfer and that\nfew-shot learning is hard to harness for many tasks. Moreover, downstream\nperformance is partially explained by the amount of available unlabelled\ntextual data for pretraining, and only weakly by the typological distance of\ntarget-source languages. We hope to encourage future research efforts in this\narea by releasing the benchmark to the community.\n","authors":["Emanuele Bugliarello","Fangyu Liu","Jonas Pfeiffer","Siva Reddy","Desmond Elliott","Edoardo Maria Ponti","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2201.11732v2.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2207.08162v1","updated":"2022-07-17T12:59:34Z","published":"2022-07-17T12:59:34Z","title":"Natural language processing for clusterization of genes according to\n  their functions","summary":"  There are hundreds of methods for analysis of data obtained in\nmRNA-sequencing. The most of them are focused on small number of genes. In this\nstudy, we propose an approach that reduces the analysis of several thousand\ngenes to analysis of several clusters. The list of genes is enriched with\ninformation from open databases. Then, the descriptions are encoded as vectors\nusing the pretrained language model (BERT) and some text processing approaches.\nThe encoded gene function pass through the dimensionality reduction and\nclusterization. Aiming to find the most efficient pipeline, 180 cases of\npipeline with different methods in the major pipeline steps were analyzed. The\nperformance was evaluated with clusterization indexes and expert review of the\nresults.\n","authors":["Vladislav Dordiuk","Ekaterina Demicheva","Fernando Polanco Espino","Konstantin Ushenin"],"pdf_url":"https://arxiv.org/pdf/2207.08162v1.pdf","comment":"Ural-Siberian Conference on Computational Technologies in Cognitive\n  Science, Genomics and Biomedicine 2022 (CSGB 2022)"},{"id":"http://arxiv.org/abs/2207.08143v1","updated":"2022-07-17T11:24:44Z","published":"2022-07-17T11:24:44Z","title":"Can large language models reason about medical questions?","summary":"  Although large language models (LLMs) often produce impressive outputs, they\nalso fail to reason and be factual. We set out to investigate how these\nlimitations affect the LLM's ability to answer and reason about difficult\nreal-world based questions. We applied the human-aligned GPT-3 (InstructGPT) to\nanswer multiple-choice medical exam questions (USMLE and MedMCQA) and medical\nresearch questions (PubMedQA). We investigated Chain-of-thought (think step by\nstep) prompts, grounding (augmenting the prompt with search results) and\nfew-shot (prepending the question with question-answer exemplars). For a subset\nof the USMLE questions, a medical domain expert reviewed and annotated the\nmodel's reasoning. Overall, GPT-3 achieved a substantial improvement in\nstate-of-the-art machine learning performance. We observed that GPT-3 is often\nknowledgeable and can reason about medical questions. GPT-3, when confronted\nwith a question it cannot answer, will still attempt to answer, often resulting\nin a biased predictive distribution. LLMs are not on par with human performance\nbut our results suggest the emergence of reasoning patterns that are compatible\nwith medical problem-solving. We speculate that scaling model and data,\nenhancing prompt alignment and allowing for better contextualization of the\ncompletions will be sufficient for LLMs to reach human-level performance on\nthis type of task.\n","authors":["Valentin Liévin","Christoffer Egeberg Hother","Ole Winther"],"pdf_url":"https://arxiv.org/pdf/2207.08143v1.pdf","comment":"30 pages, 1 figure, to be submitted"},{"id":"http://arxiv.org/abs/2207.08141v1","updated":"2022-07-17T11:20:58Z","published":"2022-07-17T11:20:58Z","title":"ELECTRA is a Zero-Shot Learner, Too","summary":"  Recently, for few-shot or even zero-shot learning, the new paradigm\n\"pre-train, prompt, and predict\" has achieved remarkable achievements compared\nwith the \"pre-train, fine-tune\" paradigm. After the success of prompt-based\nGPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)\nprompt learning methods became popular and widely used. However, another\nefficient pre-trained discriminative model, ELECTRA, has probably been\nneglected. In this paper, we attempt to accomplish several NLP tasks in the\nzero-shot scenario using a novel our proposed replaced token detection\n(RTD)-based prompt learning method. Experimental results show that ELECTRA\nmodel based on RTD-prompt learning achieves surprisingly state-of-the-art\nzero-shot performance. Numerically, compared to MLM-RoBERTa-large and\nMLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%\nimprovement on all 15 tasks. Especially on the SST-2 task, our\nRTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training\ndata. Overall, compared to the pre-trained masked language models, the\npre-trained replaced token detection model performs better in zero-shot\nlearning. Therefore, ELECTRA is an excellent zero-shot learner. The source code\nis available at: https://github.com/nishiwen1214/RTD-ELECTRA.\n","authors":["Shiwen Ni","Hung-Yu Kao"],"pdf_url":"https://arxiv.org/pdf/2207.08141v1.pdf","comment":"The source code is available at:\n  https://github.com/nishiwen1214/RTD-ELECTRA"},{"id":"http://arxiv.org/abs/2207.08112v1","updated":"2022-07-17T08:41:14Z","published":"2022-07-17T08:41:14Z","title":"United States Politicians' Tone Became More Negative with 2016 Primary\n  Campaigns","summary":"  There is a widespread belief that the tone of US political language has\nbecome more negative recently, in particular when Donald Trump entered\npolitics. At the same time, there is disagreement as to whether Trump changed\nor merely continued previous trends. To date, data-driven evidence regarding\nthese questions is scarce, partly due to the difficulty of obtaining a\ncomprehensive, longitudinal record of politicians' utterances. Here we apply\npsycholinguistic tools to a novel, comprehensive corpus of 24 million quotes\nfrom online news attributed to 18,627 US politicians in order to analyze how\nthe tone of US politicians' language evolved between 2008 and 2020. We show\nthat, whereas the frequency of negative emotion words had decreased\ncontinuously during Obama's tenure, it suddenly and lastingly increased with\nthe 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of\nthe pre-campaign mean, in a pattern that emerges across parties. The effect\nsize drops by 40% when omitting Trump's quotes, and by 50% when averaging over\nspeakers rather than quotes, implying that prominent speakers, and Trump in\nparticular, have disproportionately, though not exclusively, contributed to the\nrise in negative language. This work provides the first large-scale data-driven\nevidence of a drastic shift toward a more negative political tone following\nTrump's campaign start as a catalyst, with important implications for the\ndebate about the state of US politics.\n","authors":["Jonathan Külz","Andreas Spitz","Ahmad Abu-Akel","Stephan Günnemann","Robert West"],"pdf_url":"https://arxiv.org/pdf/2207.08112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08104v1","updated":"2022-07-17T08:16:49Z","published":"2022-07-17T08:16:49Z","title":"A Multibias-mitigated and Sentiment Knowledge Enriched Transformer for\n  Debiasing in Multimodal Conversational Emotion Recognition","summary":"  Multimodal emotion recognition in conversations (mERC) is an active research\ntopic in natural language processing (NLP), which aims to predict human's\nemotional states in communications of multiple modalities, e,g., natural\nlanguage and facial gestures. Innumerable implicit prejudices and\npreconceptions fill human language and conversations, leading to the question\nof whether the current data-driven mERC approaches produce a biased error. For\nexample, such approaches may offer higher emotional scores on the utterances by\nfemales than males. In addition, the existing debias models mainly focus on\ngender or race, where multibias mitigation is still an unexplored task in mERC.\nIn this work, we take the first step to solve these issues by proposing a\nseries of approaches to mitigate five typical kinds of bias in textual\nutterances (i.e., gender, age, race, religion and LGBTQ+) and visual\nrepresentations (i.e, gender and age), followed by a Multibias-Mitigated and\nsentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive\nexperimental results show the effectiveness of the proposed model and prove\nthat the debias operation has a great impact on the classification performance\nfor mERC. We hope our study will benefit the development of bias mitigation in\nmERC and related emotion studies.\n","authors":["Jinglin Wang","Fang Ma","Yazhou Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2207.08104v1.pdf","comment":"10 pages, 5 figures, accepted to NLPCC 2022"},{"id":"http://arxiv.org/abs/2207.08099v1","updated":"2022-07-17T07:22:19Z","published":"2022-07-17T07:22:19Z","title":"Aspect-specific Context Modeling for Aspect-based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) aims at predicting sentiment polarity\n(SC) or extracting opinion span (OE) expressed towards a given aspect. Previous\nwork in ABSA mostly relies on rather complicated aspect-specific feature\ninduction. Recently, pretrained language models (PLMs), e.g., BERT, have been\nused as context modeling layers to simplify the feature induction structures\nand achieve state-of-the-art performance. However, such PLM-based context\nmodeling can be not that aspect-specific. Therefore, a key question is left\nunder-explored: how the aspect-specific context can be better modeled through\nPLMs? To answer the question, we attempt to enhance aspect-specific context\nmodeling with PLM in a non-intrusive manner. We propose three aspect-specific\ninput transformations, namely aspect companion, aspect prompt, and aspect\nmarker. Informed by these transformations, non-intrusive aspect-specific PLMs\ncan be achieved to promote the PLM to pay more attention to the aspect-specific\ncontext in a sentence. Additionally, we craft an adversarial benchmark for ABSA\n(advABSA) to see how aspect-specific modeling can impact model robustness.\nExtensive experimental results on standard and adversarial benchmarks for SC\nand OE demonstrate the effectiveness and robustness of the proposed method,\nyielding new state-of-the-art performance on OE and competitive performance on\nSC.\n","authors":["Fang Ma","Chen Zhang","Bo Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2207.08099v1.pdf","comment":"12 pages, accepted to NLPCC 2022"},{"id":"http://arxiv.org/abs/2207.08087v1","updated":"2022-07-17T06:50:35Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various NLP and IR\ndownstream applications have benefited from ESE due to its ability to discover\nknowledge. Although existing bootstrapping methods have achieved great\nprogress, most of them still rely on manually pre-defined context patterns. A\nnon-negligible shortcoming of the pre-defined context patterns is that they\ncannot be flexibly generalized to all kinds of semantic classes, and we call\nthis phenomenon as \"semantic sensitivity\". To address this problem, we devise a\ncontext pattern generation module that utilizes autoregressive language models\n(e.g., GPT-2) to automatically generate high-quality context patterns for\nentities. In addition, we propose the GAPA, a novel ESE framework that\nleverages the aforementioned GenerAted PAtterns to expand target entities.\nExtensive experiments and detailed analyses on three widely used datasets\ndemonstrate the effectiveness of our method. All the codes of our experiments\nwill be available for reproducibility.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08083v1","updated":"2022-07-17T06:02:48Z","published":"2022-07-17T06:02:48Z","title":"Towards Explainability in NLP: Analyzing and Calculating Word Saliency\n  through Word Properties","summary":"  The wide use of black-box models in natural language processing brings great\nchallenges to the understanding of the decision basis, the trustworthiness of\nthe prediction results, and the improvement of the model performance. The words\nin text samples have properties that reflect their semantics and contextual\ninformation, such as the part of speech, the position, etc. These properties\nmay have certain relationships with the word saliency, which is of great help\nfor studying the explainability of the model predictions. In this paper, we\nexplore the relationships between the word saliency and the word properties.\nAccording to the analysis results, we further establish a mapping model,\nSeq2Saliency, from the words in a text sample and their properties to the\nsaliency values based on the idea of sequence tagging. In addition, we\nestablish a new dataset called PrSalM, which contains each word in the text\nsamples, the word properties, and the word saliency values. The experimental\nevaluations are conducted to analyze the saliency of words with different\nproperties. The effectiveness of the Seq2Saliency model is verified.\n","authors":["Jialiang Dong","Zhitao Guan","Longfei Wu","Zijian Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.08083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01206v2","updated":"2022-07-17T01:47:06Z","published":"2022-07-04T05:30:22Z","title":"WebShop: Towards Scalable Real-World Web Interaction with Grounded\n  Language Agents","summary":"  Existing benchmarks for grounding language in interactive environments either\nlack real-world linguistic elements, or prove difficult to scale up due to\nsubstantial human involvement in the collection of data or feedback signals. To\nbridge this gap, we develop WebShop -- a simulated e-commerce website\nenvironment with $1.18$ million real-world products and $12,087$ crowd-sourced\ntext instructions. Given a text instruction specifying a product requirement,\nan agent needs to navigate multiple types of webpages and issue diverse actions\nto find, customize, and purchase an item. WebShop provides several challenges\nfor language grounding including understanding compositional instructions,\nquery (re-)formulation, comprehending and acting on noisy text in webpages, and\nperforming strategic exploration. We collect over $1,600$ human demonstrations\nfor the task, and train and evaluate a diverse range of agents using\nreinforcement learning, imitation learning, and pre-trained image and language\nmodels. Our best model achieves a task success rate of $29\\%$, which\noutperforms rule-based heuristics ($9.6\\%$) but is far lower than human expert\nperformance ($59\\%$). We also analyze agent and human trajectories and ablate\nvarious model components to provide insights for developing future agents with\nstronger language understanding and decision making abilities. Finally, we show\nthat agents trained on WebShop exhibit non-trivial sim-to-real transfer when\nevaluated on amazon.com and ebay.com, indicating the potential value of WebShop\nin developing practical web-based agents that can operate in the wild.\n","authors":["Shunyu Yao","Howard Chen","John Yang","Karthik Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2207.01206v2.pdf","comment":"Project page with code, data, demos: https://webshop-pnlp.github.io.\n  v2 adds transfer to eBay"},{"id":"http://arxiv.org/abs/2207.09217v1","updated":"2022-07-17T03:12:27Z","published":"2022-07-17T03:12:27Z","title":"Contextual Similarity is More Valuable than Character Similarity:\n  Curriculum Learning for Chinese Spell Checking","summary":"  Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling\nerrors. In recent years, related researches focus on introducing the character\nsimilarity from confusion set to enhance the CSC models, ignoring the context\nof characters that contain richer information. To make better use of contextual\nsimilarity, we propose a simple yet effective curriculum learning framework for\nthe CSC task. With the help of our designed model-agnostic framework, existing\nCSC models will be trained from easy to difficult as humans learn Chinese\ncharacters and achieve further performance improvements. Extensive experiments\nand detailed analyses on widely used SIGHAN datasets show that our method\noutperforms previous state-of-the-art methods.\n","authors":["Ding Zhang","Yinghui Li","Qingyu Zhou","Shirong Ma","Yangning Li","Yunbo Cao","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2207.09217v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2207.08320v1","updated":"2022-07-17T23:55:40Z","published":"2022-07-17T23:55:40Z","title":"GANzilla: User-Driven Direction Discovery in Generative Adversarial\n  Networks","summary":"  Generative Adversarial Network (GAN) is widely adopted in numerous\napplication areas, such as data preprocessing, image editing, and creativity\nsupport. However, GAN's 'black box' nature prevents non-expert users from\ncontrolling what data a model generates, spawning a plethora of prior work that\nfocused on algorithm-driven approaches to extract editing directions to control\nGAN. Complementarily, we propose a GANzilla: a user-driven tool that empowers a\nuser with the classic scatter/gather technique to iteratively discover\ndirections to meet their editing goals. In a study with 12 participants,\nGANzilla users were able to discover directions that (i) edited images to match\nprovided examples (closed-ended tasks) and that (ii) met a high-level goal,\ne.g., making the face happier, while showing diversity across individuals\n(open-ended tasks).\n","authors":["Noyan Evirgen","Xiang 'Anthony' Chen"],"pdf_url":"https://arxiv.org/pdf/2207.08320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08317v1","updated":"2022-07-17T23:20:19Z","published":"2022-07-17T23:20:19Z","title":"Molecular-orbital-based Machine Learning for Open-shell and\n  Multi-reference Systems with Kernel Addition Gaussian Process Regression","summary":"  We introduce a novel machine learning strategy, kernel addition Gaussian\nprocess regression (KA-GPR), in molecular-orbital-based machine learning\n(MOB-ML) to learn the total correlation energies of general electronic\nstructure theories for closed- and open-shell systems by introducing a machine\nlearning strategy. The learning efficiency of MOB-ML (KA-GPR) is the same as\nthe original MOB-ML method for the smallest criegee molecule, which is a\nclosed-shell molecule with multi-reference characters. In addition, the\nprediction accuracies of different small free radicals could reach the chemical\naccuracy of 1 kcal/mol by training on one example structure. Accurate potential\nenergy surfaces for the H10 chain (closed-shell) and water OH bond dissociation\n(open-shell) could also be generated by MOB-ML (KA-GPR). To explore the breadth\nof chemical systems that KA-GPR can describe, we further apply MOB-ML to\naccurately predict the large benchmark datasets for closed- (QM9, QM7b-T,\nGDB-13-T) and open-shell (QMSpin) molecules.\n","authors":["Lixue Cheng","Jiace Sun","J. Emiliano Deustua","Vignesh C. Bhethanabotla","Thomas F. Miller III"],"pdf_url":"https://arxiv.org/pdf/2207.08317v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2207.02093v2","updated":"2022-07-17T23:17:18Z","published":"2022-07-05T14:55:16Z","title":"Predicting Out-of-Domain Generalization with Local Manifold Smoothness","summary":"  Understanding how machine learning models generalize to new environments is a\ncritical part of their safe deployment. Recent work has proposed a variety of\ncomplexity measures that directly predict or theoretically bound the\ngeneralization capacity of a model. However, these methods rely on a strong set\nof assumptions that in practice are not always satisfied. Motivated by the\nlimited settings in which existing measures can be applied, we propose a novel\ncomplexity measure based on the local manifold smoothness of a classifier. We\ndefine local manifold smoothness as a classifier's output sensitivity to\nperturbations in the manifold neighborhood around a given test point.\nIntuitively, a classifier that is less sensitive to these perturbations should\ngeneralize better. To estimate smoothness we sample points using data\naugmentation and measure the fraction of these points classified into the\nmajority class. Our method only requires selecting a data augmentation method\nand makes no other assumptions about the model or data distributions, meaning\nit can be applied even in out-of-domain (OOD) settings where existing methods\ncannot. In experiments on robustness benchmarks in image classification,\nsentiment analysis, and natural language inference, we demonstrate a strong and\nrobust correlation between our manifold smoothness measure and actual OOD\ngeneralization on over 3,000 models evaluated on over 100 train/test domain\npairs.\n","authors":["Nathan Ng","Neha Hulkund","Kyunghyun Cho","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2207.02093v2.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2205.13061v2","updated":"2022-07-17T23:11:49Z","published":"2022-05-25T21:53:48Z","title":"RENs: Relevance Encoding Networks","summary":"  The manifold assumption for high-dimensional data assumes that the data is\ngenerated by varying a set of parameters obtained from a low-dimensional latent\nspace. Deep generative models (DGMs) are widely used to learn data\nrepresentations in an unsupervised way. DGMs parameterize the underlying\nlow-dimensional manifold in the data space using bottleneck architectures such\nas variational autoencoders (VAEs). The bottleneck dimension for VAEs is\ntreated as a hyperparameter that depends on the dataset and is fixed at design\ntime after extensive tuning. As the intrinsic dimensionality of most real-world\ndatasets is unknown, often, there is a mismatch between the intrinsic\ndimensionality and the latent dimensionality chosen as a hyperparameter. This\nmismatch can negatively contribute to the model performance for representation\nlearning and sample generation tasks. This paper proposes relevance encoding\nnetworks (RENs): a novel probabilistic VAE-based framework that uses the\nautomatic relevance determination (ARD) prior in the latent space to learn the\ndata-specific bottleneck dimensionality. The relevance of each latent dimension\nis directly learned from the data along with the other model parameters using\nstochastic gradient descent and a reparameterization trick adapted to\nnon-Gaussian priors. We leverage the concept of DeepSets to capture permutation\ninvariant statistical properties in both data and latent spaces for relevance\ndetermination. The proposed framework is general and flexible and can be used\nfor the state-of-the-art VAE models that leverage regularizers to impose\nspecific characteristics in the latent space (e.g., disentanglement). With\nextensive experimentation on synthetic and public image datasets, we show that\nthe proposed model learns the relevant latent bottleneck dimensionality without\ncompromising the representation and generation quality of the samples.\n","authors":["Krithika Iyer","Riddhish Bhalodia","Shireen Elhabian"],"pdf_url":"https://arxiv.org/pdf/2205.13061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08309v1","updated":"2022-07-17T22:08:10Z","published":"2022-07-17T22:08:10Z","title":"CULT: Continual Unsupervised Learning with Typicality-Based Environment\n  Detection","summary":"  We introduce CULT (Continual Unsupervised Representation Learning with\nTypicality-Based Environment Detection), a new algorithm for continual\nunsupervised learning with variational auto-encoders. CULT uses a simple\ntypicality metric in the latent space of a VAE to detect distributional shifts\nin the environment, which is used in conjunction with generative replay and an\nauxiliary environmental classifier to limit catastrophic forgetting in\nunsupervised representation learning. In our experiments, CULT significantly\noutperforms baseline continual unsupervised learning approaches. Code for this\npaper can be found here: https://github.com/oliveradk/cult\n","authors":["Oliver Daniels-Koch"],"pdf_url":"https://arxiv.org/pdf/2207.08309v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.08217v1","updated":"2022-07-17T16:25:27Z","published":"2022-07-17T16:25:27Z","title":"Extracting and Visualizing Wildlife Trafficking Events from Wildlife\n  Trafficking Reports","summary":"  Experts combating wildlife trafficking manually sift through articles about\nseizures and arrests, which is time consuming and make identifying trends\ndifficult. We apply natural language processing techniques to automatically\nextract data from reports published by the Eco Activists for Governance and Law\nEnforcement (EAGLE). We expanded Python spaCy's pre-trained pipeline and added\na custom named entity ruler, which identified 15 fully correct and 36 partially\ncorrect events in 15 reports against an existing baseline, which did not\nidentify any fully correct events. The extracted wildlife trafficking events\nwere inserted to a database. Then, we created visualizations to display trends\nover time and across regions to support domain experts. These are accessible on\nour website, Wildlife Trafficking in Africa\n(https://wildlifemqp.github.io/Visualizations/).\n","authors":["Devin Coughlin","Maylee Gagnon","Victoria Grasso","Guanyi Mou","Kyumin Lee","Renata Konrad","Patricia Raxter","Meredith Gore"],"pdf_url":"https://arxiv.org/pdf/2207.08217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05731v2","updated":"2022-07-17T15:59:33Z","published":"2022-06-12T13:03:47Z","title":"Human Mobility Prediction with Causal and Spatial-constrained Multi-task\n  Network","summary":"  Modeling human mobility helps to understand how people are accessing\nresources and physically contacting with each other in cities, and thus\ncontributes to various applications such as urban planning, epidemic control,\nand location-based advertisement. Next location prediction is one decisive task\nin individual human mobility modeling and is usually viewed as sequence\nmodeling, solved with Markov or RNN-based methods. However, the existing models\npaid little attention to the logic of individual travel decisions and the\nreproducibility of the collective behavior of population. To this end, we\npropose a Causal and Spatial-constrained Long and Short-term Learner (CSLSL)\nfor next location prediction. CSLSL utilizes a causal structure based on\nmulti-task learning to explicitly model the\n\"when$\\rightarrow$what$\\rightarrow$where\", a.k.a.\n\"time$\\rightarrow$activity$\\rightarrow$location\" decision logic. We next\npropose a spatial-constrained loss function as an auxiliary task, to ensure the\nconsistency between the predicted and actual spatial distribution of travelers'\ndestinations. Moreover, CSLSL adopts modules named Long and Short-term Capturer\n(LSC) to learn the transition regularities across different time spans.\nExtensive experiments on three real-world datasets show a 33.4% performance\nimprovement of CSLSL over baselines and confirm the effectiveness of\nintroducing the causality and consistency constraints. The implementation is\navailable at https://github.com/urbanmobility/CSLSL.\n","authors":["Zongyuan Huang","Shengyuan Xu","Menghan Wang","Hansi Wu","Yanyan Xu","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2206.05731v2.pdf","comment":"The experimental results in the paper need to be updated, so we\n  withdraw the original version"},{"id":"http://arxiv.org/abs/2205.09626v5","updated":"2022-07-17T15:37:42Z","published":"2022-05-19T15:40:27Z","title":"BARS: Towards Open Benchmarking for Recommender Systems","summary":"  The past two decades have witnessed the rapid development of personalized\nrecommendation techniques. Despite significant progress made in both research\nand practice of recommender systems, to date, there is a lack of a\nwidely-recognized benchmarking standard in this field. Many existing studies\nperform model evaluations and comparisons in an ad-hoc manner, for example, by\nemploying their own private data splits or using different experimental\nsettings. Such conventions not only increase the difficulty in reproducing\nexisting studies, but also lead to inconsistent experimental results among\nthem. This largely limits the credibility and practical value of research\nresults in this field. To tackle these issues, we present an initiative project\n(namely BARS) aiming for open benchmarking for recommender systems. In\ncomparison to some earlier attempts towards this goal, we take a further step\nby setting up a standardized benchmarking pipeline for reproducible research,\nwhich integrates all the details about datasets, source code, hyper-parameter\nsettings, running logs, and evaluation results. The benchmark is designed with\ncomprehensiveness and sustainability in mind. It covers both matching and\nranking tasks, and also enables researchers to easily follow and contribute to\nthe research in this field. This project will not only reduce the redundant\nefforts of researchers to re-implement or re-run existing baselines, but also\ndrive more solid and reproducible research on recommender systems. We would\nlike to call upon everyone to use the BARS benchmark for future evaluation, and\ncontribute to the project through the portal at:\nhttps://openbenchmark.github.io/BARS.\n","authors":["Jieming Zhu","Quanyu Dai","Liangcai Su","Rong Ma","Jinyang Liu","Guohao Cai","Xi Xiao","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.09626v5.pdf","comment":"Accepted by SIGIR 2022. Note that version v5 is updated to keep\n  consistency with the ACM camera-ready version"},{"id":"http://arxiv.org/abs/2207.08087v1","updated":"2022-07-17T06:50:35Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various NLP and IR\ndownstream applications have benefited from ESE due to its ability to discover\nknowledge. Although existing bootstrapping methods have achieved great\nprogress, most of them still rely on manually pre-defined context patterns. A\nnon-negligible shortcoming of the pre-defined context patterns is that they\ncannot be flexibly generalized to all kinds of semantic classes, and we call\nthis phenomenon as \"semantic sensitivity\". To address this problem, we devise a\ncontext pattern generation module that utilizes autoregressive language models\n(e.g., GPT-2) to automatically generate high-quality context patterns for\nentities. In addition, we propose the GAPA, a novel ESE framework that\nleverages the aforementioned GenerAted PAtterns to expand target entities.\nExtensive experiments and detailed analyses on three widely used datasets\ndemonstrate the effectiveness of our method. All the codes of our experiments\nwill be available for reproducibility.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1409.6182v5","updated":"2022-07-17T01:22:34Z","published":"2014-09-22T14:21:33Z","title":"A Benchmark Suite for Template Detection and Content Extraction","summary":"  Template detection and content extraction are two of the main areas of\ninformation retrieval applied to the Web. They perform different analyses over\nthe structure and content of webpages to extract some part of the document.\nHowever, their objective is different. While template detection identifies the\ntemplate of a webpage (usually comparing with other webpages of the same\nwebsite), content extraction identifies the main content of the webpage\ndiscarding the other part. Therefore, they are somehow complementary, because\nthe main content is not part of the template. It has been measured that\ntemplates represent between 40% and 50% of data on the Web. Therefore,\nidentifying templates is essential for indexing tasks because templates usually\ncontain irrelevant information such as advertisements, menus and banners.\nProcessing and storing this information is likely to lead to a waste of\nresources (storage space, bandwidth, etc.). Similarly, identifying the main\ncontent is essential for many information retrieval tasks. In this paper, we\npresent a benchmark suite to test different approaches for template detection\nand content extraction. The suite is public, and it contains real heterogeneous\nwebpages that have been labelled so that different techniques can be suitable\n(and automatically) compared.\n","authors":["Julián Alarte","Josep Silva"],"pdf_url":"https://arxiv.org/pdf/1409.6182v5.pdf","comment":"13 pages, 3 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.08184v1","updated":"2022-07-17T13:59:46Z","published":"2022-07-17T13:59:46Z","title":"Zero-Shot Temporal Action Detection via Vision-Language Prompting","summary":"  Existing temporal action detection (TAD) methods rely on large training data\nincluding segment-level annotations, limited to recognizing previously seen\nclasses alone during inference. Collecting and annotating a large training set\nfor each class of interest is costly and hence unscalable. Zero-shot TAD\n(ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize\nany unseen action classes. Meanwhile, ZS-TAD is also much more challenging with\nsignificantly less investigation. Inspired by the success of zero-shot image\nclassification aided by vision-language (ViL) models such as CLIP, we aim to\ntackle the more complex TAD task. An intuitive method is to integrate an\noff-the-shelf proposal detector with CLIP style classification. However, due to\nthe sequential localization (e.g, proposal generation) and classification\ndesign, it is prone to localization error propagation. To overcome this\nproblem, in this paper we propose a novel zero-Shot Temporal Action detection\nmodel via Vision-LanguagE prompting (STALE). Such a novel design effectively\neliminates the dependence between localization and classification by breaking\nthe route for error propagation in-between. We further introduce an interaction\nmechanism between classification and localization for improved optimization.\nExtensive experiments on standard ZS-TAD video benchmarks show that our STALE\nsignificantly outperforms state-of-the-art alternatives. Besides, our model\nalso yields superior results on supervised TAD over recent strong competitors.\nThe PyTorch implementation of STALE is available at\nhttps://github.com/sauradip/STALE.\n","authors":["Sauradip Nag","Xiatian Zhu","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2207.08184v1.pdf","comment":"ECCV 2022; Code available at https://github.com/sauradip/STALE"},{"id":"http://arxiv.org/abs/2207.08164v1","updated":"2022-07-17T13:04:44Z","published":"2022-07-17T13:04:44Z","title":"Action-conditioned On-demand Motion Generation","summary":"  We propose a novel framework, On-Demand MOtion Generation (ODMO), for\ngenerating realistic and diverse long-term 3D human motion sequences\nconditioned only on action types with an additional capability of\ncustomization. ODMO shows improvements over SOTA approaches on all traditional\nmotion evaluation metrics when evaluated on three public datasets (HumanAct12,\nUESTC, and MoCap). Furthermore, we provide both qualitative evaluations and\nquantitative metrics demonstrating several first-known customization\ncapabilities afforded by our framework, including mode discovery,\ninterpolation, and trajectory customization. These capabilities significantly\nwiden the spectrum of potential applications of such motion generation models.\nThe novel on-demand generative capabilities are enabled by innovations in both\nthe encoder and decoder architectures: (i) Encoder: Utilizing contrastive\nlearning in low-dimensional latent space to create a hierarchical embedding of\nmotion sequences, where not only the codes of different action types form\ndifferent groups, but within an action type, codes of similar inherent patterns\n(motion styles) cluster together, making them readily discoverable; (ii)\nDecoder: Using a hierarchical decoding strategy where the motion trajectory is\nreconstructed first and then used to reconstruct the whole motion sequence.\nSuch an architecture enables effective trajectory control. Our code is released\non the Github page: https://github.com/roychowdhuryresearch/ODMO\n","authors":["Qiujing Lu","Yipeng Zhang","Mingjian Lu","Vwani Roychowdhury"],"pdf_url":"https://arxiv.org/pdf/2207.08164v1.pdf","comment":"Accepted by ACMMM 2022, 13 pages, 5 figures"}]},"2022-07-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2108.12944v3","updated":"2022-07-16T22:37:41Z","published":"2021-08-30T01:11:10Z","title":"Selective Differential Privacy for Language Modeling","summary":"  With the increasing applications of language models, it has become crucial to\nprotect these models from leaking private information. Previous work has\nattempted to tackle this challenge by training RNN-based language models with\ndifferential privacy guarantees. However, applying classical differential\nprivacy to language models leads to poor model performance as the underlying\nprivacy notion is over-pessimistic and provides undifferentiated protection for\nall tokens in the data. Given that the private information in natural language\nis sparse (for example, the bulk of an email might not carry personally\nidentifiable information), we propose a new privacy notion, selective\ndifferential privacy, to provide rigorous privacy guarantees on the sensitive\nportion of the data to improve model utility. To realize such a new notion, we\ndevelop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based\nlanguage models. Besides language modeling, we also apply the method to a more\nconcrete application--dialog systems. Experiments on both language modeling and\ndialog system building show that the proposed privacy-preserving mechanism\nachieves better utilities while remaining safe under various privacy attacks\ncompared to the baselines. The data and code are released at\nhttps://github.com/wyshi/lm_privacy to facilitate future research .\n","authors":["Weiyan Shi","Aiqi Cui","Evan Li","Ruoxi Jia","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2108.12944v3.pdf","comment":"NAACL 2022"},{"id":"http://arxiv.org/abs/2207.08012v1","updated":"2022-07-16T20:37:46Z","published":"2022-07-16T20:37:46Z","title":"Meta-Referential Games to Learn Compositional Learning Behaviours","summary":"  Human beings use compositionality to generalise from past experiences to\nactual or fictive, novel experiences. To do so, we separate our experiences\ninto fundamental atomic components. These atomic components can then be\nrecombined in novel ways to support our ability to imagine and engage with\nnovel experiences. We frame this as the ability to learn to generalise\ncompositionally. And, we will refer to behaviours making use of this ability as\ncompositional learning behaviours (CLBs).\n  A central problem to learning CLBs is the resolution of a binding problem\n(BP) (by learning to, firstly, segregate the supportive stimulus components\nfrom the observation of multiple stimuli, and then, combine them in a single\nepisodic experience). While it is another feat of intelligence that human\nbeings perform with ease, it is not the case for state-of-the-art artificial\nagents.\n  Thus, in order to build artificial agents able to collaborate with human\nbeings, we propose to develop a novel benchmark to investigate agents'\nabilities to exhibit CLBs by solving a domain-agnostic version of the BP. We\ntake inspiration from the language emergence and grounding framework of\nreferential games and propose a meta-learning extension of referential games,\nentitled Meta-Referential Games, and use this framework to build our benchmark,\nthat we name Symbolic Behaviour Benchmark (S2B).\n  While it has the potential to test for more symbolic behaviours, rather than\nsolely CLBs, in the present paper, though, we solely focus on the single-agent\nlanguage grounding task that tests for CLBs. We provide baseline results for\nit, using state-of-the-art RL agents, and show that our proposed benchmark is a\ncompelling challenge that we hope will spur the research community towards\ndeveloping more capable artificial agents.\n","authors":["Kevin Denamganaï","Sondess Missaoui","James Alfred Walker"],"pdf_url":"https://arxiv.org/pdf/2207.08012v1.pdf","comment":"work in progress / under review at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2204.06535v3","updated":"2022-07-16T18:53:32Z","published":"2022-04-13T17:28:23Z","title":"Multilingual Event Linking to Wikidata","summary":"  We present a task of multilingual linking of events to a knowledge base. We\nautomatically compile a large-scale dataset for this task, comprising of 1.8M\nmentions across 44 languages referring to over 10.9K events from Wikidata. We\npropose two variants of the event linking task: 1) multilingual, where event\ndescriptions are from the same language as the mention, and 2) crosslingual,\nwhere all event descriptions are in English. On the two proposed tasks, we\ncompare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and\nmultilingual adaptations of the biencoder and crossencoder architectures from\nBLINK (Wu et al., 2020). In our experiments on the two task variants, we find\nboth biencoder and crossencoder models significantly outperform the BM25+\nbaseline. Our results also indicate that the crosslingual task is in general\nmore challenging than the multilingual task. To test the out-of-domain\ngeneralization of the proposed linking systems, we additionally create a\nWikinews-based evaluation set. We present qualitative analysis highlighting\nvarious aspects captured by the proposed dataset, including the need for\ntemporal reasoning over context and tackling diverse event descriptions across\nlanguages.\n","authors":["Adithya Pratapa","Rishubh Gupta","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2204.06535v3.pdf","comment":"Camera-ready for Multilingual Information Access workshop at NAACL\n  2022"},{"id":"http://arxiv.org/abs/2206.12879v2","updated":"2022-07-16T17:40:14Z","published":"2022-06-26T13:40:25Z","title":"Data Augmentation for Dementia Detection in Spoken Language","summary":"  Dementia is a growing problem as our society ages, and detection methods are\noften invasive and expensive. Recent deep-learning techniques can offer a\nfaster diagnosis and have shown promising results. However, they require large\namounts of labelled data which is not easily available for the task of dementia\ndetection. One effective solution to sparse data problems is data augmentation,\nthough the exact methods need to be selected carefully. To date, there has been\nno empirical study of data augmentation on Alzheimer's disease (AD) datasets\nfor NLP and speech processing. In this work, we investigate data augmentation\ntechniques for the task of AD detection and perform an empirical evaluation of\nthe different approaches on two kinds of models for both the text and audio\ndomains. We use a transformer-based model for both domains, and SVM and Random\nForest models for the text and audio domains, respectively. We generate\nadditional samples using traditional as well as deep learning based methods and\nshow that data augmentation improves performance for both the text- and\naudio-based models and that such results are comparable to state-of-the-art\nresults on the popular ADReSS set, with carefully crafted architectures and\nfeatures.\n","authors":["Anna Hlédiková","Dominika Woszczyk","Alican Akman","Soteris Demetriou","Björn Schuller"],"pdf_url":"https://arxiv.org/pdf/2206.12879v2.pdf","comment":"Accepted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2207.05221v3","updated":"2022-07-16T15:24:01Z","published":"2022-07-11T22:59:39Z","title":"Language Models (Mostly) Know What They Know","summary":"  We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.\n","authors":["Saurav Kadavath","Tom Conerly","Amanda Askell","Tom Henighan","Dawn Drain","Ethan Perez","Nicholas Schiefer","Zac Hatfield Dodds","Nova DasSarma","Eli Tran-Johnson","Scott Johnston","Sheer El-Showk","Andy Jones","Nelson Elhage","Tristan Hume","Anna Chen","Yuntao Bai","Sam Bowman","Stanislav Fort","Deep Ganguli","Danny Hernandez","Josh Jacobson","Jackson Kernion","Shauna Kravec","Liane Lovitt","Kamal Ndousse","Catherine Olsson","Sam Ringer","Dario Amodei","Tom Brown","Jack Clark","Nicholas Joseph","Ben Mann","Sam McCandlish","Chris Olah","Jared Kaplan"],"pdf_url":"https://arxiv.org/pdf/2207.05221v3.pdf","comment":"23+17 pages; refs added, typos fixed"},{"id":"http://arxiv.org/abs/2203.04616v2","updated":"2022-07-16T14:22:01Z","published":"2022-03-09T10:05:10Z","title":"PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of\n  Transformers for Patronizing and Condescending Language Detection","summary":"  Patronizing and condescending language (PCL) has a large harmful impact and\nis difficult to detect, both for human judges and existing NLP systems. At\nSemEval-2022 Task 4, we propose a novel Transformer-based model and its\nensembles to accurately understand such language context for PCL detection. To\nfacilitate comprehension of the subtle and subjective nature of PCL, two\nfine-tuning strategies are applied to capture discriminative features from\ndiverse linguistic behaviour and categorical distribution. The system achieves\nremarkable results on the official ranking, including 1st in Subtask 1 and 5th\nin Subtask 2. Extensive experiments on the task demonstrate the effectiveness\nof our system and its strategies.\n","authors":["Dou Hu","Mengyuan Zhou","Xiyang Du","Mengfei Yuan","Meizhi Jin","Lianxin Jiang","Yang Mo","Xiaofeng Shi"],"pdf_url":"https://arxiv.org/pdf/2203.04616v2.pdf","comment":"8 pages, submitted in SemEval-2022 Workshop (co-located with NAACL)"},{"id":"http://arxiv.org/abs/2207.07934v1","updated":"2022-07-16T13:02:54Z","published":"2022-07-16T13:02:54Z","title":"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative\n  Pretrained Language Model","summary":"  Text response generation for multimodal task-oriented dialog systems, which\naims to generate the proper text response given the multimodal context, is an\nessential yet challenging task. Although existing efforts have achieved\ncompelling success, they still suffer from two pivotal limitations: 1) overlook\nthe benefit of generative pre-training, and 2) ignore the textual context\nrelated knowledge. To address these limitations, we propose a novel dual\nknowledge-enhanced generative pretrained language model for multimodal\ntask-oriented dialog systems (DKMD), consisting of three key components: dual\nknowledge selection, dual knowledge-enhanced context learning, and\nknowledge-enhanced response generation. To be specific, the dual knowledge\nselection component aims to select the related knowledge according to both\ntextual and visual modalities of the given context. Thereafter, the dual\nknowledge-enhanced context learning component targets seamlessly integrating\nthe selected knowledge into the multimodal context learning from both global\nand local perspectives, where the cross-modal semantic relation is also\nexplored. Moreover, the knowledge-enhanced response generation component\ncomprises a revised BART decoder, where an additional dot-product\nknowledge-decoder attention sub-layer is introduced for explicitly utilizing\nthe knowledge to advance the text response generation. Extensive experiments on\na public dataset verify the superiority of the proposed DKMD over\nstate-of-the-art competitors.\n","authors":["Xiaolin Chen","Xuemeng Song","Liqiang Jing","Shuo Li","Linmei Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2207.07934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10705v2","updated":"2022-07-16T07:09:25Z","published":"2022-03-21T02:11:35Z","title":"Compression of Generative Pre-trained Language Models via Quantization","summary":"  The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.\n","authors":["Chaofan Tao","Lu Hou","Wei Zhang","Lifeng Shang","Xin Jiang","Qun Liu","Ping Luo","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2203.10705v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2205.05739v3","updated":"2022-07-16T06:06:53Z","published":"2022-05-11T19:14:39Z","title":"Learning to Retrieve Videos by Asking Questions","summary":"  The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be sub-optimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog, where the user refines retrieved\nresults by answering questions generated by an AI agent. Our novel multimodal\nquestion generator learns to ask questions that maximize the subsequent video\nretrieval performance using (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. We also demonstrate that our proposed\napproach generalizes to the real-world settings that involve interactions with\nreal humans, thus, demonstrating the robustness and generality of our framework\n","authors":["Avinash Madasu","Junier Oliva","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2205.05739v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12257v3","updated":"2022-07-16T05:41:40Z","published":"2022-03-23T08:07:32Z","title":"IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument\n  Mining Tasks","summary":"  Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.\n","authors":["Liying Cheng","Lidong Bing","Ruidan He","Qian Yu","Yan Zhang","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2203.12257v3.pdf","comment":"11 pages, 3 figures, accepted by ACL 2022"},{"id":"http://arxiv.org/abs/2207.07816v1","updated":"2022-07-16T02:48:54Z","published":"2022-07-16T02:48:54Z","title":"Sotto Voce: Federated Speech Recognition with Differential Privacy\n  Guarantees","summary":"  Speech data is expensive to collect, and incredibly sensitive to its sources.\nIt is often the case that organizations independently collect small datasets\nfor their own use, but often these are not performant for the demands of\nmachine learning. Organizations could pool these datasets together and jointly\nbuild a strong ASR system; sharing data in the clear, however, comes with\ntremendous risk, in terms of intellectual property loss as well as loss of\nprivacy of the individuals who exist in the dataset. In this paper, we offer a\npotential solution for learning an ML model across multiple organizations where\nwe can provide mathematical guarantees limiting privacy loss. We use a\nFederated Learning approach built on a strong foundation of Differential\nPrivacy techniques. We apply these to a senone classification prototype and\ndemonstrate that the model improves with the addition of private data while\nstill respecting privacy.\n","authors":["Michael Shoemate","Kevin Jett","Ethan Cowan","Sean Colbath","James Honaker","Prasanna Muthukumar"],"pdf_url":"https://arxiv.org/pdf/2207.07816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.10080v2","updated":"2022-07-16T02:27:58Z","published":"2021-05-21T01:10:03Z","title":"Boosting Span-based Joint Entity and Relation Extraction via Squence\n  Tagging Mechanism","summary":"  Span-based joint extraction simultaneously conducts named entity recognition\n(NER) and relation extraction (RE) in text span form. Recent studies have shown\nthat token labels can convey crucial task-specific information and enrich token\nsemantics. However, as far as we know, due to completely abstain from sequence\ntagging mechanism, all prior span-based work fails to use token label\nin-formation. To solve this problem, we pro-pose Sequence Tagging enhanced\nSpan-based Network (STSN), a span-based joint extrac-tion network that is\nenhanced by token BIO label information derived from sequence tag-ging based\nNER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral\narchitecture to build STSN, and each atten-tion layer consists of three basic\nattention units. The deep neural architecture first learns seman-tic\nrepresentations for token labels and span-based joint extraction, and then\nconstructs in-formation interactions between them, which also realizes\nbidirectional information interac-tions between span-based NER and RE.\nFur-thermore, we extend the BIO tagging scheme to make STSN can extract\noverlapping en-tity. Experiments on three benchmark datasets show that our\nmodel consistently outperforms previous optimal models by a large margin,\ncreating new state-of-the-art results.\n","authors":["Bin Ji","Shasha Li","Jie Yu","Jun Ma","Huijun Liu"],"pdf_url":"https://arxiv.org/pdf/2105.10080v2.pdf","comment":"10pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2207.03300v2","updated":"2022-07-16T02:21:04Z","published":"2022-07-07T13:52:06Z","title":"Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity\n  Recognition","summary":"  For Named Entity Recognition (NER), sequence labeling-based and span-based\nparadigms are quite different. Previous research has demonstrated that the two\nparadigms have clear complementary advantages, but few models have attempted to\nleverage these advantages in a single NER model as far as we know. In our\nprevious work, we proposed a paradigm known as Bundling Learning (BL) to\naddress the above problem. The BL paradigm bundles the two NER paradigms,\nenabling NER models to jointly tune their parameters by weighted summing each\nparadigm's training loss. However, three critical issues remain unresolved:\nWhen does BL work? Why does BL work? Can BL enhance the existing\nstate-of-the-art (SOTA) NER models? To address the first two issues, we\nimplement three NER models, involving a sequence labeling-based model--SeqNER,\na span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER\ntogether. We draw two conclusions regarding the two issues based on the\nexperimental results on eleven NER datasets from five domains. We then apply BL\nto five existing SOTA NER models to investigate the third issue, consisting of\nthree sequence labeling-based models and two span-based models. Experimental\nresults indicate that BL consistently enhances their performance, suggesting\nthat it is possible to construct a new SOTA NER system by incorporating BL into\nthe current SOTA system. Moreover, we find that BL reduces both entity boundary\nand type prediction errors. In addition, we compare two commonly used labeling\ntagging methods as well as three types of span semantic representations.\n","authors":["Bin Ji","Shasha Li","Jie Yu","Jun Ma","Huijun Liu"],"pdf_url":"https://arxiv.org/pdf/2207.03300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.10936v2","updated":"2022-07-16T01:27:59Z","published":"2022-02-18T15:15:46Z","title":"A Survey of Vision-Language Pre-Trained Models","summary":"  As transformer evolves, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve downstream task\nperformance becomes a focus of multimodal learning. In this paper, we review\nthe recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the\ncore content, we first briefly introduce several ways to encode raw images and\ntexts to single-modal embeddings before pre-training. Then, we dive into the\nmainstream architectures of VL-PTMs in modeling the interaction between text\nand image representations. We further present widely-used pre-training tasks,\nand then we introduce some common downstream tasks. We finally conclude this\npaper and present some promising research directions. Our survey aims to\nprovide researchers with synthesis and pointer to related research.\n","authors":["Yifan Du","Zikang Liu","Junyi Li","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2202.10936v2.pdf","comment":"Accepted by IJCAI-2022 survey track"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.07940v1","updated":"2022-07-16T13:29:26Z","published":"2022-07-16T13:29:26Z","title":"HQANN: Efficient and Robust Similarity Search for Hybrid Queries with\n  Structured and Unstructured Constraints","summary":"  The in-memory approximate nearest neighbor search (ANNS) algorithms have\nachieved great success for fast high-recall query processing, but are extremely\ninefficient when handling hybrid queries with unstructured (i.e., feature\nvectors) and structured (i.e., related attributes) constraints. In this paper,\nwe present HQANN, a simple yet highly efficient hybrid query processing\nframework which can be easily embedded into existing proximity graph-based ANNS\nalgorithms. We guarantee both low latency and high recall by leveraging\nnavigation sense among attributes and fusing vector similarity search with\nattribute filtering. Experimental results on both public and in-house datasets\ndemonstrate that HQANN is 10x faster than the state-of-the-art hybrid ANNS\nsolutions to reach the same recall quality and its performance is hardly\naffected by the complexity of attributes. It can reach 99\\% recall@10 in just\naround 50 microseconds On GLOVE-1.2M with thousands of attribute constraints.\n","authors":["Wei Wu","Junlin He","Yu Qiao","Guoheng Fu","Li Liu","Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2207.07940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07790v1","updated":"2022-07-16T00:10:12Z","published":"2022-07-16T00:10:12Z","title":"BCRLSP: An Offline Reinforcement Learning Framework for Sequential\n  Targeted Promotion","summary":"  We utilize an offline reinforcement learning (RL) model for sequential\ntargeted promotion in the presence of budget constraints in a real-world\nbusiness environment. In our application, the mobile app aims to boost customer\nretention by sending cash bonuses to customers and control the costs of such\ncash bonuses during each time period. To achieve the multi-task goal, we\npropose the Budget Constrained Reinforcement Learning for Sequential Promotion\n(BCRLSP) framework to determine the value of cash bonuses to be sent to users.\nWe first find out the target policy and the associated Q-values that maximizes\nthe user retention rate using an RL model. A linear programming (LP) model is\nthen added to satisfy the constraints of promotion costs. We solve the LP\nproblem by maximizing the Q-values of actions learned from the RL model given\nthe budget constraints. During deployment, we combine the offline RL model with\nthe LP model to generate a robust policy under the budget constraints. Using\nboth online and offline experiments, we demonstrate the efficacy of our\napproach by showing that BCRLSP achieves a higher long-term customer retention\nrate and a lower cost than various baselines. Taking advantage of the near\nreal-time cost control method, the proposed framework can easily adapt to data\nwith a noisy behavioral policy and/or meet flexible budget constraints.\n","authors":["Fanglin Chen","Xiao Liu","Bo Tang","Feiyu Xiong","Serim Hwang","Guomian Zhuang"],"pdf_url":"https://arxiv.org/pdf/2207.07790v1.pdf","comment":"8 pages, DRL4IR@SIGIR"}],"Multimedia":[{"id":"http://arxiv.org/abs/2006.15984v6","updated":"2022-07-16T16:24:55Z","published":"2020-06-29T12:32:16Z","title":"New Framework for Code-Mapping-based Reversible Data Hiding in JPEG\n  Images","summary":"  Code mapping (CM) is an efficient technique for reversible data hiding (RDH)\nin JPEG images, which embeds data by constructing a mapping relationship\nbetween the used and unused codes in the JPEG bitstream. This study presents a\nnew framework for designing a CM-based RDH method. First, a new code mapping\nstrategy is proposed to suppress file size expansion and improve applicability.\nBased on our proposed strategy, the mapped codes are redefined by creating a\nnew Huffman table rather than selecting them from the unused codes in the\noriginal Huffman table. The critical issue of designing the CM-based RDH\nmethod, that is, constructing code mapping, is converted into a combinatorial\noptimization problem. This study proposes a novel CM-based RDH method that\nutilizes a genetic algorithm (GA). The experimental results demonstrate that\nthe proposed method achieves a high embedding capacity with no signal\ndistortion while suppressing file size expansion.\n","authors":["Yang Du","Zhaoxia Yin"],"pdf_url":"https://arxiv.org/pdf/2006.15984v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.14448v5","updated":"2022-07-16T14:40:40Z","published":"2021-11-29T11:02:41Z","title":"AVA-AVD: Audio-Visual Speaker Diarization in the Wild","summary":"  Audio-visual speaker diarization aims at detecting \"who spoke when\" using\nboth auditory and visual signals. Existing audio-visual diarization datasets\nare mainly focused on indoor environments like meeting rooms or news studios,\nwhich are quite different from in-the-wild videos in many scenarios such as\nmovies, documentaries, and audience sitcoms. To develop diarization methods for\nthese challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)\ndataset. Our experiments demonstrate that adding AVA-AVD into training set can\nproduce significantly better diarization models for in-the-wild videos despite\nthat the data is relatively small. Moreover, this benchmark is challenging due\nto the diverse scenes, complicated acoustic conditions, and completely\noff-screen speakers. As a first step towards addressing the challenges, we\ndesign the Audio-Visual Relation Network (AVR-Net) which introduces a simple\nyet effective modality mask to capture discriminative information based on face\nvisibility. Experiments show that our method not only can outperform\nstate-of-the-art methods but is more robust as varying the ratio of off-screen\nspeakers. Our data and code has been made publicly available at\nhttps://github.com/showlab/AVA-AVD.\n","authors":["Eric Zhongcong Xu","Zeyang Song","Satoshi Tsutsui","Chao Feng","Mang Ye","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2111.14448v5.pdf","comment":"ACMMM 2022"},{"id":"http://arxiv.org/abs/2207.07935v1","updated":"2022-07-16T13:09:25Z","published":"2022-07-16T13:09:25Z","title":"Visually-aware Acoustic Event Detection using Heterogeneous Graphs","summary":"  Perception of auditory events is inherently multimodal relying on both audio\nand visual cues. A large number of existing multimodal approaches process each\nmodality using modality-specific models and then fuse the embeddings to encode\nthe joint information. In contrast, we employ heterogeneous graphs to\nexplicitly capture the spatial and temporal relationships between the\nmodalities and represent detailed information about the underlying signal.\nUsing heterogeneous graph approaches to address the task of visually-aware\nacoustic event classification, which serves as a compact, efficient and\nscalable way to represent data in the form of graphs. Through heterogeneous\ngraphs, we show efficiently modelling of intra- and inter-modality\nrelationships both at spatial and temporal scales. Our model can easily be\nadapted to different scales of events through relevant hyperparameters.\nExperiments on AudioSet, a large benchmark, shows that our model achieves\nstate-of-the-art performance.\n","authors":["Amir Shirian","Krishna Somandepalli","Victor Sanchez","Tanaya Guha"],"pdf_url":"https://arxiv.org/pdf/2207.07935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07934v1","updated":"2022-07-16T13:02:54Z","published":"2022-07-16T13:02:54Z","title":"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative\n  Pretrained Language Model","summary":"  Text response generation for multimodal task-oriented dialog systems, which\naims to generate the proper text response given the multimodal context, is an\nessential yet challenging task. Although existing efforts have achieved\ncompelling success, they still suffer from two pivotal limitations: 1) overlook\nthe benefit of generative pre-training, and 2) ignore the textual context\nrelated knowledge. To address these limitations, we propose a novel dual\nknowledge-enhanced generative pretrained language model for multimodal\ntask-oriented dialog systems (DKMD), consisting of three key components: dual\nknowledge selection, dual knowledge-enhanced context learning, and\nknowledge-enhanced response generation. To be specific, the dual knowledge\nselection component aims to select the related knowledge according to both\ntextual and visual modalities of the given context. Thereafter, the dual\nknowledge-enhanced context learning component targets seamlessly integrating\nthe selected knowledge into the multimodal context learning from both global\nand local perspectives, where the cross-modal semantic relation is also\nexplored. Moreover, the knowledge-enhanced response generation component\ncomprises a revised BART decoder, where an additional dot-product\nknowledge-decoder attention sub-layer is introduced for explicitly utilizing\nthe knowledge to advance the text response generation. Extensive experiments on\na public dataset verify the superiority of the proposed DKMD over\nstate-of-the-art competitors.\n","authors":["Xiaolin Chen","Xuemeng Song","Liqiang Jing","Shuo Li","Linmei Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2207.07934v1.pdf","comment":null}]},"2022-07-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2112.00969v2","updated":"2022-07-19T17:43:05Z","published":"2021-12-02T03:56:09Z","title":"Object-Centric Unsupervised Image Captioning","summary":"  Image captioning is a longstanding problem in the field of computer vision\nand natural language processing. To date, researchers have produced impressive\nstate-of-the-art performance in the age of deep learning. Most of these\nstate-of-the-art, however, requires large volume of annotated image-caption\npairs in order to train their models. When given an image dataset of interests,\npractitioner needs to annotate the caption for each image in the training set\nand this process needs to happen for each newly collected image dataset. In\nthis paper, we explore the task of unsupervised image captioning which utilizes\nunpaired images and texts to train the model so that the texts can come from\ndifferent sources than the images. A main school of research on this topic that\nhas been shown to be effective is to construct pairs from the images and texts\nin the training set according to their overlap of objects. Unlike in the\nsupervised setting, these constructed pairings are however not guaranteed to\nhave fully overlapping set of objects. Our work in this paper overcomes this by\nharvesting objects corresponding to a given sentence from the training set,\neven if they don't belong to the same image. When used as input to a\ntransformer, such mixture of objects enables larger if not full object\ncoverage, and when supervised by the corresponding sentence, produced results\nthat outperform current state of the art unsupervised methods by a significant\nmargin. Building upon this finding, we further show that (1) additional\ninformation on relationship between objects and attributes of objects also\nhelps in boosting performance; and (2) our method also extends well to\nnon-English image captioning, which usually suffers from a scarcer level of\nannotations. Our findings are supported by strong empirical results. Our code\nis available at https://github.com/zihangm/obj-centric-unsup-caption.\n","authors":["Zihang Meng","David Yang","Xuefei Cao","Ashish Shah","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2112.00969v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.05280v2","updated":"2022-07-19T15:25:04Z","published":"2022-07-12T03:23:02Z","title":"Effective Few-Shot Named Entity Linking by Meta-Learning","summary":"  Entity linking aims to link ambiguous mentions to their corresponding\nentities in a knowledge base, which is significant and fundamental for various\ndownstream applications, e.g., knowledge base completion, question answering,\nand information extraction. While great efforts have been devoted to this task,\nmost of these studies follow the assumption that large-scale labeled data is\navailable. However, when the labeled data is insufficient for specific domains\ndue to labor-intensive annotation work, the performance of existing algorithms\nwill suffer an intolerable decline. In this paper, we endeavor to solve the\nproblem of few-shot entity linking, which only requires a minimal amount of\nin-domain labeled data and is more practical in real situations. Specifically,\nwe firstly propose a novel weak supervision strategy to generate non-trivial\nsynthetic entity-mention pairs based on mention rewriting. Since the quality of\nthe synthetic data has a critical impact on effective model training, we\nfurther design a meta-learning mechanism to assign different weights to each\nsynthetic entity-mention pair automatically. Through this way, we can\nprofoundly exploit rich and precious semantic information to derive a\nwell-trained entity linking model under the few-shot setting. The experiments\non real-world datasets show that the proposed method can extensively improve\nthe state-of-the-art few-shot entity linking model and achieve impressive\nperformance when only a small amount of labeled data is available. Moreover, we\nalso demonstrate the outstanding ability of the model's transferability.\n","authors":["Xiuxing Li","Zhenyu Li","Zhengyan Zhang","Ning Liu","Haitao Yuan","Wei Zhang","Zhiyuan Liu","Jianyong Wang"],"pdf_url":"https://arxiv.org/pdf/2207.05280v2.pdf","comment":"14 pages, 4 figures. Accepted at IEEE ICDE 2022"},{"id":"http://arxiv.org/abs/2207.05751v2","updated":"2022-07-19T14:32:56Z","published":"2022-07-12T04:11:05Z","title":"A Synergistic Compilation Workflow for Tackling Crosstalk in Quantum\n  Machines","summary":"  Near-term quantum systems tend to be noisy. Crosstalk noise has been\nrecognized as one of several major types of noises in superconducting Noisy\nIntermediate-Scale Quantum (NISQ) devices. Crosstalk arises from the concurrent\nexecution of two-qubit gates on nearby qubits, such as \\texttt{CX}. It might\nsignificantly raise the error rate of gates in comparison to running them\nindividually. Crosstalk can be mitigated through scheduling or hardware machine\ntuning. Prior scientific studies, however, manage crosstalk at a really late\nphase in the compilation process, usually after hardware mapping is done. It\nmay miss great opportunities of optimizing algorithm logic, routing, and\ncrosstalk at the same time. In this paper, we push the envelope by considering\nall these factors simultaneously at the very early compilation stage. We\npropose a crosstalk-aware quantum program compilation framework called CQC that\ncan enhance crosstalk mitigation while achieving satisfactory circuit depth.\nMoreover, we identify opportunities for translation from intermediate\nrepresentation to the circuit for application-specific crosstalk mitigation,\nfor instance, the \\texttt{CX} ladder construction in variational quantum\neigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices\nshow that our framework can significantly reduce the error rate by up to\n6$\\times$, with only $\\sim$60\\% circuit depth compared to state-of-the-art gate\nscheduling approaches. In particular, for VQE, we demonstrate 49\\% circuit\ndepth reduction with 9.6\\% fidelity improvement over prior art on the H4\nmolecule using IBMQ Guadalupe. Our CQC framework will be released on GitHub.\n","authors":["Fei Hua","Yuwei Jin","Ang Li","Yanhao Chen","Chi Zhang","Ari Hayes","Hang Gao","Eddy Z. Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.05751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09238v1","updated":"2022-07-19T12:49:02Z","published":"2022-07-19T12:49:02Z","title":"Formal Algorithms for Transformers","summary":"  This document aims to be a self-contained, mathematically precise overview of\ntransformer architectures and algorithms (*not* results). It covers what\ntransformers are, how they are trained, what they are used for, their key\narchitectural components, and a preview of the most prominent models. The\nreader is assumed to be familiar with basic ML terminology and simpler neural\nnetwork architectures such as MLPs.\n","authors":["Mary Phuong","Marcus Hutter"],"pdf_url":"https://arxiv.org/pdf/2207.09238v1.pdf","comment":"16 pages, 15 algorithms"},{"id":"http://arxiv.org/abs/2207.09163v1","updated":"2022-07-19T10:11:22Z","published":"2022-07-19T10:11:22Z","title":"Urdu Speech and Text Based Sentiment Analyzer","summary":"  Discovering what other people think has always been a key aspect of our\ninformation-gathering strategy. People can now actively utilize information\ntechnology to seek out and comprehend the ideas of others, thanks to the\nincreased availability and popularity of opinion-rich resources such as online\nreview sites and personal blogs. Because of its crucial function in\nunderstanding people's opinions, sentiment analysis (SA) is a crucial task.\nExisting research, on the other hand, is primarily focused on the English\nlanguage, with just a small amount of study devoted to low-resource languages.\nFor sentiment analysis, this work presented a new multi-class Urdu dataset\nbased on user evaluations. The tweeter website was used to get Urdu dataset.\nOur proposed dataset includes 10,000 reviews that have been carefully\nclassified into two categories by human experts: positive, negative. The\nprimary purpose of this research is to construct a manually annotated dataset\nfor Urdu sentiment analysis and to establish the baseline result. Five\ndifferent lexicon- and rule-based algorithms including Naivebayes, Stanza,\nTextblob, Vader, and Flair are employed and the experimental results show that\nFlair with an accuracy of 70% outperforms other tested algorithms.\n","authors":["Waqar Ahmad","Maryam Edalati"],"pdf_url":"https://arxiv.org/pdf/2207.09163v1.pdf","comment":"Sentiment Analysis, Opinion Mining, Urdu language, polarity\n  assessment, lexicon-based method"},{"id":"http://arxiv.org/abs/2203.12886v5","updated":"2022-07-19T10:07:32Z","published":"2022-03-24T07:15:24Z","title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool\n  Children","summary":"  Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying Automatic Speech Recognition(ASR) system is useless, since they of\npre-trained on voices, that are different from children's voices in terms of\nfrequency and amplitude. We constructed an ASR for our cognitive test system to\nsolve this issue using the Wav2Vec 2.0 model with a new pre-training objective\ncalled Random Frequency Pitch(RFP). In addition, we used our new dataset to\nfine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)\ntests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian\nsection of the CommonVoice dataset. Furthermore, our novel methodology produces\npositive outcomes in zero- and few-shot scenarios.\n","authors":["Amirhossein Abaskohi","Fatemeh Mortazavi","Hadi Moradi"],"pdf_url":"https://arxiv.org/pdf/2203.12886v5.pdf","comment":"8 pages, 5 figures, 4 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2207.09157v1","updated":"2022-07-19T09:55:04Z","published":"2022-07-19T09:55:04Z","title":"On the cross-lingual transferability of multilingual prototypical models\n  across NLU tasks","summary":"  Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n","authors":["Oralie Cattan","Christophe Servan","Sophie Rosset"],"pdf_url":"https://arxiv.org/pdf/2207.09157v1.pdf","comment":"Accepted to the ACL workshop METANLP 2021"},{"id":"http://arxiv.org/abs/2207.09152v1","updated":"2022-07-19T09:47:08Z","published":"2022-07-19T09:47:08Z","title":"Benchmarking Transformers-based models on French Spoken Language\n  Understanding tasks","summary":"  In the last five years, the rise of the self-attentional Transformer-based\narchitectures led to state-of-the-art performances over many natural language\ntasks. Although these approaches are increasingly popular, they require large\namounts of data and computational resources. There is still a substantial need\nfor benchmarking methodologies ever upwards on under-resourced languages in\ndata-scarce application conditions. Most pre-trained language models were\nmassively studied using the English language and only a few of them were\nevaluated on French. In this paper, we propose a unified benchmark, focused on\nevaluating models quality and their ecological impact on two well-known French\nspoken language understanding tasks. Especially we benchmark thirteen\nwell-established Transformer-based models on the two available spoken language\nunderstanding tasks for French: MEDIA and ATIS-FR. Within this framework, we\nshow that compact models can reach comparable results to bigger ones while\ntheir ecological impact is considerably lower. However, this assumption is\nnuanced and depends on the considered compression method.\n","authors":["Oralie Cattan","Sahar Ghannay","Christophe Servan","Sophie Rosset"],"pdf_url":"https://arxiv.org/pdf/2207.09152v1.pdf","comment":"Accepted paper at INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2207.09150v1","updated":"2022-07-19T09:46:15Z","published":"2022-07-19T09:46:15Z","title":"On the Usability of Transformers-based models for a French\n  Question-Answering task","summary":"  For many tasks, state-of-the-art results have been achieved with\nTransformer-based architectures, resulting in a paradigmatic shift in practices\nfrom the use of task-specific architectures to the fine-tuning of pre-trained\nlanguage models. The ongoing trend consists in training models with an\never-increasing amount of data and parameters, which requires considerable\nresources. It leads to a strong search to improve resource efficiency based on\nalgorithmic and hardware improvements evaluated only for English. This raises\nquestions about their usability when applied to small-scale learning problems,\nfor which a limited amount of training data is available, especially for\nunder-resourced languages tasks. The lack of appropriately sized corpora is a\nhindrance to applying data-driven and transfer learning-based approaches with\nstrong instability cases. In this paper, we establish a state-of-the-art of the\nefforts dedicated to the usability of Transformer-based models and propose to\nevaluate these improvements on the question-answering performances of French\nlanguage which have few resources. We address the instability relating to data\nscarcity by investigating various training strategies with data augmentation,\nhyperparameters optimization and cross-lingual transfer. We also introduce a\nnew compact model for French FrALBERT which proves to be competitive in\nlow-resource settings.\n","authors":["Oralie Cattan","Christophe Servan","Sophie Rosset"],"pdf_url":"https://arxiv.org/pdf/2207.09150v1.pdf","comment":"French compact model paper: FrALBERT, Accepted to RANLP 2021"},{"id":"http://arxiv.org/abs/2204.06508v2","updated":"2022-07-19T09:43:43Z","published":"2022-04-13T16:45:33Z","title":"FactGraph: Evaluating Factuality in Summarization with Semantic Graph\n  Representations","summary":"  Despite recent improvements in abstractive summarization, most current\napproaches generate summaries that are not factually consistent with the source\ndocument, severely restricting their trust and usage in real-world\napplications. Recent works have shown promising improvements in factuality\nerror identification using text or dependency arc entailments; however, they do\nnot consider the entire semantic graph simultaneously. To this end, we propose\nFactGraph, a method that decomposes the document and the summary into\nstructured meaning representations (MR), which are more suitable for factuality\nevaluation. MRs describe core semantic concepts and their relations,\naggregating the main content in both document and summary in a canonical form,\nand reducing data sparsity. FactGraph encodes such graphs using a graph encoder\naugmented with structure-aware adapters to capture interactions among the\nconcepts based on the graph connectivity, along with text representations using\nan adapter-based text encoder. Experiments on different benchmarks for\nevaluating factuality show that FactGraph outperforms previous approaches by up\nto 15%. Furthermore, FactGraph improves performance on identifying content\nverifiability errors and better captures subsentence-level factual\ninconsistencies.\n","authors":["Leonardo F. R. Ribeiro","Mengwen Liu","Iryna Gurevych","Markus Dreyer","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2204.06508v2.pdf","comment":"NAACL 2022 (15 pages)"},{"id":"http://arxiv.org/abs/2204.08387v3","updated":"2022-07-19T06:41:15Z","published":"2022-04-18T16:19:52Z","title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image\n  Masking","summary":"  Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\n\\url{https://aka.ms/layoutlmv3}.\n","authors":["Yupan Huang","Tengchao Lv","Lei Cui","Yutong Lu","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2204.08387v3.pdf","comment":"ACM Multimedia 2022"},{"id":"http://arxiv.org/abs/2207.09099v1","updated":"2022-07-19T06:30:37Z","published":"2022-07-19T06:30:37Z","title":"Analyzing Bagging Methods for Language Models","summary":"  Modern language models leverage increasingly large numbers of parameters to\nachieve performance on natural language understanding tasks. Ensembling these\nmodels in specific configurations for downstream tasks show even further\nperformance improvements. In this paper, we perform an analysis of bagging\nlanguage models and compare single language models to bagged ensembles that are\nroughly equivalent in terms of final model size. We explore an array of model\nbagging configurations for natural language understanding tasks with final\nensemble sizes ranging from 300M parameters to 1.5B parameters and determine\nthat our ensembling methods are at best roughly equivalent to single LM\nbaselines. We note other positive effects of bagging and pruning in specific\nscenarios according to findings in our experiments such as variance reduction\nand minor performance improvements.\n","authors":["Pranab Islam","Shaan Khosla","Arthur Lok","Mudit Saxena"],"pdf_url":"https://arxiv.org/pdf/2207.09099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09094v1","updated":"2022-07-19T06:09:55Z","published":"2022-07-19T06:09:55Z","title":"MoEC: Mixture of Expert Clusters","summary":"  Sparsely Mixture of Experts (MoE) has received great interest due to its\npromising scaling capability with affordable computational overhead. MoE\nconverts dense layers into sparse experts, and utilizes a gated routing network\nto make experts conditionally activated. However, as the number of experts\ngrows, MoE with outrageous parameters suffers from overfitting and sparse data\nallocation. Such problems are especially severe on tasks with limited data,\nthus hindering the progress for MoE models to improve performance by scaling\nup. In this work, we propose Mixture of Expert Clusters - a general approach to\nenable expert layers to learn more diverse and appropriate knowledge by\nimposing variance-based constraints on the routing stage. We further propose a\ncluster-level expert dropout strategy specifically designed for the expert\ncluster structure. Our experiments reveal that MoEC could improve performance\non machine translation and natural language understanding tasks, and raise the\nperformance upper bound for scaling up experts under limited data. We also\nverify that MoEC plays a positive role in mitigating overfitting and sparse\ndata allocation.\n","authors":["Yuan Xie","Shaohan Huang","Tianyu Chen","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2207.09094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09085v1","updated":"2022-07-19T05:43:49Z","published":"2022-07-19T05:43:49Z","title":"Can You Fool AI by Doing a 180? $\\unicode{x2013}$ A Case Study on\n  Authorship Analysis of Texts by Arata Osada","summary":"  This paper is our attempt at answering a twofold question covering the areas\nof ethics and authorship analysis. Firstly, since the methods used for\nperforming authorship analysis imply that an author can be recognized by the\ncontent he or she creates, we were interested in finding out whether it would\nbe possible for an author identification system to correctly attribute works to\nauthors if in the course of years they have undergone a major psychological\ntransition. Secondly, and from the point of view of the evolution of an\nauthor's ethical values, we checked what it would mean if the authorship\nattribution system encounters difficulties in detecting single authorship. We\nset out to answer those questions through performing a binary authorship\nanalysis task using a text classifier based on a pre-trained transformer model\nand a baseline method relying on conventional similarity metrics. For the test\nset, we chose works of Arata Osada, a Japanese educator and specialist in the\nhistory of education, with half of them being books written before the World\nWar II and another half in the 1950s, in between which he underwent a\ntransformation in terms of political opinions. As a result, we were able to\nconfirm that in the case of texts authored by Arata Osada in a time span of\nmore than 10 years, while the classification accuracy drops by a large margin\nand is substantially lower than for texts by other non-fiction writers,\nconfidence scores of the predictions remain at a similar level as in the case\nof a shorter time span, indicating that the classifier was in many instances\ntricked into deciding that texts written over a time span of multiple years\nwere actually written by two different people, which in turn leads us to\nbelieve that such a change can affect authorship analysis, and that historical\nevents have great impact on a person's ethical outlook as expressed in their\nwritings.\n","authors":["Jagna Nieuwazny","Karol Nowakowski","Michal Ptaszynski","Fumito Masui"],"pdf_url":"https://arxiv.org/pdf/2207.09085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09083v1","updated":"2022-07-19T05:42:14Z","published":"2022-07-19T05:42:14Z","title":"Relational Future Captioning Model for Explaining Likely Collisions in\n  Daily Tasks","summary":"  Domestic service robots that support daily tasks are a promising solution for\nelderly or disabled people. It is crucial for domestic service robots to\nexplain the collision risk before they perform actions. In this paper, our aim\nis to generate a caption about a future event. We propose the Relational Future\nCaptioning Model (RFCM), a crossmodal language generation model for the future\ncaptioning task. The RFCM has the Relational Self-Attention Encoder to extract\nthe relationships between events more effectively than the conventional\nself-attention in transformers. We conducted comparison experiments, and the\nresults show the RFCM outperforms a baseline method on two datasets.\n","authors":["Motonari Kambara","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2207.09083v1.pdf","comment":"Accepted for presentation at ICIP2022"},{"id":"http://arxiv.org/abs/2207.09078v1","updated":"2022-07-19T05:24:13Z","published":"2022-07-19T05:24:13Z","title":"ILASR: Privacy-Preserving Incremental Learning for AutomaticSpeech\n  Recognition at Production Scale","summary":"  Incremental learning is one paradigm to enable model building and updating at\nscale with streaming data. For end-to-end automatic speech recognition (ASR)\ntasks, the absence of human annotated labels along with the need for privacy\npreserving policies for model building makes it a daunting challenge. Motivated\nby these challenges, in this paper we use a cloud based framework for\nproduction systems to demonstrate insights from privacy preserving incremental\nlearning for automatic speech recognition (ILASR). By privacy preserving, we\nmean, usage of ephemeral data which are not human annotated. This system is a\nstep forward for production levelASR models for incremental/continual learning\nthat offers near real-time test-bed for experimentation in the cloud for\nend-to-end ASR, while adhering to privacy-preserving policies. We show that the\nproposed system can improve the production models significantly(3%) over a new\ntime period of six months even in the absence of human annotated labels with\nvarying levels of weak supervision and large batch sizes in incremental\nlearning. This improvement is 20% over test sets with new words and phrases in\nthe new time period. We demonstrate the effectiveness of model building in a\nprivacy-preserving incremental fashion for ASR while further exploring the\nutility of having an effective teacher model and use of large batch sizes.\n","authors":["Gopinath Chennupati","Milind Rao","Gurpreet Chadha","Aaron Eakin","Anirudh Raju","Gautam Tiwari","Anit Kumar Sahu","Ariya Rastrow","Jasha Droppo","Andy Oberlin","Buddha Nandanoor","Prahalad Venkataramanan","Zheng Wu","Pankaj Sitpure"],"pdf_url":"https://arxiv.org/pdf/2207.09078v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2207.09076v1","updated":"2022-07-19T05:23:18Z","published":"2022-07-19T05:23:18Z","title":"Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation","summary":"  Some Transformer-based models can perform cross-lingual transfer learning:\nthose models can be trained on a specific task in one language and give\nrelatively good results on the same task in another language, despite having\nbeen pre-trained on monolingual tasks only. But, there is no consensus yet on\nwhether those transformer-based models learn universal patterns across\nlanguages. We propose a word-level task-agnostic method to evaluate the\nalignment of contextualized representations built by such models. We show that\nour method provides more accurate translated word pairs than previous methods\nto evaluate word-level alignment. And our results show that some inner layers\nof multilingual Transformer-based models outperform other explicitly aligned\nrepresentations, and even more so according to a stricter definition of\nmultilingual alignment.\n","authors":["Félix Gaschi","François Plesse","Parisa Rastin","Yannick Toussaint"],"pdf_url":"https://arxiv.org/pdf/2207.09076v1.pdf","comment":"accepted at IJCNN 2022"},{"id":"http://arxiv.org/abs/2207.09068v1","updated":"2022-07-19T04:45:41Z","published":"2022-07-19T04:45:41Z","title":"PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic\n  Search","summary":"  Since BERT (Devlin et al., 2018), learning contextualized word embeddings has\nbeen a de-facto standard in NLP. However, the progress of learning\ncontextualized phrase embeddings is hindered by the lack of a human-annotated,\nphrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of\n~28K of noun phrases accompanied by their contextual Wikipedia pages and a\nsuite of three tasks of increasing difficulty for evaluating the quality of\nphrase embeddings. We find that training on our dataset improves ranking\nmodels' accuracy and remarkably pushes Question Answering (QA) models to\nnear-human accuracy which is 95% Exact Match (EM) on semantic search given a\nquery phrase and a passage. Interestingly, we find evidence that such\nimpressive performance is because the QA models learn to better capture the\ncommon meaning of a phrase regardless of its actual context. That is, on our\nPhrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially\n(60% EM), failing to differentiate between two different senses of the same\nphrase under two different contexts. Further results on our 3-task PiC\nbenchmark reveal that learning contextualized phrase embeddings remains an\ninteresting, open challenge.\n","authors":["Thang M. Pham","Seunghyun Yoon","Trung Bui","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2207.09068v1.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.08087v2","updated":"2022-07-19T03:40:45Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various NLP and IR\ndownstream applications have benefited from ESE due to its ability to discover\nknowledge. Although existing bootstrapping methods have achieved great\nprogress, most of them still rely on manually pre-defined context patterns. A\nnon-negligible shortcoming of the pre-defined context patterns is that they\ncannot be flexibly generalized to all kinds of semantic classes, and we call\nthis phenomenon as \"semantic sensitivity\". To address this problem, we devise a\ncontext pattern generation module that utilizes autoregressive language models\n(e.g., GPT-2) to automatically generate high-quality context patterns for\nentities. In addition, we propose the GAPA, a novel ESE framework that\nleverages the aforementioned GenerAted PAtterns to expand target entities.\nExtensive experiments and detailed analyses on three widely used datasets\ndemonstrate the effectiveness of our method. All the codes of our experiments\nwill be available for reproducibility.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2110.08534v3","updated":"2022-07-19T02:09:00Z","published":"2021-10-16T09:59:33Z","title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging\n  Corpora","summary":"  Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.\n","authors":["Xisen Jin","Dejiao Zhang","Henghui Zhu","Wei Xiao","Shang-Wen Li","Xiaokai Wei","Andrew Arnold","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2110.08534v3.pdf","comment":"Accepted at NAACL 2022; fixed Figure 7 (a)(b) in Appendix"},{"id":"http://arxiv.org/abs/2112.03799v2","updated":"2022-07-19T23:34:56Z","published":"2021-12-07T16:16:01Z","title":"A pragmatic account of the weak evidence effect","summary":"  Language is not only used to inform. We often seek to persuade by arguing in\nfavor of a particular view. Persuasion raises a number of challenges for\nclassical accounts of belief updating, as information cannot be taken at face\nvalue. How should listeners account for a speaker's \"hidden agenda\" when\nincorporating new information? Here, we extend recent probabilistic models of\nrecursive social reasoning to allow for persuasive goals and show that our\nmodel provides a new pragmatic explanation for why weakly favorable arguments\nmay backfire, a phenomenon known as the weak evidence effect. Critically, our\nmodel predicts a relationship between belief updating and speaker expectations:\nweak evidence should only backfire when speakers are expected to act under\npersuasive goals, implying the absence of stronger evidence. We introduce a\nsimple experimental paradigm called the Stick Contest to measure the extent to\nwhich the weak evidence effect depends on speaker expectations, and show that a\npragmatic listener model accounts for the empirical data better than\nalternative models. Our findings suggest potential avenues for rational models\nof social reasoning to further illuminate decision-making phenomena.\n","authors":["Samuel A. Barnett","Thomas L. Griffiths","Robert D. Hawkins"],"pdf_url":"https://arxiv.org/pdf/2112.03799v2.pdf","comment":"in press at Open Mind"},{"id":"http://arxiv.org/abs/2111.14188v3","updated":"2022-07-19T22:56:56Z","published":"2021-11-28T16:02:13Z","title":"An Empirical Study of Topic Transition in Dialogue","summary":"  Transitioning between topics is a natural component of human-human dialog.\nAlthough topic transition has been studied in dialogue for decades, only a\nhandful of corpora based studies have been performed to investigate the\nsubtleties of topic transitions. Thus, this study annotates 215 conversations\nfrom the switchboard corpus and investigates how variables such as length,\nnumber of topic transitions, topic transitions share by participants and\nturns/topic are related. This work presents an empirical study on topic\ntransition in switchboard corpus followed by modelling topic transition with a\nprecision of 83% for in-domain(id) test set and 82% on 10 out-of-domain}(ood)\ntest set. It is envisioned that this work will help in emulating human-human\nlike topic transition in open-domain dialog systems.\n","authors":["Mayank Soni","Brendan Spillane","Emer Gilmartin","Christian Saam","Benjamin R. Cowan","Vincent Wade"],"pdf_url":"https://arxiv.org/pdf/2111.14188v3.pdf","comment":"5 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2207.09566v1","updated":"2022-07-19T21:52:37Z","published":"2022-07-19T21:52:37Z","title":"Human-guided Collaborative Problem Solving: A Natural Language based\n  Framework","summary":"  We consider the problem of human-machine collaborative problem solving as a\nplanning task coupled with natural language communication. Our framework\nconsists of three components -- a natural language engine that parses the\nlanguage utterances to a formal representation and vice-versa, a concept\nlearner that induces generalized concepts for plans based on limited\ninteractions with the user, and an HTN planner that solves the task based on\nhuman interaction. We illustrate the ability of this framework to address the\nkey challenges of collaborative problem solving by demonstrating it on a\ncollaborative building task in a Minecraft-based blocksworld domain. The\naccompanied demo video is available at https://youtu.be/q1pWe4aahF0.\n","authors":["Harsha Kokel","Mayukh Das","Rakibul Islam","Julia Bonn","Jon Cai","Soham Dan","Anjali Narayan-Chen","Prashant Jayannavar","Janardhan Rao Doppa","Julia Hockenmaier","Sriraam Natarajan","Martha Palmer","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2207.09566v1.pdf","comment":"ICAPS 2021 (demo track)"},{"id":"http://arxiv.org/abs/2207.09562v1","updated":"2022-07-19T21:32:59Z","published":"2022-07-19T21:32:59Z","title":"QuoteKG: A Multilingual Knowledge Graph of Quotes","summary":"  Quotes of public figures can mark turning points in history. A quote can\nexplain its originator's actions, foreshadowing political or personal decisions\nand revealing character traits. Impactful quotes cross language barriers and\ninfluence the general population's reaction to specific stances, always facing\nthe risk of being misattributed or taken out of context. The provision of a\ncross-lingual knowledge graph of quotes that establishes the authenticity of\nquotes and their contexts is of great importance to allow the exploration of\nthe lives of important people as well as topics from the perspective of what\nwas actually said. In this paper, we present QuoteKG, the first multilingual\nknowledge graph of quotes. We propose the QuoteKG creation pipeline that\nextracts quotes from Wikiquote, a free and collaboratively created collection\nof quotes in many languages, and aligns different mentions of the same quote.\nQuoteKG includes nearly one million quotes in $55$ languages, said by more than\n$69,000$ people of public interest across a wide range of topics. QuoteKG is\npublicly available and can be accessed via a SPARQL endpoint.\n","authors":["Tin Kuculo","Simon Gottschalk","Elena Demidova"],"pdf_url":"https://arxiv.org/pdf/2207.09562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.17190v3","updated":"2022-07-19T19:19:47Z","published":"2022-03-31T17:12:26Z","title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme\n  Representations for Text to Speech","summary":"  Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT\n","authors":["Guangyan Zhang","Kaitao Song","Xu Tan","Daxin Tan","Yuzi Yan","Yanqing Liu","Gang Wang","Wei Zhou","Tao Qin","Tan Lee","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2203.17190v3.pdf","comment":"Accepted by interspeech 2022"},{"id":"http://arxiv.org/abs/2207.09519v1","updated":"2022-07-19T19:12:11Z","published":"2022-07-19T19:12:11Z","title":"Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification","summary":"  Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations using large-scale image-text\npairs. It shows impressive performance on downstream tasks by zero-shot\nknowledge transfer. To further enhance CLIP's adaption capability, existing\nmethods proposed to fine-tune additional learnable modules, which significantly\nimproves the few-shot performance but introduces extra training time and\ncomputational resources. In this paper, we propose a training-free adaption\nmethod for CLIP to conduct few-shot classification, termed as Tip-Adapter,\nwhich not only inherits the training-free advantage of zero-shot CLIP but also\nperforms comparably to those training-required approaches. Tip-Adapter\nconstructs the adapter via a key-value cache model from the few-shot training\nset, and updates the prior knowledge encoded in CLIP by feature retrieval. On\ntop of that, the performance of Tip-Adapter can be further boosted to be\nstate-of-the-art on ImageNet by fine-tuning the cache model for 10$\\times$\nfewer epochs than existing methods, which is both effective and efficient. We\nconduct extensive experiments of few-shot classification on 11 datasets to\ndemonstrate the superiority of our proposed methods. Code is released at\nhttps://github.com/gaopengcuhk/Tip-Adapter.\n","authors":["Renrui Zhang","Zhang Wei","Rongyao Fang","Peng Gao","Kunchang Li","Jifeng Dai","Yu Qiao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2207.09519v1.pdf","comment":"Accepted by ECCV 2022. arXiv admin note: substantial text overlap\n  with arXiv:2111.03930"},{"id":"http://arxiv.org/abs/2207.09514v1","updated":"2022-07-19T18:55:29Z","published":"2022-07-19T18:55:29Z","title":"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition,\n  Translation, and Understanding","summary":"  This paper presents recent progress on integrating speech separation and\nenhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE\nwork, numerous features have been added, including recent state-of-the-art\nspeech enhancement models with their respective training and evaluation\nrecipes. Importantly, a new interface has been designed to flexibly combine\nspeech enhancement front-ends with other tasks, including automatic speech\nrecognition (ASR), speech translation (ST), and spoken language understanding\n(SLU). To showcase such integration, we performed experiments on carefully\ndesigned synthetic datasets for noisy-reverberant multi-channel ST and SLU\ntasks, which can be used as benchmark corpora for future research. In addition\nto these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and\nsingle-channel SE approaches. Results show that the integration of SE\nfront-ends with back-end tasks is a promising research direction even for tasks\nbesides ASR, especially in the multi-channel scenario. The code is available\nonline at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU\ndatasets, which are another contribution of this work, are released on\nHuggingFace.\n","authors":["Yen-Ju Lu","Xuankai Chang","Chenda Li","Wangyou Zhang","Samuele Cornell","Zhaoheng Ni","Yoshiki Masuyama","Brian Yan","Robin Scheibler","Zhong-Qiu Wang","Yu Tsao","Yanmin Qian","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2207.09514v1.pdf","comment":"To appear in Interspeech 2022"},{"id":"http://arxiv.org/abs/2205.09224v2","updated":"2022-07-19T18:41:36Z","published":"2022-05-18T21:52:11Z","title":"Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner","summary":"  Large language models have achieved high performance on various question\nanswering (QA) benchmarks, but the explainability of their output remains\nelusive. Structured explanations, called entailment trees, were recently\nsuggested as a way to explain and inspect a QA system's answer. In order to\nbetter generate such entailment trees, we propose an architecture called\nIterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a\ngiven hypothesis by systematically generating a step-by-step explanation from\ntextual premises. The IRGR model iteratively searches for suitable premises,\nconstructing a single entailment step at a time. Contrary to previous\napproaches, our method combines generation steps and retrieval of premises,\nallowing the model to leverage intermediate conclusions, and mitigating the\ninput size limit of baseline encoder-decoder models. We conduct experiments\nusing the EntailmentBank dataset, where we outperform existing benchmarks on\npremise retrieval and entailment tree generation, with around 300% gain in\noverall correctness.\n","authors":["Danilo Ribeiro","Shen Wang","Xiaofei Ma","Rui Dong","Xiaokai Wei","Henry Zhu","Xinchi Chen","Zhiheng Huang","Peng Xu","Andrew Arnold","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2205.09224v2.pdf","comment":"published in NAACL 2022"},{"id":"http://arxiv.org/abs/2207.10572v1","updated":"2022-07-19T19:17:10Z","published":"2022-07-19T19:17:10Z","title":"Big Data and Education: using big data analytics in language learning","summary":"  Working with big data using data mining tools is rapidly becoming a trend in\neducation industry. The combination of the current capacity to collect, store,\nmanage and process data in a timely manner, and data from online educational\nplatforms represents an unprecedented opportunity for educational institutes,\nlearners, educators, and researchers. In this position paper, we consider some\nbasic concepts as well as most popular tools, methods and techniques regarding\nEducational Data Mining and Learning Analytics, and discuss big data\napplications in language learning, in particular.\n","authors":["Vahid Ashrafimoghari"],"pdf_url":"https://arxiv.org/pdf/2207.10572v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2207.09450v1","updated":"2022-07-19T17:59:59Z","published":"2022-07-19T17:59:59Z","title":"Human-to-Robot Imitation in the Wild","summary":"  We approach the problem of learning by watching humans in the wild. While\ntraditional approaches in Imitation and Reinforcement Learning are promising\nfor learning in the real world, they are either sample inefficient or are\nconstrained to lab settings. Meanwhile, there has been a lot of success in\nprocessing passive, unstructured human data. We propose tackling this problem\nvia an efficient one-shot robot learning algorithm, centered around learning\nfrom a third-person perspective. We call our method WHIRL: In-the-Wild Human\nImitating Robot Learning. WHIRL extracts a prior over the intent of the human\ndemonstrator, using it to initialize our agent's policy. We introduce an\nefficient real-world policy learning scheme that improves using interactions.\nOur key contributions are a simple sampling-based policy optimization approach,\na novel objective function for aligning human and robot videos as well as an\nexploration method to boost sample efficiency. We show one-shot generalization\nand success in real-world settings, including 20 different manipulation tasks\nin the wild. Videos and talk at https://human2robot.github.io\n","authors":["Shikhar Bahl","Abhinav Gupta","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2207.09450v1.pdf","comment":"Published at RSS 2022. Demos at https://human2robot.github.io"},{"id":"http://arxiv.org/abs/2207.09446v1","updated":"2022-07-19T17:59:01Z","published":"2022-07-19T17:59:01Z","title":"ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model","summary":"  We present ShapeCrafter, a neural network for recursive text-conditioned 3D\nshape generation. Existing methods to generate text-conditioned 3D shapes\nconsume an entire text prompt to generate a 3D shape in a single step. However,\nhumans tend to describe shapes recursively-we may start with an initial\ndescription and progressively add details based on intermediate results. To\ncapture this recursive process, we introduce a method to generate a 3D shape\ndistribution, conditioned on an initial phrase, that gradually evolves as more\nphrases are added. Since existing datasets are insufficient for training this\napproach, we present Text2Shape++, a large dataset of 369K shape-text pairs\nthat supports recursive shape generation. To capture local details that are\noften used to refine shape descriptions, we build on top of vector-quantized\ndeep implicit functions that generate a distribution of high-quality shapes.\nResults show that our method can generate shapes consistent with text\ndescriptions, and shapes evolve gradually as more phrases are added. Our method\nsupports shape editing, extrapolation, and can enable new applications in\nhuman-machine collaboration for creative design.\n","authors":["Rao Fu","Xiao Zhan","Yiwen Chen","Daniel Ritchie","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2207.09446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09445v1","updated":"2022-07-19T17:58:33Z","published":"2022-07-19T17:58:33Z","title":"PoserNet: Refining Relative Camera Poses Exploiting Object Detections","summary":"  The estimation of the camera poses associated with a set of images commonly\nrelies on feature matches between the images. In contrast, we are the first to\naddress this challenge by using objectness regions to guide the pose estimation\nproblem rather than explicit semantic object detections. We propose Pose\nRefiner Network (PoserNet) a light-weight Graph Neural Network to refine the\napproximate pair-wise relative camera poses. PoserNet exploits associations\nbetween the objectness regions - concisely expressed as bounding boxes - across\nmultiple views to globally refine sparsely connected view graphs. We evaluate\non the 7-Scenes dataset across varied sizes of graphs and show how this process\ncan be beneficial to optimisation-based Motion Averaging algorithms improving\nthe median error on the rotation by 62 degrees with respect to the initial\nestimates obtained based on bounding boxes. Code and data are available at\nhttps://github.com/IIT-PAVIS/PoserNet.\n","authors":["Matteo Taiana","Matteo Toso","Stuart James","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2207.09445v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09442v1","updated":"2022-07-19T17:57:40Z","published":"2022-07-19T17:57:40Z","title":"Theseus: A Library for Differentiable Nonlinear Optimization","summary":"  We present Theseus, an efficient application-agnostic open source library for\ndifferentiable nonlinear least squares (DNLS) optimization built on PyTorch,\nproviding a common framework for end-to-end structured learning in robotics and\nvision. Existing DNLS implementations are application specific and do not\nalways incorporate many ingredients important for efficiency. Theseus is\napplication-agnostic, as we illustrate with several example applications that\nare built using the same underlying differentiable components, such as\nsecond-order optimizers, standard costs functions, and Lie groups. For\nefficiency, Theseus incorporates support for sparse solvers, automatic\nvectorization, batching, GPU acceleration, and gradient computation with\nimplicit differentiation and direct loss minimization. We do extensive\nperformance evaluation in a set of applications, demonstrating significant\nefficiency gains and better scalability when these features are incorporated.\nProject page: https://sites.google.com/view/theseus-ai\n","authors":["Luis Pineda","Taosha Fan","Maurizio Monge","Shobha Venkataraman","Paloma Sodhi","Ricky Chen","Joseph Ortiz","Daniel DeTone","Austin Wang","Stuart Anderson","Jing Dong","Brandon Amos","Mustafa Mukadam"],"pdf_url":"https://arxiv.org/pdf/2207.09442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07795v2","updated":"2022-07-19T17:52:13Z","published":"2022-07-16T01:02:52Z","title":"RCRN: Real-world Character Image Restoration Network via Skeleton\n  Extraction","summary":"  Constructing high-quality character image datasets is challenging because\nreal-world images are often affected by image degradation. There are\nlimitations when applying current image restoration methods to such real-world\ncharacter images, since (i) the categories of noise in character images are\ndifferent from those in general images; (ii) real-world character images\nusually contain more complex image degradation, e.g., mixed noise at different\nnoise levels. To address these problems, we propose a real-world character\nrestoration network (RCRN) to effectively restore degraded character images,\nwhere character skeleton information and scale-ensemble feature extraction are\nutilized to obtain better restoration performance. The proposed method consists\nof a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet\naims to preserve the structural consistency of the character and normalize\ncomplex noise. Then, CiRNet reconstructs clean images from degraded character\nimages and their skeletons. Due to the lack of benchmarks for real-world\ncharacter image restoration, we constructed a dataset containing 1,606\ncharacter images with real-world degradation to evaluate the validity of the\nproposed method. The experimental results demonstrate that RCRN outperforms\nstate-of-the-art methods quantitatively and qualitatively.\n","authors":["Daqian Shi","Xiaolei Diao","Hao Tang","Xiaomin Li","Hao Xing","Hao Xu"],"pdf_url":"https://arxiv.org/pdf/2207.07795v2.pdf","comment":"Accepted to ACM MM 2022"},{"id":"http://arxiv.org/abs/2207.07798v2","updated":"2022-07-19T17:46:58Z","published":"2022-07-16T01:11:30Z","title":"CharFormer: A Glyph Fusion based Attentive Framework for High-precision\n  Character Image Denoising","summary":"  Degraded images commonly exist in the general sources of character images,\nleading to unsatisfactory character recognition results. Existing methods have\ndedicated efforts to restoring degraded character images. However, the\ndenoising results obtained by these methods do not appear to improve character\nrecognition performance. This is mainly because current methods only focus on\npixel-level information and ignore critical features of a character, such as\nits glyph, resulting in character-glyph damage during the denoising process. In\nthis paper, we introduce a novel generic framework based on glyph fusion and\nattention mechanisms, i.e., CharFormer, for precisely recovering character\nimages without changing their inherent glyphs. Unlike existing frameworks,\nCharFormer introduces a parallel target task for capturing additional\ninformation and injecting it into the image denoising backbone, which will\nmaintain the consistency of character glyphs during character image denoising.\nMoreover, we utilize attention-based networks for global-local feature\ninteraction, which will help to deal with blind denoising and enhance denoising\nperformance. We compare CharFormer with state-of-the-art methods on multiple\ndatasets. The experimental results show the superiority of CharFormer\nquantitatively and qualitatively.\n","authors":["Daqian Shi","Xiaolei Diao","Lida Shi","Hao Tang","Yang Chi","Chuntao Li","Hao Xu"],"pdf_url":"https://arxiv.org/pdf/2207.07798v2.pdf","comment":"Accepted by ACM MM 2022"},{"id":"http://arxiv.org/abs/2112.00969v2","updated":"2022-07-19T17:43:05Z","published":"2021-12-02T03:56:09Z","title":"Object-Centric Unsupervised Image Captioning","summary":"  Image captioning is a longstanding problem in the field of computer vision\nand natural language processing. To date, researchers have produced impressive\nstate-of-the-art performance in the age of deep learning. Most of these\nstate-of-the-art, however, requires large volume of annotated image-caption\npairs in order to train their models. When given an image dataset of interests,\npractitioner needs to annotate the caption for each image in the training set\nand this process needs to happen for each newly collected image dataset. In\nthis paper, we explore the task of unsupervised image captioning which utilizes\nunpaired images and texts to train the model so that the texts can come from\ndifferent sources than the images. A main school of research on this topic that\nhas been shown to be effective is to construct pairs from the images and texts\nin the training set according to their overlap of objects. Unlike in the\nsupervised setting, these constructed pairings are however not guaranteed to\nhave fully overlapping set of objects. Our work in this paper overcomes this by\nharvesting objects corresponding to a given sentence from the training set,\neven if they don't belong to the same image. When used as input to a\ntransformer, such mixture of objects enables larger if not full object\ncoverage, and when supervised by the corresponding sentence, produced results\nthat outperform current state of the art unsupervised methods by a significant\nmargin. Building upon this finding, we further show that (1) additional\ninformation on relationship between objects and attributes of objects also\nhelps in boosting performance; and (2) our method also extends well to\nnon-English image captioning, which usually suffers from a scarcer level of\nannotations. Our findings are supported by strong empirical results. Our code\nis available at https://github.com/zihangm/obj-centric-unsup-caption.\n","authors":["Zihang Meng","David Yang","Xuefei Cao","Ashish Shah","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2112.00969v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09425v1","updated":"2022-07-19T17:36:55Z","published":"2022-07-19T17:36:55Z","title":"Geometric Features Informed Multi-person Human-object Interaction\n  Recognition in Videos","summary":"  Human-Object Interaction (HOI) recognition in videos is important for\nanalyzing human activity. Most existing work focusing on visual features\nusually suffer from occlusion in the real-world scenarios. Such a problem will\nbe further complicated when multiple people and objects are involved in HOIs.\nConsider that geometric features such as human pose and object position provide\nmeaningful information to understand HOIs, we argue to combine the benefits of\nboth visual and geometric features in HOI recognition, and propose a novel\nTwo-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The\ngeometric-level graph models the interdependency between geometric features of\nhumans and objects, while the fusion-level graph further fuses them with visual\nfeatures of humans and objects. To demonstrate the novelty and effectiveness of\nour method in challenging scenarios, we propose a new multi-person HOI dataset\n(MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120\n(single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our\nsuperior performance compared to state-of-the-arts.\n","authors":["Tanqiu Qiao","Qianhui Men","Frederick W. B. Li","Yoshiki Kubotani","Shigeo Morishima","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2207.09425v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2203.14714v2","updated":"2022-07-19T17:33:56Z","published":"2022-03-28T13:00:06Z","title":"Primitive-based Shape Abstraction via Nonparametric Bayesian Inference","summary":"  3D shape abstraction has drawn great interest over the years. Apart from\nlow-level representations such as meshes and voxels, researchers also seek to\nsemantically abstract complex objects with basic geometric primitives. Recent\ndeep learning methods rely heavily on datasets, with limited generality to\nunseen categories. Furthermore, abstracting an object accurately yet with a\nsmall number of primitives still remains a challenge. In this paper, we propose\na novel non-parametric Bayesian statistical method to infer an abstraction,\nconsisting of an unknown number of geometric primitives, from a point cloud. We\nmodel the generation of points as observations sampled from an infinite mixture\nof Gaussian Superquadric Taper Models (GSTM). Our approach formulates the\nabstraction as a clustering problem, in which: 1) each point is assigned to a\ncluster via the Chinese Restaurant Process (CRP); 2) a primitive representation\nis optimized for each cluster, and 3) a merging post-process is incorporated to\nprovide a concise representation. We conduct extensive experiments on two\ndatasets. The results indicate that our method outperforms the state-of-the-art\nin terms of accuracy and is generalizable to various types of objects.\n","authors":["Yuwei Wu","Weixiao Liu","Sipu Ruan","Gregory S. Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2203.14714v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2204.00604v2","updated":"2022-07-19T17:17:14Z","published":"2022-04-01T17:53:39Z","title":"Quantized GAN for Complex Music Generation from Dance Videos","summary":"  We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal\nframework that generates complex musical samples conditioned on dance videos.\nOur proposed framework takes dance video frames and human body motions as\ninput, and learns to generate music samples that plausibly accompany the\ncorresponding input. Unlike most existing conditional music generation works\nthat generate specific types of mono-instrumental sounds using symbolic audio\nrepresentations (e.g., MIDI), and that usually rely on pre-defined musical\nsynthesizers, in this work we generate dance music in complex styles (e.g.,\npop, breaking, etc.) by employing a Vector Quantized (VQ) audio representation,\nand leverage both its generality and high abstraction capacity of its symbolic\nand continuous counterparts. By performing an extensive set of experiments on\nmultiple datasets, and following a comprehensive evaluation protocol, we assess\nthe generative qualities of our proposal against alternatives. The attained\nquantitative results, which measure the music consistency, beats\ncorrespondence, and music diversity, demonstrate the effectiveness of our\nproposed method. Last but not least, we curate a challenging dance-music\ndataset of in-the-wild TikTok videos, which we use to further demonstrate the\nefficacy of our approach in real-world applications -- and which we hope to\nserve as a starting point for relevant future research.\n","authors":["Ye Zhu","Kyle Olszewski","Yu Wu","Panos Achlioptas","Menglei Chai","Yan Yan","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2204.00604v2.pdf","comment":"Dataset and code at https://github.com/L-YeZhu/D2M-GAN"},{"id":"http://arxiv.org/abs/2207.02196v3","updated":"2022-07-19T17:15:35Z","published":"2022-07-05T17:55:42Z","title":"Accelerating Score-based Generative Models with Preconditioned Diffusion\n  Sampling","summary":"  Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. However, a fundamental limitation is that their\ninference is very slow due to a need for many (e.g., 2000) iterations of\nsequential computations. An intuitive acceleration method is to reduce the\nsampling iterations which however causes severe performance degradation. We\ninvestigate this problem by viewing the diffusion sampling process as a\nMetropolis adjusted Langevin algorithm, which helps reveal the underlying cause\nto be ill-conditioned curvature. Under this insight, we propose a\nmodel-agnostic preconditioned diffusion sampling (PDS) method that leverages\nmatrix preconditioning to alleviate the aforementioned problem. Crucially, PDS\nis proven theoretically to converge to the original target distribution of a\nSGM, no need for retraining. Extensive experiments on three image datasets with\na variety of resolutions and diversity validate that PDS consistently\naccelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In\nparticular, PDS can accelerate by up to 29x on more challenging high resolution\n(1024x1024) image generation.\n","authors":["Hengyuan Ma","Li Zhang","Xiatian Zhu","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2207.02196v3.pdf","comment":"ECCV 2022. Code is available at https://github.com/fudan-zvg/PDS"},{"id":"http://arxiv.org/abs/2207.09413v1","updated":"2022-07-19T17:13:06Z","published":"2022-07-19T17:13:06Z","title":"SphereFed: Hyperspherical Federated Learning","summary":"  Federated Learning aims at training a global model from multiple\ndecentralized devices (i.e. clients) without exchanging their private local\ndata. A key challenge is the handling of non-i.i.d. (independent identically\ndistributed) data across multiple clients that may induce disparities of their\nlocal features. We introduce the Hyperspherical Federated Learning (SphereFed)\nframework to address the non-i.i.d. issue by constraining learned\nrepresentations of data points to be on a unit hypersphere shared by clients.\nSpecifically, all clients learn their local representations by minimizing the\nloss with respect to a fixed classifier whose weights span the unit\nhypersphere. After federated training in improving the global model, this\nclassifier is further calibrated with a closed-form solution by minimizing a\nmean squared loss. We show that the calibration solution can be computed\nefficiently and distributedly without direct access of local data. Extensive\nexperiments indicate that our SphereFed approach is able to improve the\naccuracy of multiple existing federated learning algorithms by a considerable\nmargin (up to 6% on challenging datasets) with enhanced computation and\ncommunication efficiency across datasets and model architectures.\n","authors":["Xin Dong","Sai Qian Zhang","Ang Li","H. T. Kung"],"pdf_url":"https://arxiv.org/pdf/2207.09413v1.pdf","comment":"European Conference on Computer Vision 2022"},{"id":"http://arxiv.org/abs/2207.09412v1","updated":"2022-07-19T17:12:48Z","published":"2022-07-19T17:12:48Z","title":"Det6D: A Ground-Aware Full-Pose 3D Object Detector for Improving Terrain\n  Robustness","summary":"  Accurate 3D object detection with LiDAR is critical for autonomous driving.\nExisting research is all based on the flat-world assumption. However, the\nactual road can be complex with steep sections, which breaks the premise.\nCurrent methods suffer from performance degradation in this case due to\ndifficulty correctly detecting objects on sloped terrain. In this work, we\npropose Det6D, the first full-degree-of-freedom 3D object detector without\nspatial and postural limitations, to improve terrain robustness. We choose the\npoint-based framework by founding their capability of detecting objects in the\nentire spatial range. To predict full-degree poses, including pitch and roll,\nwe design a ground-aware orientation branch that leverages the local ground\nconstraints. Given the difficulty of long-tail non-flat scene data collection\nand 6D pose annotation, we present Slope-Aug, a data augmentation method for\nsynthesizing non-flat terrain from existing datasets recorded in flat scenes.\nExperiments on various datasets demonstrate the effectiveness and robustness of\nour method in different terrains. We further conducted an extended experiment\nto explore how the network predicts the two extra poses. The proposed modules\nare plug-and-play for existing point-based frameworks. The code is available at\nhttps://github.com/HITSZ-NRSL/De6D.\n","authors":["Junyuan Ouyang","Haoyao Chen"],"pdf_url":"https://arxiv.org/pdf/2207.09412v1.pdf","comment":"8 pages, 9 figures, submit to RA-L"},{"id":"http://arxiv.org/abs/2205.00186v2","updated":"2022-07-19T17:08:46Z","published":"2022-04-30T07:19:03Z","title":"Reliable Label Correction is a Good Booster When Learning with Extremely\n  Noisy Labels","summary":"  Learning with noisy labels has aroused much research interest since data\nannotations, especially for large-scale datasets, may be inevitably imperfect.\nRecent approaches resort to a semi-supervised learning problem by dividing\ntraining samples into clean and noisy sets. This paradigm, however, is prone to\nsignificant degeneration under heavy label noise, as the number of clean\nsamples is too small for conventional methods to behave well. In this paper, we\nintroduce a novel framework, termed as LC-Booster, to explicitly tackle\nlearning under extreme noise. The core idea of LC-Booster is to incorporate\nlabel correction into the sample selection, so that more purified samples,\nthrough the reliable label correction, can be utilized for training, thereby\nalleviating the confirmation bias. Experiments show that LC-Booster advances\nstate-of-the-art results on several noisy-label benchmarks, including CIFAR-10,\nCIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\\% noise\nratio, LC-Booster achieves 92.9\\% and 48.4\\% accuracy on CIFAR-10 and\nCIFAR-100, surpassing state-of-the-art methods by a large margin.\n","authors":["Kai Wang","Xiangyu Peng","Shuo Yang","Jianfei Yang","Zheng Zhu","Xinchao Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2205.00186v2.pdf","comment":"LC-Booster is an efficient method for LNL problems"},{"id":"http://arxiv.org/abs/2203.04099v2","updated":"2022-07-19T16:54:03Z","published":"2022-03-08T14:08:47Z","title":"VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer","summary":"  This paper presents an audio-visual approach for voice separation which\nproduces state-of-the-art results at a low latency in two scenarios: speech and\nsinging voice. The model is based on a two-stage network. Motion cues are\nobtained with a lightweight graph convolutional network that processes face\nlandmarks. Then, both audio and motion features are fed to an audio-visual\ntransformer which produces a fairly good estimation of the isolated target\nsource. In a second stage, the predominant voice is enhanced with an audio-only\nnetwork. We present different ablation studies and comparison to\nstate-of-the-art methods. Finally, we explore the transferability of models\ntrained for speech separation in the task of singing voice separation. The\ndemos, code, and weights are available in https://ipcv.github.io/VoViT/\n","authors":["Juan F. Montesinos","Venkatesh S. Kadandale","Gloria Haro"],"pdf_url":"https://arxiv.org/pdf/2203.04099v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2112.01398v2","updated":"2022-07-19T16:51:51Z","published":"2021-12-02T16:39:35Z","title":"TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation","summary":"  In this paper, we conduct a study on the state-of-the-art methods for\ntext-to-image synthesis and propose a framework to evaluate these methods. We\nconsider syntheses where an image contains a single or multiple objects. Our\nstudy outlines several issues in the current evaluation pipeline: (i) for image\nquality assessment, a commonly used metric, e.g., Inception Score (IS), is\noften either miscalibrated for the single-object case or misused for the\nmulti-object case; (ii) for text relevance and object accuracy assessment,\nthere is an overfitting phenomenon in the existing R-precision (RP) and\nSemantic Object Accuracy (SOA) metrics, respectively; (iii) for multi-object\ncase, many vital factors for evaluation, e.g., object fidelity, positional\nalignment, counting alignment, are largely dismissed; (iv) the ranking of the\nmethods based on current metrics is highly inconsistent with real images. To\novercome these issues, we propose a combined set of existing and new metrics to\nsystematically evaluate the methods. For existing metrics, we offer an improved\nversion of IS named IS* by using temperature scaling to calibrate the\nconfidence of the classifier used by IS; we also propose a solution to mitigate\nthe overfitting issues of RP and SOA. For new metrics, we develop counting\nalignment, positional alignment, object-centric IS, and object-centric FID\nmetrics for evaluating the multi-object case. We show that benchmarking with\nour bag of metrics results in a highly consistent ranking among existing\nmethods that is well-aligned with human evaluation. As a by-product, we create\nAttnGAN++, a simple but strong baseline for the benchmark by stabilizing the\ntraining of AttnGAN using spectral normalization. We also release our toolbox,\nso-called TISE, for advocating fair and consistent evaluation of text-to-image\nmodels.\n","authors":["Tan M. Dinh","Rang Nguyen","Binh-Son Hua"],"pdf_url":"https://arxiv.org/pdf/2112.01398v2.pdf","comment":"Accepted to ECCV 2022; TISE toolbox is available at\n  https://github.com/VinAIResearch/tise-toolbox"},{"id":"http://arxiv.org/abs/2207.09399v1","updated":"2022-07-19T16:48:39Z","published":"2022-07-19T16:48:39Z","title":"RCLane: Relay Chain Prediction for Lane Detection","summary":"  Lane detection is an important component of many real-world autonomous\nsystems. Despite a wide variety of lane detection approaches have been\nproposed, reporting steady benchmark improvements over time, lane detection\nremains a largely unsolved problem. This is because most of the existing lane\ndetection methods either treat the lane detection as a dense prediction or a\ndetection task, few of them consider the unique topologies (Y-shape,\nFork-shape, nearly horizontal lane) of the lane markers, which leads to\nsub-optimal solution. In this paper, we present a new method for lane detection\nbased on relay chain prediction. Specifically, our model predicts a\nsegmentation map to classify the foreground and background region. For each\npixel point in the foreground region, we go through the forward branch and\nbackward branch to recover the whole lane. Each branch decodes a transfer map\nand a distance map to produce the direction moving to the next point, and how\nmany steps to progressively predict a relay station (next point). As such, our\nmodel is able to capture the keypoints along the lanes. Despite its simplicity,\nour strategy allows us to establish new state-of-the-art on four major\nbenchmarks including TuSimple, CULane, CurveLanes and LLAMAS.\n","authors":["Shenghua Xu","Xinyue Cai","Bin Zhao","Li Zhang","Hang Xu","Yanwei Fu","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2207.09399v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09389v1","updated":"2022-07-19T16:38:48Z","published":"2022-07-19T16:38:48Z","title":"Image Synthesis with Disentangled Attributes for Chest X-Ray Nodule\n  Augmentation and Detection","summary":"  Lung nodule detection in chest X-ray (CXR) images is common to early\nscreening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis\n(CAD) systems can support radiologists for nodule screening in CXR. However, it\nrequires large-scale and diverse medical data with high-quality annotations to\ntrain such robust and accurate CADs. To alleviate the limited availability of\nsuch datasets, lung nodule synthesis methods are proposed for the sake of data\naugmentation. Nevertheless, previous methods lack the ability to generate\nnodules that are realistic with the size attribute desired by the detector. To\naddress this issue, we introduce a novel lung nodule synthesis framework in\nthis paper, which decomposes nodule attributes into three main aspects\nincluding shape, size, and texture, respectively. A GAN-based Shape Generator\nfirstly models nodule shapes by generating diverse shape masks. The following\nSize Modulation then enables quantitative control on the diameters of the\ngenerated nodule shapes in pixel-level granularity. A coarse-to-fine gated\nconvolutional Texture Generator finally synthesizes visually plausible nodule\ntextures conditioned on the modulated shape masks. Moreover, we propose to\nsynthesize nodule CXR images by controlling the disentangled nodule attributes\nfor data augmentation, in order to better compensate for the nodules that are\neasily missed in the detection task. Our experiments demonstrate the enhanced\nimage quality, diversity, and controllability of the proposed lung nodule\nsynthesis framework. We also validate the effectiveness of our data\naugmentation on greatly improving nodule detection performance.\n","authors":["Zhenrong Shen","Xi Ouyang","Bin Xiao","Jie-Zhi Cheng","Qian Wang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2207.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13579v4","updated":"2022-07-19T16:24:56Z","published":"2021-11-26T16:24:03Z","title":"VL-LTR: Learning Class-wise Visual-Linguistic Representation for\n  Long-Tailed Visual Recognition","summary":"  Deep learning-based models encounter challenges when processing long-tailed\ndata in the real world. Existing solutions usually employ some balancing\nstrategies or transfer learning to deal with the class imbalance problem, based\non the image modality. In this work, we present a visual-linguistic long-tailed\nrecognition framework, termed VL-LTR, and conduct empirical studies on the\nbenefits of introducing text modality for long-tailed recognition (LTR).\nCompared to existing approaches, the proposed VL-LTR has the following merits.\n(1) Our method can not only learn visual representation from images but also\nlearn corresponding linguistic representation from noisy class-level text\ndescriptions collected from the Internet; (2) Our method can effectively use\nthe learned visual-linguistic representation to improve the visual recognition\nperformance, especially for classes with fewer image samples. We also conduct\nextensive experiments and set the new state-of-the-art performance on\nwidely-used LTR benchmarks. Notably, our method achieves 77.2% overall accuracy\non ImageNet-LT, which significantly outperforms the previous best method by\nover 17 points, and is close to the prevailing performance training on the full\nImageNet. Code is available at https://github.com/ChangyaoTian/VL-LTR.\n","authors":["Changyao Tian","Wenhai Wang","Xizhou Zhu","Jifeng Dai","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2111.13579v4.pdf","comment":"Accepted by ECCV 2022; 14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2207.09373v1","updated":"2022-07-19T16:18:53Z","published":"2022-07-19T16:18:53Z","title":"Emotion Recognition based on Multi-Task Learning Framework in the ABAW4\n  Challenge","summary":"  This paper presents our submission to the Multi-Task Learning (MTL) Challenge\nof the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Based on\nvisual feature representations, we utilize three types of temporal encoder to\ncapture the temporal context information in the video, including the\ntransformer based encoder, LSTM based encoder and GRU based encoder. With the\ntemporal context-aware representations, we employ multi-task framework to\npredict the valence, arousal, expression and AU values of the images. In\naddition, smoothing processing is applied to refine the initial valence and\narousal predictions, and a model ensemble strategy is used to combine multiple\nresults from different model setups. Our system achieves the performance of\n$1.742$ on MTL Challenge validation dataset.\n","authors":["Tenggan Zhang","Chuanhe Liu","Xiaolong Liu","Yuchen Liu","Liyu Meng","Lei Sun","Wenqiang Jiang","Fengyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.09373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09729v2","updated":"2022-07-19T16:12:49Z","published":"2022-03-18T04:04:45Z","title":"REALY: Rethinking the Evaluation of 3D Face Reconstruction","summary":"  The evaluation of 3D face reconstruction results typically relies on a rigid\nshape alignment between the estimated 3D model and the ground-truth scan. We\nobserve that aligning two shapes with different reference points can largely\naffect the evaluation results. This poses difficulties for precisely diagnosing\nand improving a 3D face reconstruction method. In this paper, we propose a\nnovel evaluation approach with a new benchmark REALY, consists of 100 globally\naligned face scans with accurate facial keypoints, high-quality region masks,\nand topology-consistent meshes. Our approach performs region-wise shape\nalignment and leads to more accurate, bidirectional correspondences during\ncomputing the shape errors. The fine-grained, region-wise evaluation results\nprovide us detailed understandings about the performance of state-of-the-art 3D\nface reconstruction methods. For example, our experiments on single-image based\nreconstruction methods reveal that DECA performs the best on nose regions,\nwhile GANFit performs better on cheek regions. Besides, a new and high-quality\n3DMM basis, HIFI3D++, is further derived using the same procedure as we\nconstruct REALY to align and retopologize several 3D face datasets. We will\nrelease REALY, HIFI3D++, and our new evaluation pipeline at\nhttps://realy3dface.com.\n","authors":["Zenghao Chai","Haoxian Zhang","Jing Ren","Di Kang","Zhengzhuo Xu","Xuefei Zhe","Chun Yuan","Linchao Bao"],"pdf_url":"https://arxiv.org/pdf/2203.09729v2.pdf","comment":"Accepted to ECCV 2022, camera-ready version; Project page:\n  https://realy3dface.com; Code: https://github.com/czh-98/REALY"},{"id":"http://arxiv.org/abs/2207.09367v1","updated":"2022-07-19T16:10:16Z","published":"2022-07-19T16:10:16Z","title":"Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and\n  Editability","summary":"  GAN inversion aims to invert an input image into the latent space of a\npre-trained GAN. Despite the recent advances in GAN inversion, there remain\nchallenges to mitigate the tradeoff between distortion and editability, i.e.\nreconstructing the input image accurately and editing the inverted image with a\nsmall visual quality drop. The recently proposed pivotal tuning model makes\nsignificant progress towards reconstruction and editability, by using a\ntwo-step approach that first inverts the input image into a latent code, called\npivot code, and then alters the generator so that the input image can be\naccurately mapped into the pivot code. Here, we show that both reconstruction\nand editability can be improved by a proper design of the pivot code. We\npresent a simple yet effective method, named cycle encoding, for a high-quality\npivot code. The key idea of our method is to progressively train an encoder in\nvarying spaces according to a cycle scheme: W->W+->W. This training methodology\npreserves the properties of both W and W+ spaces, i.e. high editability of W\nand low distortion of W+. To further decrease the distortion, we also propose\nto refine the pivot code with an optimization-based method, where a\nregularization term is introduced to reduce the degradation in editability.\nQualitative and quantitative comparisons to several state-of-the-art methods\ndemonstrate the superiority of our approach.\n","authors":["Xudong Mao","Liujuan Cao","Aurele T. Gnanha","Zhenguo Yang","Qing Li","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2207.09367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.09443v2","updated":"2022-07-19T16:01:02Z","published":"2022-04-20T13:17:39Z","title":"GIMO: Gaze-Informed Human Motion Prediction in Context","summary":"  Predicting human motion is critical for assistive robots and AR/VR\napplications, where the interaction with humans needs to be safe and\ncomfortable. Meanwhile, an accurate prediction depends on understanding both\nthe scene context and human intentions. Even though many works study\nscene-aware human motion prediction, the latter is largely underexplored due to\nthe lack of ego-centric views that disclose human intent and the limited\ndiversity in motion and scenes. To reduce the gap, we propose a large-scale\nhuman motion dataset that delivers high-quality body pose sequences, scene\nscans, as well as ego-centric views with the eye gaze that serves as a\nsurrogate for inferring human intent. By employing inertial sensors for motion\ncapture, our data collection is not tied to specific scenes, which further\nboosts the motion dynamics observed from our subjects. We perform an extensive\nstudy of the benefits of leveraging the eye gaze for ego-centric human motion\nprediction with various state-of-the-art architectures. Moreover, to realize\nthe full potential of the gaze, we propose a novel network architecture that\nenables bidirectional communication between the gaze and motion branches. Our\nnetwork achieves the top performance in human motion prediction on the proposed\ndataset, thanks to the intent information from eye gaze and the denoised gaze\nfeature modulated by the motion. Code and data can be found at\nhttps://github.com/y-zheng18/GIMO.\n","authors":["Yang Zheng","Yanchao Yang","Kaichun Mo","Jiaman Li","Tao Yu","Yebin Liu","C. Karen Liu","Leonidas J. Guibas"],"pdf_url":"https://arxiv.org/pdf/2204.09443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09352v1","updated":"2022-07-19T15:59:40Z","published":"2022-07-19T15:59:40Z","title":"Computer Vision to the Rescue: Infant Postural Symmetry Estimation from\n  Incongruent Annotations","summary":"  Bilateral postural symmetry plays a key role as a potential risk marker for\nautism spectrum disorder (ASD) and as a symptom of congenital muscular\ntorticollis (CMT) in infants, but current methods of assessing symmetry require\nlaborious clinical expert assessments. In this paper, we develop a computer\nvision based infant symmetry assessment system, leveraging 3D human pose\nestimation for infants. Evaluation and calibration of our system against ground\ntruth assessments is complicated by our findings from a survey of human ratings\nof angle and symmetry, that such ratings exhibit low inter-rater reliability.\nTo rectify this, we develop a Bayesian estimator of the ground truth derived\nfrom a probabilistic graphical model of fallible human raters. We show that the\n3D infant pose estimation model can achieve 68% area under the receiver\noperating characteristic curve performance in predicting the Bayesian aggregate\nlabels, compared to only 61% from a 2D infant pose estimation model and 60%\nfrom a 3D adult pose estimation model, highlighting the importance of 3D poses\nand infant domain knowledge in assessing infant body symmetry. Our survey\nanalysis also suggests that human ratings are susceptible to higher levels of\nbias and inconsistency, and hence our final 3D pose-based symmetry assessment\nsystem is calibrated but not directly supervised by Bayesian aggregate human\nratings, yielding higher levels of consistency and lower levels of inter-limb\nassessment bias.\n","authors":["Xiaofei Huang","Michael Wan","Lingfei Luan","Bethany Tunik","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2207.09352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09339v1","updated":"2022-07-19T15:49:35Z","published":"2022-07-19T15:49:35Z","title":"Visual Representation Learning with Transformer: A Sequence-to-Sequence\n  Perspective","summary":"  Visual representation learning is the key of solving various vision problems.\nRelying on the seminal grid structure priors, convolutional neural networks\n(CNNs) have been the de facto standard architectures of most deep vision\nmodels. For instance, classical semantic segmentation methods often adopt a\nfully-convolutional network (FCN) with an encoder-decoder architecture. The\nencoder progressively reduces the spatial resolution and learns more abstract\nvisual concepts with larger receptive fields. Since context modeling is\ncritical for segmentation, the latest efforts have been focused on increasing\nthe receptive field, through either dilated (i.e., atrous) convolutions or\ninserting attention modules. However, the FCN-based architecture remains\nunchanged. In this paper, we aim to provide an alternative perspective by\ntreating visual representation learning generally as a sequence-to-sequence\nprediction task. Specifically, we deploy a pure Transformer to encode an image\nas a sequence of patches, without local convolution and resolution reduction.\nWith the global context modeled in every layer of the Transformer, stronger\nvisual representation can be learned for better tackling vision tasks. In\nparticular, our segmentation model, termed as SEgmentation TRansformer (SETR),\nexcels on ADE20K (50.28% mIoU, the first position in the test leaderboard on\nthe day of submission), Pascal Context (55.83% mIoU) and reaches competitive\nresults on Cityscapes. Further, we formulate a family of Hierarchical\nLocal-Global (HLG) Transformers characterized by local attention within windows\nand global-attention across windows in a hierarchical and pyramidal\narchitecture. Extensive experiments show that our method achieves appealing\nperformance on a variety of visual recognition tasks (e.g., image\nclassification, object detection and instance segmentation and semantic\nsegmentation).\n","authors":["Li Zhang","Sixiao Zheng","Jiachen Lu","Xinxuan Zhao","Xiatian Zhu","Yanwei Fu","Tao Xiang","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2207.09339v1.pdf","comment":"Extended version of CVPR 2021 paper arXiv:2012.15840"},{"id":"http://arxiv.org/abs/2207.09336v1","updated":"2022-07-19T15:44:59Z","published":"2022-07-19T15:44:59Z","title":"Uncertainty in Contrastive Learning: On the Predictability of Downstream\n  Performance","summary":"  The superior performance of some of today's state-of-the-art deep learning\nmodels is to some extent owed to extensive (self-)supervised contrastive\npretraining on large-scale datasets. In contrastive learning, the network is\npresented with pairs of positive (similar) and negative (dissimilar) datapoints\nand is trained to find an embedding vector for each datapoint, i.e., a\nrepresentation, which can be further fine-tuned for various downstream tasks.\nIn order to safely deploy these models in critical decision-making systems, it\nis crucial to equip them with a measure of their uncertainty or reliability.\nHowever, due to the pairwise nature of training a contrastive model, and the\nlack of absolute labels on the output (an abstract embedding vector), adapting\nconventional uncertainty estimation techniques to such models is non-trivial.\nIn this work, we study whether the uncertainty of such a representation can be\nquantified for a single datapoint in a meaningful way. In other words, we\nexplore if the downstream performance on a given datapoint is predictable,\ndirectly from its pre-trained embedding. We show that this goal can be achieved\nby directly estimating the distribution of the training data in the embedding\nspace and accounting for the local consistency of the representations. Our\nexperiments show that this notion of uncertainty for an embedding vector often\nstrongly correlates with its downstream accuracy.\n","authors":["Shervin Ardeshir","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2207.09336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09332v1","updated":"2022-07-19T15:35:23Z","published":"2022-07-19T15:35:23Z","title":"Rethinking IoU-based Optimization for Single-stage 3D Object Detection","summary":"  Since Intersection-over-Union (IoU) based optimization maintains the\nconsistency of the final IoU prediction metric and losses, it has been widely\nused in both regression and classification branches of single-stage 2D object\ndetectors. Recently, several 3D object detection methods adopt IoU-based\noptimization and directly replace the 2D IoU with 3D IoU. However, such a\ndirect computation in 3D is very costly due to the complex implementation and\ninefficient backward operations. Moreover, 3D IoU-based optimization is\nsub-optimal as it is sensitive to rotation and thus can cause training\ninstability and detection performance deterioration. In this paper, we propose\na novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the\nrotation-sensitivity issue, and produce more efficient optimization objectives\ncompared with 3D IoU during the training stage. Specifically, our RDIoU\nsimplifies the complex interactions of regression parameters by decoupling the\nrotation variable as an independent term, yet preserving the geometry of 3D\nIoU. By incorporating RDIoU into both the regression and classification\nbranches, the network is encouraged to learn more precise bounding boxes and\nconcurrently overcome the misalignment issue between classification and\nregression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset\nvalidate that our RDIoU method can bring substantial improvement for the\nsingle-stage 3D object detection.\n","authors":["Hualian Sheng","Sijia Cai","Na Zhao","Bing Deng","Jianqiang Huang","Xian-Sheng Hua","Min-Jian Zhao","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2207.09332v1.pdf","comment":"ECCV2022 accepted. The code is available at\n  https://github.com/hlsheng1/RDIoU"},{"id":"http://arxiv.org/abs/2203.07724v2","updated":"2022-07-19T15:32:48Z","published":"2022-03-15T08:32:56Z","title":"CODA: A Real-World Road Corner Case Dataset for Object Detection in\n  Autonomous Driving","summary":"  Contemporary deep-learning object detection methods for autonomous driving\nusually assume prefixed categories of common traffic participants, such as\npedestrians and cars. Most existing detectors are unable to detect uncommon\nobjects and corner cases (e.g., a dog crossing a street), which may lead to\nsevere accidents in some situations, making the timeline for the real-world\napplication of reliable autonomous driving uncertain. One main reason that\nimpedes the development of truly reliably self-driving systems is the lack of\npublic datasets for evaluating the performance of object detectors on corner\ncases. Hence, we introduce a challenging dataset named CODA that exposes this\ncritical problem of vision-based detectors. The dataset consists of 1500\ncarefully selected real-world driving scenes, each containing four object-level\ncorner cases (on average), spanning more than 30 object categories. On CODA,\nthe performance of standard object detectors trained on large-scale autonomous\ndriving datasets significantly drops to no more than 12.8% in mAR. Moreover, we\nexperiment with the state-of-the-art open-world object detector and find that\nit also fails to reliably identify the novel objects in CODA, suggesting that a\nrobust perception system for autonomous driving is probably still far from\nreach. We expect our CODA dataset to facilitate further research in reliable\ndetection for real-world autonomous driving. Our dataset will be released at\nhttps://coda-dataset.github.io.\n","authors":["Kaican Li","Kai Chen","Haoyu Wang","Lanqing Hong","Chaoqiang Ye","Jianhua Han","Yukuai Chen","Wei Zhang","Chunjing Xu","Dit-Yan Yeung","Xiaodan Liang","Zhenguo Li","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2203.07724v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09314v1","updated":"2022-07-19T15:01:36Z","published":"2022-07-19T15:01:36Z","title":"Self-Supervised Interactive Object Segmentation Through a\n  Singulation-and-Grasping Approach","summary":"  Instance segmentation with unseen objects is a challenging problem in\nunstructured environments. To solve this problem, we propose a robot learning\napproach to actively interact with novel objects and collect each object's\ntraining label for further fine-tuning to improve the segmentation model\nperformance, while avoiding the time-consuming process of manually labeling a\ndataset. The Singulation-and-Grasping (SaG) policy is trained through\nend-to-end reinforcement learning. Given a cluttered pile of objects, our\napproach chooses pushing and grasping motions to break the clutter and conducts\nobject-agnostic grasping for which the SaG policy takes as input the visual\nobservations and imperfect segmentation. We decompose the problem into three\nsubtasks: (1) the object singulation subtask aims to separate the objects from\neach other, which creates more space that alleviates the difficulty of (2) the\ncollision-free grasping subtask; (3) the mask generation subtask to obtain the\nself-labeled ground truth masks by using an optical flow-based binary\nclassifier and motion cue post-processing for transfer learning. Our system\nachieves 70% singulation success rate in simulated cluttered scenes. The\ninteractive segmentation of our system achieves 87.8%, 73.9%, and 69.3% average\nprecision for toy blocks, YCB objects in simulation and real-world novel\nobjects, respectively, which outperforms several baselines.\n","authors":["Houjian Yu","Changhyun Choi"],"pdf_url":"https://arxiv.org/pdf/2207.09314v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09313v1","updated":"2022-07-19T14:59:14Z","published":"2022-07-19T14:59:14Z","title":"Content-aware Scalable Deep Compressed Sensing","summary":"  To more efficiently address image compressed sensing (CS) problems, we\npresent a novel content-aware scalable network dubbed CASNet which collectively\nachieves adaptive sampling rate allocation, fine granular scalability and\nhigh-quality reconstruction. We first adopt a data-driven saliency detector to\nevaluate the importances of different image regions and propose a\nsaliency-based block ratio aggregation (BRA) strategy for sampling rate\nallocation. A unified learnable generating matrix is then developed to produce\nsampling matrix of any CS ratio with an ordered structure. Being equipped with\nthe optimization-inspired recovery subnet guided by saliency information and a\nmulti-block training scheme preventing blocking artifacts, CASNet jointly\nreconstructs the image blocks sampled at various sampling rates with one single\nmodel. To accelerate training convergence and improve network robustness, we\npropose an SVD-based initialization scheme and a random transformation\nenhancement (RTE) strategy, which are extensible without introducing extra\nparameters. All the CASNet components can be combined and learned end-to-end.\nWe further provide a four-stage implementation for evaluation and practical\ndeployments. Experiments demonstrate that CASNet outperforms other CS networks\nby a large margin, validating the collaboration and mutual supports among its\ncomponents and strategies. Codes are available at\nhttps://github.com/Guaishou74851/CASNet.\n","authors":["Bin Chen","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.09313v1.pdf","comment":"Accepted for publication as a Regular paper in the IEEE Transactions\n  on Image Processing (T-IP)"},{"id":"http://arxiv.org/abs/2207.09312v1","updated":"2022-07-19T14:55:42Z","published":"2022-07-19T14:55:42Z","title":"Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for\n  COVID-19 Screening With Chest Radiography","summary":"  Building AI models with trustworthiness is important especially in regulated\nareas such as healthcare. In tackling COVID-19, previous work uses\nconvolutional neural networks as the backbone architecture, which has shown to\nbe prone to over-caution and overconfidence in making decisions, rendering them\nless trustworthy -- a crucial flaw in the context of medical imaging. In this\nstudy, we propose a feature learning approach using Vision Transformers, which\nuse an attention-based mechanism, and examine the representation learning\ncapability of Transformers as a new backbone architecture for medical imaging.\nThrough the task of classifying COVID-19 chest radiographs, we investigate into\nwhether generalization capabilities benefit solely from Vision Transformers'\narchitectural advances. Quantitative and qualitative evaluations are conducted\non the trustworthiness of the models, through the use of \"trust score\"\ncomputation and a visual explainability technique. We conclude that the\nattention-based feature learning approach is promising in building trustworthy\ndeep learning models for healthcare.\n","authors":["Kai Ma","Pengcheng Xi","Karim Habashy","Ashkan Ebadi","Stéphane Tremblay","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2207.09312v1.pdf","comment":"Accepted to 39th International Conference on Machine Learning,\n  Workshop on Healthcare AI and COVID-19"},{"id":"http://arxiv.org/abs/2008.09041v5","updated":"2022-07-19T14:47:32Z","published":"2020-08-19T02:36:53Z","title":"A New Perspective on Stabilizing GANs training: Direct Adversarial\n  Training","summary":"  Generative Adversarial Networks (GANs) are the most popular image generation\nmodels that have achieved remarkable progress on various computer vision tasks.\nHowever, training instability is still one of the open problems for all\nGAN-based algorithms. Quite a number of methods have been proposed to stabilize\nthe training of GANs, the focuses of which were respectively put on the loss\nfunctions, regularization and normalization technologies, training algorithms,\nand model architectures. Different from the above methods, in this paper, a new\nperspective on stabilizing GANs training is presented. It is found that\nsometimes the images produced by the generator act like adversarial examples of\nthe discriminator during the training process, which may be part of the reason\ncausing the unstable training of GANs. With this finding, we propose the Direct\nAdversarial Training (DAT) method to stabilize the training process of GANs.\nFurthermore, we prove that the DAT method is able to minimize the Lipschitz\nconstant of the discriminator adaptively. The advanced performance of DAT is\nverified on multiple loss functions, network architectures, hyper-parameters,\nand datasets. Specifically, DAT achieves significant improvements of 11.5% FID\non CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10\nunconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom\nunconditional generation based on SSGAN. Code will be available at\nhttps://github.com/iceli1007/DAT-GAN\n","authors":["Ziqiang Li","Pengfei Xia","Rentuo Tao","Hongjing Niu","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2008.09041v5.pdf","comment":"Accepted to IEEE Transactions on Emerging Topics in Computational\n  Intelligence"},{"id":"http://arxiv.org/abs/2207.08455v2","updated":"2022-07-19T14:43:47Z","published":"2022-07-18T09:20:04Z","title":"Open-world Semantic Segmentation via Contrasting and Clustering\n  Vision-Language Embedding","summary":"  To bridge the gap between supervised semantic segmentation and real-world\napplications that acquires one model to recognize arbitrary new concepts,\nrecent zero-shot segmentation attracts a lot of attention by exploring the\nrelationships between unseen and seen object categories, yet requiring large\namounts of densely-annotated data with diverse base classes. In this paper, we\npropose a new open-world semantic segmentation pipeline that makes the first\nattempt to learn to segment semantic objects of various open-world categories\nwithout any efforts on dense annotations, by purely exploiting the\nimage-caption data that naturally exist on the Internet. Our method,\nVision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a\ntext encoder to generate visual and text embeddings for the image-caption data,\nwith two core components that endow its segmentation ability: First, the image\nencoder is jointly trained with a vision-based contrasting and a cross-modal\ncontrasting, which encourage the visual embeddings to preserve both\nfine-grained semantics and high-level category information that are crucial for\nthe segmentation task. Furthermore, an online clustering head is devised over\nthe image encoder, which allows to dynamically segment the visual embeddings\ninto distinct semantic groups such that they can be classified by comparing\nwith various text embeddings to complete our segmentation pipeline. Experiments\nshow that without using any data with dense annotations, our method can\ndirectly segment objects of arbitrary categories, outperforming zero-shot\nsegmentation methods that require data labeling on three benchmark datasets.\n","authors":["Quande Liu","Youpeng Wen","Jianhua Han","Chunjing Xu","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2207.08455v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09303v1","updated":"2022-07-19T14:37:46Z","published":"2022-07-19T14:37:46Z","title":"DH-AUG: DH Forward Kinematics Model Driven Augmentation for 3D Human\n  Pose Estimation","summary":"  Due to the lack of diversity of datasets, the generalization ability of the\npose estimator is poor. To solve this problem, we propose a pose augmentation\nsolution via DH forward kinematics model, which we call DH-AUG. We observe that\nthe previous work is all based on single-frame pose augmentation, if it is\ndirectly applied to video pose estimator, there will be several previously\nignored problems: (i) angle ambiguity in bone rotation (multiple solutions);\n(ii) the generated skeleton video lacks movement continuity. To solve these\nproblems, we propose a special generator based on DH forward kinematics model,\nwhich is called DH-generator. Extensive experiments demonstrate that DH-AUG can\ngreatly increase the generalization ability of the video pose estimator. In\naddition, when applied to a single-frame 3D pose estimator, our method\noutperforms the previous best pose augmentation method. The source code has\nbeen released at\nhttps://github.com/hlz0606/DH-AUG-DH-Forward-Kinematics-Model-Driven-Augmentation-for-3D-Human-Pose-Estimation.\n","authors":["Linzhi Huang","Jiahao Liang","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2207.09303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09302v1","updated":"2022-07-19T14:35:42Z","published":"2022-07-19T14:35:42Z","title":"Deep Semantic Statistics Matching (D2SM) Denoising Network","summary":"  The ultimate aim of image restoration like denoising is to find an exact\ncorrelation between the noisy and clear image domains. But the optimization of\nend-to-end denoising learning like pixel-wise losses is performed in a\nsample-to-sample manner, which ignores the intrinsic correlation of images,\nespecially semantics. In this paper, we introduce the Deep Semantic Statistics\nMatching (D2SM) Denoising Network. It exploits semantic features of pretrained\nclassification networks, then it implicitly matches the probabilistic\ndistribution of clear images at the semantic feature space. By learning to\npreserve the semantic distribution of denoised images, we empirically find our\nmethod significantly improves the denoising capabilities of networks, and the\ndenoised results can be better understood by high-level vision tasks.\nComprehensive experiments conducted on the noisy Cityscapes dataset demonstrate\nthe superiority of our method on both the denoising performance and semantic\nsegmentation accuracy. Moreover, the performance improvement observed on our\nextended tasks including super-resolution and dehazing experiments shows its\npotentiality as a new general plug-and-play component.\n","authors":["Kangfu Mei","Vishal M. Patel","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2207.09302v1.pdf","comment":"ECCV2022, for Project Page, see https://kfmei.page/d2sm/"},{"id":"http://arxiv.org/abs/2107.03815v2","updated":"2022-07-19T14:29:13Z","published":"2021-07-08T12:44:41Z","title":"Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with\n  100M FLOPs","summary":"  In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.\n","authors":["Yikang Zhang","Zhuo Chen","Zhao Zhong"],"pdf_url":"https://arxiv.org/pdf/2107.03815v2.pdf","comment":"ICML"},{"id":"http://arxiv.org/abs/2207.09295v1","updated":"2022-07-19T14:26:12Z","published":"2022-07-19T14:26:12Z","title":"The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object\n  Tracking and Counting","summary":"  We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for\ndetecting, tracking, and counting fish in sonar videos. We identify sonar\nvideos as a rich source of data for advancing low signal-to-noise computer\nvision applications and tackling domain generalization in multiple-object\ntracking (MOT) and counting. In comparison to existing MOT and counting\ndatasets, which are largely restricted to videos of people and vehicles in\ncities, CFC is sourced from a natural-world domain where targets are not easily\nresolvable and appearance features cannot be easily leveraged for target\nre-identification. With over half a million annotations in over 1,500 videos\nsourced from seven different sonar cameras, CFC allows researchers to train MOT\nand counting algorithms and evaluate generalization performance at unseen test\nlocations. We perform extensive baseline experiments and identify key\nchallenges and opportunities for advancing the state of the art in\ngeneralization in MOT and counting.\n","authors":["Justin Kay","Peter Kulits","Suzanne Stathatos","Siqi Deng","Erik Young","Sara Beery","Grant Van Horn","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2207.09295v1.pdf","comment":"ECCV 2022. 33 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09291v1","updated":"2022-07-19T14:22:28Z","published":"2022-07-19T14:22:28Z","title":"3D Room Layout Estimation from a Cubemap of Panorama Image via Deep\n  Manhattan Hough Transform","summary":"  Significant geometric structures can be compactly described by global\nwireframes in the estimation of 3D room layout from a single panoramic image.\nBased on this observation, we present an alternative approach to estimate the\nwalls in 3D space by modeling long-range geometric patterns in a learnable\nHough Transform block. We transform the image feature from a cubemap tile to\nthe Hough space of a Manhattan world and directly map the feature to the\ngeometric output. The convolutional layers not only learn the local\ngradient-like line features, but also utilize the global information to\nsuccessfully predict occluded walls with a simple network structure. Unlike\nmost previous work, the predictions are performed individually on each cubemap\ntile, and then assembled to get the layout estimation. Experimental results\nshow that we achieve comparable results with recent state-of-the-art in\nprediction accuracy and performance. Code is available at\nhttps://github.com/Starrah/DMH-Net.\n","authors":["Yining Zhao","Chao Wen","Zhou Xue","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2207.09291v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2105.08997v2","updated":"2022-07-19T14:08:42Z","published":"2021-05-19T09:03:02Z","title":"When Deep Classifiers Agree: Analyzing Correlations between Learning\n  Order and Image Statistics","summary":"  Although a plethora of architectural variants for deep classification has\nbeen introduced over time, recent works have found empirical evidence towards\nsimilarities in their training process. It has been hypothesized that neural\nnetworks converge not only to similar representations, but also exhibit a\nnotion of empirical agreement on which data instances are learned first.\nFollowing in the latter works$'$ footsteps, we define a metric to quantify the\nrelationship between such classification agreement over time, and posit that\nthe agreement phenomenon can be mapped to core statistics of the investigated\ndataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,\nImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to\nbe independent of specific architectures, training hyper-parameters or labels,\nalbeit follows an ordering according to image statistics.\n","authors":["Iuliia Pliushch","Martin Mundt","Nicolas Lupp","Visvanathan Ramesh"],"pdf_url":"https://arxiv.org/pdf/2105.08997v2.pdf","comment":"Accepted for publication at ECCV 2022. Version includes supplementary\n  material"},{"id":"http://arxiv.org/abs/2207.00095v2","updated":"2022-07-19T13:57:47Z","published":"2022-06-30T20:30:33Z","title":"End-to-end Learning for Image-based Detection of Molecular Alterations\n  in Digital Pathology","summary":"  Current approaches for classification of whole slide images (WSI) in digital\npathology predominantly utilize a two-stage learning pipeline. The first stage\nidentifies areas of interest (e.g. tumor tissue), while the second stage\nprocesses cropped tiles from these areas in a supervised fashion. During\ninference, a large number of tiles are combined into a unified prediction for\nthe entire slide. A major drawback of such approaches is the requirement for\ntask-specific auxiliary labels which are not acquired in clinical routine. We\npropose a novel learning pipeline for WSI classification that is trainable\nend-to-end and does not require any auxiliary annotations. We apply our\napproach to predict molecular alterations for a number of different use-cases,\nincluding detection of microsatellite instability in colorectal tumors and\nprediction of specific mutations for colon, lung, and breast cancer cases from\nThe Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to\nbe competitive with state of the art two-stage pipelines. We believe our\napproach can facilitate future research in digital pathology and contribute to\nsolve a large range of problems around the prediction of cancer phenotypes,\nhopefully enabling personalized therapies for more patients in future.\n","authors":["Marvin Teichmann","Andre Aichert","Hanibal Bohnenberger","Philipp Ströbel","Tobias Heimann"],"pdf_url":"https://arxiv.org/pdf/2207.00095v2.pdf","comment":"MICCAI 2022; 8.5 Pages, 4 Figures"},{"id":"http://arxiv.org/abs/2207.09280v1","updated":"2022-07-19T13:49:30Z","published":"2022-07-19T13:49:30Z","title":"Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain\n  Adaptation","summary":"  Universal domain adaptation (UDA) aims to transfer the knowledge of common\nclasses from source domain to target domain without any prior knowledge on the\nlabel set, which requires to distinguish the unknown samples from the known\nones in the target domain. Recent methods preferred to increase the\ninter-sample affinity within a known class, while they ignored the inter-sample\naffinity between the unknown samples and the known ones. This paper reveals\nthat exploiting such inter-sample affinity can significantly improve the\nperformance of UDA and proposes a knowability-aware UDA framework based on it.\nFirst, we estimate the knowability of each target sample by searching its\nneighboring samples in the source domain. Then, we propose an auto-thresholding\nscheme applied to the estimated knowability to determine whether a target\nsample is unknown or known. Next, in addition to increasing the inter-sample\naffinity within each known class like previous methods, we design new losses\nbased on the estimated knowability to reduce the inter-sample affinity between\nthe unknown target samples and the known ones. Finally, experiments on four\npublic datasets demonstrate that our method significantly outperforms existing\nstate-of-the-art methods.\n","authors":["Yifan Wang","Lin Zhang","Ran Song","Lin Ma","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.09280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09270v1","updated":"2022-07-19T13:29:05Z","published":"2022-07-19T13:29:05Z","title":"Action Quality Assessment with Temporal Parsing Transformer","summary":"  Action Quality Assessment(AQA) is important for action understanding and\nresolving the task poses unique challenges due to subtle visual differences.\nExisting state-of-the-art methods typically rely on the holistic video\nrepresentations for score regression or ranking, which limits the\ngeneralization to capture fine-grained intra-class variation. To overcome the\nabove limitation, we propose a temporal parsing transformer to decompose the\nholistic feature into temporal part-level representations. Specifically, we\nutilize a set of learnable queries to represent the atomic temporal patterns\nfor a specific action. Our decoding process converts the frame representations\nto a fixed number of temporally ordered part representations. To obtain the\nquality score, we adopt the state-of-the-art contrastive regression based on\nthe part representations. Since existing AQA datasets do not provide temporal\npart-level labels or partitions, we propose two novel loss functions on the\ncross attention responses of the decoder: a ranking loss to ensure the\nlearnable queries to satisfy the temporal order in cross attention and a\nsparsity loss to encourage the part representations to be more discriminative.\nExtensive experiments show that our proposed method outperforms prior work on\nthree public AQA benchmarks by a considerable margin.\n","authors":["Yang Bai","Desen Zhou","Songyang Zhang","Jian Wang","Errui Ding","Yu Guan","Yang Long","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09270v1.pdf","comment":"accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2204.02548v2","updated":"2022-07-19T13:24:11Z","published":"2022-04-06T02:49:06Z","title":"Style-Hallucinated Dual Consistency Learning for Domain Generalized\n  Semantic Segmentation","summary":"  In this paper, we study the task of synthetic-to-real domain generalized\nsemantic segmentation, which aims to learn a model that is robust to unseen\nreal-world scenes using only synthetic data. The large domain shift between\nsynthetic and real-world data, including the limited source environmental\nvariations and the large distribution gap between synthetic and real-world\ndata, significantly hinders the model performance on unseen real-world scenes.\nIn this work, we propose the Style-HAllucinated Dual consistEncy learning\n(SHADE) framework to handle such domain shift. Specifically, SHADE is\nconstructed based on two consistency constraints, Style Consistency (SC) and\nRetrospection Consistency (RC). SC enriches the source situations and\nencourages the model to learn consistent representation across\nstyle-diversified samples. RC leverages real-world knowledge to prevent the\nmodel from overfitting to synthetic data and thus largely keeps the\nrepresentation consistent between the synthetic and real-world models.\nFurthermore, we present a novel style hallucination module (SHM) to generate\nstyle-diversified samples that are essential to consistency learning. SHM\nselects basis styles from the source distribution, enabling the model to\ndynamically generate diverse and realistic samples during training. Experiments\nshow that our SHADE yields significant improvement and outperforms\nstate-of-the-art methods by 5.05% and 8.35% on the average mIoU of three\nreal-world datasets on single- and multi-source settings, respectively.\n","authors":["Yuyang Zhao","Zhun Zhong","Na Zhao","Nicu Sebe","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2204.02548v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2111.12928v2","updated":"2022-07-19T13:12:34Z","published":"2021-11-25T05:59:27Z","title":"Facial Depth and Normal Estimation using Single Dual-Pixel Camera","summary":"  Many mobile manufacturers recently have adopted Dual-Pixel (DP) sensors in\ntheir flagship models for faster auto-focus and aesthetic image captures.\nDespite their advantages, research on their usage for 3D facial understanding\nhas been limited due to the lack of datasets and algorithmic designs that\nexploit parallax in DP images. This is because the baseline of sub-aperture\nimages is extremely narrow and parallax exists in the defocus blur region. In\nthis paper, we introduce a DP-oriented Depth/Normal network that reconstructs\nthe 3D facial geometry. For this purpose, we collect a DP facial data with more\nthan 135K images for 101 persons captured with our multi-camera structured\nlight systems. It contains the corresponding ground-truth 3D models including\ndepth map and surface normal in metric scale. Our dataset allows the proposed\nmatching network to be generalized for 3D facial depth/normal estimation. The\nproposed network consists of two novel modules: Adaptive Sampling Module and\nAdaptive Normal Module, which are specialized in handling the defocus blur in\nDP images. Finally, the proposed method achieves state-of-the-art performances\nover recent DP-based depth/normal estimation methods. We also demonstrate the\napplicability of the estimated depth/normal to face spoofing and relighting.\n","authors":["Minjun Kang","Jaesung Choe","Hyowon Ha","Hae-Gon Jeon","Sunghoon Im","In So Kweon","KuK-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2111.12928v2.pdf","comment":"Github page : https://github.com/MinJunKang/DualPixelFace To be\n  appeared in ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09248v1","updated":"2022-07-19T13:03:14Z","published":"2022-07-19T13:03:14Z","title":"Don't Stop Learning: Towards Continual Learning for the CLIP Model","summary":"  The Contrastive Language-Image Pre-training (CLIP) Model is a recently\nproposed large-scale pre-train model which attracts increasing attention in the\ncomputer vision community. Benefiting from its gigantic image-text training\nset, the CLIP model has learned outstanding capabilities in zero-shot learning\nand image-text matching. To boost the recognition performance of CLIP on some\ntarget visual concepts, it is often desirable to further update the CLIP model\nby fine-tuning some classes-of-interest on extra training data. This operation,\nhowever, raises an important concern: will the update hurt the zero-shot\nlearning or image-text matching capability of the CLIP, i.e., the catastrophic\nforgetting issue? If yes, could existing continual learning algorithms be\nadapted to alleviate the risk of catastrophic forgetting? To answer these\nquestions, this work conducts a systemic study on the continual learning issue\nof the CLIP model. We construct evaluation protocols to measure the impact of\nfine-tuning updates and explore different ways to upgrade existing continual\nlearning methods to mitigate the forgetting issue of the CLIP model. Our study\nreveals the particular challenges of CLIP continual learning problem and lays a\nfoundation for further researches. Moreover, we propose a new algorithm, dubbed\nLearning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact\neffectiveness for alleviating the forgetting issue of the CLIP model.\n","authors":["Yuxuan Ding","Lingqiao Liu","Chunna Tian","Jingyuan Yang","Haoxuan Ding"],"pdf_url":"https://arxiv.org/pdf/2207.09248v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2108.08760v3","updated":"2022-07-19T12:52:19Z","published":"2021-08-19T16:00:58Z","title":"Robust outlier detection by de-biasing VAE likelihoods","summary":"  Deep networks often make confident, yet, incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models (DGMs) are a candidate metric\nfor outlier detection with unlabeled data. Yet, previous studies have shown\nthat DGM likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest of DGMs. We propose novel\nanalytical and algorithmic approaches to ameliorate key biases with VAE\nlikelihoods. Our bias corrections are sample-specific, computationally\ninexpensive, and readily computed for various decoder visible distributions.\nNext, we show that a well-known image pre-processing technique -- contrast\nstretching -- extends the effectiveness of bias correction to further improve\noutlier detection. Our approach achieves state-of-the-art accuracies with nine\ngrayscale and natural image datasets, and demonstrates significant advantages\n-- both with speed and performance -- over four recent, competing approaches.\nIn summary, lightweight remedies suffice to achieve robust outlier detection\nwith VAEs.\n","authors":["Kushal Chauhan","Barath Mohan U","Pradeep Shenoy","Manish Gupta","Devarajan Sridharan"],"pdf_url":"https://arxiv.org/pdf/2108.08760v3.pdf","comment":"CVPR 2022. 20 pages and 19 figures"},{"id":"http://arxiv.org/abs/2203.03821v4","updated":"2022-07-19T12:45:08Z","published":"2022-03-08T02:57:49Z","title":"CF-ViT: A General Coarse-to-Fine Method for Vision Transformer","summary":"  Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.\n","authors":["Mengzhao Chen","Mingbao Lin","Ke Li","Yunhang Shen","Yongjian Wu","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2203.03821v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06058v2","updated":"2022-07-19T12:31:23Z","published":"2022-07-13T09:05:35Z","title":"Structure PLP-SLAM: Efficient Sparse Mapping and Localization using\n  Point, Line and Plane for Monocular, RGB-D and Stereo Cameras","summary":"  This paper demonstrates a visual SLAM system that utilizes point and line\ncloud for robust camera localization, simultaneously, with an embedded\npiece-wise planar reconstruction (PPR) module which in all provides a\nstructural map. To build a scale consistent map in parallel with tracking, such\nas employing a single camera brings the challenge of reconstructing geometric\nprimitives with scale ambiguity, and further introduces the difficulty in graph\noptimization of bundle adjustment (BA). We address these problems by proposing\nseveral run-time optimizations on the reconstructed lines and planes. The\nsystem is then extended with depth and stereo sensors based on the design of\nthe monocular framework. The results show that our proposed SLAM tightly\nincorporates the semantic features to boost both frontend tracking as well as\nbackend optimization. We evaluate our system exhaustively on various datasets,\nand open-source our code for the community\n(https://github.com/PeterFWS/Structure-PLP-SLAM).\n","authors":["Fangwen Shu","Jiaxuan Wang","Alain Pagani","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2207.06058v2.pdf","comment":"The pre-print version, v2 add supplementary materials, code\n  open-source: https://github.com/PeterFWS/Structure-PLP-SLAM"},{"id":"http://arxiv.org/abs/2207.09228v1","updated":"2022-07-19T12:31:17Z","published":"2022-07-19T12:31:17Z","title":"Image Super-Resolution with Deep Dictionary","summary":"  Since the first success of Dong et al., the deep-learning-based approach has\nbecome dominant in the field of single-image super-resolution. This replaces\nall the handcrafted image processing steps of traditional sparse-coding-based\nmethods with a deep neural network. In contrast to sparse-coding-based methods,\nwhich explicitly create high/low-resolution dictionaries, the dictionaries in\ndeep-learning-based methods are implicitly acquired as a nonlinear combination\nof multiple convolutions. One disadvantage of deep-learning-based methods is\nthat their performance is degraded for images created differently from the\ntraining dataset (out-of-domain images). We propose an end-to-end\nsuper-resolution network with a deep dictionary (SRDD), where a high-resolution\ndictionary is explicitly learned without sacrificing the advantages of deep\nlearning. Extensive experiments show that explicit learning of high-resolution\ndictionary makes the network more robust for out-of-domain test images while\nmaintaining the performance of the in-domain test images.\n","authors":["Shunta Maeda"],"pdf_url":"https://arxiv.org/pdf/2207.09228v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2111.11430v6","updated":"2022-07-19T12:02:49Z","published":"2021-11-22T18:59:29Z","title":"Class-agnostic Object Detection with Multi-modal Transformer","summary":"  What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and novel objects. In this paper, we advocate\nthat existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. For the first time in literature, we\ndemonstrate that Multi-modal Vision Transformers (MViT) trained with aligned\nimage-text pairs can effectively bridge this gap. Our extensive experiments\nacross various domains and novel objects show the state-of-the-art performance\nof MViTs to localize generic objects in images. Based on the observation that\nexisting MViTs do not include multi-scale feature processing and usually\nrequire longer training schedules, we develop an efficient MViT architecture\nusing multi-scale deformable attention and late vision-language fusion. We show\nthe significance of MViT proposals in a diverse range of applications including\nopen-world object detection, salient and camouflage object detection,\nsupervised and self-supervised detection tasks. Further, MViTs can adaptively\ngenerate proposals given a specific language query and thus offer enhanced\ninteractability. Code: \\url{https://git.io/J1HPY}.\n","authors":["Muhammad Maaz","Hanoona Rasheed","Salman Khan","Fahad Shahbaz Khan","Rao Muhammad Anwer","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2111.11430v6.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2112.04966v2","updated":"2022-07-19T11:52:47Z","published":"2021-12-09T14:54:59Z","title":"CA-SSL: Class-Agnostic Semi-Supervised Learning for Detection and\n  Segmentation","summary":"  To improve instance-level detection/segmentation performance, existing\nself-supervised and semi-supervised methods extract either task-unrelated or\ntask-specific training signals from unlabeled data. We show that these two\napproaches, at the two extreme ends of the task-specificity spectrum, are\nsuboptimal for the task performance. Utilizing too little task-specific\ntraining signals causes underfitting to the ground-truth labels of downstream\ntasks, while the opposite causes overfitting to the ground-truth labels. To\nthis end, we propose a novel Class-Agnostic Semi-Supervised Learning (CA-SSL)\nframework to achieve a more favorable task-specificity balance in extracting\ntraining signals from unlabeled data. CA-SSL has three training stages that act\non either ground-truth labels (labeled data) or pseudo labels (unlabeled data).\nThis decoupling strategy avoids the complicated scheme in traditional SSL\nmethods that balances the contributions from both data types. Especially, we\nintroduce a warmup training stage to achieve a more optimal balance in task\nspecificity by ignoring class information in the pseudo labels, while\npreserving localization training signals. As a result, our warmup model can\nbetter avoid underfitting/overfitting when fine-tuned on the ground-truth\nlabels in detection and segmentation tasks. Using 3.6M unlabeled data, we\nachieve a significant performance gain of 4.7% over ImageNet-pretrained\nbaseline on FCOS object detection. In addition, our warmup model demonstrates\nexcellent transferability to other detection and segmentation frameworks.\n","authors":["Lu Qi","Jason Kuen","Zhe Lin","Jiuxiang Gu","Fengyun Rao","Dian Li","Weidong Guo","Zhen Wen","Ming-Hsuan Yang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2112.04966v2.pdf","comment":"Appeared in ECCV2022"},{"id":"http://arxiv.org/abs/2207.09210v1","updated":"2022-07-19T11:49:21Z","published":"2022-07-19T11:49:21Z","title":"KinD-LCE Curve Estimation And Retinex Fusion On Low-Light Image","summary":"  The problems of low light image noise and chromatic aberration is a\nchallenging problem for tasks such as object detection, semantic segmentation,\ninstance segmentation, etc. In this paper, we propose the algorithm for low\nillumination enhancement. KinD-LCE uses the light curve estimation module in\nthe network structure to enhance the illumination map in the Retinex decomposed\nimage, which improves the image brightness; we proposed the illumination map\nand reflection map fusion module to restore the restored image details and\nreduce the detail loss. Finally, we included a total variation loss function to\neliminate noise. Our method uses the GladNet dataset as the training set, and\nthe LOL dataset as the test set and is validated using ExDark as the dataset\nfor downstream tasks. Extensive Experiments on the benchmarks demonstrate the\nadvantages of our method and are close to the state-of-the-art results, which\nachieve a PSNR of 19.7216 and SSIM of 0.8213 in terms of metrics.\n","authors":["Xiaochun Lei","Junlin Xie","Zetao Jiang","Weiliang Mai","Zhaoting Gong","Chang Lu","Linjun Lu","Ziqi Shan"],"pdf_url":"https://arxiv.org/pdf/2207.09210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08220v2","updated":"2022-07-19T11:45:08Z","published":"2022-07-17T16:28:41Z","title":"Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial\n  Patches","summary":"  Contrastive-based self-supervised learning methods achieved great success in\nrecent years. However, self-supervision requires extremely long training epochs\n(e.g., 800 epochs for MoCo v3) to achieve promising results, which is\nunacceptable for the general academic community and hinders the development of\nthis topic. This work revisits the momentum-based contrastive learning\nframeworks and identifies the inefficiency in which two augmented views\ngenerate only one positive pair. We propose Fast-MoCo - a novel framework that\nutilizes combinatorial patches to construct multiple positive pairs from two\naugmented views, which provides abundant supervision signals that bring\nsignificant acceleration with neglectable extra computational cost. Fast-MoCo\ntrained with 100 epochs achieves 73.5% linear evaluation accuracy, similar to\nMoCo v3 (ResNet-50 backbone) trained with 800 epochs. Extra training (200\nepochs) further improves the result to 75.1%, which is on par with\nstate-of-the-art methods. Experiments on several downstream tasks also confirm\nthe effectiveness of Fast-MoCo.\n","authors":["Yuanzheng Ci","Chen Lin","Lei Bai","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2207.08220v2.pdf","comment":"Accepted for publication at the 2022 European Conference on Computer\n  Vision (ECCV 2022)"},{"id":"http://arxiv.org/abs/2207.09204v1","updated":"2022-07-19T11:30:41Z","published":"2022-07-19T11:30:41Z","title":"VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data","summary":"  We present VoloGAN, an adversarial domain adaptation network that translates\nsynthetic RGB-D images of a high-quality 3D model of a person, into RGB-D\nimages that could be generated with a consumer depth sensor. This system is\nespecially useful to generate high amount training data for single-view 3D\nreconstruction algorithms replicating the real-world capture conditions, being\nable to imitate the style of different sensor types, for the same high-end 3D\nmodel database. The network uses a CycleGAN framework with a U-Net architecture\nfor the generator and a discriminator inspired by SIV-GAN. We use different\noptimizers and learning rate schedules to train the generator and the\ndiscriminator. We further construct a loss function that considers image\nchannels individually and, among other metrics, evaluates the structural\nsimilarity. We demonstrate that CycleGANs can be used to apply adversarial\ndomain adaptation of synthetic 3D data to train a volumetric video generator\nmodel having only few training samples.\n","authors":["Sascha Kirch","Rafael Pagés","Sergio Arnaldo","Sergio Martín"],"pdf_url":"https://arxiv.org/pdf/2207.09204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09202v1","updated":"2022-07-19T11:22:32Z","published":"2022-07-19T11:22:32Z","title":"Exploring Disentangled Content Information for Face Forgery Detection","summary":"  Convolutional neural network based face forgery detection methods have\nachieved remarkable results during training, but struggled to maintain\ncomparable performance during testing. We observe that the detector is prone to\nfocus more on content information than artifact traces, suggesting that the\ndetector is sensitive to the intrinsic bias of the dataset, which leads to\nsevere overfitting. Motivated by this key observation, we design an easily\nembeddable disentanglement framework for content information removal, and\nfurther propose a Content Consistency Constraint (C2C) and a Global\nRepresentation Contrastive Constraint (GRCC) to enhance the independence of\ndisentangled features. Furthermore, we cleverly construct two unbalanced\ndatasets to investigate the impact of the content bias. Extensive\nvisualizations and experiments demonstrate that our framework can not only\nignore the interference of content information, but also guide the detector to\nmine suspicious artifact traces and achieve competitive performance.\n","authors":["Jiahao Liang","Huafeng Shi","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2207.09202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00306v2","updated":"2022-07-19T11:16:36Z","published":"2022-03-01T09:30:01Z","title":"Comprehensive Analysis of the Object Detection Pipeline on UAVs","summary":"  An object detection pipeline comprises a camera that captures the scene and\nan object detector that processes these images. The quality of the images\ndirectly affects the performance of the object detector. Many works nowadays\nfocus either on improving the image quality or improving the object detection\nmodels independently, but neglect the importance of joint optimization of the\ntwo subsystems. The goal of this paper is to tune the detection throughput and\naccuracy of existing object detectors in the remote sensing scenario by\nfocusing on optimizing the input images tailored to the object detector. To\nachieve this, we empirically analyze the influence of two selected camera\ncalibration parameters (camera distortion correction and gamma correction) and\nfive image parameters (quantization, compression, resolution, color model,\nadditional channels) for these applications. For our experiments, we utilize\nthree UAV data sets from different domains and a mixture of large and small\nstate-of-the-art object detector models to provide an extensive evaluation of\nthe influence of the pipeline parameters. Finally, we realize an object\ndetection pipeline prototype on an embedded platform for an UAV and give a best\npractice recommendation for building object detection pipelines based on our\nfindings. We show that not all parameters have an equal impact on detection\naccuracy and data throughput, and that by using a suitable compromise between\nparameters we are able to achieve higher detection accuracy for lightweight\nobject detection models, while keeping the same data throughput.\n","authors":["Leon Amadeus Varga","Sebastian Koch","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2203.00306v2.pdf","comment":"Submitted WACV23"},{"id":"http://arxiv.org/abs/2207.09193v1","updated":"2022-07-19T10:55:41Z","published":"2022-07-19T10:55:41Z","title":"NDF: Neural Deformable Fields for Dynamic Human Modelling","summary":"  We propose Neural Deformable Fields (NDF), a new representation for dynamic\nhuman digitization from a multi-view video. Recent works proposed to represent\na dynamic human body with shared canonical neural radiance fields which links\nto the observation space with deformation fields estimations. However, the\nlearned canonical representation is static and the current design of the\ndeformation fields is not able to represent large movements or detailed\ngeometry changes. In this paper, we propose to learn a neural deformable field\nwrapped around a fitted parametric body model to represent the dynamic human.\nThe NDF is spatially aligned by the underlying reference surface. A neural\nnetwork is then learned to map pose to the dynamics of NDF. The proposed NDF\nrepresentation can synthesize the digitized performer with novel views and\nnovel poses with a detailed and reasonable dynamic appearance. Experiments show\nthat our method significantly outperforms recent human synthesis methods.\n","authors":["Ruiqi Zhang","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2207.09193v1.pdf","comment":"16 pages, 7 figures. Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2110.03967v2","updated":"2022-07-19T10:31:52Z","published":"2021-10-08T08:32:26Z","title":"GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using\n  Unsupervised Learning","summary":"  Numerous studies in the literature have already shown the potential of\nbiometrics on mobile devices for authentication purposes. However, it has been\nshown that, the learning processes associated to biometric systems might expose\nsensitive personal information about the subjects. This study proposes\nGaitPrivacyON, a novel mobile gait biometrics verification approach that\nprovides accurate authentication results while preserving the sensitive\ninformation of the subject. It comprises two modules: i) a convolutional\nAutoencoder that transforms attributes of the biometric raw data, such as the\ngender or the activity being performed, into a new privacy-preserving\nrepresentation; and ii) a mobile gait verification system based on the\ncombination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs) with a Siamese architecture. The main advantage of\nGaitPrivacyON is that the first module (convolutional Autoencoder) is trained\nin an unsupervised way, without specifying the sensitive attributes of the\nsubject to protect. The experimental results achieved using two popular\ndatabases (MotionSense and MobiAct) suggest the potential of GaitPrivacyON to\nsignificantly improve the privacy of the subject while keeping user\nauthentication results higher than 99% Area Under the Curve (AUC). To the best\nof our knowledge, this is the first mobile gait verification approach that\nconsiders privacy-preserving methods trained in an unsupervised way.\n","authors":["Paula Delgado-Santos","Ruben Tolosana","Richard Guest","Ruben Vera","Farzin Deravi","Aythami Morales"],"pdf_url":"https://arxiv.org/pdf/2110.03967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09176v1","updated":"2022-07-19T10:23:40Z","published":"2022-07-19T10:23:40Z","title":"Self-Supervision Can Be a Good Few-Shot Learner","summary":"  Existing few-shot learning (FSL) methods rely on training with a large\nlabeled dataset, which prevents them from leveraging abundant unlabeled data.\nFrom an information-theoretic perspective, we propose an effective unsupervised\nFSL method, learning representations with self-supervision. Following the\nInfoMax principle, our method learns comprehensive representations by capturing\nthe intrinsic structure of the data. Specifically, we maximize the mutual\ninformation (MI) of instances and their representations with a low-bias MI\nestimator to perform self-supervised pre-training. Rather than supervised\npre-training focusing on the discriminable features of the seen classes, our\nself-supervised model has less bias toward the seen classes, resulting in\nbetter generalization for unseen classes. We explain that supervised\npre-training and self-supervised pre-training are actually maximizing different\nMI objectives. Extensive experiments are further conducted to analyze their FSL\nperformance with various training settings. Surprisingly, the results show that\nself-supervised pre-training can outperform supervised pre-training under the\nappropriate conditions. Compared with state-of-the-art FSL methods, our\napproach achieves comparable performance on widely used FSL benchmarks without\nany labels of the base classes.\n","authors":["Yuning Lu","Liangjian Wen","Jianzhuang Liu","Yajing Liu","Xinmei Tian"],"pdf_url":"https://arxiv.org/pdf/2207.09176v1.pdf","comment":"ECCV 2022, code: https://github.com/bbbdylan/unisiam"},{"id":"http://arxiv.org/abs/2101.05307v2","updated":"2022-07-19T10:20:52Z","published":"2021-01-13T19:09:38Z","title":"Explainability of deep vision-based autonomous driving systems: Review\n  and challenges","summary":"  This survey reviews explainability methods for vision-based self-driving\nsystems trained with behavior cloning. The concept of explainability has\nseveral facets and the need for explainability is strong in driving, a\nsafety-critical application. Gathering contributions from several research\nfields, namely computer vision, deep learning, autonomous driving, explainable\nAI (X-AI), this survey tackles several points. First, it discusses definitions,\ncontext, and motivation for gaining more interpretability and explainability\nfrom self-driving systems, as well as the challenges that are specific to this\napplication. Second, methods providing explanations to a black-box self-driving\nsystem in a post-hoc fashion are comprehensively organized and detailed. Third,\napproaches from the literature that aim at building more interpretable\nself-driving systems by design are presented and discussed in detail. Finally,\nremaining open-challenges and potential future research directions are\nidentified and examined.\n","authors":["Éloi Zablocki","Hédi Ben-Younes","Patrick Pérez","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2101.05307v2.pdf","comment":"IJCV 2022"},{"id":"http://arxiv.org/abs/2207.09165v1","updated":"2022-07-19T10:12:26Z","published":"2022-07-19T10:12:26Z","title":"A Multi-Stage Framework for the 2022 Multi-Structure Segmentation for\n  Renal Cancer Treatment","summary":"  Three-dimensional (3D) kidney parsing on computed tomography angiography\n(CTA) images is of great clinical significance. Automatic segmentation of\nkidney, renal tumor, renal vein and renal artery benefits a lot on\nsurgery-based renal cancer treatment. In this paper, we propose a new\nnnhra-unet network, and use a multi-stage framework which is based on it to\nsegment the multi-structure of kidney and participate in the KiPA2022\nchallenge.\n","authors":["Yusheng Liu","Zhongchen Zhao","Lisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09162v1","updated":"2022-07-19T10:10:49Z","published":"2022-07-19T10:10:49Z","title":"Global and Local Features through Gaussian Mixture Models on Image\n  Semantic Segmentation","summary":"  The semantic segmentation task aims at dense classification at the pixel-wise\nlevel. Deep models exhibited progress in tackling this task. However, one\nremaining problem with these approaches is the loss of spatial precision, often\nproduced at the segmented objects' boundaries. Our proposed model addresses\nthis problem by providing an internal structure for the feature representations\nwhile extracting a global representation that supports the former. To fit the\ninternal structure, during training, we predict a Gaussian Mixture Model from\nthe data, which, merged with the skip connections and the decoding stage, helps\navoid wrong inductive biases. Furthermore, our results show that we can improve\nsemantic segmentation by providing both learning representations (global and\nlocal) with a clustering behavior and combining them. Finally, we present\nresults demonstrating our advances in Cityscapes and Synthia datasets.\n","authors":["Darwin Saire","Adín Ramírez Rivera"],"pdf_url":"https://arxiv.org/pdf/2207.09162v1.pdf","comment":"Pre-print to appear in IEEE Access. Code available at\n  https://gitlab.com/mipl/phgmm"},{"id":"http://arxiv.org/abs/2207.09161v1","updated":"2022-07-19T10:01:31Z","published":"2022-07-19T10:01:31Z","title":"Single Stage Virtual Try-on via Deformable Attention Flows","summary":"  Virtual try-on aims to generate a photo-realistic fitting result given an\nin-shop garment and a reference person image. Existing methods usually build up\nmulti-stage frameworks to deal with clothes warping and body blending\nrespectively, or rely heavily on intermediate parser-based labels which may be\nnoisy or even inaccurate. To solve the above challenges, we propose a\nsingle-stage try-on framework by developing a novel Deformable Attention Flow\n(DAFlow), which applies the deformable attention scheme to multi-flow\nestimation. With pose keypoints as the guidance only, the self- and\ncross-deformable attention flows are estimated for the reference person and the\ngarment images, respectively. By sampling multiple flow fields, the\nfeature-level and pixel-level information from different semantic areas are\nsimultaneously extracted and merged through the attention mechanism. It enables\nclothes warping and body synthesizing at the same time which leads to\nphoto-realistic results in an end-to-end manner. Extensive experiments on two\ntry-on datasets demonstrate that our proposed method achieves state-of-the-art\nperformance both qualitatively and quantitatively. Furthermore, additional\nexperiments on the other two image editing tasks illustrate the versatility of\nour method for multi-view synthesis and image animation.\n","authors":["Shuai Bai","Huiling Zhou","Zhikang Li","Chang Zhou","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2207.09161v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2203.11089v3","updated":"2022-07-19T10:00:22Z","published":"2022-03-21T16:12:53Z","title":"PersFormer: 3D Lane Detection via Perspective Transformer and the\n  OpenLane Benchmark","summary":"  Methods for 3D lane detection have been recently proposed to address the\nissue of inaccurate lane layouts in many autonomous driving scenarios\n(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to\ntheir simple designs of the spatial transformation between front view and\nbird's eye view (BEV) and the lack of a realistic dataset. Towards these\nissues, we present PersFormer: an end-to-end monocular 3D lane detector with a\nnovel Transformer-based spatial feature transformation module. Our model\ngenerates BEV features by attending to related front-view local regions with\ncamera parameters as a reference. PersFormer adopts a unified 2D/3D anchor\ndesign and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing\nthe feature consistency and sharing the benefits of multi-task learning.\nMoreover, we release one of the first large-scale real-world 3D lane datasets:\nOpenLane, with high-quality annotation and scenario diversity. OpenLane\ncontains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories,\nalong with scene tags and the closed-in-path object annotations to encourage\nthe development of lane detection and more industrial-related autonomous\ndriving methods. We show that PersFormer significantly outperforms competitive\nbaselines in the 3D lane detection task on our new OpenLane dataset as well as\nApollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art\nalgorithms in the 2D task on OpenLane. The project page is available at\nhttps://github.com/OpenPerceptionX/PersFormer_3DLane and OpenLane dataset is\nprovided at https://github.com/OpenPerceptionX/OpenLane.\n","authors":["Li Chen","Chonghao Sima","Yang Li","Zehan Zheng","Jiajie Xu","Xiangwei Geng","Hongyang Li","Conghui He","Jianping Shi","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2203.11089v3.pdf","comment":"Accepted by ECCV 2022 (Oral). Project page:\n  https://github.com/OpenPerceptionX/PersFormer_3DLane | OpenLane dataset:\n  https://github.com/OpenPerceptionX/OpenLane"},{"id":"http://arxiv.org/abs/2207.09158v1","updated":"2022-07-19T09:56:44Z","published":"2022-07-19T09:56:44Z","title":"FedX: Unsupervised Federated Learning with Cross Knowledge Distillation","summary":"  This paper presents FedX, an unsupervised federated learning framework. Our\nmodel learns unbiased representation from decentralized and heterogeneous local\ndata. It employs a two-sided knowledge distillation with contrastive learning\nas a core component, allowing the federated system to function without\nrequiring clients to share any data features. Furthermore, its adaptable\narchitecture can be used as an add-on module for existing unsupervised\nalgorithms in federated settings. Experiments show that our model improves\nperformance significantly (1.58--5.52pp) on five unsupervised algorithms.\n","authors":["Sungwon Han","Sungwon Park","Fangzhao Wu","Sundong Kim","Chuhan Wu","Xing Xie","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2207.09158v1.pdf","comment":"Accepted and will be published at ECCV2022"},{"id":"http://arxiv.org/abs/2207.09156v1","updated":"2022-07-19T09:54:17Z","published":"2022-07-19T09:54:17Z","title":"Learning Mutual Modulation for Self-Supervised Cross-Modal\n  Super-Resolution","summary":"  Self-supervised cross-modal super-resolution (SR) can overcome the difficulty\nof acquiring paired training data, but is challenging because only\nlow-resolution (LR) source and high-resolution (HR) guide images from different\nmodalities are available. Existing methods utilize pseudo or weak supervision\nin LR space and thus deliver results that are blurry or not faithful to the\nsource modality. To address this issue, we present a mutual modulation SR\n(MMSR) model, which tackles the task by a mutual modulation strategy, including\na source-to-guide modulation and a guide-to-source modulation. In these\nmodulations, we develop cross-domain adaptive filters to fully exploit\ncross-modal spatial dependency and help induce the source to emulate the\nresolution of the guide and induce the guide to mimic the modality\ncharacteristics of the source. Moreover, we adopt a cycle consistency\nconstraint to train MMSR in a fully self-supervised manner. Experiments on\nvarious tasks demonstrate the state-of-the-art performance of our MMSR.\n","authors":["Xiaoyu Dong","Naoto Yokoya","Longguang Wang","Tatsumi Uezato"],"pdf_url":"https://arxiv.org/pdf/2207.09156v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08467v2","updated":"2022-07-19T09:45:01Z","published":"2022-07-18T09:36:44Z","title":"Segmenting white matter hyperintensities on isotropic three-dimensional\n  Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison\n  of Deep learning tools on a Norwegian national imaging database","summary":"  Automated segmentation of white matter hyperintensities (WMHs) is an\nessential step in neuroimaging analysis of Magnetic Resonance Imaging (MRI).\nFluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast that is\nparticularly useful to visualize and quantify WMHs, a hallmark of cerebral\nsmall vessel disease and Alzheimer's disease (AD). Clinical MRI protocols\nmigrate to a three-dimensional (3D) FLAIR-weighted acquisition to enable high\nspatial resolution in all three voxel dimensions. The current study details the\ndeployment of deep learning tools to enable automated WMH segmentation and\ncharacterization from 3D FLAIR-weighted images acquired as part of a national\nAD imaging initiative.\n  Among 642 participants (283 male, mean age: (65.18 +/- 9.33) years) from the\nDDI study, two in-house networks were trained and validated across five\nnational collection sites. Three models were tested on a held-out subset of the\ninternal data from the 642 participants and an external dataset with 29 cases\nfrom an international collaborator. These test sets were evaluated\nindependently. Five established WMH performance metrics were used for\ncomparison against ground truth human-in-the-loop segmentation.\n  Results of the three networks tested, the 3D nnU-Net had the best performance\nwith an average dice similarity coefficient score of 0.78 +/- 0.10, performing\nbetter than both the in-house developed 2.5D model and the SOTA Deep Bayesian\nnetwork.\n  With the increasing use of 3D FLAIR-weighted images in MRI protocols, our\nresults suggest that WMH segmentation models can be trained on 3D data and\nyield WMH segmentation performance that is comparable to or better than\nstate-of-the-art without the need for including T1-weighted image series.\n","authors":["Martin Soria Roevang","Per Selnes","Bradley John MacIntosh","Inge Rasmus Groote","Lene Paalhaugen","Carole Sudre","Tormod Fladby","Atle Bjoernerud"],"pdf_url":"https://arxiv.org/pdf/2207.08467v2.pdf","comment":"14 Pages, 7 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2207.07316v3","updated":"2022-07-19T09:43:15Z","published":"2022-07-15T07:15:36Z","title":"Privacy-Preserving Face Recognition with Learnable Privacy Budgets in\n  Frequency Domain","summary":"  Face recognition technology has been used in many fields due to its high\nrecognition accuracy, including the face unlocking of mobile devices, community\naccess control systems, and city surveillance. As the current high accuracy is\nguaranteed by very deep network structures, facial images often need to be\ntransmitted to third-party servers with high computational power for inference.\nHowever, facial images visually reveal the user's identity information. In this\nprocess, both untrusted service providers and malicious users can significantly\nincrease the risk of a personal privacy breach. Current privacy-preserving\napproaches to face recognition are often accompanied by many side effects, such\nas a significant increase in inference time or a noticeable decrease in\nrecognition accuracy. This paper proposes a privacy-preserving face recognition\nmethod using differential privacy in the frequency domain. Due to the\nutilization of differential privacy, it offers a guarantee of privacy in\ntheory. Meanwhile, the loss of accuracy is very slight. This method first\nconverts the original image to the frequency domain and removes the direct\ncomponent termed DC. Then a privacy budget allocation method can be learned\nbased on the loss of the back-end face recognition network within the\ndifferential privacy framework. Finally, it adds the corresponding noise to the\nfrequency domain features. Our method performs very well with several classical\nface recognition test sets according to the extensive experiments.\n","authors":["Jiazhen Ji","Huan Wang","Yuge Huang","Jiaxiang Wu","Xingkun Xu","Shouhong Ding","ShengChuan Zhang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2207.07316v3.pdf","comment":"ECCV 2022; Code is available at\n  https://github.com/Tencent/TFace/tree/master/recognition/tasks/dctdp"},{"id":"http://arxiv.org/abs/2207.09143v1","updated":"2022-07-19T09:27:05Z","published":"2022-07-19T09:27:05Z","title":"What Matters for 3D Scene Flow Network","summary":"  3D scene flow estimation from point clouds is a low-level 3D motion\nperception task in computer vision. Flow embedding is a commonly used technique\nin scene flow estimation, and it encodes the point motion between two\nconsecutive frames. Thus, it is critical for the flow embeddings to capture the\ncorrect overall direction of the motion. However, previous works only search\nlocally to determine a soft correspondence, ignoring the distant points that\nturn out to be the actual matching ones. In addition, the estimated\ncorrespondence is usually from the forward direction of the adjacent point\nclouds, and may not be consistent with the estimated correspondence acquired\nfrom the backward direction. To tackle these problems, we propose a novel\nall-to-all flow embedding layer with backward reliability validation during the\ninitial scene flow estimation. Besides, we investigate and compare several\ndesign choices in key components of the 3D scene flow network, including the\npoint similarity calculation, input elements of predictor, and predictor &\nrefinement level design. After carefully choosing the most effective designs,\nwe are able to present a model that achieves the state-of-the-art performance\non FlyingThings3D and KITTI Scene Flow datasets. Our proposed model surpasses\nall existing methods by at least 38.2% on FlyingThings3D dataset and 24.7% on\nKITTI Scene Flow dataset for EPE3D metric. We release our codes at\nhttps://github.com/IRMVLab/3DFlow.\n","authors":["Guangming Wang","Yunzhe Hu","Zhe Liu","Yiyang Zhou","Masayoshi Tomizuka","Wei Zhan","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09143v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08581v2","updated":"2022-07-19T09:24:09Z","published":"2022-07-18T13:18:34Z","title":"Study of the performance and scalability of federated learning for\n  medical imaging with intermittent clients","summary":"  Federated learning is a data decentralization privacy-preserving technique\nused to perform machine or deep learning in a secure way. In this paper we\npresent theoretical aspects about federated learning, such as the presentation\nof an aggregation operator, different types of federated learning, and issues\nto be taken into account in relation to the distribution of data from the\nclients, together with the exhaustive analysis of a use case where the number\nof clients varies. Specifically, a use case of medical image analysis is\nproposed, using chest X-ray images obtained from an open data repository. In\naddition to the advantages related to privacy, improvements in predictions (in\nterms of accuracy and area under the curve) and reduction of execution times\nwill be studied with respect to the classical case (the centralized approach).\nDifferent clients will be simulated from the training data, selected in an\nunbalanced manner, i.e., they do not all have the same number of data. The\nresults of considering three or ten clients are exposed and compared between\nthem and against the centralized case. Two approaches to follow will be\nanalyzed in the case of intermittent clients, as in a real scenario some\nclients may leave the training, and some new ones may enter the training. The\nevolution of the results for the test set in terms of accuracy, area under the\ncurve and execution time is shown as the number of clients into which the\noriginal data is divided increases. Finally, improvements and future work in\nthe field are proposed.\n","authors":["Judith Sáinz-Pardo Díaz","Álvaro López García"],"pdf_url":"https://arxiv.org/pdf/2207.08581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08485v2","updated":"2022-07-19T09:23:22Z","published":"2022-07-18T10:10:14Z","title":"Hierarchical Feature Alignment Network for Unsupervised Video Object\n  Segmentation","summary":"  Optical flow is an easily conceived and precious cue for advancing\nunsupervised video object segmentation (UVOS). Most of the previous methods\ndirectly extract and fuse the motion and appearance features for segmenting\ntarget objects in the UVOS setting. However, optical flow is intrinsically an\ninstantaneous velocity of all pixels among consecutive frames, thus making the\nmotion features not aligned well with the primary objects among the\ncorresponding frames. To solve the above challenge, we propose a concise,\npractical, and efficient architecture for appearance and motion feature\nalignment, dubbed hierarchical feature alignment network (HFAN). Specifically,\nthe key merits in HFAN are the sequential Feature AlignMent (FAM) module and\nthe Feature AdaptaTion (FAT) module, which are leveraged for processing the\nappearance and motion features hierarchically. FAM is capable of aligning both\nappearance and motion features with the primary object semantic\nrepresentations, respectively. Further, FAT is explicitly designed for the\nadaptive fusion of appearance and motion features to achieve a desirable\ntrade-off between cross-modal features. Extensive experiments demonstrate the\neffectiveness of the proposed HFAN, which reaches a new state-of-the-art\nperformance on DAVIS-16, achieving 88.7 $\\mathcal{J}\\&\\mathcal{F}$ Mean, i.e.,\na relative improvement of 3.5% over the best published result.\n","authors":["Gensheng Pei","Fumin Shen","Yazhou Yao","Guo-Sen Xie","Zhenmin Tang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2207.08485v2.pdf","comment":"Accepted by ECCV-2022"},{"id":"http://arxiv.org/abs/2207.09137v1","updated":"2022-07-19T09:19:45Z","published":"2022-07-19T09:19:45Z","title":"ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving\n  Cameras in the Wild","summary":"  Estimating the pose of a moving camera from monocular video is a challenging\nproblem, especially due to the presence of moving objects in dynamic\nenvironments, where the performance of existing camera pose estimation methods\nare susceptible to pixels that are not geometrically consistent. To tackle this\nchallenge, we present a robust dense indirect structure-from-motion method for\nvideos that is based on dense correspondence initialized from pairwise optical\nflow. Our key idea is to optimize long-range video correspondence as dense\npoint trajectories and use it to learn robust estimation of motion\nsegmentation. A novel neural network architecture is proposed for processing\nirregular point trajectory data. Camera poses are then estimated and optimized\nwith global bundle adjustment over the portion of long-range point trajectories\nthat are classified as static. Experiments on MPI Sintel dataset show that our\nsystem produces significantly more accurate camera trajectories compared to\nexisting state-of-the-art methods. In addition, our method is able to retain\nreasonable accuracy of camera poses on fully static scenes, which consistently\noutperforms strong state-of-the-art dense correspondence based methods with\nend-to-end deep learning, demonstrating the potential of dense indirect methods\nbased on optical flow and point trajectories. As the point trajectory\nrepresentation is general, we further present results and comparisons on\nin-the-wild monocular videos with complex motion of dynamic objects. Code is\navailable at https://github.com/bytedance/particle-sfm.\n","authors":["Wang Zhao","Shaohui Liu","Hengkai Guo","Wenping Wang","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2207.09137v1.pdf","comment":"ECCV 2022. Project page: http://b1ueber2y.me/projects/ParticleSfM/"},{"id":"http://arxiv.org/abs/2207.09135v1","updated":"2022-07-19T09:11:43Z","published":"2022-07-19T09:11:43Z","title":"Shrinking the Semantic Gap: Spatial Pooling of Local Moment Invariants\n  for Copy-Move Forgery Detection","summary":"  Copy-move forgery is a manipulation of copying and pasting specific patches\nfrom and to an image, with potentially illegal or unethical uses. Recent\nadvances in the forensic methods for copy-move forgery have shown increasing\nsuccess in detection accuracy and robustness. However, for images with high\nself-similarity or strong signal corruption, the existing algorithms often\nexhibit inefficient processes and unreliable results. This is mainly due to the\ninherent semantic gap between low-level visual representation and high-level\nsemantic concept. In this paper, we present a very first study of trying to\nmitigate the semantic gap problem in copy-move forgery detection, with spatial\npooling of local moment invariants for midlevel image representation. Our\ndetection method expands the traditional works on two aspects: 1) we introduce\nthe bag-of-visual-words model into this field for the first time, may meaning a\nnew perspective of forensic study; 2) we propose a word-to-phrase feature\ndescription and matching pipeline, covering the spatial structure and visual\nsaliency information of digital images. Extensive experimental results show the\nsuperior performance of our framework over state-of-the-art algorithms in\novercoming the related problems caused by the semantic gap.\n","authors":["Chao Wang","Zhiqiu Huang","Shuren Qi","Yaoshen Yu","Guohua Shen"],"pdf_url":"https://arxiv.org/pdf/2207.09135v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2207.04808v4","updated":"2022-07-19T09:10:40Z","published":"2022-07-11T12:09:41Z","title":"CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer","summary":"  In this paper, we aim to devise a universally versatile style transfer method\ncapable of performing artistic, photo-realistic, and video style transfer\njointly, without seeing videos during training. Previous single-frame methods\nassume a strong constraint on the whole image to maintain temporal consistency,\nwhich could be violated in many cases. Instead, we make a mild and reasonable\nassumption that global inconsistency is dominated by local inconsistencies and\ndevise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local\npatches. CCPL can preserve the coherence of the content source during style\ntransfer without degrading stylization. Moreover, it owns a neighbor-regulating\nmechanism, resulting in a vast reduction of local distortions and considerable\nvisual quality improvement. Aside from its superior performance on versatile\nstyle transfer, it can be easily extended to other tasks, such as\nimage-to-image translation. Besides, to better fuse content and style features,\nwe propose Simple Covariance Transformation (SCT) to effectively align\nsecond-order statistics of the content feature with the style feature.\nExperiments demonstrate the effectiveness of the resulting model for versatile\nstyle transfer, when armed with CCPL.\n","authors":["Zijie Wu","Zhen Zhu","Junping Du","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2207.04808v4.pdf","comment":"Accepted by ECCV2022 as an oral paper; code url:\n  https://github.com/JarrentWu1031/CCPL Video demo:\n  https://youtu.be/scZuJCXhL14"},{"id":"http://arxiv.org/abs/2112.07957v2","updated":"2022-07-19T09:03:17Z","published":"2021-12-15T08:28:55Z","title":"FEAR: Fast, Efficient, Accurate and Robust Visual Tracker","summary":"  We present FEAR, a family of fast, efficient, accurate, and robust Siamese\nvisual trackers. We present a novel and efficient way to benefit from\ndual-template representation for object model adaption, which incorporates\ntemporal information with only a single learnable parameter. We further improve\nthe tracker architecture with a pixel-wise fusion block. By plugging-in\nsophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L\ntrackers surpass most Siamese trackers on several academic benchmarks in both\naccuracy and efficiency. Employed with the lightweight backbone, the optimized\nversion FEAR-XS offers more than 10 times faster tracking than current Siamese\ntrackers while maintaining near state-of-the-art results. FEAR-XS tracker is\n2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In\naddition, we expand the definition of the model efficiency by introducing FEAR\nbenchmark that assesses energy consumption and execution speed. We show that\nenergy consumption is a limiting factor for trackers on mobile devices. Source\ncode, pretrained models, and evaluation protocol are available at\nhttps://github.com/PinataFarms/FEARTracker.\n","authors":["Vasyl Borsuk","Roman Vei","Orest Kupyn","Tetiana Martyniuk","Igor Krashenyi","Jiři Matas"],"pdf_url":"https://arxiv.org/pdf/2112.07957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.03247v4","updated":"2022-07-19T08:56:21Z","published":"2021-05-07T13:27:01Z","title":"MOTR: End-to-End Multiple-Object Tracking with Transformer","summary":"  Temporal modeling of objects is a key challenge in multiple object tracking\n(MOT). Existing methods track by associating detections through motion-based\nand appearance-based similarity heuristics. The post-processing nature of\nassociation prevents end-to-end exploitation of temporal variations in video\nsequence. In this paper, we propose MOTR, which extends DETR and introduces\ntrack query to model the tracked instances in the entire video. Track query is\ntransferred and updated frame-by-frame to perform iterative prediction over\ntime. We propose tracklet-aware label assignment to train track queries and\nnewborn object queries. We further propose temporal aggregation network and\ncollective average loss to enhance temporal relation modeling. Experimental\nresults on DanceTrack show that MOTR significantly outperforms state-of-the-art\nmethod, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our\nconcurrent works, TrackFormer and TransTrack, on association performance. MOTR\ncan serve as a stronger baseline for future research on temporal modeling and\nTransformer-based trackers. Code is available at\nhttps://github.com/megvii-research/MOTR.\n","authors":["Fangao Zeng","Bin Dong","Yuang Zhang","Tiancai Wang","Xiangyu Zhang","Yichen Wei"],"pdf_url":"https://arxiv.org/pdf/2105.03247v4.pdf","comment":"Accepted by ECCV 2022. Code is available at\n  https://github.com/megvii-research/MOTR"},{"id":"http://arxiv.org/abs/2207.02541v2","updated":"2022-07-19T08:45:02Z","published":"2022-07-06T09:41:17Z","title":"Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection","summary":"  To date, the most powerful semi-supervised object detectors (SS-OD) are based\non pseudo-boxes, which need a sequence of post-processing with fine-tuned\nhyper-parameters. In this work, we propose replacing the sparse pseudo-boxes\nwith the dense prediction as a united and straightforward form of pseudo-label.\nCompared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any\npost-processing method, thus retaining richer information. We also introduce a\nregion selection technique to highlight the key information while suppressing\nthe noise carried by dense labels. We name our proposed SS-OD algorithm that\nleverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows\nsuperior performance under various settings compared with the pseudo-box-based\nmethods.\n","authors":["Hongyu Zhou","Zheng Ge","Songtao Liu","Weixin Mao","Zeming Li","Haiyan Yu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2207.02541v2.pdf","comment":"ECCV2022"},{"id":"http://arxiv.org/abs/2203.01386v2","updated":"2022-07-19T08:41:22Z","published":"2022-03-02T20:05:00Z","title":"Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot\n  Image Classification","summary":"  The main question we address in this paper is how to scale up visual\nrecognition of unseen classes, also known as zero-shot learning, to tens of\nthousands of categories as in the ImageNet-21K benchmark. At this scale,\nespecially with many fine-grained categories included in ImageNet-21K, it is\ncritical to learn quality visual semantic representations that are\ndiscriminative enough to recognize unseen classes and distinguish them from\nseen ones. We propose a \\emph{H}ierarchical \\emph{G}raphical knowledge\n\\emph{R}epresentation framework for the confidence-based classification method,\ndubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp\nclass inheritance relations by utilizing hierarchical conceptual knowledge. Our\nmethod significantly outperformed all existing techniques, boosting the\nperformance by 7\\% compared to the runner-up approach on the ImageNet-21K\nbenchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We\nalso analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and\n3-hops, demonstrating its generalization ability. Our benchmark and code are\navailable at https://kaiyi.me/p/hgrnet.html.\n","authors":["Kai Yi","Xiaoqian Shen","Yunhao Gou","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2203.01386v2.pdf","comment":"ECCV 2022, camera-ready version"},{"id":"http://arxiv.org/abs/2203.05625v3","updated":"2022-07-19T08:30:57Z","published":"2022-03-10T20:33:28Z","title":"PETR: Position Embedding Transformation for Multi-View 3D Object\n  Detection","summary":"  In this paper, we develop position embedding transformation (PETR) for\nmulti-view 3D object detection. PETR encodes the position information of 3D\ncoordinates into image features, producing the 3D position-aware features.\nObject query can perceive the 3D position-aware features and perform end-to-end\nobject detection. PETR achieves state-of-the-art performance (50.4% NDS and\n44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.\nIt can serve as a simple yet strong baseline for future research. Code is\navailable at \\url{https://github.com/megvii-research/PETR}.\n","authors":["Yingfei Liu","Tiancai Wang","Xiangyu Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2203.05625v3.pdf","comment":"Accepted by ECCV 2022. Code is available at\n  \\url{https://github.com/megvii-research/PETR}"},{"id":"http://arxiv.org/abs/2012.06257v3","updated":"2022-07-19T08:30:19Z","published":"2020-12-11T11:45:39Z","title":"On Learning the Right Attention Point for Feature Enhancement","summary":"  We present a novel attention-based mechanism to learn enhanced point features\nfor point cloud processing tasks, e.g., classification and segmentation. Unlike\nprior works, which were trained to optimize the weights of a pre-selected set\nof attention points, our approach learns to locate the best attention points to\nmaximize the performance of a specific task, e.g., point cloud classification.\nImportantly, we advocate the use of single attention point to facilitate\nsemantic understanding in point feature learning. Specifically, we formulate a\nnew and simple convolution, which combines convolutional features from an input\npoint and its corresponding learned attention point, or LAP, for short. Our\nattention mechanism can be easily incorporated into state-of-the-art point\ncloud classification and segmentation networks. Extensive experiments on common\nbenchmarks such as ModelNet40, ShapeNetPart, and S3DIS all demonstrate that our\nLAP-enabled networks consistently outperform the respective original networks,\nas well as other competitive alternatives, which employ multiple attention\npoints, either pre-selected or learned under our LAP framework.\n","authors":["Liqiang Lin","Pengdi Huang","Chi-Wing Fu","Kai Xu","Hao Zhang","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2012.06257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00705v2","updated":"2022-07-19T08:22:38Z","published":"2022-05-02T07:53:29Z","title":"3D Object Detection with a Self-supervised Lidar Scene Flow Backbone","summary":"  State-of-the-art lidar-based 3D object detection methods rely on supervised\nlearning and large labeled datasets. However, annotating lidar data is\nresource-consuming, and depending only on supervised learning limits the\napplicability of trained models. Self-supervised training strategies can\nalleviate these issues by learning a general point cloud backbone model for\ndownstream 3D vision tasks. Against this backdrop, we show the relationship\nbetween self-supervised multi-frame flow representations and single-frame 3D\ndetection hypotheses. Our main contribution leverages learned flow and motion\nrepresentations and combines a self-supervised backbone with a supervised 3D\ndetection head. First, a self-supervised scene flow estimation model is trained\nwith cycle consistency. Then, the point cloud encoder of this model is used as\nthe backbone of a single-frame 3D object detection head model. This second 3D\nobject detection model learns to utilize motion representations to distinguish\ndynamic objects exhibiting different movement patterns. Experiments on KITTI\nand nuScenes benchmarks show that the proposed self-supervised pre-training\nincreases 3D detection performance significantly.\nhttps://github.com/emecercelik/ssl-3d-detection.git\n","authors":["Ekim Yurtsever","Emeç Erçelik","Mingyu Liu","Zhijie Yang","Hanzhen Zhang","Pınar Topçam","Maximilian Listl","Yılmaz Kaan Çaylı","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2205.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.05515v3","updated":"2022-07-19T08:20:52Z","published":"2022-07-12T13:17:38Z","title":"Compound Prototype Matching for Few-shot Action Recognition","summary":"  Few-shot action recognition aims to recognize novel action classes using only\na small number of labeled training samples. In this work, we propose a novel\napproach that first summarizes each video into compound prototypes consisting\nof a group of global prototypes and a group of focused prototypes, and then\ncompares video similarity based on the prototypes. Each global prototype is\nencouraged to summarize a specific aspect from the entire video, for example,\nthe start/evolution of the action. Since no clear annotation is provided for\nthe global prototypes, we use a group of focused prototypes to focus on certain\ntimestamps in the video. We compare video similarity by matching the compound\nprototypes between the support and query videos. The global prototypes are\ndirectly matched to compare videos from the same perspective, for example, to\ncompare whether two actions start similarly. For the focused prototypes, since\nactions have various temporal variations in the videos, we apply bipartite\nmatching to allow the comparison of actions with different temporal positions\nand shifts. Experiments demonstrate that our proposed method achieves\nstate-of-the-art results on multiple benchmarks.\n","authors":["Lijin Yang","Yifei Huang","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2207.05515v3.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09120v1","updated":"2022-07-19T08:20:05Z","published":"2022-07-19T08:20:05Z","title":"Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios","summary":"  Clustering traffic scenarios and detecting novel scenario types are required\nfor scenario-based testing of autonomous vehicles. These tasks benefit from\neither good similarity measures or good representations for the traffic\nscenarios. In this work, an expert-knowledge aided representation learning for\ntraffic scenarios is presented. The latent space so formed is used for\nsuccessful clustering and novel scenario type detection. Expert-knowledge is\nused to define objectives that the latent representations of traffic scenarios\nshall fulfill. It is presented, how the network architecture and loss is\ndesigned from these objectives, thereby incorporating expert-knowledge. An\nautomatic mining strategy for traffic scenarios is presented, such that no\nmanual labeling is required. Results show the performance advantage compared to\nbaseline methods. Additionally, extensive analysis of the latent space is\nperformed.\n","authors":["Jonas Wurst","Lakshman Balasubramanian","Michael Botsch","Wolfgang Utschick"],"pdf_url":"https://arxiv.org/pdf/2207.09120v1.pdf","comment":"Copyright 2022 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2204.13317v4","updated":"2022-07-19T08:05:58Z","published":"2022-04-28T07:31:00Z","title":"MMRotate: A Rotated Object Detection Benchmark using PyTorch","summary":"  We present an open-source toolbox, named MMRotate, which provides a coherent\nalgorithm framework of training, inferring, and evaluation for the popular\nrotated object detection algorithm based on deep learning. MMRotate implements\n18 state-of-the-art algorithms and supports the three most frequently used\nangle definition methods. To facilitate future research and industrial\napplications of rotated object detection-related problems, we also provide a\nlarge number of trained models and detailed benchmarks to give insights into\nthe performance of rotated object detection. MMRotate is publicly released at\nhttps://github.com/open-mmlab/mmrotate.\n","authors":["Yue Zhou","Xue Yang","Gefan Zhang","Jiabao Wang","Yanyi Liu","Liping Hou","Xue Jiang","Xingzhao Liu","Junchi Yan","Chengqi Lyu","Wenwei Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2204.13317v4.pdf","comment":"5 pages, 2 tables, MMRotate is accepted by ACM MM 2022 (OS Track).\n  Yue Zhou and Xue Yang provided equal contribution. The code is publicly\n  released at https://github.com/open-mmlab/mmrotate"},{"id":"http://arxiv.org/abs/2207.09108v1","updated":"2022-07-19T07:39:04Z","published":"2022-07-19T07:39:04Z","title":"eCDT: Event Clustering for Simultaneous Feature Detection and Tracking-","summary":"  Contrary to other standard cameras, event cameras interpret the world in an\nentirely different manner; as a collection of asynchronous events. Despite\nevent camera's unique data output, many event feature detection and tracking\nalgorithms have shown significant progress by making detours to frame-based\ndata representations. This paper questions the need to do so and proposes a\nnovel event data-friendly method that achieve simultaneous feature detection\nand tracking, called event Clustering-based Detection and Tracking (eCDT). Our\nmethod employs a novel clustering method, named as k-NN Classifier-based\nSpatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent\npolarity events to retrieve event trajectories.With the aid of a Head and Tail\nDescriptor Matching process, event clusters that reappear in a different\npolarity are continually tracked, elongating the feature tracks. Thanks to our\nclustering approach in spatio-temporal space, our method automatically solves\nfeature detection and feature tracking simultaneously. Also, eCDT can extract\nfeature tracks at any frequency with an adjustable time window, which does not\ncorrupt the high temporal resolution of the original event data. Our method\nachieves 30% better feature tracking ages compared with the state-of-the-art\napproach while also having a low error approximately equal to it.\n","authors":["Sumin Hu","Yeeun Kim","Hyungtae Lim","Alex Junho Lee","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2207.09108v1.pdf","comment":"IROS2022 accepted paper"},{"id":"http://arxiv.org/abs/2207.09107v1","updated":"2022-07-19T07:25:43Z","published":"2022-07-19T07:25:43Z","title":"MONet: Multi-scale Overlap Network for Duplication Detection in\n  Biomedical Images","summary":"  Manipulation of biomedical images to misrepresent experimental results has\nplagued the biomedical community for a while. Recent interest in the problem\nled to the curation of a dataset and associated tasks to promote the\ndevelopment of biomedical forensic methods. Of these, the largest manipulation\ndetection task focuses on the detection of duplicated regions between images.\nTraditional computer-vision based forensic models trained on natural images are\nnot designed to overcome the challenges presented by biomedical images. We\npropose a multi-scale overlap detection model to detect duplicated image\nregions. Our model is structured to find duplication hierarchically, so as to\nreduce the number of patch operations. It achieves state-of-the-art performance\noverall and on multiple biomedical image categories.\n","authors":["Ekraam Sabir","Soumyaroop Nandi","Wael AbdAlmageed","Prem Natarajan"],"pdf_url":"https://arxiv.org/pdf/2207.09107v1.pdf","comment":"To appear at ICIP 2022"},{"id":"http://arxiv.org/abs/2203.03833v2","updated":"2022-07-19T06:57:13Z","published":"2022-03-08T03:44:49Z","title":"Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point\n  Clouds for Closing Domain Gap","summary":"  Semantic analyses of object point clouds are largely driven by releasing of\nbenchmarking datasets, including synthetic ones whose instances are sampled\nfrom object CAD models. However, learning from synthetic data may not\ngeneralize to practical scenarios, where point clouds are typically incomplete,\nnon-uniformly distributed, and noisy. Such a challenge of Simulation-to-Reality\n(Sim2Real) domain gap could be mitigated via learning algorithms of domain\nadaptation; however, we argue that generation of synthetic point clouds via\nmore physically realistic rendering is a powerful alternative, as systematic\nnon-uniform noise patterns can be captured. To this end, we propose an\nintegrated scheme consisting of physically realistic synthesis of object point\nclouds via rendering stereo images via projection of speckle patterns onto CAD\nmodels and a novel quasi-balanced self-training designed for more balanced data\ndistribution by sparsity-driven selection of pseudo labeled samples for long\ntailed classes. Experiment results can verify the effectiveness of our method\nas well as both of its modules for unsupervised domain adaptation on point\ncloud classification, achieving the state-of-the-art performance. Source codes\nand the SpeckleNet synthetic dataset are available at\nhttps://github.com/Gorilla-Lab-SCUT/QS3.\n","authors":["Yongwei Chen","Zihao Wang","Longkun Zou","Ke Chen","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2203.03833v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2204.08387v3","updated":"2022-07-19T06:41:15Z","published":"2022-04-18T16:19:52Z","title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image\n  Masking","summary":"  Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\n\\url{https://aka.ms/layoutlmv3}.\n","authors":["Yupan Huang","Tengchao Lv","Lei Cui","Yutong Lu","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2204.08387v3.pdf","comment":"ACM Multimedia 2022"},{"id":"http://arxiv.org/abs/2110.07920v4","updated":"2022-07-19T06:21:11Z","published":"2021-10-15T08:04:59Z","title":"Content Preserving Image Translation with Texture Co-occurrence and\n  Spatial Self-Similarity for Texture Debiasing and Domain Adaptation","summary":"  Models trained on datasets with texture bias usually perform poorly on\nout-of-distribution samples since biased representations are embedded into the\nmodel. Recently, various image translation and debiasing methods have attempted\nto disentangle texture biased representations for downstream tasks, but\naccurately discarding biased features without altering other relevant\ninformation is still challenging. In this paper, we propose a novel framework\nthat leverages image translation to generate additional training images using\nthe content of a source image and the texture of a target image with a\ndifferent bias property to explicitly mitigate texture bias when training a\nmodel on a target task. Our model ensures texture similarity between the target\nand generated images via a texture co-occurrence loss while preserving content\ndetails from source images with a spatial self-similarity loss. Both the\ngenerated and original training images are combined to train improved\nclassification or segmentation models robust to inconsistent texture bias.\nEvaluation on five classification- and two segmentation-datasets with known\ntexture biases demonstrates the utility of our method, and reports significant\nimprovements over recent state-of-the-art methods in all cases.\n","authors":["Myeongkyun Kang","Dongkyu Won","Miguel Luna","Philip Chikontwe","Kyung Soo Hong","June Hong Ahn","Sang Hyun Park"],"pdf_url":"https://arxiv.org/pdf/2110.07920v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09086v1","updated":"2022-07-19T05:47:03Z","published":"2022-07-19T05:47:03Z","title":"MHR-Net: Multiple-Hypothesis Reconstruction of Non-Rigid Shapes from 2D\n  Views","summary":"  We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from\nMotion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a\n2D view, and it also selects the most likely reconstruction from the set. To\ndeal with the challenging unsupervised generation of non-rigid shapes, we\ndevelop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net.\nThe non-rigid shape is first expressed as the sum of a coarse shape basis and a\nflexible shape deformation, then multiple hypotheses are generated with\nuncertainty modeling of the deformation part. MHR-Net is optimized with\nreprojection loss on the basis and the best hypothesis. Furthermore, we design\na new Procrustean Residual Loss, which reduces the rigid rotations between\nsimilar shapes and further improves the performance. Experiments show that\nMHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL\nand 300-VW datasets.\n","authors":["Haitian Zeng","Xin Yu","Jiaxu Miao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2207.09086v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09084v1","updated":"2022-07-19T05:43:14Z","published":"2022-07-19T05:43:14Z","title":"Dual Adaptive Transformations for Weakly Supervised Point Cloud\n  Segmentation","summary":"  Weakly supervised point cloud segmentation, i.e. semantically segmenting a\npoint cloud with only a few labeled points in the whole 3D scene, is highly\ndesirable due to the heavy burden of collecting abundant dense annotations for\nthe model training. However, existing methods remain challenging to accurately\nsegment 3D point clouds since limited annotated data may lead to insufficient\nguidance for label propagation to unlabeled data. Considering the\nsmoothness-based methods have achieved promising progress, in this paper, we\nadvocate applying the consistency constraint under various perturbations to\neffectively regularize unlabeled 3D points. Specifically, we propose a novel\nDAT (\\textbf{D}ual \\textbf{A}daptive \\textbf{T}ransformations) model for weakly\nsupervised point cloud segmentation, where the dual adaptive transformations\nare performed via an adversarial strategy at both point-level and region-level,\naiming at enforcing the local and structural smoothness constraints on 3D point\nclouds. We evaluate our proposed DAT model with two popular backbones on the\nlarge-scale S3DIS and ScanNet-V2 datasets. Extensive experiments demonstrate\nthat our model can effectively leverage the unlabeled 3D points and achieve\nsignificant performance gains on both datasets, setting new state-of-the-art\nperformance for weakly supervised point cloud segmentation.\n","authors":["Zhonghua Wu","Yicheng Wu","Guosheng Lin","Jianfei Cai","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2207.09084v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09083v1","updated":"2022-07-19T05:42:14Z","published":"2022-07-19T05:42:14Z","title":"Relational Future Captioning Model for Explaining Likely Collisions in\n  Daily Tasks","summary":"  Domestic service robots that support daily tasks are a promising solution for\nelderly or disabled people. It is crucial for domestic service robots to\nexplain the collision risk before they perform actions. In this paper, our aim\nis to generate a caption about a future event. We propose the Relational Future\nCaptioning Model (RFCM), a crossmodal language generation model for the future\ncaptioning task. The RFCM has the Relational Self-Attention Encoder to extract\nthe relationships between events more effectively than the conventional\nself-attention in transformers. We conducted comparison experiments, and the\nresults show the RFCM outperforms a baseline method on two datasets.\n","authors":["Motonari Kambara","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2207.09083v1.pdf","comment":"Accepted for presentation at ICIP2022"},{"id":"http://arxiv.org/abs/2207.09074v1","updated":"2022-07-19T05:21:14Z","published":"2022-07-19T05:21:14Z","title":"Incremental Task Learning with Incremental Rank Updates","summary":"  Incremental Task learning (ITL) is a category of continual learning that\nseeks to train a single network for multiple tasks (one after another), where\ntraining data for each task is only available during the training of that task.\nNeural networks tend to forget older tasks when they are trained for the newer\ntasks; this property is often known as catastrophic forgetting. To address this\nissue, ITL methods use episodic memory, parameter regularization, masking and\npruning, or extensible network structures. In this paper, we propose a new\nincremental task learning framework based on low-rank factorization. In\nparticular, we represent the network weights for each layer as a linear\ncombination of several rank-1 matrices. To update the network for a new task,\nwe learn a rank-1 (or low-rank) matrix and add that to the weights of every\nlayer. We also introduce an additional selector vector that assigns different\nweights to the low-rank matrices learned for the previous tasks. We show that\nour approach performs better than the current state-of-the-art methods in terms\nof accuracy and forgetting. Our method also offers better memory efficiency\ncompared to episodic memory- and mask-based approaches. Our code will be\navailable at https://github.com/CSIPlab/task-increment-rank-update.git\n","authors":["Rakib Hyder","Ken Shao","Boyu Hou","Panos Markopoulos","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2207.09074v1.pdf","comment":"Code will be available at\n  https://github.com/CSIPlab/task-increment-rank-update.git"},{"id":"http://arxiv.org/abs/2107.04286v3","updated":"2022-07-19T05:20:26Z","published":"2021-07-09T07:56:46Z","title":"Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset","summary":"  We present UrbanScene3D, a large-scale data platform for research of urban\nscene perception and reconstruction. UrbanScene3D contains over 128k\nhigh-resolution images covering 16 scenes including large-scale real urban\nregions and synthetic cities with 136 km^2 area in total. The dataset also\ncontains high-precision LiDAR scans and hundreds of image sets with different\nobservation patterns, which provide a comprehensive benchmark to design and\nevaluate aerial path planning and 3D reconstruction algorithms. In addition,\nthe dataset, which is built on Unreal Engine and Airsim simulator together with\nthe manually annotated unique instance label for each building in the dataset,\nenables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D\nbounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with\nphysical engine and lighting system not only produce variety of data but also\nenable users to simulate cars or drones in the proposed urban environment for\nfuture research.\n","authors":["Liqiang Lin","Yilin Liu","Yue Hu","Xingguang Yan","Ke Xie","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2107.04286v3.pdf","comment":"ECCV 2022 camera ready; Project page: https://vcc.tech/UrbanScene3D/"},{"id":"http://arxiv.org/abs/2112.00378v2","updated":"2022-07-19T05:07:05Z","published":"2021-12-01T09:55:01Z","title":"$\\ell_\\infty$-Robustness and Beyond: Unleashing Efficient Adversarial\n  Training","summary":"  Neural networks are vulnerable to adversarial attacks: adding well-crafted,\nimperceptible perturbations to their input can modify their output. Adversarial\ntraining is one of the most effective approaches in training robust models\nagainst such attacks. However, it is much slower than vanilla training of\nneural networks since it needs to construct adversarial examples for the entire\ntraining data at every iteration, hampering its effectiveness. Recently, Fast\nAdversarial Training (FAT) was proposed that can obtain robust models\nefficiently. However, the reasons behind its success are not fully understood,\nand more importantly, it can only train robust models for $\\ell_\\infty$-bounded\nattacks as it uses FGSM during training. In this paper, by leveraging the\ntheory of coreset selection, we show how selecting a small subset of training\ndata provides a general, more principled approach toward reducing the time\ncomplexity of robust training. Unlike existing methods, our approach can be\nadapted to a wide variety of training objectives, including TRADES,\n$\\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental\nresults indicate that our approach speeds up adversarial training by 2-3 times\nwhile experiencing a slight reduction in the clean and robust accuracy.\n","authors":["Hadi M. Dolatabadi","Sarah Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2112.00378v2.pdf","comment":"Accepted to the 17th European Conference on Computer Vision (ECCV\n  2022)"},{"id":"http://arxiv.org/abs/2207.08352v2","updated":"2022-07-19T04:59:56Z","published":"2022-07-18T02:32:48Z","title":"Show Me What I Like: Detecting User-Specific Video Highlights Using\n  Content-Based Multi-Head Attention","summary":"  We propose a method to detect individualized highlights for users on given\ntarget videos based on their preferred highlight clips marked on previous\nvideos they have watched. Our method explicitly leverages the contents of both\nthe preferred clips and the target videos using pre-trained features for the\nobjects and the human activities. We design a multi-head attention mechanism to\nadaptively weigh the preferred clips based on their object- and\nhuman-activity-based contents, and fuse them using these weights into a single\nfeature representation for each user. We compute similarities between these\nper-user feature representations and the per-frame features computed from the\ndesired target videos to estimate the user-specific highlight clips from the\ntarget videos. We test our method on a large-scale highlight detection dataset\ncontaining the annotated highlights of individual users. Compared to current\nbaselines, we observe an absolute improvement of 2-4% in the mean average\nprecision of the detected highlights. We also perform extensive ablation\nexperiments on the number of preferred highlight clips associated with each\nuser as well as on the object- and human-activity-based feature representations\nto validate that our method is indeed both content-based and user-specific.\n","authors":["Uttaran Bhattacharya","Gang Wu","Stefano Petrangeli","Viswanathan Swaminathan","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2207.08352v2.pdf","comment":"14 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2206.07990v3","updated":"2022-07-19T04:52:24Z","published":"2022-06-16T08:01:19Z","title":"Patch-level Representation Learning for Self-supervised Vision\n  Transformers","summary":"  Recent self-supervised learning (SSL) methods have shown impressive results\nin learning visual representations from unlabeled images. This paper aims to\nimprove their performance further by utilizing the architectural advantages of\nthe underlying neural network, as the current state-of-the-art visual pretext\ntasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.\nIn particular, we focus on Vision Transformers (ViTs), which have gained much\nattention recently as a better architectural choice, often outperforming\nconvolutional networks for various visual tasks. The unique characteristic of\nViT is that it takes a sequence of disjoint patches from an image and processes\npatch-level representations internally. Inspired by this, we design a simple\nyet effective visual pretext task, coined SelfPatch, for learning better\npatch-level representations. To be specific, we enforce invariance against each\npatch and its neighbors, i.e., each patch treats similar neighboring patches as\npositive samples. Consequently, training ViTs with SelfPatch learns more\nsemantically meaningful relations among patches (without using human-annotated\nlabels), which can be beneficial, in particular, to downstream tasks of a dense\nprediction type. Despite its simplicity, we demonstrate that it can\nsignificantly improve the performance of existing SSL methods for various\nvisual tasks, including object detection and semantic segmentation.\nSpecifically, SelfPatch significantly improves the recent self-supervised ViT,\nDINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance\nsegmentation, and +2.9 mIoU on ADE20K semantic segmentation.\n","authors":["Sukmin Yun","Hankook Lee","Jaehyung Kim","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2206.07990v3.pdf","comment":"Accepted to CVPR 2022 (Oral). Code is available at\n  https://github.com/alinlab/SelfPatch"},{"id":"http://arxiv.org/abs/2207.09070v1","updated":"2022-07-19T04:51:39Z","published":"2022-07-19T04:51:39Z","title":"Context Unaware Knowledge Distillation for Image Retrieval","summary":"  Existing data-dependent hashing methods use large backbone networks with\nmillions of parameters and are computationally complex. Existing knowledge\ndistillation methods use logits and other features of the deep (teacher) model\nand as knowledge for the compact (student) model, which requires the teacher's\nnetwork to be fine-tuned on the context in parallel with the student model on\nthe context. Training teacher on the target context requires more time and\ncomputational resources. In this paper, we propose context unaware knowledge\ndistillation that uses the knowledge of the teacher model without fine-tuning\nit on the target context. We also propose a new efficient student model\narchitecture for knowledge distillation. The proposed approach follows a\ntwo-step process. The first step involves pre-training the student model with\nthe help of context unaware knowledge distillation from the teacher model. The\nsecond step involves fine-tuning the student model on the context of image\nretrieval. In order to show the efficacy of the proposed approach, we compare\nthe retrieval results, no. of parameters and no. of operations of the student\nmodels with the teacher models under different retrieval frameworks, including\ndeep cauchy hashing (DCH) and central similarity quantization (CSQ). The\nexperimental results confirm that the proposed approach provides a promising\ntrade-off between the retrieval results and efficiency. The code used in this\npaper is released publicly at \\url{https://github.com/satoru2001/CUKDFIR}.\n","authors":["Bytasandram Yaswanth Reddy","Shiv Ram Dubey","Rakesh Kumar Sanodiya","Ravi Ranjan Prasad Karn"],"pdf_url":"https://arxiv.org/pdf/2207.09070v1.pdf","comment":"Accepted in International Conference on Computer Vision and Machine\n  Intelligence (CVMI), 2022"},{"id":"http://arxiv.org/abs/2207.09067v1","updated":"2022-07-19T04:44:08Z","published":"2022-07-19T04:44:08Z","title":"Time Is MattEr: Temporal Self-supervision for Video Transformers","summary":"  Understanding temporal dynamics of video is an essential aspect of learning\nbetter video representations. Recently, transformer-based architectural designs\nhave been extensively explored for video tasks due to their capability to\ncapture long-term dependency of input sequences. However, we found that these\nVideo Transformers are still biased to learn spatial dynamics rather than\ntemporal ones, and debiasing the spurious correlation is critical for their\nperformance. Based on the observations, we design simple yet effective\nself-supervised tasks for video models to learn temporal dynamics better.\nSpecifically, for debiasing the spatial bias, our method learns the temporal\norder of video frames as extra self-supervision and enforces the randomly\nshuffled frames to have low-confidence outputs. Also, our method learns the\ntemporal flow direction of video tokens among consecutive frames for enhancing\nthe correlation toward temporal dynamics. Under various video action\nrecognition tasks, we demonstrate the effectiveness of our method and its\ncompatibility with state-of-the-art Video Transformers.\n","authors":["Sukmin Yun","Jaehyung Kim","Dongyoon Han","Hwanjun Song","Jung-Woo Ha","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2207.09067v1.pdf","comment":"Accepted to ICML 2022. Code is available at\n  https://github.com/alinlab/temporal-selfsupervision"},{"id":"http://arxiv.org/abs/2207.09066v1","updated":"2022-07-19T04:38:01Z","published":"2022-07-19T04:38:01Z","title":"Moment Centralization based Gradient Descent Optimizers for\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) have shown very appealing performance\nfor many computer vision applications. The training of CNNs is generally\nperformed using stochastic gradient descent (SGD) based optimization\ntechniques. The adaptive momentum-based SGD optimizers are the recent trends.\nHowever, the existing optimizers are not able to maintain a zero mean in the\nfirst-order moment and struggle with optimization. In this paper, we propose a\nmoment centralization-based SGD optimizer for CNNs. Specifically, we impose the\nzero mean constraints on the first-order moment explicitly. The proposed moment\ncentralization is generic in nature and can be integrated with any of the\nexisting adaptive momentum-based optimizers. The proposed idea is tested with\nthree state-of-the-art optimization techniques, including Adam, Radam, and\nAdabelief on benchmark CIFAR10, CIFAR100, and TinyImageNet datasets for image\nclassification. The performance of the existing optimizers is generally\nimproved when integrated with the proposed moment centralization. Further, The\nresults of the proposed moment centralization are also better than the existing\ngradient centralization. The analytical analysis using the toy example shows\nthat the proposed method leads to a shorter and smoother optimization\ntrajectory. The source code is made publicly available at\n\\url{https://github.com/sumanthsadhu/MC-optimizer}.\n","authors":["Sumanth Sadu","Shiv Ram Dubey","SR Sreeja"],"pdf_url":"https://arxiv.org/pdf/2207.09066v1.pdf","comment":"Accepted in International Conference on Computer Vision and Machine\n  Intelligence (CVMI), 2022"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2207.09450v1","updated":"2022-07-19T17:59:59Z","published":"2022-07-19T17:59:59Z","title":"Human-to-Robot Imitation in the Wild","summary":"  We approach the problem of learning by watching humans in the wild. While\ntraditional approaches in Imitation and Reinforcement Learning are promising\nfor learning in the real world, they are either sample inefficient or are\nconstrained to lab settings. Meanwhile, there has been a lot of success in\nprocessing passive, unstructured human data. We propose tackling this problem\nvia an efficient one-shot robot learning algorithm, centered around learning\nfrom a third-person perspective. We call our method WHIRL: In-the-Wild Human\nImitating Robot Learning. WHIRL extracts a prior over the intent of the human\ndemonstrator, using it to initialize our agent's policy. We introduce an\nefficient real-world policy learning scheme that improves using interactions.\nOur key contributions are a simple sampling-based policy optimization approach,\na novel objective function for aligning human and robot videos as well as an\nexploration method to boost sample efficiency. We show one-shot generalization\nand success in real-world settings, including 20 different manipulation tasks\nin the wild. Videos and talk at https://human2robot.github.io\n","authors":["Shikhar Bahl","Abhinav Gupta","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2207.09450v1.pdf","comment":"Published at RSS 2022. Demos at https://human2robot.github.io"},{"id":"http://arxiv.org/abs/2207.09442v1","updated":"2022-07-19T17:57:40Z","published":"2022-07-19T17:57:40Z","title":"Theseus: A Library for Differentiable Nonlinear Optimization","summary":"  We present Theseus, an efficient application-agnostic open source library for\ndifferentiable nonlinear least squares (DNLS) optimization built on PyTorch,\nproviding a common framework for end-to-end structured learning in robotics and\nvision. Existing DNLS implementations are application specific and do not\nalways incorporate many ingredients important for efficiency. Theseus is\napplication-agnostic, as we illustrate with several example applications that\nare built using the same underlying differentiable components, such as\nsecond-order optimizers, standard costs functions, and Lie groups. For\nefficiency, Theseus incorporates support for sparse solvers, automatic\nvectorization, batching, GPU acceleration, and gradient computation with\nimplicit differentiation and direct loss minimization. We do extensive\nperformance evaluation in a set of applications, demonstrating significant\nefficiency gains and better scalability when these features are incorporated.\nProject page: https://sites.google.com/view/theseus-ai\n","authors":["Luis Pineda","Taosha Fan","Maurizio Monge","Shobha Venkataraman","Paloma Sodhi","Ricky Chen","Joseph Ortiz","Daniel DeTone","Austin Wang","Stuart Anderson","Jing Dong","Brandon Amos","Mustafa Mukadam"],"pdf_url":"https://arxiv.org/pdf/2207.09442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09440v1","updated":"2022-07-19T17:55:22Z","published":"2022-07-19T17:55:22Z","title":"A Convolutional Neural Network Approach to Supernova Time-Series\n  Classification","summary":"  One of the brightest objects in the universe, supernovae (SNe) are powerful\nexplosions marking the end of a star's lifetime. Supernova (SN) type is defined\nby spectroscopic emission lines, but obtaining spectroscopy is often\nlogistically unfeasible. Thus, the ability to identify SNe by type using\ntime-series image data alone is crucial, especially in light of the increasing\nbreadth and depth of upcoming telescopes. We present a convolutional neural\nnetwork method for fast supernova time-series classification, with observed\nbrightness data smoothed in both the wavelength and time directions with\nGaussian process regression. We apply this method to full duration and\ntruncated SN time-series, to simulate retrospective as well as real-time\nclassification performance. Retrospective classification is used to\ndifferentiate cosmologically useful Type Ia SNe from other SN types, and this\nmethod achieves >99% accuracy on this task. We are also able to differentiate\nbetween 6 SN types with 60% accuracy given only two nights of data and 98%\naccuracy retrospectively.\n","authors":["Helen Qu","Masao Sako","Anais Moller","Cyrille Doux"],"pdf_url":"https://arxiv.org/pdf/2207.09440v1.pdf","comment":"Accepted at the ICML 2022 Workshop on Machine Learning for\n  Astrophysics"},{"id":"http://arxiv.org/abs/2207.09435v1","updated":"2022-07-19T17:48:54Z","published":"2022-07-19T17:48:54Z","title":"Regret Minimization with Noisy Observations","summary":"  In a typical optimization problem, the task is to pick one of a number of\noptions with the lowest cost or the highest value. In practice, these\ncost/value quantities often come through processes such as measurement or\nmachine learning, which are noisy, with quantifiable noise distributions. To\ntake these noise distributions into account, one approach is to assume a prior\nfor the values, use it to build a posterior, and then apply standard stochastic\noptimization to pick a solution. However, in many practical applications, such\nprior distributions may not be available. In this paper, we study such\nscenarios using a regret minimization model.\n  In our model, the task is to pick the highest one out of $n$ values. The\nvalues are unknown and chosen by an adversary, but can be observed through\nnoisy channels, where additive noises are stochastically drawn from known\ndistributions. The goal is to minimize the regret of our selection, defined as\nthe expected difference between the highest and the selected value on the\nworst-case choices of values. We show that the na\\\"ive algorithm of picking the\nhighest observed value has regret arbitrarily worse than the optimum, even when\n$n = 2$ and the noises are unbiased in expectation. On the other hand, we\npropose an algorithm which gives a constant-approximation to the optimal regret\nfor any $n$. Our algorithm is conceptually simple, computationally efficient,\nand requires only minimal knowledge of the noise distributions.\n","authors":["Mohammad Mahdian","Jieming Mao","Kangning Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09432v1","updated":"2022-07-19T17:46:20Z","published":"2022-07-19T17:46:20Z","title":"Deep equilibrium networks are sensitive to initialization statistics","summary":"  Deep equilibrium networks (DEQs) are a promising way to construct models\nwhich trade off memory for compute. However, theoretical understanding of these\nmodels is still lacking compared to traditional networks, in part because of\nthe repeated application of a single set of weights. We show that DEQs are\nsensitive to the higher order statistics of the matrix families from which they\nare initialized. In particular, initializing with orthogonal or symmetric\nmatrices allows for greater stability in training. This gives us a practical\nprescription for initializations which allow for training with a broader range\nof initial weight scales.\n","authors":["Atish Agarwala","Samuel S. Schoenholz"],"pdf_url":"https://arxiv.org/pdf/2207.09432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09418v1","updated":"2022-07-19T17:25:56Z","published":"2022-07-19T17:25:56Z","title":"Unrolled algorithms for group synchronization","summary":"  The group synchronization problem involves estimating a collection of group\nelements from noisy measurements of their pairwise ratios. This task is a key\ncomponent in many computational problems, including the molecular\nreconstruction problem in single-particle cryo-electron microscopy (cryo-EM).\nThe standard methods to estimate the group elements are based on iteratively\napplying linear and non-linear operators. Motivated by the structural\nsimilarity to deep neural networks, we adopt the concept of algorithm\nunrolling, where training data is used to optimize the algorithm. We design\nunrolled algorithms for several group synchronization instances, including\nsynchronization over the group of 3-D rotations: the synchronization problem in\ncryo-EM. We also apply a similar approach to the multi-reference alignment\nproblem. We show by numerical experiments that the unrolling strategy\noutperforms existing synchronization algorithms in a wide variety of scenarios.\n","authors":["Noam Janco","Tamir Bendory"],"pdf_url":"https://arxiv.org/pdf/2207.09418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00949v2","updated":"2022-07-19T17:20:47Z","published":"2022-03-02T08:58:07Z","title":"GAP: Differentially Private Graph Neural Networks with Aggregation\n  Perturbation","summary":"  In this paper, we study the problem of learning Graph Neural Networks (GNNs)\nwith Differential Privacy (DP). We propose a novel differentially private GNN\nbased on Aggregation Perturbation (GAP), which adds stochastic noise to the\nGNN's aggregation function to statistically obfuscate the presence of a single\nedge (edge-level privacy) or a single node and all its adjacent edges\n(node-level privacy). Tailored to the specifics of private learning, GAP's new\narchitecture is composed of three separate modules: (i) the encoder module,\nwhere we learn private node embeddings without relying on the edge information;\n(ii) the aggregation module, where we compute noisy aggregated node embeddings\nbased on the graph structure; and (iii) the classification module, where we\ntrain a neural network on the private aggregations for node classification\nwithout further querying the graph edges. GAP's major advantage over previous\napproaches is that it can benefit from multi-hop neighborhood aggregations, and\nguarantees both edge-level and node-level DP not only for training, but also at\ninference with no additional costs beyond the training's privacy budget. We\nanalyze GAP's formal privacy guarantees using R\\'enyi DP and conduct empirical\nexperiments over three real-world graph datasets. We demonstrate that GAP\noffers significantly better accuracy-privacy trade-offs than state-of-the-art\nDP-GNN approaches and naive MLP-based baselines.\n","authors":["Sina Sajadmanesh","Ali Shahin Shamsabadi","Aurélien Bellet","Daniel Gatica-Perez"],"pdf_url":"https://arxiv.org/pdf/2203.00949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05126v2","updated":"2022-07-19T17:13:07Z","published":"2022-03-10T02:54:56Z","title":"PACTran: PAC-Bayesian Metrics for Estimating the Transferability of\n  Pretrained Models to Classification Tasks","summary":"  With the increasing abundance of pretrained models in recent years, the\nproblem of selecting the best pretrained checkpoint for a particular downstream\nclassification task has been gaining increased attention. Although several\nmethods have recently been proposed to tackle the selection problem (e.g. LEEP,\nH-score), these methods resort to applying heuristics that are not well\nmotivated by learning theory. In this paper we present PACTran, a theoretically\ngrounded family of metrics for pretrained model selection and transferability\nmeasurement. We first show how to derive PACTran metrics from the optimal\nPAC-Bayesian bound under the transfer learning setting. We then empirically\nevaluate three metric instantiations of PACTran on a number of vision tasks\n(VTAB) as well as a language-and-vision (OKVQA) task. An analysis of the\nresults shows PACTran is a more consistent and effective transferability\nmeasure compared to existing selection methods.\n","authors":["Nan Ding","Xi Chen","Tomer Levinboim","Beer Changpinyo","Radu Soricut"],"pdf_url":"https://arxiv.org/pdf/2203.05126v2.pdf","comment":"European Conference on Computer Vision 2022 (oral)"},{"id":"http://arxiv.org/abs/2207.09413v1","updated":"2022-07-19T17:13:06Z","published":"2022-07-19T17:13:06Z","title":"SphereFed: Hyperspherical Federated Learning","summary":"  Federated Learning aims at training a global model from multiple\ndecentralized devices (i.e. clients) without exchanging their private local\ndata. A key challenge is the handling of non-i.i.d. (independent identically\ndistributed) data across multiple clients that may induce disparities of their\nlocal features. We introduce the Hyperspherical Federated Learning (SphereFed)\nframework to address the non-i.i.d. issue by constraining learned\nrepresentations of data points to be on a unit hypersphere shared by clients.\nSpecifically, all clients learn their local representations by minimizing the\nloss with respect to a fixed classifier whose weights span the unit\nhypersphere. After federated training in improving the global model, this\nclassifier is further calibrated with a closed-form solution by minimizing a\nmean squared loss. We show that the calibration solution can be computed\nefficiently and distributedly without direct access of local data. Extensive\nexperiments indicate that our SphereFed approach is able to improve the\naccuracy of multiple existing federated learning algorithms by a considerable\nmargin (up to 6% on challenging datasets) with enhanced computation and\ncommunication efficiency across datasets and model architectures.\n","authors":["Xin Dong","Sai Qian Zhang","Ang Li","H. T. Kung"],"pdf_url":"https://arxiv.org/pdf/2207.09413v1.pdf","comment":"European Conference on Computer Vision 2022"},{"id":"http://arxiv.org/abs/2102.06278v3","updated":"2022-07-19T17:06:13Z","published":"2021-02-11T21:32:59Z","title":"Unsupervised Ground Metric Learning using Wasserstein Singular Vectors","summary":"  Defining meaningful distances between samples in a dataset is a fundamental\nproblem in machine learning. Optimal Transport (OT) lifts a distance between\nfeatures (the \"ground metric\") to a geometrically meaningful distance between\nsamples. However, there is usually no straightforward choice of ground metric.\nSupervised ground metric learning approaches exist but require labeled data. In\nabsence of labels, only ad-hoc ground metrics remain. Unsupervised ground\nmetric learning is thus a fundamental problem to enable data-driven\napplications of OT. In this paper, we propose for the first time a canonical\nanswer by simultaneously computing an OT distance between samples and between\nfeatures of a dataset. These distance matrices emerge naturally as positive\nsingular vectors of the function mapping ground metrics to OT distances. We\nprovide criteria to ensure the existence and uniqueness of these singular\nvectors. We then introduce scalable computational methods to approximate them\nin high-dimensional settings, using stochastic approximation and entropic\nregularization. Finally, we showcase Wasserstein Singular Vectors on a\nsingle-cell RNA-sequencing dataset.\n","authors":["Geert-Jan Huizing","Laura Cantini","Gabriel Peyré"],"pdf_url":"https://arxiv.org/pdf/2102.06278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09219v3","updated":"2022-07-19T17:05:13Z","published":"2022-05-18T21:18:16Z","title":"A Classification of $G$-invariant Shallow Neural Networks","summary":"  When trying to fit a deep neural network (DNN) to a $G$-invariant target\nfunction with respect to a group $G$, it only makes sense to constrain the DNN\nto be $G$-invariant as well. However, there can be many different ways to do\nthis, thus raising the problem of \"$G$-invariant neural architecture design\":\nWhat is the optimal $G$-invariant architecture for a given problem? Before we\ncan consider the optimization problem itself, we must understand the search\nspace, the architectures in it, and how they relate to one another. In this\npaper, we take a first step towards this goal; we prove a theorem that gives a\nclassification of all $G$-invariant single-hidden-layer or \"shallow\" neural\nnetwork ($G$-SNN) architectures with ReLU activation for any finite orthogonal\ngroup $G$. The proof is based on a correspondence of every $G$-SNN to a signed\npermutation representation of $G$ acting on the hidden neurons. The\nclassification is equivalently given in terms of the first cohomology classes\nof $G$, thus admitting a topological interpretation. Based on a code\nimplementation, we enumerate the $G$-SNN architectures for some example groups\n$G$ and visualize their structure. We draw the network morphisms between the\nenumerated architectures that can be leveraged during neural architecture\nsearch (NAS). Finally, we prove that architectures corresponding to\ninequivalent cohomology classes in a given cohomology ring coincide in function\nspace only when their weight matrices are zero, and we discuss the implications\nof this in the context of NAS.\n","authors":["Devanshu Agrawal","James Ostrowski"],"pdf_url":"https://arxiv.org/pdf/2205.09219v3.pdf","comment":"29 pages, 8 figures; corrected proof of Lemma 20, fixed typos\n  including in the statement of Thm. 5"},{"id":"http://arxiv.org/abs/2207.09408v1","updated":"2022-07-19T17:05:02Z","published":"2022-07-19T17:05:02Z","title":"Bounding generalization error with input compression: An empirical study\n  with infinite-width networks","summary":"  Estimating the Generalization Error (GE) of Deep Neural Networks (DNNs) is an\nimportant task that often relies on availability of held-out data. The ability\nto better predict GE based on a single training set may yield overarching DNN\ndesign principles to reduce a reliance on trial-and-error, along with other\nperformance assessment advantages. In search of a quantity relevant to GE, we\ninvestigate the Mutual Information (MI) between the input and final layer\nrepresentations, using the infinite-width DNN limit to bound MI. An existing\ninput compression-based GE bound is used to link MI and GE. To the best of our\nknowledge, this represents the first empirical study of this bound. In our\nattempt to empirically falsify the theoretical bound, we find that it is often\ntight for best-performing models. Furthermore, it detects randomization of\ntraining labels in many cases, reflects test-time perturbation robustness, and\nworks well given only few training samples. These results are promising given\nthat input compression is broadly applicable where MI can be estimated with\nconfidence.\n","authors":["Angus Galloway","Anna Golubeva","Mahmoud Salem","Mihai Nica","Yani Ioannou","Graham W. Taylor"],"pdf_url":"https://arxiv.org/pdf/2207.09408v1.pdf","comment":"12 pages main content, 26 pages total"},{"id":"http://arxiv.org/abs/2207.09405v1","updated":"2022-07-19T16:57:38Z","published":"2022-07-19T16:57:38Z","title":"Bayesian Generational Population-Based Training","summary":"  Reinforcement learning (RL) offers the potential for training generally\ncapable agents that can interact autonomously in the real world. However, one\nkey limitation is the brittleness of RL algorithms to core hyperparameters and\nnetwork architecture choice. Furthermore, non-stationarities such as evolving\ntraining data and increased agent complexity mean that different\nhyperparameters and architectures may be optimal at different points of\ntraining. This motivates AutoRL, a class of methods seeking to automate these\ndesign choices. One prominent class of AutoRL methods is Population-Based\nTraining (PBT), which have led to impressive performance in several large scale\nsettings. In this paper, we introduce two new innovations in PBT-style methods.\nFirst, we employ trust-region based Bayesian Optimization, enabling full\ncoverage of the high-dimensional mixed hyperparameter search space. Second, we\nshow that using a generational approach, we can also learn both architectures\nand hyperparameters jointly on-the-fly in a single training run. Leveraging the\nnew highly parallelizable Brax physics engine, we show that these innovations\nlead to large performance gains, significantly outperforming the tuned baseline\nwhile learning entire configurations on the fly. Code is available at\nhttps://github.com/xingchenwan/bgpbt.\n","authors":["Xingchen Wan","Cong Lu","Jack Parker-Holder","Philip J. Ball","Vu Nguyen","Binxin Ru","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2207.09405v1.pdf","comment":"AutoML Conference 2022. 10 pages, 4 figure, 3 tables (28 pages, 10\n  figures, 7 tables including references and appendices)"},{"id":"http://arxiv.org/abs/2203.04099v2","updated":"2022-07-19T16:54:03Z","published":"2022-03-08T14:08:47Z","title":"VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer","summary":"  This paper presents an audio-visual approach for voice separation which\nproduces state-of-the-art results at a low latency in two scenarios: speech and\nsinging voice. The model is based on a two-stage network. Motion cues are\nobtained with a lightweight graph convolutional network that processes face\nlandmarks. Then, both audio and motion features are fed to an audio-visual\ntransformer which produces a fairly good estimation of the isolated target\nsource. In a second stage, the predominant voice is enhanced with an audio-only\nnetwork. We present different ablation studies and comparison to\nstate-of-the-art methods. Finally, we explore the transferability of models\ntrained for speech separation in the task of singing voice separation. The\ndemos, code, and weights are available in https://ipcv.github.io/VoViT/\n","authors":["Juan F. Montesinos","Venkatesh S. Kadandale","Gloria Haro"],"pdf_url":"https://arxiv.org/pdf/2203.04099v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09390v1","updated":"2022-07-19T16:39:16Z","published":"2022-07-19T16:39:16Z","title":"Neural Greedy Pursuit for Feature Selection","summary":"  We propose a greedy algorithm to select $N$ important features among $P$\ninput features for a non-linear prediction problem. The features are selected\none by one sequentially, in an iterative loss minimization procedure. We use\nneural networks as predictors in the algorithm to compute the loss and hence,\nwe refer to our method as neural greedy pursuit (NGP). NGP is efficient in\nselecting $N$ features when $N \\ll P$, and it provides a notion of feature\nimportance in a descending order following the sequential selection procedure.\nWe experimentally show that NGP provides better performance than several\nfeature selection methods such as DeepLIFT and Drop-one-out loss. In addition,\nwe experimentally show a phase transition behavior in which perfect selection\nof all $N$ features without false positives is possible when the training data\nsize exceeds a threshold.\n","authors":["Sandipan Das","Alireza M. Javid","Prakash Borpatra Gohain","Yonina C. Eldar","Saikat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2207.09390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09387v1","updated":"2022-07-19T16:37:24Z","published":"2022-07-19T16:37:24Z","title":"Green, Quantized Federated Learning over Wireless Networks: An\n  Energy-Efficient Design","summary":"  In this paper, a green, quantized FL framework, which represents data with a\nfinite precision level in both local training and uplink transmission, is\nproposed. Here, the finite precision level is captured through the use of\nquantized neural networks (QNNs) that quantize weights and activations in\nfixed-precision format. In the considered FL model, each device trains its QNN\nand transmits a quantized training result to the base station. Energy models\nfor the local training and the transmission with quantization are rigorously\nderived. To minimize the energy consumption and the number of communication\nrounds simultaneously, a multi-objective optimization problem is formulated\nwith respect to the number of local iterations, the number of selected devices,\nand the precision levels for both local training and transmission while\nensuring convergence under a target accuracy constraint. To solve this problem,\nthe convergence rate of the proposed FL system is analytically derived with\nrespect to the system control variables. Then, the Pareto boundary of the\nproblem is characterized to provide efficient solutions using the normal\nboundary inspection method. Design insights on balancing the tradeoff between\nthe two objectives are drawn from using the Nash bargaining solution and\nanalyzing the derived convergence rate. Simulation results show that the\nproposed FL framework can reduce energy consumption until convergence by up to\n52% compared to a baseline FL algorithm that represents data with full\nprecision.\n","authors":["Minsu Kim","Walid Saad","Mohammad Mozaffari","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2207.09387v1.pdf","comment":"Submitted to IEEE Transactions on Wireless Communications"},{"id":"http://arxiv.org/abs/2203.03279v3","updated":"2022-07-19T16:31:31Z","published":"2022-03-07T10:51:40Z","title":"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning\n  Strategies for Model Fusion","summary":"  Techniques of hybridisation and ensemble learning are popular model fusion\ntechniques for improving the predictive power of forecasting methods. With\nlimited research that instigates combining these two promising approaches, this\npaper focuses on the utility of the Exponential-Smoothing-Recurrent Neural\nNetwork (ES-RNN) in the pool of base models for different ensembles. We compare\nagainst some state of the art ensembling techniques and arithmetic model\naveraging as a benchmark. We experiment with the M4 forecasting data set of\n100,000 time-series, and the results show that the Feature-based Forecast Model\nAveraging (FFORMA), on average, is the best technique for late data fusion with\nthe ES-RNN. However, considering the M4's Daily subset of data, stacking was\nthe only successful ensemble at dealing with the case where all base model\nperformances are similar. Our experimental results indicate that we attain\nstate of the art forecasting results compared to N-BEATS as a benchmark. We\nconclude that model averaging is a more robust ensemble than model selection\nand stacking strategies. Further, the results show that gradient boosting is\nsuperior for implementing ensemble learning strategies.\n","authors":["Pieter Cawood","Terence van Zyl"],"pdf_url":"https://arxiv.org/pdf/2203.03279v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.02032v3","updated":"2022-07-19T16:22:28Z","published":"2020-06-03T04:00:52Z","title":"A Unified Single-loop Alternating Gradient Projection Algorithm for\n  Nonconvex-Concave and Convex-Nonconcave Minimax Problems","summary":"  Much recent research effort has been directed to the development of efficient\nalgorithms for solving minimax problems with theoretical convergence guarantees\ndue to the relevance of these problems to a few emergent applications. In this\npaper, we propose a unified single-loop alternating gradient projection (AGP)\nalgorithm for solving smooth nonconvex-(strongly) concave and (strongly)\nconvex-nonconcave minimax problems. AGP employs simple gradient projection\nsteps for updating the primal and dual variables alternatively at each\niteration. We show that it can find an $\\varepsilon$-stationary point of the\nobjective function in $\\mathcal{O}\\left( \\varepsilon ^{-2} \\right)$ (resp.\n$\\mathcal{O}\\left( \\varepsilon ^{-4} \\right)$) iterations under\nnonconvex-strongly concave (resp. nonconvex-concave) setting. Moreover, its\ngradient complexity to obtain an $\\varepsilon$-stationary point of the\nobjective function is bounded by $\\mathcal{O}\\left( \\varepsilon ^{-2} \\right)$\n(resp., $\\mathcal{O}\\left( \\varepsilon ^{-4} \\right)$) under the strongly\nconvex-nonconcave (resp., convex-nonconcave) setting. To the best of our\nknowledge, this is the first time that a simple and unified single-loop\nalgorithm is developed for solving both nonconvex-(strongly) concave and\n(strongly) convex-nonconcave minimax problems. Moreover, the complexity results\nfor solving the latter (strongly) convex-nonconcave minimax problems have never\nbeen obtained before in the literature. Numerical results show the efficiency\nof the proposed AGP algorithm. Furthermore, we extend the AGP algorithm by\npresenting a block alternating proximal gradient (BAPG) algorithm for solving\nmore general multi-block nonsmooth nonconvex-(strongly) concave and (strongly)\nconvex-nonconcave minimax problems. We can similarly establish the gradient\ncomplexity of the proposed algorithm under these four different settings.\n","authors":["Zi Xu","Huiling Zhang","Yang Xu","Guanghui Lan"],"pdf_url":"https://arxiv.org/pdf/2006.02032v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09370v1","updated":"2022-07-19T16:15:11Z","published":"2022-07-19T16:15:11Z","title":"Data-Centric Epidemic Forecasting: A Survey","summary":"  The COVID-19 pandemic has brought forth the importance of epidemic\nforecasting for decision makers in multiple domains, ranging from public health\nto the economy as a whole. While forecasting epidemic progression is frequently\nconceptualized as being analogous to weather forecasting, however it has some\nkey differences and remains a non-trivial task. The spread of diseases is\nsubject to multiple confounding factors spanning human behavior, pathogen\ndynamics, weather and environmental conditions. Research interest has been\nfueled by the increased availability of rich data sources capturing previously\nunobservable facets and also due to initiatives from government public health\nand funding agencies. This has resulted, in particular, in a spate of work on\n'data-centered' solutions which have shown potential in enhancing our\nforecasting capabilities by leveraging non-traditional data sources as well as\nrecent innovations in AI and machine learning. This survey delves into various\ndata-driven methodological and practical advancements and introduces a\nconceptual framework to navigate through them. First, we enumerate the large\nnumber of epidemiological datasets and novel data streams that are relevant to\nepidemic forecasting, capturing various factors like symptomatic online\nsurveys, retail and commerce, mobility, genomics data and more. Next, we\ndiscuss methods and modeling paradigms focusing on the recent data-driven\nstatistical and deep-learning based methods as well as on the novel class of\nhybrid models that combine domain knowledge of mechanistic models with the\neffectiveness and flexibility of statistical approaches. We also discuss\nexperiences and challenges that arise in real-world deployment of these\nforecasting systems including decision-making informed by forecasts. Finally,\nwe highlight some challenges and open problems found across the forecasting\npipeline.\n","authors":["Alexander Rodríguez","Harshavardhan Kamarthi","Pulak Agarwal","Javen Ho","Mira Patel","Suchet Sapre","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2207.09370v1.pdf","comment":"67 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09353v1","updated":"2022-07-19T16:00:57Z","published":"2022-07-19T16:00:57Z","title":"Beyond Transmitting Bits: Context, Semantics, and Task-Oriented\n  Communications","summary":"  Communication systems to date primarily aim at reliably communicating bit\nsequences. Such an approach provides efficient engineering designs that are\nagnostic to the meanings of the messages or to the goal that the message\nexchange aims to achieve. Next generation systems, however, can be potentially\nenriched by folding message semantics and goals of communication into their\ndesign. Further, these systems can be made cognizant of the context in which\ncommunication exchange takes place, providing avenues for novel design\ninsights. This tutorial summarizes the efforts to date, starting from its early\nadaptations, semantic-aware and task-oriented communications, covering the\nfoundations, algorithms and potential implementations. The focus is on\napproaches that utilize information theory to provide the foundations, as well\nas the significant role of learning in semantics and task-aware communications.\n","authors":["Deniz Gunduz","Zhijin Qin","Inaki Estella Aguerri","Harpreet S. Dhillon","Zhaohui Yang","Aylin Yener","Kai Kit Wong","Chan-Byoung Chae"],"pdf_url":"https://arxiv.org/pdf/2207.09353v1.pdf","comment":"28 pages, 14 figures"},{"id":"http://arxiv.org/abs/2207.09350v1","updated":"2022-07-19T15:58:27Z","published":"2022-07-19T15:58:27Z","title":"Riemannian Stochastic Gradient Method for Nested Composition\n  Optimization","summary":"  This work considers optimization of composition of functions in a nested form\nover Riemannian manifolds where each function contains an expectation. This\ntype of problems is gaining popularity in applications such as policy\nevaluation in reinforcement learning or model customization in meta-learning.\nThe standard Riemannian stochastic gradient methods for non-compositional\noptimization cannot be directly applied as stochastic approximation of inner\nfunctions create bias in the gradients of the outer functions. For two-level\ncomposition optimization, we present a Riemannian Stochastic Composition\nGradient Descent (R-SCGD) method that finds an approximate stationary point,\nwith expected squared Riemannian gradient smaller than $\\epsilon$, in\n$O(\\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer\nfunction and stochastic function and gradient oracles of the inner function.\nFurthermore, we generalize the R-SCGD algorithms for problems with multi-level\nnested compositional structures, with the same complexity of $O(\\epsilon^{-2})$\nfor the first-order stochastic oracle. Finally, the performance of the R-SCGD\nmethod is numerically evaluated over a policy evaluation problem in\nreinforcement learning.\n","authors":["Dewei Zhang","Sam Davanloo Tajbakhsh"],"pdf_url":"https://arxiv.org/pdf/2207.09350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09344v1","updated":"2022-07-19T15:51:25Z","published":"2022-07-19T15:51:25Z","title":"Online Dynamics Learning for Predictive Control with an Application to\n  Aerial Robots","summary":"  In this work, we consider the task of improving the accuracy of dynamic\nmodels for model predictive control (MPC) in an online setting. Even though\nprediction models can be learned and applied to model-based controllers, these\nmodels are often learned offline. In this offline setting, training data is\nfirst collected and a prediction model is learned through an elaborated\ntraining procedure. After the model is trained to a desired accuracy, it is\nthen deployed in a model predictive controller. However, since the model is\nlearned offline, it does not adapt to disturbances or model errors observed\nduring deployment. To improve the adaptiveness of the model and the controller,\nwe propose an online dynamics learning framework that continually improves the\naccuracy of the dynamic model during deployment. We adopt knowledge-based\nneural ordinary differential equations (KNODE) as the dynamic models, and use\ntechniques inspired by transfer learning to continually improve the model\naccuracy. We demonstrate the efficacy of our framework with a quadrotor robot,\nand verify the framework in both simulations and physical experiments. Results\nshow that the proposed approach is able to account for disturbances that are\npossibly time-varying, while maintaining good trajectory tracking performance.\n","authors":["Tom Z. Jiahao","Kong Yao Chee","M. Ani Hsieh"],"pdf_url":"https://arxiv.org/pdf/2207.09344v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.06325v2","updated":"2022-07-19T15:50:32Z","published":"2022-07-13T16:25:35Z","title":"Non-Myopic Multifidelity Bayesian Optimization","summary":"  Bayesian optimization is a popular framework for the optimization of black\nbox functions. Multifidelity methods allows to accelerate Bayesian optimization\nby exploiting low-fidelity representations of expensive objective functions.\nPopular multifidelity Bayesian strategies rely on sampling policies that\naccount for the immediate reward obtained evaluating the objective function at\na specific input, precluding greater informative gains that might be obtained\nlooking ahead more steps. This paper proposes a non-myopic multifidelity\nBayesian framework to grasp the long-term reward from future steps of the\noptimization. Our computational strategy comes with a two-step lookahead\nmultifidelity acquisition function that maximizes the cumulative reward\nobtained measuring the improvement in the solution over two steps ahead. We\ndemonstrate that the proposed algorithm outperforms a standard multifidelity\nBayesian framework on popular benchmark optimization problems.\n","authors":["Francesco Di Fiore","Laura Mainini"],"pdf_url":"https://arxiv.org/pdf/2207.06325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09340v1","updated":"2022-07-19T15:49:41Z","published":"2022-07-19T15:49:41Z","title":"A coherence parameter characterizing generative compressed sensing with\n  Fourier measurements","summary":"  In Bora et al. (2017), a mathematical framework was developed for compressed\nsensing guarantees in the setting where the measurement matrix is Gaussian and\nthe signal structure is the range of a generative neural network (GNN). The\nproblem of compressed sensing with GNNs has since been extensively analyzed\nwhen the measurement matrix and/or network weights follow a subgaussian\ndistribution. We move beyond the subgaussian assumption, to measurement\nmatrices that are derived by sampling uniformly at random rows of a unitary\nmatrix (including subsampled Fourier measurements as a special case).\nSpecifically, we prove the first known restricted isometry guarantee for\ngenerative compressed sensing with subsampled isometries, and provide recovery\nbounds with nearly order-optimal sample complexity, addressing an open problem\nof Scarlett et al. (2022, p. 10). Recovery efficacy is characterized by the\ncoherence, a new parameter, which measures the interplay between the range of\nthe network and the measurement matrix. Our approach relies on subspace\ncounting arguments and ideas central to high-dimensional probability.\nFurthermore, we propose a regularization strategy for training GNNs to have\nfavourable coherence with the measurement operator. We provide compelling\nnumerical simulations that support this regularized training strategy: our\nstrategy yields low coherence networks that require fewer measurements for\nsignal recovery. This, together with our theoretical results, supports\ncoherence as a natural quantity for characterizing generative compressed\nsensing with subsampled isometries.\n","authors":["Aaron Berk","Simone Brugiapaglia","Babhru Joshi","Yaniv Plan","Matthew Scott","Özgür Yilmaz"],"pdf_url":"https://arxiv.org/pdf/2207.09340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09336v1","updated":"2022-07-19T15:44:59Z","published":"2022-07-19T15:44:59Z","title":"Uncertainty in Contrastive Learning: On the Predictability of Downstream\n  Performance","summary":"  The superior performance of some of today's state-of-the-art deep learning\nmodels is to some extent owed to extensive (self-)supervised contrastive\npretraining on large-scale datasets. In contrastive learning, the network is\npresented with pairs of positive (similar) and negative (dissimilar) datapoints\nand is trained to find an embedding vector for each datapoint, i.e., a\nrepresentation, which can be further fine-tuned for various downstream tasks.\nIn order to safely deploy these models in critical decision-making systems, it\nis crucial to equip them with a measure of their uncertainty or reliability.\nHowever, due to the pairwise nature of training a contrastive model, and the\nlack of absolute labels on the output (an abstract embedding vector), adapting\nconventional uncertainty estimation techniques to such models is non-trivial.\nIn this work, we study whether the uncertainty of such a representation can be\nquantified for a single datapoint in a meaningful way. In other words, we\nexplore if the downstream performance on a given datapoint is predictable,\ndirectly from its pre-trained embedding. We show that this goal can be achieved\nby directly estimating the distribution of the training data in the embedding\nspace and accounting for the local consistency of the representations. Our\nexperiments show that this notion of uncertainty for an embedding vector often\nstrongly correlates with its downstream accuracy.\n","authors":["Shervin Ardeshir","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2207.09336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07724v2","updated":"2022-07-19T15:32:48Z","published":"2022-03-15T08:32:56Z","title":"CODA: A Real-World Road Corner Case Dataset for Object Detection in\n  Autonomous Driving","summary":"  Contemporary deep-learning object detection methods for autonomous driving\nusually assume prefixed categories of common traffic participants, such as\npedestrians and cars. Most existing detectors are unable to detect uncommon\nobjects and corner cases (e.g., a dog crossing a street), which may lead to\nsevere accidents in some situations, making the timeline for the real-world\napplication of reliable autonomous driving uncertain. One main reason that\nimpedes the development of truly reliably self-driving systems is the lack of\npublic datasets for evaluating the performance of object detectors on corner\ncases. Hence, we introduce a challenging dataset named CODA that exposes this\ncritical problem of vision-based detectors. The dataset consists of 1500\ncarefully selected real-world driving scenes, each containing four object-level\ncorner cases (on average), spanning more than 30 object categories. On CODA,\nthe performance of standard object detectors trained on large-scale autonomous\ndriving datasets significantly drops to no more than 12.8% in mAR. Moreover, we\nexperiment with the state-of-the-art open-world object detector and find that\nit also fails to reliably identify the novel objects in CODA, suggesting that a\nrobust perception system for autonomous driving is probably still far from\nreach. We expect our CODA dataset to facilitate further research in reliable\ndetection for real-world autonomous driving. Our dataset will be released at\nhttps://coda-dataset.github.io.\n","authors":["Kaican Li","Kai Chen","Haoyu Wang","Lanqing Hong","Chaoqiang Ye","Jianhua Han","Yukuai Chen","Wei Zhang","Chunjing Xu","Dit-Yan Yeung","Xiaodan Liang","Zhenguo Li","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2203.07724v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2203.13450v3","updated":"2022-07-19T15:31:09Z","published":"2022-03-25T05:17:24Z","title":"A Comparative Survey of Deep Active Learning","summary":"  While deep learning (DL) is data-hungry and usually relies on extensive\nlabeled data to deliver good performance, Active Learning (AL) reduces labeling\ncosts by selecting a small proportion of samples from unlabeled data for\nlabeling and training. Therefore, Deep Active Learning (DAL) has risen as a\nfeasible solution for maximizing model performance under a limited labeling\ncost/budget in recent years. Although abundant methods of DAL have been\ndeveloped and various literature reviews conducted, the performance evaluation\nof DAL methods under fair comparison settings is not yet available. Our work\nintends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by\nre-implementing 19 highly-cited DAL methods. We survey and categorize\nDAL-related works and construct comparative experiments across frequently used\ndatasets and DAL algorithms. Additionally, we explore some factors (e.g., batch\nsize, number of epochs in the training process) that influence the efficacy of\nDAL, which provides better references for researchers to design their DAL\nexperiments or carry out DAL-related applications.\n","authors":["Xueying Zhan","Qingzhong Wang","Kuan-hao Huang","Haoyi Xiong","Dejing Dou","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2203.13450v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2207.09325v1","updated":"2022-07-19T15:27:10Z","published":"2022-07-19T15:27:10Z","title":"Multi-parametric Analysis for Mixed Integer Linear Programming: An\n  Application to Transmission Planning and Congestion Control","summary":"  Enhancing existing transmission lines is a useful tool to combat transmission\ncongestion and guarantee transmission security with increasing demand and\nboosting the renewable energy source. This study concerns the selection of\nlines whose capacity should be expanded and by how much from the perspective of\nindependent system operator (ISO) to minimize the system cost with the\nconsideration of transmission line constraints and electricity generation and\ndemand balance conditions, and incorporating ramp-up and startup ramp rates,\nshutdown ramp rates, ramp-down rate limits and minimum up and minimum down\ntimes. For that purpose, we develop the ISO unit commitment and economic\ndispatch model and show it as a right-hand side uncertainty multiple parametric\nanalysis for the mixed integer linear programming (MILP) problem. We first\nrelax the binary variable to continuous variables and employ the Lagrange\nmethod and Karush-Kuhn-Tucker conditions to obtain optimal solutions (optimal\ndecision variables and objective function) and critical regions associated with\nactive and inactive constraints. Further, we extend the traditional branch and\nbound method for the large-scale MILP problem by determining the upper bound of\nthe problem at each node, then comparing the difference between the upper and\nlower bounds and reaching the approximate optimal solution within the decision\nmakers' tolerated error range. In additional, the objective function's first\nderivative on the parameters of each line is used to inform the selection of\nlines to ease congestion and maximize social welfare. Finally, the amount of\ncapacity upgrade will be chosen by balancing the cost-reduction rate of the\nobjective function on parameters and the cost of the line upgrade. Our findings\nare supported by numerical simulation and provide transmission line planners\nwith decision-making guidance.\n","authors":["Jian Liu","Rui Bo","Siyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08389v2","updated":"2022-07-19T15:07:17Z","published":"2022-07-18T05:47:29Z","title":"MLGOPerf: An ML Guided Inliner to Optimize Performance","summary":"  For the past 25 years, we have witnessed an extensive application of Machine\nLearning to the Compiler space; the selection and the phase-ordering problem.\nHowever, limited works have been upstreamed into the state-of-the-art\ncompilers, i.e., LLVM, to seamlessly integrate the former into the optimization\npipeline of a compiler to be readily deployed by the user. MLGO was among the\nfirst of such projects and it only strives to reduce the code size of a binary\nwith an ML-based Inliner using Reinforcement Learning.\n  This paper presents MLGOPerf; the first end-to-end framework capable of\noptimizing performance using LLVM's ML-Inliner. It employs a secondary ML model\nto generate rewards used for training a retargeted Reinforcement learning\nagent, previously used as the primary model by MLGO. It does so by predicting\nthe post-inlining speedup of a function under analysis and it enables a fast\ntraining framework for the primary model which otherwise wouldn't be practical.\nThe experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with\nrespect to LLVM's optimization at O3 when trained for performance on SPEC\nCPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach\nprovides up to 26% increased opportunities to autotune code regions for our\nbenchmarks which can be translated into an additional 3.7% speedup value.\n","authors":["Amir H. Ashouri","Mostafa Elhoushi","Yuzhe Hua","Xiang Wang","Muhammad Asif Manzoor","Bryan Chan","Yaoqing Gao"],"pdf_url":"https://arxiv.org/pdf/2207.08389v2.pdf","comment":"Version 2: Added the missing Table 6. The short version of this work\n  is accepted at ACM/IEEE CASES 2022"},{"id":"http://arxiv.org/abs/2207.09315v1","updated":"2022-07-19T15:04:14Z","published":"2022-07-19T15:04:14Z","title":"Metadata Representations for Queryable ML Model Zoos","summary":"  Machine learning (ML) practitioners and organizations are building model zoos\nof pre-trained models, containing metadata describing properties of the ML\nmodels and datasets that are useful for reporting, auditing, reproducibility,\nand interpretability purposes. The metatada is currently not standardised; its\nexpressivity is limited; and there is no interoperable way to store and query\nit. Consequently, model search, reuse, comparison, and composition are\nhindered. In this paper, we advocate for standardized ML model meta-data\nrepresentation and management, proposing a toolkit supported to help\npractitioners manage and query that metadata.\n","authors":["Ziyu Li","Rihan Hai","Alessandro Bozzon","Asterios Katsifodimos"],"pdf_url":"https://arxiv.org/pdf/2207.09315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1710.10345v5","updated":"2022-07-19T14:58:19Z","published":"2017-10-27T21:47:58Z","title":"The Implicit Bias of Gradient Descent on Separable Data","summary":"  We examine gradient descent on unregularized logistic regression problems,\nwith homogeneous linear predictors on linearly separable datasets. We show the\npredictor converges to the direction of the max-margin (hard margin SVM)\nsolution. The result also generalizes to other monotone decreasing loss\nfunctions with an infimum at infinity, to multi-class problems, and to training\na weight layer in a deep network in a certain restricted setting. Furthermore,\nwe show this convergence is very slow, and only logarithmic in the convergence\nof the loss itself. This can help explain the benefit of continuing to optimize\nthe logistic or cross-entropy loss even after the training error is zero and\nthe training loss is extremely small, and, as we show, even if the validation\nloss increases. Our methodology can also aid in understanding implicit\nregularization n more complex models and with other optimization methods.\n","authors":["Daniel Soudry","Elad Hoffer","Mor Shpigel Nacson","Suriya Gunasekar","Nathan Srebro"],"pdf_url":"https://arxiv.org/pdf/1710.10345v5.pdf","comment":"Fixed a few minor issues in v4: typo in assumption 2, Latex issue in\n  Lemma 1, and added a few words to proof sketch"},{"id":"http://arxiv.org/abs/2207.09312v1","updated":"2022-07-19T14:55:42Z","published":"2022-07-19T14:55:42Z","title":"Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for\n  COVID-19 Screening With Chest Radiography","summary":"  Building AI models with trustworthiness is important especially in regulated\nareas such as healthcare. In tackling COVID-19, previous work uses\nconvolutional neural networks as the backbone architecture, which has shown to\nbe prone to over-caution and overconfidence in making decisions, rendering them\nless trustworthy -- a crucial flaw in the context of medical imaging. In this\nstudy, we propose a feature learning approach using Vision Transformers, which\nuse an attention-based mechanism, and examine the representation learning\ncapability of Transformers as a new backbone architecture for medical imaging.\nThrough the task of classifying COVID-19 chest radiographs, we investigate into\nwhether generalization capabilities benefit solely from Vision Transformers'\narchitectural advances. Quantitative and qualitative evaluations are conducted\non the trustworthiness of the models, through the use of \"trust score\"\ncomputation and a visual explainability technique. We conclude that the\nattention-based feature learning approach is promising in building trustworthy\ndeep learning models for healthcare.\n","authors":["Kai Ma","Pengcheng Xi","Karim Habashy","Ashkan Ebadi","Stéphane Tremblay","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2207.09312v1.pdf","comment":"Accepted to 39th International Conference on Machine Learning,\n  Workshop on Healthcare AI and COVID-19"},{"id":"http://arxiv.org/abs/2008.09041v5","updated":"2022-07-19T14:47:32Z","published":"2020-08-19T02:36:53Z","title":"A New Perspective on Stabilizing GANs training: Direct Adversarial\n  Training","summary":"  Generative Adversarial Networks (GANs) are the most popular image generation\nmodels that have achieved remarkable progress on various computer vision tasks.\nHowever, training instability is still one of the open problems for all\nGAN-based algorithms. Quite a number of methods have been proposed to stabilize\nthe training of GANs, the focuses of which were respectively put on the loss\nfunctions, regularization and normalization technologies, training algorithms,\nand model architectures. Different from the above methods, in this paper, a new\nperspective on stabilizing GANs training is presented. It is found that\nsometimes the images produced by the generator act like adversarial examples of\nthe discriminator during the training process, which may be part of the reason\ncausing the unstable training of GANs. With this finding, we propose the Direct\nAdversarial Training (DAT) method to stabilize the training process of GANs.\nFurthermore, we prove that the DAT method is able to minimize the Lipschitz\nconstant of the discriminator adaptively. The advanced performance of DAT is\nverified on multiple loss functions, network architectures, hyper-parameters,\nand datasets. Specifically, DAT achieves significant improvements of 11.5% FID\non CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10\nunconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom\nunconditional generation based on SSGAN. Code will be available at\nhttps://github.com/iceli1007/DAT-GAN\n","authors":["Ziqiang Li","Pengfei Xia","Rentuo Tao","Hongjing Niu","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2008.09041v5.pdf","comment":"Accepted to IEEE Transactions on Emerging Topics in Computational\n  Intelligence"},{"id":"http://arxiv.org/abs/2207.09304v1","updated":"2022-07-19T14:38:52Z","published":"2022-07-19T14:38:52Z","title":"A sharp uniform-in-time error estimate for Stochastic Gradient Langevin\n  Dynamics","summary":"  We establish a sharp uniform-in-time error estimate for the Stochastic\nGradient Langevin Dynamics (SGLD), which is a popular sampling algorithm. Under\nmild assumptions, we obtain a uniform-in-time $O(\\eta^2)$ bound for the\nKL-divergence between the SGLD iteration and the Langevin diffusion, where\n$\\eta$ is the step size (or learning rate). Our analysis is also valid for\nvarying step sizes. Based on this, we are able to obtain an $O(\\eta)$ bound for\nthe distance between the SGLD iteration and the invariant distribution of the\nLangevin diffusion, in terms of Wasserstein or total variation distances.\n","authors":["Lei Li","Yuliang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.03815v2","updated":"2022-07-19T14:29:13Z","published":"2021-07-08T12:44:41Z","title":"Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with\n  100M FLOPs","summary":"  In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.\n","authors":["Yikang Zhang","Zhuo Chen","Zhao Zhong"],"pdf_url":"https://arxiv.org/pdf/2107.03815v2.pdf","comment":"ICML"},{"id":"http://arxiv.org/abs/2207.09295v1","updated":"2022-07-19T14:26:12Z","published":"2022-07-19T14:26:12Z","title":"The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object\n  Tracking and Counting","summary":"  We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for\ndetecting, tracking, and counting fish in sonar videos. We identify sonar\nvideos as a rich source of data for advancing low signal-to-noise computer\nvision applications and tackling domain generalization in multiple-object\ntracking (MOT) and counting. In comparison to existing MOT and counting\ndatasets, which are largely restricted to videos of people and vehicles in\ncities, CFC is sourced from a natural-world domain where targets are not easily\nresolvable and appearance features cannot be easily leveraged for target\nre-identification. With over half a million annotations in over 1,500 videos\nsourced from seven different sonar cameras, CFC allows researchers to train MOT\nand counting algorithms and evaluate generalization performance at unseen test\nlocations. We perform extensive baseline experiments and identify key\nchallenges and opportunities for advancing the state of the art in\ngeneralization in MOT and counting.\n","authors":["Justin Kay","Peter Kulits","Suzanne Stathatos","Siqi Deng","Erik Young","Sara Beery","Grant Van Horn","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2207.09295v1.pdf","comment":"ECCV 2022. 33 pages, 12 figures"},{"id":"http://arxiv.org/abs/2105.08997v2","updated":"2022-07-19T14:08:42Z","published":"2021-05-19T09:03:02Z","title":"When Deep Classifiers Agree: Analyzing Correlations between Learning\n  Order and Image Statistics","summary":"  Although a plethora of architectural variants for deep classification has\nbeen introduced over time, recent works have found empirical evidence towards\nsimilarities in their training process. It has been hypothesized that neural\nnetworks converge not only to similar representations, but also exhibit a\nnotion of empirical agreement on which data instances are learned first.\nFollowing in the latter works$'$ footsteps, we define a metric to quantify the\nrelationship between such classification agreement over time, and posit that\nthe agreement phenomenon can be mapped to core statistics of the investigated\ndataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,\nImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to\nbe independent of specific architectures, training hyper-parameters or labels,\nalbeit follows an ordering according to image statistics.\n","authors":["Iuliia Pliushch","Martin Mundt","Nicolas Lupp","Visvanathan Ramesh"],"pdf_url":"https://arxiv.org/pdf/2105.08997v2.pdf","comment":"Accepted for publication at ECCV 2022. Version includes supplementary\n  material"},{"id":"http://arxiv.org/abs/2207.02192v2","updated":"2022-07-19T13:17:52Z","published":"2022-07-05T17:48:54Z","title":"CEN : Cooperatively Evolving Networks","summary":"  GANs contain two competing modules: the generator module is trained to\ngenerate new examples, and the discriminator module is trained to discriminate\nreal examples from generated examples. Training procedure of GAN is modeled as\na finitely repeated simultaneous game. Each module tries to increase it's\nperformance at every repetition of the base game (at every batch of training\ndata) in a non-cooperative manner. We observed that each module can perform\nbetter, if training is modeled as an infinitely repeated simultaneous game. At\nevery repetition of the base game (at every batch of training data) the\nstronger module (whose performance is increased or remains the same compared to\nthe previous batch of training data) cooperate with weaker module (whose\nperformance is decreased compared to the previous batch of training data) and\nonly weaker module is allowed to increase it's performance.\n","authors":["Sobhan Babu","Ravindra Guravannavar"],"pdf_url":"https://arxiv.org/pdf/2207.02192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07753v2","updated":"2022-07-19T12:56:54Z","published":"2022-07-15T21:03:11Z","title":"Do Not Sleep on Linear Models: Simple and Interpretable Techniques\n  Outperform Deep Learning for Sleep Scoring","summary":"  Over the last few years, research in automatic sleep scoring has mainly\nfocused on developing increasingly complex deep learning architectures.\nHowever, recently these approaches achieved only marginal improvements, often\nat the expense of requiring more data and more expensive training procedures.\nDespite all these efforts and their satisfactory performance, automatic sleep\nstaging solutions are not widely adopted in a clinical context yet. We argue\nthat most deep learning solutions for sleep scoring are limited in their\nreal-world applicability as they are hard to train, deploy, and reproduce.\nMoreover, these solutions lack interpretability and transparency, which are\noften key to increase adoption rates. In this work, we revisit the problem of\nsleep stage classification using classical machine learning. Results show that\nstate-of-the-art performance can be achieved with a conventional machine\nlearning pipeline consisting of preprocessing, feature extraction, and a simple\nmachine learning model. In particular, we analyze the performance of a linear\nmodel and a non-linear (gradient boosting) model. Our approach surpasses\nstate-of-the-art (that uses the same data) on two public datasets: Sleep-EDF\nSC-20 (MF1 0.810) and Sleep-EDF ST (MF1 0.795), while achieving competitive\nresults on Sleep-EDF SC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that,\nfor the sleep stage scoring task, the expressiveness of an engineered feature\nvector is on par with the internally learned representations of deep learning\nmodels. This observation opens the door to clinical adoption, as a\nrepresentative feature vector allows to leverage both the interpretability and\nsuccessful track record of traditional machine learning models.\n","authors":["Jeroen Van Der Donckt","Jonas Van Der Donckt","Emiel Deprost","Nicolas Vandenbussche","Michael Rademaker","Gilles Vandewiele","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2207.07753v2.pdf","comment":"The first two authors contributed equally. Submitted to Biomedical\n  Signal Processing and Control"},{"id":"http://arxiv.org/abs/2207.09243v1","updated":"2022-07-19T12:56:41Z","published":"2022-07-19T12:56:41Z","title":"Abstract Demonstrations and Adaptive Exploration for Efficient and\n  Stable Multi-step Sparse Reward Reinforcement Learning","summary":"  Although Deep Reinforcement Learning (DRL) has been popular in many\ndisciplines including robotics, state-of-the-art DRL algorithms still struggle\nto learn long-horizon, multi-step and sparse reward tasks, such as stacking\nseveral blocks given only a task-completion reward signal. To improve learning\nefficiency for such tasks, this paper proposes a DRL exploration technique,\ntermed A^2, which integrates two components inspired by human experiences:\nAbstract demonstrations and Adaptive exploration. A^2 starts by decomposing a\ncomplex task into subtasks, and then provides the correct orders of subtasks to\nlearn. During training, the agent explores the environment adaptively, acting\nmore deterministically for well-mastered subtasks and more stochastically for\nill-learnt subtasks. Ablation and comparative experiments are conducted on\nseveral grid-world tasks and three robotic manipulation tasks. We demonstrate\nthat A^2 can aid popular DRL algorithms (DQN, DDPG, and SAC) to learn more\nefficiently and stably in these environments.\n","authors":["Xintong Yang","Ze Ji","Jing Wu","Yu-kun Lai"],"pdf_url":"https://arxiv.org/pdf/2207.09243v1.pdf","comment":"Accepted by The 27th IEEE International Conference on Automation and\n  Computing (ICAC2022)"},{"id":"http://arxiv.org/abs/2202.04985v3","updated":"2022-07-19T12:55:28Z","published":"2022-02-10T12:30:45Z","title":"Generalization Bounds via Convex Analysis","summary":"  Since the celebrated works of Russo and Zou (2016,2019) and Xu and Raginsky\n(2017), it has been well known that the generalization error of supervised\nlearning algorithms can be bounded in terms of the mutual information between\ntheir input and the output, given that the loss of any fixed hypothesis has a\nsubgaussian tail. In this work, we generalize this result beyond the standard\nchoice of Shannon's mutual information to measure the dependence between the\ninput and the output. Our main result shows that it is indeed possible to\nreplace the mutual information by any strongly convex function of the joint\ninput-output distribution, with the subgaussianity condition on the losses\nreplaced by a bound on an appropriately chosen norm capturing the geometry of\nthe dependence measure. This allows us to derive a range of generalization\nbounds that are either entirely new or strengthen previously known ones.\nExamples include bounds stated in terms of $p$-norm divergences and the\nWasserstein-2 distance, which are respectively applicable for heavy-tailed loss\ndistributions and highly smooth loss functions. Our analysis is entirely based\non elementary tools from convex analysis by tracking the growth of a potential\nfunction associated with the dependence measure and the loss function.\n","authors":["Gábor Lugosi","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2202.04985v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09239v1","updated":"2022-07-19T12:52:33Z","published":"2022-07-19T12:52:33Z","title":"Assaying Out-Of-Distribution Generalization in Transfer Learning","summary":"  Since out-of-distribution generalization is a generally ill-posed problem,\nvarious proxy targets (e.g., calibration, adversarial robustness, algorithmic\ncorruptions, invariance across shifts) were studied across different research\nprograms resulting in different recommendations. While sharing the same\naspirational goal, these approaches have never been tested under the same\nexperimental conditions on real data. In this paper, we take a unified view of\nprevious work, highlighting message discrepancies that we address empirically,\nand providing recommendations on how to measure the robustness of a model and\nhow to improve it. To this end, we collect 172 publicly available dataset pairs\nfor training and out-of-distribution evaluation of accuracy, calibration error,\nadversarial attacks, environment invariance, and synthetic corruptions. We\nfine-tune over 31k networks, from nine different architectures in the many- and\nfew-shot setting. Our findings confirm that in- and out-of-distribution\naccuracies tend to increase jointly, but show that their relation is largely\ndataset-dependent, and in general more nuanced and more complex than posited by\nprevious, smaller scale studies.\n","authors":["Florian Wenzel","Andrea Dittadi","Peter Vincent Gehler","Carl-Johann Simon-Gabriel","Max Horn","Dominik Zietlow","David Kernert","Chris Russell","Thomas Brox","Bernt Schiele","Bernhard Schölkopf","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2207.09239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08760v3","updated":"2022-07-19T12:52:19Z","published":"2021-08-19T16:00:58Z","title":"Robust outlier detection by de-biasing VAE likelihoods","summary":"  Deep networks often make confident, yet, incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models (DGMs) are a candidate metric\nfor outlier detection with unlabeled data. Yet, previous studies have shown\nthat DGM likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest of DGMs. We propose novel\nanalytical and algorithmic approaches to ameliorate key biases with VAE\nlikelihoods. Our bias corrections are sample-specific, computationally\ninexpensive, and readily computed for various decoder visible distributions.\nNext, we show that a well-known image pre-processing technique -- contrast\nstretching -- extends the effectiveness of bias correction to further improve\noutlier detection. Our approach achieves state-of-the-art accuracies with nine\ngrayscale and natural image datasets, and demonstrates significant advantages\n-- both with speed and performance -- over four recent, competing approaches.\nIn summary, lightweight remedies suffice to achieve robust outlier detection\nwith VAEs.\n","authors":["Kushal Chauhan","Barath Mohan U","Pradeep Shenoy","Manish Gupta","Devarajan Sridharan"],"pdf_url":"https://arxiv.org/pdf/2108.08760v3.pdf","comment":"CVPR 2022. 20 pages and 19 figures"},{"id":"http://arxiv.org/abs/2207.09238v1","updated":"2022-07-19T12:49:02Z","published":"2022-07-19T12:49:02Z","title":"Formal Algorithms for Transformers","summary":"  This document aims to be a self-contained, mathematically precise overview of\ntransformer architectures and algorithms (*not* results). It covers what\ntransformers are, how they are trained, what they are used for, their key\narchitectural components, and a preview of the most prominent models. The\nreader is assumed to be familiar with basic ML terminology and simpler neural\nnetwork architectures such as MLPs.\n","authors":["Mary Phuong","Marcus Hutter"],"pdf_url":"https://arxiv.org/pdf/2207.09238v1.pdf","comment":"16 pages, 15 algorithms"},{"id":"http://arxiv.org/abs/2207.09237v1","updated":"2022-07-19T12:49:00Z","published":"2022-07-19T12:49:00Z","title":"Semi-supervised Predictive Clustering Trees for (Hierarchical)\n  Multi-label Classification","summary":"  Semi-supervised learning (SSL) is a common approach to learning predictive\nmodels using not only labeled examples, but also unlabeled examples. While SSL\nfor the simple tasks of classification and regression has received a lot of\nattention from the research community, this is not properly investigated for\ncomplex prediction tasks with structurally dependent variables. This is the\ncase of multi-label classification and hierarchical multi-label classification\ntasks, which may require additional information, possibly coming from the\nunderlying distribution in the descriptive space provided by unlabeled\nexamples, to better face the challenging task of predicting simultaneously\nmultiple class labels.\n  In this paper, we investigate this aspect and propose a (hierarchical)\nmulti-label classification method based on semi-supervised learning of\npredictive clustering trees. We also extend the method towards ensemble\nlearning and propose a method based on the random forest approach. Extensive\nexperimental evaluation conducted on 23 datasets shows significant advantages\nof the proposed method and its extension with respect to their supervised\ncounterparts. Moreover, the method preserves interpretability and reduces the\ntime complexity of classical tree-based models.\n","authors":["Jurica Levatić","Michelangelo Ceci","Dragi Kocev","Sašo Džeroski"],"pdf_url":"https://arxiv.org/pdf/2207.09237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09232v1","updated":"2022-07-19T12:42:12Z","published":"2022-07-19T12:42:12Z","title":"Over-the-Air Federated Edge Learning with Hierarchical Clustering","summary":"  We examine federated learning (FL) with over-the-air (OTA) aggregation, where\nmobile users (MUs) aim to reach a consensus on a global model with the help of\na parameter server (PS) that aggregates the local gradients. In OTA FL, MUs\ntrain their models using local data at every training round and transmit their\ngradients simultaneously using the same frequency band in an uncoded fashion.\nBased on the received signal of the superposed gradients, the PS performs a\nglobal model update. While the OTA FL has a significantly decreased\ncommunication cost, it is susceptible to adverse channel effects and noise.\nEmploying multiple antennas at the receiver side can reduce these effects, yet\nthe path-loss is still a limiting factor for users located far away from the\nPS. To ameliorate this issue, in this paper, we propose a wireless-based\nhierarchical FL scheme that uses intermediate servers (ISs) to form clusters at\nthe areas where the MUs are more densely located. Our scheme utilizes OTA\ncluster aggregations for the communication of the MUs with their corresponding\nIS, and OTA global aggregations from the ISs to the PS. We present a\nconvergence analysis for the proposed algorithm, and show through numerical\nevaluations of the derived analytical expressions and experimental results that\nutilizing ISs results in a faster convergence and a better performance than the\nOTA FL alone while using less transmit power. We also validate the results on\nthe performance using different number of cluster iterations with different\ndatasets and data distributions. We conclude that the best choice of cluster\naggregations depends on the data distribution among the MUs and the clusters.\n","authors":["Ozan Aygün","Mohammad Kazemi","Deniz Gündüz","Tolga M. Duman"],"pdf_url":"https://arxiv.org/pdf/2207.09232v1.pdf","comment":"30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2207.03036v2","updated":"2022-07-19T12:29:17Z","published":"2022-07-07T01:33:25Z","title":"Not All Models Are Equal: Predicting Model Transferability in a\n  Self-challenging Fisher Space","summary":"  This paper addresses an important problem of ranking the pre-trained deep\nneural networks and screening the most transferable ones for downstream tasks.\nIt is challenging because the ground-truth model ranking for each task can only\nbe generated by fine-tuning the pre-trained models on the target dataset, which\nis brute-force and computationally expensive. Recent advanced methods proposed\nseveral lightweight transferability metrics to predict the fine-tuning results.\nHowever, these approaches only capture static representations but neglect the\nfine-tuning dynamics. To this end, this paper proposes a new transferability\nmetric, called \\textbf{S}elf-challenging \\textbf{F}isher \\textbf{D}iscriminant\n\\textbf{A}nalysis (\\textbf{SFDA}), which has many appealing benefits that\nexisting works do not have. First, SFDA can embed the static features into a\nFisher space and refine them for better separability between classes. Second,\nSFDA uses a self-challenging mechanism to encourage different pre-trained\nmodels to differentiate on hard examples. Third, SFDA can easily select\nmultiple pre-trained models for the model ensemble. Extensive experiments on\n$33$ pre-trained models of $11$ downstream tasks show that SFDA is efficient,\neffective, and robust when measuring the transferability of pre-trained models.\nFor instance, compared with the state-of-the-art method NLEEP, SFDA\ndemonstrates an average of $59.1$\\% gain while bringing $22.5$x speedup in\nwall-clock time. The code will be available at\n\\url{https://github.com/TencentARC/SFDA}.\n","authors":["Wenqi Shao","Xun Zhao","Yixiao Ge","Zhaoyang Zhang","Lei Yang","Xiaogang Wang","Ying Shan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2207.03036v2.pdf","comment":"ECCV 2022 camera ready. 24 pages, 11 tables, 5 figures"},{"id":"http://arxiv.org/abs/2207.09225v1","updated":"2022-07-19T12:23:08Z","published":"2022-07-19T12:23:08Z","title":"Similarity of Pre-trained and Fine-tuned Representations","summary":"  In transfer learning, only the last part of the networks - the so-called head\n- is often fine-tuned. Representation similarity analysis shows that the most\nsignificant change still occurs in the head even if all weights are updatable.\nHowever, recent results from few-shot learning have shown that representation\nchange in the early layers, which are mostly convolutional, is beneficial,\nespecially in the case of cross-domain adaption. In our paper, we find out\nwhether that also holds true for transfer learning. In addition, we analyze the\nchange of representation in transfer learning, both during pre-training and\nfine-tuning, and find out that pre-trained structure is unlearned if not\nusable.\n","authors":["Thomas Goerttler","Klaus Obermayer"],"pdf_url":"https://arxiv.org/pdf/2207.09225v1.pdf","comment":"Workshop of Updatable Machine Learning at ICML 2022"},{"id":"http://arxiv.org/abs/2205.08455v2","updated":"2022-07-19T11:40:52Z","published":"2022-05-17T15:56:31Z","title":"Utterance Weighted Multi-Dilation Temporal Convolutional Networks for\n  Monaural Speech Dereverberation","summary":"  Speech dereverberation is an important stage in many speech technology\napplications. Recent work in this area has been dominated by deep neural\nnetwork models. Temporal convolutional networks (TCNs) are deep learning models\nthat have been proposed for sequence modelling in the task of dereverberating\nspeech. In this work a weighted multi-dilation depthwise-separable convolution\nis proposed to replace standard depthwise-separable convolutions in TCN models.\nThis proposed convolution enables the TCN to dynamically focus on more or less\nlocal information in its receptive field at each convolutional block in the\nnetwork. It is shown that this weighted multi-dilation temporal convolutional\nnetwork (WD-TCN) consistently outperforms the TCN across various model\nconfigurations and using the WD-TCN model is a more parameter efficient method\nto improve the performance of the model than increasing the number of\nconvolutional blocks. The best performance improvement over the baseline TCN is\n0.55 dB scale-invariant signal-to-distortion ratio (SISDR) and the best\nperforming WD-TCN model attains 12.26 dB SISDR on the WHAMR dataset.\n","authors":["William Ravenscroft","Stefan Goetze","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2205.08455v2.pdf","comment":"Accepted at IWAENC 2022"},{"id":"http://arxiv.org/abs/2207.09204v1","updated":"2022-07-19T11:30:41Z","published":"2022-07-19T11:30:41Z","title":"VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data","summary":"  We present VoloGAN, an adversarial domain adaptation network that translates\nsynthetic RGB-D images of a high-quality 3D model of a person, into RGB-D\nimages that could be generated with a consumer depth sensor. This system is\nespecially useful to generate high amount training data for single-view 3D\nreconstruction algorithms replicating the real-world capture conditions, being\nable to imitate the style of different sensor types, for the same high-end 3D\nmodel database. The network uses a CycleGAN framework with a U-Net architecture\nfor the generator and a discriminator inspired by SIV-GAN. We use different\noptimizers and learning rate schedules to train the generator and the\ndiscriminator. We further construct a loss function that considers image\nchannels individually and, among other metrics, evaluates the structural\nsimilarity. We demonstrate that CycleGANs can be used to apply adversarial\ndomain adaptation of synthetic 3D data to train a volumetric video generator\nmodel having only few training samples.\n","authors":["Sascha Kirch","Rafael Pagés","Sergio Arnaldo","Sergio Martín"],"pdf_url":"https://arxiv.org/pdf/2207.09204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07654v2","updated":"2022-07-19T11:11:17Z","published":"2022-07-16T10:41:41Z","title":"Learning inducing points and uncertainty on molecular data","summary":"  Uncertainty control and scalability to large datasets are the two main issues\nfor the deployment of Gaussian process models into the autonomous material and\nchemical space exploration pipelines. One way to address both of these issues\nis by introducing the latent inducing variables and choosing the right\napproximation for the marginal log-likelihood objective. Here, we show that\nvariational learning of the inducing points in the high-dimensional molecular\ndescriptor space significantly improves both the prediction quality and\nuncertainty estimates on test configurations from a sample molecular dynamics\ndataset. Additionally, we show that inducing points can learn to represent the\nconfigurations of the molecules of different types that were not present within\nthe initialization set of inducing points. Among several evaluated approximate\nmarginal log-likelihood objectives, we show that the predictive log-likelihood\nprovides both the predictive quality comparable to the exact Gaussian process\nmodel and excellent uncertainty control. Finally, we comment on whether a\nmachine learning model makes predictions by interpolating the molecular\nconfigurations in high-dimensional descriptor space. We show that despite our\nintuition, and even for densely sampled molecular dynamics datasets, most of\nthe predictions are done in the extrapolation regime.\n","authors":["Mikhail Tsitsvero"],"pdf_url":"https://arxiv.org/pdf/2207.07654v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.08640v2","updated":"2022-07-19T11:01:17Z","published":"2022-07-18T14:38:11Z","title":"Lightweight Automated Feature Monitoring for Data Streams","summary":"  Monitoring the behavior of automated real-time stream processing systems has\nbecome one of the most relevant problems in real world applications. Such\nsystems have grown in complexity relying heavily on high dimensional input\ndata, and data hungry Machine Learning (ML) algorithms. We propose a flexible\nsystem, Feature Monitoring (FM), that detects data drifts in such data sets,\nwith a small and constant memory footprint and a small computational cost in\nstreaming applications. The method is based on a multi-variate statistical test\nand is data driven by design (full reference distributions are estimated from\nthe data). It monitors all features that are used by the system, while\nproviding an interpretable features ranking whenever an alarm occurs (to aid in\nroot cause analysis). The computational and memory lightness of the system\nresults from the use of Exponential Moving Histograms. In our experimental\nstudy, we analyze the system's behavior with its parameters and, more\nimportantly, show examples where it detects problems that are not directly\nrelated to a single feature. This illustrates how FM eliminates the need to add\ncustom signals to detect specific types of problems and that monitoring the\navailable space of features is often enough.\n","authors":["João Conde","Ricardo Moreira","João Torres","Pedro Cardoso","Hugo R. C. Ferreira","Marco O. P. Sampaio","João Tiago Ascensão","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2207.08640v2.pdf","comment":"10 pages, 5 figures. AutoML, KDD22, August 14-17, 2022, Washington,\n  DC, US"},{"id":"http://arxiv.org/abs/2207.09196v1","updated":"2022-07-19T11:00:33Z","published":"2022-07-19T11:00:33Z","title":"Adaptive Learning for the Resource-Constrained Classification Problem","summary":"  Resource-constrained classification tasks are common in real-world\napplications such as allocating tests for disease diagnosis, hiring decisions\nwhen filling a limited number of positions, and defect detection in\nmanufacturing settings under a limited inspection budget. Typical\nclassification algorithms treat the learning process and the resource\nconstraints as two separate and sequential tasks. Here we design an adaptive\nlearning approach that considers resource constraints and learning jointly by\niteratively fine-tuning misclassification costs. Via a structured experimental\nstudy using a publicly available data set, we evaluate a decision tree\nclassifier that utilizes the proposed approach. The adaptive learning approach\nperforms significantly better than alternative approaches, especially for\ndifficult classification problems in which the performance of common approaches\nmay be unsatisfactory. We envision the adaptive learning approach as an\nimportant addition to the repertoire of techniques for handling\nresource-constrained classification problems.\n","authors":["Danit Shifman Abukasis","Izack Cohen","Xiaochen Xian","Kejun Huang","Gonen Singer"],"pdf_url":"https://arxiv.org/pdf/2207.09196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.13086v2","updated":"2022-07-19T10:59:29Z","published":"2021-10-25T16:26:37Z","title":"Quantum Algorithms and Lower Bounds for Linear Regression with Norm\n  Constraints","summary":"  Lasso and Ridge are important minimization problems in machine learning and\nstatistics. They are versions of linear regression with squared loss where the\nvector $\\theta\\in\\mathbb{R}^d$ of coefficients is constrained in either\n$\\ell_1$-norm (for Lasso) or in $\\ell_2$-norm (for Ridge). We study the\ncomplexity of quantum algorithms for finding $\\varepsilon$-minimizers for these\nminimization problems. We show that for Lasso we can get a quadratic quantum\nspeedup in terms of $d$ by speeding up the cost-per-iteration of the\nFrank-Wolfe algorithm, while for Ridge the best quantum algorithms are linear\nin $d$, as are the best classical algorithms. As a byproduct of our quantum\nlower bound for Lasso, we also prove the first classical lower bound for Lasso\nthat is tight up to polylog-factors.\n","authors":["Yanlin Chen","Ronald de Wolf"],"pdf_url":"https://arxiv.org/pdf/2110.13086v2.pdf","comment":"v2: Main changes are the addition of a tight classical lower bound\n  for Lasso, and small improvements in the existing text. 38 pages LaTeX"},{"id":"http://arxiv.org/abs/2207.03830v3","updated":"2022-07-19T10:47:11Z","published":"2022-07-08T11:33:53Z","title":"Safe reinforcement learning for multi-energy management systems with\n  known constraint functions","summary":"  Reinforcement learning (RL) is a promising optimal control technique for\nmulti-energy management systems. It does not require a model a priori -\nreducing the upfront and ongoing project-specific engineering effort and is\ncapable of learning better representations of the underlying system dynamics.\nHowever, vanilla RL does not provide constraint satisfaction guarantees -\nresulting in various potentially unsafe interactions within its safety-critical\nenvironment. In this paper, we present two novel safe RL methods, namely\nSafeFallback and GiveSafe, where the safety constraint formulation is decoupled\nfrom the RL formulation and which provides hard-constraint satisfaction\nguarantees both during training a (near) optimal policy (which involves\nexploratory and exploitative, i.e. greedy, steps) as well as during deployment\nof any policy (e.g. random agents or offline trained RL agents). In a simulated\nmulti-energy systems case study we have shown that both methods start with a\nsignificantly higher utility (i.e. useful policy) compared to a vanilla RL\nbenchmark (94,6% and 82,8% compared to 35,5%) and that the proposed\nSafeFallback method even can outperform the vanilla RL benchmark (102,9% to\n100%). We conclude that both methods are viably safety constraint handling\ntechniques applicable beyond RL, as demonstrated with random policies while\nstill providing hard-constraint guarantees. Finally, we propose directions for\nfuture work to i.a. improve the constraint functions itself as more data\nbecomes available.\n","authors":["Glenn Ceusters","Luis Ramirez Camargo","Rüdiger Franke","Ann Nowé","Maarten Messagie"],"pdf_url":"https://arxiv.org/pdf/2207.03830v3.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09185v1","updated":"2022-07-19T10:46:02Z","published":"2022-07-19T10:46:02Z","title":"Multi-view hierarchical Variational AutoEncoders with Factor Analysis\n  latent space","summary":"  Real-world databases are complex, they usually present redundancy and shared\ncorrelations between heterogeneous and multiple representations of the same\ndata. Thus, exploiting and disentangling shared information between views is\ncritical. For this purpose, recent studies often fuse all views into a shared\nnonlinear complex latent space but they lose the interpretability. To overcome\nthis limitation, here we propose a novel method to combine multiple Variational\nAutoEncoders (VAE) architectures with a Factor Analysis latent space (FA-VAE).\nConcretely, we use a VAE to learn a private representation of each\nheterogeneous view in a continuous latent space. Then, we model the shared\nlatent space by projecting every private variable to a low-dimensional latent\nspace using a linear projection matrix. Thus, we create an interpretable\nhierarchical dependency between private and shared information. This way, the\nnovel model is able to simultaneously: (i) learn from multiple heterogeneous\nviews, (ii) obtain an interpretable hierarchical shared space, and, (iii)\nperform transfer learning between generative models.\n","authors":["Alejandro Guerrero-López","Carlos Sevilla-Salcedo","Vanessa Gómez-Verdejo","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2207.09185v1.pdf","comment":"20 pages main work, 2 pages supplementary, 14 figures"},{"id":"http://arxiv.org/abs/2207.09179v1","updated":"2022-07-19T10:32:11Z","published":"2022-07-19T10:32:11Z","title":"SCARA: Scalable Graph Neural Networks with Feature-Oriented Optimization","summary":"  Recent advances in data processing have stimulated the demand for learning\ngraphs of very large scales. Graph Neural Networks (GNNs), being an emerging\nand powerful approach in solving graph learning tasks, are known to be\ndifficult to scale up. Most scalable models apply node-based techniques in\nsimplifying the expensive graph message-passing propagation procedure of GNN.\nHowever, we find such acceleration insufficient when applied to million- or\neven billion-scale graphs. In this work, we propose SCARA, a scalable GNN with\nfeature-oriented optimization for graph computation. SCARA efficiently computes\ngraph embedding from node features, and further selects and reuses feature\ncomputation results to reduce overhead. Theoretical analysis indicates that our\nmodel achieves sub-linear time complexity with a guaranteed precision in\npropagation process as well as GNN training and inference. We conduct extensive\nexperiments on various datasets to evaluate the efficacy and efficiency of\nSCARA. Performance comparison with baselines shows that SCARA can reach up to\n100x graph propagation acceleration than current state-of-the-art methods with\nfast convergence and comparable accuracy. Most notably, it is efficient to\nprocess precomputation on the largest available billion-scale GNN dataset\nPapers100M (111M nodes, 1.6B edges) in 100 seconds.\n","authors":["Ningyi Liao","Dingheng Mo","Siqiang Luo","Xiang Li","Pengcheng Yin"],"pdf_url":"https://arxiv.org/pdf/2207.09179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.05307v2","updated":"2022-07-19T10:20:52Z","published":"2021-01-13T19:09:38Z","title":"Explainability of deep vision-based autonomous driving systems: Review\n  and challenges","summary":"  This survey reviews explainability methods for vision-based self-driving\nsystems trained with behavior cloning. The concept of explainability has\nseveral facets and the need for explainability is strong in driving, a\nsafety-critical application. Gathering contributions from several research\nfields, namely computer vision, deep learning, autonomous driving, explainable\nAI (X-AI), this survey tackles several points. First, it discusses definitions,\ncontext, and motivation for gaining more interpretability and explainability\nfrom self-driving systems, as well as the challenges that are specific to this\napplication. Second, methods providing explanations to a black-box self-driving\nsystem in a post-hoc fashion are comprehensively organized and detailed. Third,\napproaches from the literature that aim at building more interpretable\nself-driving systems by design are presented and discussed in detail. Finally,\nremaining open-challenges and potential future research directions are\nidentified and examined.\n","authors":["Éloi Zablocki","Hédi Ben-Younes","Patrick Pérez","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2101.05307v2.pdf","comment":"IJCV 2022"},{"id":"http://arxiv.org/abs/2207.09158v1","updated":"2022-07-19T09:56:44Z","published":"2022-07-19T09:56:44Z","title":"FedX: Unsupervised Federated Learning with Cross Knowledge Distillation","summary":"  This paper presents FedX, an unsupervised federated learning framework. Our\nmodel learns unbiased representation from decentralized and heterogeneous local\ndata. It employs a two-sided knowledge distillation with contrastive learning\nas a core component, allowing the federated system to function without\nrequiring clients to share any data features. Furthermore, its adaptable\narchitecture can be used as an add-on module for existing unsupervised\nalgorithms in federated settings. Experiments show that our model improves\nperformance significantly (1.58--5.52pp) on five unsupervised algorithms.\n","authors":["Sungwon Han","Sungwon Park","Fangzhao Wu","Sundong Kim","Chuhan Wu","Xing Xie","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2207.09158v1.pdf","comment":"Accepted and will be published at ECCV2022"},{"id":"http://arxiv.org/abs/2207.09154v1","updated":"2022-07-19T09:50:34Z","published":"2022-07-19T09:50:34Z","title":"On the development of a Bayesian optimisation framework for complex\n  unknown systems","summary":"  Bayesian optimisation provides an effective method to optimise expensive\nblack box functions. It has recently been applied to problems in fluid\ndynamics. This paper studies and compares common Bayesian optimisation\nalgorithms empirically on a range of synthetic test functions. It investigates\nthe choice of acquisition function and number of training samples, exact\ncalculation of acquisition functions and Monte Carlo based approaches and both\nsingle-point and multi-point optimisation. The test functions considered cover\na wide selection of challenges and therefore serve as an ideal test bed to\nunderstand the performance of Bayesian optimisation and to identify general\nsituations where Bayesian optimisation performs well and poorly. This knowledge\ncan be utilised in applications, including those in fluid dynamics, where\nobjective functions are unknown. The results of this investigation show that\nthe choices to be made are less relevant for relatively simple functions, while\noptimistic acquisition functions such as Upper Confidence Bound should be\npreferred for more complex objective functions. Furthermore, results from the\nMonte Carlo approach are comparable to results from analytical acquisition\nfunctions. In instances where the objective function allows parallel\nevaluations, the multi-point approach offers a quicker alternative, yet it may\npotentially require more objective function evaluations.\n","authors":["Mike Diessner","Yu Guan","Kevin J. Wilson","Richard D. Whalley"],"pdf_url":"https://arxiv.org/pdf/2207.09154v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2204.06508v2","updated":"2022-07-19T09:43:43Z","published":"2022-04-13T16:45:33Z","title":"FactGraph: Evaluating Factuality in Summarization with Semantic Graph\n  Representations","summary":"  Despite recent improvements in abstractive summarization, most current\napproaches generate summaries that are not factually consistent with the source\ndocument, severely restricting their trust and usage in real-world\napplications. Recent works have shown promising improvements in factuality\nerror identification using text or dependency arc entailments; however, they do\nnot consider the entire semantic graph simultaneously. To this end, we propose\nFactGraph, a method that decomposes the document and the summary into\nstructured meaning representations (MR), which are more suitable for factuality\nevaluation. MRs describe core semantic concepts and their relations,\naggregating the main content in both document and summary in a canonical form,\nand reducing data sparsity. FactGraph encodes such graphs using a graph encoder\naugmented with structure-aware adapters to capture interactions among the\nconcepts based on the graph connectivity, along with text representations using\nan adapter-based text encoder. Experiments on different benchmarks for\nevaluating factuality show that FactGraph outperforms previous approaches by up\nto 15%. Furthermore, FactGraph improves performance on identifying content\nverifiability errors and better captures subsentence-level factual\ninconsistencies.\n","authors":["Leonardo F. R. Ribeiro","Mengwen Liu","Iryna Gurevych","Markus Dreyer","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2204.06508v2.pdf","comment":"NAACL 2022 (15 pages)"},{"id":"http://arxiv.org/abs/2207.08548v2","updated":"2022-07-19T09:28:56Z","published":"2022-07-18T12:12:24Z","title":"GATE: Gated Additive Tree Ensemble for Tabular Classification and\n  Regression","summary":"  We propose a novel high-performance, parameter and computationally efficient\ndeep learning architecture for tabular data, Gated Additive Tree\nEnsemble(GATE). GATE uses a gating mechanism, inspired from GRU, as a feature\nrepresentation learning unit with an in-built feature selection mechanism. We\ncombine it with an ensemble of differentiable, non-linear decision trees,\nre-weighted with simple self-attention to predict our desired output. We\ndemonstrate that GATE is a competitive alternative to SOTA approaches like\nGBDTs, NODE, FT Transformers, etc. by experiments on several public datasets\n(both classification and regression). The code will be uploaded as soon as the\npaper comes out of review.\n","authors":["Manu Joseph","Harsh Raj"],"pdf_url":"https://arxiv.org/pdf/2207.08548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08581v2","updated":"2022-07-19T09:24:09Z","published":"2022-07-18T13:18:34Z","title":"Study of the performance and scalability of federated learning for\n  medical imaging with intermittent clients","summary":"  Federated learning is a data decentralization privacy-preserving technique\nused to perform machine or deep learning in a secure way. In this paper we\npresent theoretical aspects about federated learning, such as the presentation\nof an aggregation operator, different types of federated learning, and issues\nto be taken into account in relation to the distribution of data from the\nclients, together with the exhaustive analysis of a use case where the number\nof clients varies. Specifically, a use case of medical image analysis is\nproposed, using chest X-ray images obtained from an open data repository. In\naddition to the advantages related to privacy, improvements in predictions (in\nterms of accuracy and area under the curve) and reduction of execution times\nwill be studied with respect to the classical case (the centralized approach).\nDifferent clients will be simulated from the training data, selected in an\nunbalanced manner, i.e., they do not all have the same number of data. The\nresults of considering three or ten clients are exposed and compared between\nthem and against the centralized case. Two approaches to follow will be\nanalyzed in the case of intermittent clients, as in a real scenario some\nclients may leave the training, and some new ones may enter the training. The\nevolution of the results for the test set in terms of accuracy, area under the\ncurve and execution time is shown as the number of clients into which the\noriginal data is divided increases. Finally, improvements and future work in\nthe field are proposed.\n","authors":["Judith Sáinz-Pardo Díaz","Álvaro López García"],"pdf_url":"https://arxiv.org/pdf/2207.08581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09141v1","updated":"2022-07-19T09:21:21Z","published":"2022-07-19T09:21:21Z","title":"Using Neural Networks by Modelling Semi-Active Shock Absorber","summary":"  A permanently increasing number of on-board automotive control systems\nrequires new approaches to their digital mapping that improves functionality in\nterms of adaptability and robustness as well as enables their easier on-line\nsoftware update. As it can be concluded from many recent studies, various\nmethods applying neural networks (NN) can be good candidates for relevant\ndigital twin (DT) tools in automotive control system design, for example, for\ncontroller parameterization and condition monitoring. However, the NN-based DT\nhas strong requirements to an adequate amount of data to be used in training\nand design. In this regard, the paper presents an approach, which demonstrates\nhow the regression tasks can be efficiently handled by the modeling of a\nsemi-active shock absorber within the DT framework. The approach is based on\nthe adaptation of time series augmentation techniques to the stationary data\nthat increases the variance of the latter. Such a solution gives a background\nto elaborate further data engineering methods for the data preparation of\nsophisticated databases.\n","authors":["Moritz Zink","Martin Schiele","Valentin Ivanov"],"pdf_url":"https://arxiv.org/pdf/2207.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09139v1","updated":"2022-07-19T09:21:01Z","published":"2022-07-19T09:21:01Z","title":"Heterogeneous Treatment Effect with Trained Kernels of the\n  Nadaraya-Watson Regression","summary":"  A new method for estimating the conditional average treatment effect is\nproposed in the paper. It is called TNW-CATE (the Trainable Nadaraya-Watson\nregression for CATE) and based on the assumption that the number of controls is\nrather large whereas the number of treatments is small. TNW-CATE uses the\nNadaraya-Watson regression for predicting outcomes of patients from the control\nand treatment groups. The main idea behind TNW-CATE is to train kernels of the\nNadaraya-Watson regression by using a weight sharing neural network of a\nspecific form. The network is trained on controls, and it replaces standard\nkernels with a set of neural subnetworks with shared parameters such that every\nsubnetwork implements the trainable kernel, but the whole network implements\nthe Nadaraya-Watson estimator. The network memorizes how the feature vectors\nare located in the feature space. The proposed approach is similar to the\ntransfer learning when domains of source and target data are similar, but tasks\nare different. Various numerical simulation experiments illustrate TNW-CATE and\ncompare it with the well-known T-learner, S-learner and X-learner for several\ntypes of the control and treatment outcome functions. The code of proposed\nalgorithms implementing TNW-CATE is available in\nhttps://github.com/Stasychbr/TNW-CATE.\n","authors":["Andrei V. Konstantinov","Stanislav R. Kirpichenko","Lev V. Utkin"],"pdf_url":"https://arxiv.org/pdf/2207.09139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.15327v3","updated":"2022-07-19T08:20:03Z","published":"2021-12-31T07:25:18Z","title":"Sufficient Statistic Memory AMP","summary":"  Approximate message passing (AMP) type algorithms have been widely used in\nthe signal reconstruction of certain large random linear systems. A key feature\nof the AMP-type algorithms is that their dynamics can be correctly described by\nstate evolution. However, the state evolution does not necessarily be\nconvergent. To solve the convergence problem of the state evolution of AMP-type\nalgorithms in principle, this paper proposes a memory AMP (MAMP) under a\nsufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We\nshow that the covariance matrices of SS-MAMP are L-banded and convergent. Given\nan arbitrary MAMP, we can construct an SS-MAMP by damping, which not only\nensures the convergence of the state evolution, but also preserves the\northogonality, i.e., its dynamics can be correctly described by state\nevolution. As a byproduct, we prove that the Bayes-optimal orthogonal/vector\nAMP (BO-OAMP/VAMP) is an SS-MAMP. As a result, we reveal two interesting\nproperties of BO-OAMP/VAMP for large systems: 1) the covariance matrices are\nL-banded and are convergent, and 2) damping and memory are not needed (i.e., do\nnot bring performance improvement). As an example, we construct a sufficient\nstatistic Bayes-optimal MAMP (SS-BO-MAMP) whose state evolution converges to\nthe minimum (i.e., Bayes-optimal) mean square error (MSE) predicted by replica\nmethods. In addition, the MSE of SS-BO-MAMP is not worse than the original\nBO-MAMP. Finally, simulations are provided to verify the theoretical results.\n","authors":["Lei Liu","Shunqi Huang","Brian M. Kurkoski"],"pdf_url":"https://arxiv.org/pdf/2112.15327v3.pdf","comment":"Double-column, 16 pages, submitted to IEEE Transactions on\n  Information Theory"},{"id":"http://arxiv.org/abs/2207.08258v2","updated":"2022-07-19T07:56:35Z","published":"2022-07-17T18:53:56Z","title":"Minimum Description Length Control","summary":"  We propose a novel framework for multitask reinforcement learning based on\nthe minimum description length (MDL) principle. In this approach, which we term\nMDL-control (MDL-C), the agent learns the common structure among the tasks with\nwhich it is faced and then distills it into a simpler representation which\nfacilitates faster convergence and generalization to new tasks. In doing so,\nMDL-C naturally balances adaptation to each task with epistemic uncertainty\nabout the task distribution. We motivate MDL-C via formal connections between\nthe MDL principle and Bayesian inference, derive theoretical performance\nguarantees, and demonstrate MDL-C's empirical effectiveness on both discrete\nand high-dimensional continuous control tasks. %Empirically, this framework is\nused to modify existing policy optimization approaches and improves their\nmultitask performance in both discrete and high-dimensional continuous control\nproblems.\n","authors":["Ted Moskovitz","Ta-Chu Kao","Maneesh Sahani","Matthew M. Botvinick"],"pdf_url":"https://arxiv.org/pdf/2207.08258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06804v2","updated":"2022-07-19T07:44:30Z","published":"2022-02-14T15:48:27Z","title":"Flexible learning of quantum states with generative query neural\n  networks","summary":"  Deep neural networks are a powerful tool for the characterization of quantum\nstates.\n  Existing networks are typically trained with experimental data gathered from\nthe specific quantum state that needs to be characterized.\n  But is it possible to train a neural network offline and to make predictions\nabout quantum states other than the ones used for the training?\n  Here we introduce a model of network that can be trained with classically\nsimulated data from a fiducial set of states and measurements, and can later be\nused to characterize quantum states that share structural similarities with the\nstates in the fiducial set. With little guidance of quantum physics, the\nnetwork builds its own data-driven representation of quantum states, and then\nuses it to predict the outcome statistics of quantum measurements that have not\nbeen performed yet.\n  The state representation produced by the network can also be used for tasks\nbeyond the prediction of outcome statistics, including clustering of quantum\nstates and identification of different phases of matter.\n  Our network model provides a flexible approach that can be applied to online\nlearning scenarios, where predictions must be generated as soon as experimental\ndata become available, and to blind learning scenarios where the learner has\nonly access to an encrypted description of the quantum hardware.\n","authors":["Yan Zhu","Ya-Dong Wu","Ge Bai","Dong-Sheng Wang","Yuexuan Wang","Giulio Chiribella"],"pdf_url":"https://arxiv.org/pdf/2202.06804v2.pdf","comment":"Major improvements in the presentation; new numerical experiments\n  that illustrate the applicability of our neural network to various types of\n  states"},{"id":"http://arxiv.org/abs/2207.09109v1","updated":"2022-07-19T07:42:06Z","published":"2022-07-19T07:42:06Z","title":"Active-Learning-as-a-Service: An Efficient MLOps System for Data-Centric\n  AI","summary":"  The success of today's AI applications requires not only model training\n(Model-centric) but also data engineering (Data-centric). In data-centric AI,\nactive learning (AL) plays a vital role, but current AL tools can not perform\nAL tasks efficiently. To this end, this paper presents an efficient MLOps\nsystem for AL, named ALaaS (Active-Learning-as-a-Service). Specifically, ALaaS\nadopts a server-client architecture to support an AL pipeline and implements\nstage-level parallelism for high efficiency. Meanwhile, caching and batching\ntechniques are employed to further accelerate the AL process. In addition to\nefficiency, ALaaS ensures accessibility with the help of the design philosophy\nof configuration-as-a-service. It also abstracts an AL process to several\ncomponents and provides rich APIs for advanced users to extend the system to\nnew scenarios. Extensive experiments show that ALaaS outperforms all other\nbaselines in terms of latency and throughput. Further ablation studies\ndemonstrate the effectiveness of our design as well as ALaaS's ease to use. Our\ncode is available at \\url{https://github.com/MLSysOps/alaas}.\n","authors":["Yizheng Huang","Huaizheng Zhang","Yuanming Li","Chiew Tong Lau","Yang You"],"pdf_url":"https://arxiv.org/pdf/2207.09109v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2105.12876v2","updated":"2022-07-19T07:20:58Z","published":"2021-05-26T23:10:51Z","title":"A Hybrid Recommender System for Recommending Smartphones to Prospective\n  Customers","summary":"  Recommender Systems are a subclass of machine learning systems that employ\nsophisticated information filtering strategies to reduce the search time and\nsuggest the most relevant items to any particular user. Hybrid recommender\nsystems combine multiple recommendation strategies in different ways to benefit\nfrom their complementary advantages. Some hybrid recommender systems have\ncombined collaborative filtering and content-based approaches to build systems\nthat are more robust. In this paper, we propose a hybrid recommender system,\nwhich combines Alternating Least Squares (ALS) based collaborative filtering\nwith deep learning to enhance recommendation performance as well as overcome\nthe limitations associated with the collaborative filtering approach,\nespecially concerning its cold start problem. In essence, we use the outputs\nfrom ALS (collaborative filtering) to influence the recommendations from a Deep\nNeural Network (DNN), which combines characteristic, contextual, structural and\nsequential information, in a big data processing framework. We have conducted\nseveral experiments in testing the efficacy of the proposed hybrid architecture\nin recommending smartphones to prospective customers and compared its\nperformance with other open-source recommenders. The results have shown that\nthe proposed system has outperformed several existing hybrid recommender\nsystems.\n","authors":["Pratik K. Biswas","Songlin Liu"],"pdf_url":"https://arxiv.org/pdf/2105.12876v2.pdf","comment":"Expert Systems With Applications, 2022"},{"id":"http://arxiv.org/abs/2207.09102v1","updated":"2022-07-19T06:49:24Z","published":"2022-07-19T06:49:24Z","title":"Identity Testing for High-Dimensional Distributions via Entropy\n  Tensorization","summary":"  We present improved algorithms and matching statistical and computational\nlower bounds for the problem of identity testing $n$-dimensional distributions.\nIn the identity testing problem, we are given as input an explicit distribution\n$\\mu$, an $\\varepsilon>0$, and access to a sampling oracle for a hidden\ndistribution $\\pi$. The goal is to distinguish whether the two distributions\n$\\mu$ and $\\pi$ are identical or are at least $\\varepsilon$-far apart. When\nthere is only access to full samples from the hidden distribution $\\pi$, it is\nknown that exponentially many samples may be needed, and hence previous works\nhave studied identity testing with additional access to various conditional\nsampling oracles. We consider here a significantly weaker conditional sampling\noracle, called the Coordinate Oracle, and provide a fairly complete\ncomputational and statistical characterization of the identity testing problem\nin this new model.\n  We prove that if an analytic property known as approximate tensorization of\nentropy holds for the visible distribution $\\mu$, then there is an efficient\nidentity testing algorithm for any hidden $\\pi$ that uses\n$\\tilde{O}(n/\\varepsilon)$ queries to the Coordinate Oracle. Approximate\ntensorization of entropy is a classical tool for proving optimal mixing time\nbounds of Markov chains for high-dimensional distributions, and recently has\nbeen established for many families of distributions via spectral independence.\nWe complement our algorithmic result for identity testing with a matching\n$\\Omega(n/\\varepsilon)$ statistical lower bound for the number of queries under\nthe Coordinate Oracle. We also prove a computational phase transition: for\nsparse antiferromagnetic Ising models over $\\{+1,-1\\}^n$, in the regime where\napproximate tensorization of entropy fails, there is no efficient identity\ntesting algorithm unless RP=NP.\n","authors":["Antonio Blanca","Zongchen Chen","Daniel Štefankovič","Eric Vigoda"],"pdf_url":"https://arxiv.org/pdf/2207.09102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09099v1","updated":"2022-07-19T06:30:37Z","published":"2022-07-19T06:30:37Z","title":"Analyzing Bagging Methods for Language Models","summary":"  Modern language models leverage increasingly large numbers of parameters to\nachieve performance on natural language understanding tasks. Ensembling these\nmodels in specific configurations for downstream tasks show even further\nperformance improvements. In this paper, we perform an analysis of bagging\nlanguage models and compare single language models to bagged ensembles that are\nroughly equivalent in terms of final model size. We explore an array of model\nbagging configurations for natural language understanding tasks with final\nensemble sizes ranging from 300M parameters to 1.5B parameters and determine\nthat our ensembling methods are at best roughly equivalent to single LM\nbaselines. We note other positive effects of bagging and pruning in specific\nscenarios according to findings in our experiments such as variance reduction\nand minor performance improvements.\n","authors":["Pranab Islam","Shaan Khosla","Arthur Lok","Mudit Saxena"],"pdf_url":"https://arxiv.org/pdf/2207.09099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09097v1","updated":"2022-07-19T06:28:17Z","published":"2022-07-19T06:28:17Z","title":"Lazy Estimation of Variable Importance for Large Neural Networks","summary":"  As opaque predictive models increasingly impact many areas of modern life,\ninterest in quantifying the importance of a given input variable for making a\nspecific prediction has grown. Recently, there has been a proliferation of\nmodel-agnostic methods to measure variable importance (VI) that analyze the\ndifference in predictive power between a full model trained on all variables\nand a reduced model that excludes the variable(s) of interest. A bottleneck\ncommon to these methods is the estimation of the reduced model for each\nvariable (or subset of variables), which is an expensive process that often\ndoes not come with theoretical guarantees. In this work, we propose a fast and\nflexible method for approximating the reduced model with important inferential\nguarantees. We replace the need for fully retraining a wide neural network by a\nlinearization initialized at the full model parameters. By adding a ridge-like\npenalty to make the problem convex, we prove that when the ridge penalty\nparameter is sufficiently large, our method estimates the variable importance\nmeasure with an error rate of $O(\\frac{1}{\\sqrt{n}})$ where $n$ is the number\nof training samples. We also show that our estimator is asymptotically normal,\nenabling us to provide confidence bounds for the VI estimates. We demonstrate\nthrough simulations that our method is fast and accurate under several\ndata-generating regimes, and we demonstrate its real-world applicability on a\nseasonal climate forecasting example.\n","authors":["Yue Gao","Abby Stevens","Rebecca Willet","Garvesh Raskutti"],"pdf_url":"https://arxiv.org/pdf/2207.09097v1.pdf","comment":"Accepted to ICML'22"},{"id":"http://arxiv.org/abs/2207.09094v1","updated":"2022-07-19T06:09:55Z","published":"2022-07-19T06:09:55Z","title":"MoEC: Mixture of Expert Clusters","summary":"  Sparsely Mixture of Experts (MoE) has received great interest due to its\npromising scaling capability with affordable computational overhead. MoE\nconverts dense layers into sparse experts, and utilizes a gated routing network\nto make experts conditionally activated. However, as the number of experts\ngrows, MoE with outrageous parameters suffers from overfitting and sparse data\nallocation. Such problems are especially severe on tasks with limited data,\nthus hindering the progress for MoE models to improve performance by scaling\nup. In this work, we propose Mixture of Expert Clusters - a general approach to\nenable expert layers to learn more diverse and appropriate knowledge by\nimposing variance-based constraints on the routing stage. We further propose a\ncluster-level expert dropout strategy specifically designed for the expert\ncluster structure. Our experiments reveal that MoEC could improve performance\non machine translation and natural language understanding tasks, and raise the\nperformance upper bound for scaling up experts under limited data. We also\nverify that MoEC plays a positive role in mitigating overfitting and sparse\ndata allocation.\n","authors":["Yuan Xie","Shaohan Huang","Tianyu Chen","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2207.09094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.02438v2","updated":"2022-07-19T06:04:19Z","published":"2022-05-05T04:41:27Z","title":"Uncertainty Minimization for Personalized Federated Semi-Supervised\n  Learning","summary":"  Since federated learning (FL) has been introduced as a decentralized learning\ntechnique with privacy preservation, statistical heterogeneity of distributed\ndata stays the main obstacle to achieve robust performance and stable\nconvergence in FL applications. Model personalization methods have been studied\nto overcome this problem. However, existing approaches are mainly under the\nprerequisite of fully labeled data, which is unrealistic in practice due to the\nrequirement of expertise. The primary issue caused by partial-labeled condition\nis that, clients with deficient labeled data can suffer from unfair performance\ngain because they lack adequate insights of local distribution to customize the\nglobal model. To tackle this problem, 1) we propose a novel personalized\nsemi-supervised learning paradigm which allows partial-labeled or unlabeled\nclients to seek labeling assistance from data-related clients (helper agents),\nthus to enhance their perception of local data; 2) based on this paradigm, we\ndesign an uncertainty-based data-relation metric to ensure that selected\nhelpers can provide trustworthy pseudo labels instead of misleading the local\ntraining; 3) to mitigate the network overload introduced by helper searching,\nwe further develop a helper selection protocol to achieve efficient\ncommunication with negligible performance sacrifice. Experiments show that our\nproposed method can obtain superior performance and more stable convergence\nthan other related works with partial labeled data, especially in highly\nheterogeneous setting.\n","authors":["Yanhang Shi","Siguang Chen","Haijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.02438v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2207.09090v1","updated":"2022-07-19T05:55:02Z","published":"2022-07-19T05:55:02Z","title":"Actor-Critic based Improper Reinforcement Learning","summary":"  We consider an improper reinforcement learning setting where a learner is\ngiven $M$ base controllers for an unknown Markov decision process, and wishes\nto combine them optimally to produce a potentially new controller that can\noutperform each of the base ones. This can be useful in tuning across\ncontrollers, learnt possibly in mismatched or simulated environments, to obtain\na good controller for a given target environment with relatively few trials.\n  Towards this, we propose two algorithms: (1) a Policy Gradient-based\napproach; and (2) an algorithm that can switch between a simple Actor-Critic\n(AC) based scheme and a Natural Actor-Critic (NAC) scheme depending on the\navailable information. Both algorithms operate over a class of improper\nmixtures of the given controllers. For the first case, we derive convergence\nrate guarantees assuming access to a gradient oracle. For the AC-based approach\nwe provide convergence rate guarantees to a stationary point in the basic AC\ncase and to a global optimum in the NAC case. Numerical results on (i) the\nstandard control theoretic benchmark of stabilizing an cartpole; and (ii) a\nconstrained queueing task show that our improper policy optimization algorithm\ncan stabilize the system even when the base policies at its disposal are\nunstable.\n","authors":["Mohammadi Zaki","Avinash Mohan","Aditya Gopalan","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2207.09090v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2102.08201"},{"id":"http://arxiv.org/abs/2207.09088v1","updated":"2022-07-19T05:47:45Z","published":"2022-07-19T05:47:45Z","title":"XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection\n  and Forensics","summary":"  In this paper, we proposed XG-BoT, an explainable deep graph neural network\nmodel for botnet node detection. The proposed model is mainly composed of a\nbotnet detector and an explainer for automatic forensics. The XG-BoT detector\ncan effectively detect malicious botnet nodes under large-scale networks.\nSpecifically, it utilizes a grouped reversible residual connection with a graph\nisomorphism network to learn expressive node representations from the botnet\ncommunication graphs. The explainer in XG-BoT can perform automatic network\nforensics by highlighting suspicious network flows and related botnet nodes. We\nevaluated XG-BoT on real-world, large-scale botnet network graphs. Overall,\nXG-BoT is able to outperform the state-of-the-art in terms of evaluation\nmetrics. In addition, we show that the XG-BoT explainer can generate useful\nexplanations based on GNNExplainer for automatic network forensics.\n","authors":["Wai Weng Lo","Siamak Layeghy","Mohanad Sarhan","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2207.09088v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2207.09087v1","updated":"2022-07-19T05:47:30Z","published":"2022-07-19T05:47:30Z","title":"Is Vertical Logistic Regression Privacy-Preserving? A Comprehensive\n  Privacy Analysis and Beyond","summary":"  We consider vertical logistic regression (VLR) trained with mini-batch\ngradient descent -- a setting which has attracted growing interest among\nindustries and proven to be useful in a wide range of applications including\nfinance and medical research. We provide a comprehensive and rigorous privacy\nanalysis of VLR in a class of open-source Federated Learning frameworks, where\nthe protocols might differ between one another, yet a procedure of obtaining\nlocal gradients is implicitly shared. We first consider the honest-but-curious\nthreat model, in which the detailed implementation of protocol is neglected and\nonly the shared procedure is assumed, which we abstract as an oracle. We find\nthat even under this general setting, single-dimension feature and label can\nstill be recovered from the other party under suitable constraints of batch\nsize, thus demonstrating the potential vulnerability of all frameworks\nfollowing the same philosophy. Then we look into a popular instantiation of the\nprotocol based on Homomorphic Encryption (HE). We propose an active attack that\nsignificantly weaken the constraints on batch size in the previous analysis via\ngenerating and compressing auxiliary ciphertext. To address the privacy leakage\nwithin the HE-based protocol, we develop a simple-yet-effective countermeasure\nbased on Differential Privacy (DP), and provide both utility and privacy\nguarantees for the updated algorithm. Finally, we empirically verify the\neffectiveness of our attack and defense on benchmark datasets. Altogether, our\nfindings suggest that all vertical federated learning frameworks that solely\ndepend on HE might contain severe privacy risks, and DP, which has already\ndemonstrated its power in horizontal federated learning, can also play a\ncrucial role in the vertical setting, especially when coupled with HE or secure\nmulti-party computation (MPC) techniques.\n","authors":["Yuzheng Hu","Tianle Cai","Jinyong Shan","Shange Tang","Chaochao Cai","Ethan Song","Bo Li","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2207.09087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09085v1","updated":"2022-07-19T05:43:49Z","published":"2022-07-19T05:43:49Z","title":"Can You Fool AI by Doing a 180? $\\unicode{x2013}$ A Case Study on\n  Authorship Analysis of Texts by Arata Osada","summary":"  This paper is our attempt at answering a twofold question covering the areas\nof ethics and authorship analysis. Firstly, since the methods used for\nperforming authorship analysis imply that an author can be recognized by the\ncontent he or she creates, we were interested in finding out whether it would\nbe possible for an author identification system to correctly attribute works to\nauthors if in the course of years they have undergone a major psychological\ntransition. Secondly, and from the point of view of the evolution of an\nauthor's ethical values, we checked what it would mean if the authorship\nattribution system encounters difficulties in detecting single authorship. We\nset out to answer those questions through performing a binary authorship\nanalysis task using a text classifier based on a pre-trained transformer model\nand a baseline method relying on conventional similarity metrics. For the test\nset, we chose works of Arata Osada, a Japanese educator and specialist in the\nhistory of education, with half of them being books written before the World\nWar II and another half in the 1950s, in between which he underwent a\ntransformation in terms of political opinions. As a result, we were able to\nconfirm that in the case of texts authored by Arata Osada in a time span of\nmore than 10 years, while the classification accuracy drops by a large margin\nand is substantially lower than for texts by other non-fiction writers,\nconfidence scores of the predictions remain at a similar level as in the case\nof a shorter time span, indicating that the classifier was in many instances\ntricked into deciding that texts written over a time span of multiple years\nwere actually written by two different people, which in turn leads us to\nbelieve that such a change can affect authorship analysis, and that historical\nevents have great impact on a person's ethical outlook as expressed in their\nwritings.\n","authors":["Jagna Nieuwazny","Karol Nowakowski","Michal Ptaszynski","Fumito Masui"],"pdf_url":"https://arxiv.org/pdf/2207.09085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09081v1","updated":"2022-07-19T05:31:16Z","published":"2022-07-19T05:31:16Z","title":"Generalizing Goal-Conditioned Reinforcement Learning with Variational\n  Causal Reasoning","summary":"  As a pivotal component to attaining generalizable solutions in human\nintelligence, reasoning provides great potential for reinforcement learning\n(RL) agents' generalization towards varied goals by summarizing part-to-whole\narguments and discovering cause-and-effect relations. However, how to discover\nand represent causalities remains a huge gap that hinders the development of\ncausal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal\nGraph (CG), a structure built upon the relation between objects and events. We\nnovelly formulate the GCRL problem into variational likelihood maximization\nwith CG as latent variables. To optimize the derived objective, we propose a\nframework with theoretical performance guarantees that alternates between two\nsteps: using interventional data to estimate the posterior of CG; using CG to\nlearn generalizable models and interpretable policies. Due to the lack of\npublic benchmarks that verify generalization capability under reasoning, we\ndesign nine tasks and then empirically show the effectiveness of the proposed\nmethod against five baselines on these tasks. Further theoretical analysis\nshows that our performance improvement is attributed to the virtuous cycle of\ncausal discovery, transition modeling, and policy training, which aligns with\nthe experimental evidence in extensive ablation studies.\n","authors":["Wenhao Ding","Haohong Lin","Bo Li","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.09081v1.pdf","comment":"28 pages, 5 figures, under review"},{"id":"http://arxiv.org/abs/2207.09074v1","updated":"2022-07-19T05:21:14Z","published":"2022-07-19T05:21:14Z","title":"Incremental Task Learning with Incremental Rank Updates","summary":"  Incremental Task learning (ITL) is a category of continual learning that\nseeks to train a single network for multiple tasks (one after another), where\ntraining data for each task is only available during the training of that task.\nNeural networks tend to forget older tasks when they are trained for the newer\ntasks; this property is often known as catastrophic forgetting. To address this\nissue, ITL methods use episodic memory, parameter regularization, masking and\npruning, or extensible network structures. In this paper, we propose a new\nincremental task learning framework based on low-rank factorization. In\nparticular, we represent the network weights for each layer as a linear\ncombination of several rank-1 matrices. To update the network for a new task,\nwe learn a rank-1 (or low-rank) matrix and add that to the weights of every\nlayer. We also introduce an additional selector vector that assigns different\nweights to the low-rank matrices learned for the previous tasks. We show that\nour approach performs better than the current state-of-the-art methods in terms\nof accuracy and forgetting. Our method also offers better memory efficiency\ncompared to episodic memory- and mask-based approaches. Our code will be\navailable at https://github.com/CSIPlab/task-increment-rank-update.git\n","authors":["Rakib Hyder","Ken Shao","Boyu Hou","Panos Markopoulos","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2207.09074v1.pdf","comment":"Code will be available at\n  https://github.com/CSIPlab/task-increment-rank-update.git"},{"id":"http://arxiv.org/abs/1811.11881v8","updated":"2022-07-19T05:21:00Z","published":"2018-11-28T23:43:11Z","title":"Adversarial Bandits with Knapsacks","summary":"  We consider Bandits with Knapsacks (henceforth, BwK), a general model for\nmulti-armed bandits under supply/budget constraints. In particular, a bandit\nalgorithm needs to solve a well-known knapsack problem: find an optimal packing\nof items into a limited-size knapsack. The BwK problem is a common\ngeneralization of numerous motivating examples, which range from dynamic\npricing to repeated auctions to dynamic ad allocation to network routing and\nscheduling. While the prior work on BwK focused on the stochastic version, we\npioneer the other extreme in which the outcomes can be chosen adversarially.\nThis is a considerably harder problem, compared to both the stochastic version\nand the \"classic\" adversarial bandits, in that regret minimization is no longer\nfeasible. Instead, the objective is to minimize the competitive ratio: the\nratio of the benchmark reward to the algorithm's reward.\n  We design an algorithm with competitive ratio O(log T) relative to the best\nfixed distribution over actions, where T is the time horizon; we also prove a\nmatching lower bound. The key conceptual contribution is a new perspective on\nthe stochastic version of the problem. We suggest a new algorithm for the\nstochastic version, which builds on the framework of regret minimization in\nrepeated games and admits a substantially simpler analysis compared to prior\nwork. We then analyze this algorithm for the adversarial version and use it as\na subroutine to solve the latter.\n","authors":["Nicole Immorlica","Karthik Abinav Sankararaman","Robert Schapire","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/1811.11881v8.pdf","comment":"The extended abstract appeared in FOCS 2019. The definitive version\n  was published in JACM '22"},{"id":"http://arxiv.org/abs/2112.00378v2","updated":"2022-07-19T05:07:05Z","published":"2021-12-01T09:55:01Z","title":"$\\ell_\\infty$-Robustness and Beyond: Unleashing Efficient Adversarial\n  Training","summary":"  Neural networks are vulnerable to adversarial attacks: adding well-crafted,\nimperceptible perturbations to their input can modify their output. Adversarial\ntraining is one of the most effective approaches in training robust models\nagainst such attacks. However, it is much slower than vanilla training of\nneural networks since it needs to construct adversarial examples for the entire\ntraining data at every iteration, hampering its effectiveness. Recently, Fast\nAdversarial Training (FAT) was proposed that can obtain robust models\nefficiently. However, the reasons behind its success are not fully understood,\nand more importantly, it can only train robust models for $\\ell_\\infty$-bounded\nattacks as it uses FGSM during training. In this paper, by leveraging the\ntheory of coreset selection, we show how selecting a small subset of training\ndata provides a general, more principled approach toward reducing the time\ncomplexity of robust training. Unlike existing methods, our approach can be\nadapted to a wide variety of training objectives, including TRADES,\n$\\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental\nresults indicate that our approach speeds up adversarial training by 2-3 times\nwhile experiencing a slight reduction in the clean and robust accuracy.\n","authors":["Hadi M. Dolatabadi","Sarah Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2112.00378v2.pdf","comment":"Accepted to the 17th European Conference on Computer Vision (ECCV\n  2022)"},{"id":"http://arxiv.org/abs/2207.09071v1","updated":"2022-07-19T04:58:06Z","published":"2022-07-19T04:58:06Z","title":"Learning Action Translator for Meta Reinforcement Learning on\n  Sparse-Reward Tasks","summary":"  Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of\ntraining tasks simultaneously and quickly adapting to new tasks. It requires\nmassive amounts of data drawn from training tasks to infer the common structure\nshared among tasks. Without heavy reward engineering, the sparse rewards in\nlong-horizon tasks exacerbate the problem of sample efficiency in meta-RL.\nAnother challenge in meta-RL is the discrepancy of difficulty level among\ntasks, which might cause one easy task dominating learning of the shared policy\nand thus preclude policy adaptation to new tasks. This work introduces a novel\nobjective function to learn an action translator among training tasks. We\ntheoretically verify that the value of the transferred policy with the action\ntranslator can be close to the value of the source policy and our objective\nfunction (approximately) upper bounds the value difference. We propose to\ncombine the action translator with context-based meta-RL algorithms for better\ndata collection and more efficient exploration during meta-training. Our\napproach empirically improves the sample efficiency and performance of meta-RL\nalgorithms on sparse-reward tasks.\n","authors":["Yijie Guo","Qiucheng Wu","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2207.09071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07990v3","updated":"2022-07-19T04:52:24Z","published":"2022-06-16T08:01:19Z","title":"Patch-level Representation Learning for Self-supervised Vision\n  Transformers","summary":"  Recent self-supervised learning (SSL) methods have shown impressive results\nin learning visual representations from unlabeled images. This paper aims to\nimprove their performance further by utilizing the architectural advantages of\nthe underlying neural network, as the current state-of-the-art visual pretext\ntasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.\nIn particular, we focus on Vision Transformers (ViTs), which have gained much\nattention recently as a better architectural choice, often outperforming\nconvolutional networks for various visual tasks. The unique characteristic of\nViT is that it takes a sequence of disjoint patches from an image and processes\npatch-level representations internally. Inspired by this, we design a simple\nyet effective visual pretext task, coined SelfPatch, for learning better\npatch-level representations. To be specific, we enforce invariance against each\npatch and its neighbors, i.e., each patch treats similar neighboring patches as\npositive samples. Consequently, training ViTs with SelfPatch learns more\nsemantically meaningful relations among patches (without using human-annotated\nlabels), which can be beneficial, in particular, to downstream tasks of a dense\nprediction type. Despite its simplicity, we demonstrate that it can\nsignificantly improve the performance of existing SSL methods for various\nvisual tasks, including object detection and semantic segmentation.\nSpecifically, SelfPatch significantly improves the recent self-supervised ViT,\nDINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance\nsegmentation, and +2.9 mIoU on ADE20K semantic segmentation.\n","authors":["Sukmin Yun","Hankook Lee","Jaehyung Kim","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2206.07990v3.pdf","comment":"Accepted to CVPR 2022 (Oral). Code is available at\n  https://github.com/alinlab/SelfPatch"},{"id":"http://arxiv.org/abs/2204.00777v2","updated":"2022-07-19T04:47:50Z","published":"2022-04-02T06:25:48Z","title":"Revealing the CO2 emission reduction of ridesplitting and its\n  determinants based on real-world data","summary":"  Ridesplitting, which is a form of pooled ridesourcing service, has great\npotential to alleviate the negative impacts of ridesourcing on the environment.\nHowever, most existing studies only explored its theoretical environmental\nbenefits based on optimization models and simulations. By contrast, this study\naims to reveal the real-world emission reduction of ridesplitting and its\ndeterminants based on the observed data of ridesourcing in Chengdu, China.\nIntegrating the trip data with the COPERT model, this study calculates the CO2\nemissions of shared rides (ridesplitting) and their substituted single rides\n(regular ridesourcing) to estimate the CO2 emission reduction of each\nridesplitting trip. The results show that not all ridesplitting trips reduce\nemissions from ridesourcing in the real world. The CO2 emission reduction rate\nof ridesplitting varies from trip to trip, averaging at 43.15g/km. Then,\ninterpretable machine learning models, gradient boosting machines, are applied\nto explore the relationship between the CO2 emission reduction rate of\nridesplitting and its determinants. Based on the SHapley Additive exPlanations\n(SHAP) method, the overlap rate and detour rate of shared rides are identified\nto be the most important factors that determine the CO2 emission reduction rate\nof ridesplitting. Increasing the overlap rate, the number of shared rides,\naverage speed, and ride distance ratio while decreasing the detour rate, actual\ntrip distance, and ride distance gap can increase the CO2 emission reduction\nrate of ridesplitting. In addition, nonlinear effects and interactions of the\ndeterminants are examined through the partial dependence plots. To sum up, this\nstudy provides a scientific method for the government and ridesourcing\ncompanies to better assess and optimize the environmental benefits of\nridesplitting.\n","authors":["Wenxiang Li","Yuanyuan Li","Ziyuan Pu","Long Cheng","Lei Wang","Linchuan Yang"],"pdf_url":"https://arxiv.org/pdf/2204.00777v2.pdf","comment":"35 pages, 13 figures"},{"id":"http://arxiv.org/abs/2207.09067v1","updated":"2022-07-19T04:44:08Z","published":"2022-07-19T04:44:08Z","title":"Time Is MattEr: Temporal Self-supervision for Video Transformers","summary":"  Understanding temporal dynamics of video is an essential aspect of learning\nbetter video representations. Recently, transformer-based architectural designs\nhave been extensively explored for video tasks due to their capability to\ncapture long-term dependency of input sequences. However, we found that these\nVideo Transformers are still biased to learn spatial dynamics rather than\ntemporal ones, and debiasing the spurious correlation is critical for their\nperformance. Based on the observations, we design simple yet effective\nself-supervised tasks for video models to learn temporal dynamics better.\nSpecifically, for debiasing the spatial bias, our method learns the temporal\norder of video frames as extra self-supervision and enforces the randomly\nshuffled frames to have low-confidence outputs. Also, our method learns the\ntemporal flow direction of video tokens among consecutive frames for enhancing\nthe correlation toward temporal dynamics. Under various video action\nrecognition tasks, we demonstrate the effectiveness of our method and its\ncompatibility with state-of-the-art Video Transformers.\n","authors":["Sukmin Yun","Jaehyung Kim","Dongyoon Han","Hwanjun Song","Jung-Woo Ha","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2207.09067v1.pdf","comment":"Accepted to ICML 2022. Code is available at\n  https://github.com/alinlab/temporal-selfsupervision"},{"id":"http://arxiv.org/abs/2202.08146v3","updated":"2022-07-19T04:41:59Z","published":"2022-02-16T15:40:52Z","title":"A Prospective Approach for Human-to-Human Interaction Recognition from\n  Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural\n  Network with GUI Application Implementation","summary":"  Recent advances in 5G wireless technology and socioeconomic transformation\nhave brought a paradigm shift in sensor applications. Wi-Fi signal demonstrates\na strong correlation between its temporal variation and body movements, which\ncan be leveraged to recognize human activity. In this article, we demonstrate\nthe cognitive ability of device free mutual human-to-human interaction\nrecognition method based on the time scale Wi-Fi channel state information. The\nmutual activities examined are steady-state, approaching, departing,\nhandshaking, high-five, hugging, kicking (left-leg), kicking (right-leg),\npointing (left-hand), pointing (right-hand), punching(left-hand), punching\n(right-hand), and pushing. We explore and propose a Self-Attention furnished\nBidirectional Gated Recurrent Neural Network model to classify 13\nhuman-to-human mutual interaction types from the time-series data. Our proposed\nmodel can recognize a two subject pair mutual interaction with a maximum\nbenchmark accuracy of 94%. This has been expanded for ten subject pairs, which\nsecured a benchmark accuracy of 88% with improved classification around the\ninteraction-transition region. Also, an executable graphical user interface\n(GUI) is developed, using the PyQt5 python module, to subsequently display the\noverall mutual human-interaction recognition procedure in real-time. Finally,\nwe conclude with a brief discourse regarding the possible solutions to the\nhandicaps that resulted in curtailments observed during the study. Such, Wi-Fi\nchannel perturbation pattern analysis is believed to be an efficient,\neconomical and privacy-friendly approach to be potentially utilized in mutual\nhuman-interaction recognition for indoor activity monitoring, surveillance\nsystem, smart health monitoring systems and independent assisted living.\n","authors":["Md. Mohi Uddin Khan","Abdullah Bin Shams","Md. Mohsin Sarker Raihan"],"pdf_url":"https://arxiv.org/pdf/2202.08146v3.pdf","comment":"24 Pages. This is the Pre-print version article submitted for\n  Peer-Review to a prestigious journal"},{"id":"http://arxiv.org/abs/2111.08861v5","updated":"2022-07-19T04:29:38Z","published":"2021-11-17T01:55:01Z","title":"A label-efficient two-sample test","summary":"  Two-sample tests evaluate whether two samples are realizations of the same\ndistribution (the null hypothesis) or two different distributions (the\nalternative hypothesis). We consider a new setting for this problem where\nsample features are easily measured whereas sample labels are unknown and\ncostly to obtain. Accordingly, we devise a three-stage framework in service of\nperforming an effective two-sample test with only a small number of sample\nlabel queries: first, a classifier is trained with samples uniformly labeled to\nmodel the posterior probabilities of the labels; second, a novel query scheme\ndubbed \\emph{bimodal query} is used to query labels of samples from both\nclasses, and last, the classical Friedman-Rafsky (FR) two-sample test is\nperformed on the queried samples. Theoretical analysis and extensive\nexperiments performed on several datasets demonstrate that the proposed test\ncontrols the Type I error and has decreased Type II error relative to uniform\nquerying and certainty-based querying. Source code for our algorithms and\nexperimental results is available at\n\\url{https://github.com/wayne0908/Label-Efficient-Two-Sample}.\n","authors":["Weizhi Li","Gautam Dasarathy","Karthikeyan Natesan Ramamurthy","Visar Berisha"],"pdf_url":"https://arxiv.org/pdf/2111.08861v5.pdf","comment":"Accepted to the 38th conference on Uncertainty in Artificial\n  Intelligence (UAI2022)"},{"id":"http://arxiv.org/abs/2202.00211v3","updated":"2022-07-19T04:24:50Z","published":"2022-02-01T04:19:50Z","title":"GNNRank: Learning Global Rankings from Pairwise Comparisons via Directed\n  Graph Neural Networks","summary":"  Recovering global rankings from pairwise comparisons has wide applications\nfrom time synchronization to sports team ranking. Pairwise comparisons\ncorresponding to matches in a competition can be construed as edges in a\ndirected graph (digraph), whose nodes represent e.g. competitors with an\nunknown rank. In this paper, we introduce neural networks into the ranking\nrecovery problem by proposing the so-called GNNRank, a trainable GNN-based\nframework with digraph embedding. Moreover, new objectives are devised to\nencode ranking upsets/violations. The framework involves a ranking score\nestimation approach, and adds an inductive bias by unfolding the Fiedler vector\ncomputation of the graph constructed from a learnable similarity matrix.\nExperimental results on extensive data sets show that our methods attain\ncompetitive and often superior performance against baselines, as well as\nshowing promising transfer ability. Codes and preprocessed data are at:\n\\url{https://github.com/SherylHYX/GNNRank}.\n","authors":["Yixuan He","Quan Gan","David Wipf","Gesine Reinert","Junchi Yan","Mihai Cucuringu"],"pdf_url":"https://arxiv.org/pdf/2202.00211v3.pdf","comment":"ICML 2022 spotlight; 32 pages (9 pages for main text)"},{"id":"http://arxiv.org/abs/2207.09061v1","updated":"2022-07-19T04:22:27Z","published":"2022-07-19T04:22:27Z","title":"A-SFS: Semi-supervised Feature Selection based on Multi-task\n  Self-supervision","summary":"  Feature selection is an important process in machine learning. It builds an\ninterpretable and robust model by selecting the features that contribute the\nmost to the prediction target. However, most mature feature selection\nalgorithms, including supervised and semi-supervised, fail to fully exploit the\ncomplex potential structure between features. We believe that these structures\nare very important for the feature selection process, especially when labels\nare lacking and data is noisy.\n  To this end, we innovatively introduce a deep learning-based self-supervised\nmechanism into feature selection problems, namely batch-Attention-based\nSelf-supervision Feature Selection(A-SFS). Firstly, a multi-task\nself-supervised autoencoder is designed to uncover the hidden structure among\nfeatures with the support of two pretext tasks. Guided by the integrated\ninformation from the multi-self-supervised learning model, a batch-attention\nmechanism is designed to generate feature weights according to batch-based\nfeature selection patterns to alleviate the impacts introduced by a handful of\nnoisy data. This method is compared to 14 major strong benchmarks, including\nLightGBM and XGBoost. Experimental results show that A-SFS achieves the highest\naccuracy in most datasets. Furthermore, this design significantly reduces the\nreliance on labels, with only 1/10 labeled data needed to achieve the same\nperformance as those state of art baselines. Results show that A-SFS is also\nmost robust to the noisy and missing data.\n","authors":["Zhifeng Qiu","Wanxin Zeng","Dahua Liao","Ning Gui"],"pdf_url":"https://arxiv.org/pdf/2207.09061v1.pdf","comment":"18 pages, 7 figures, accepted by knowledge-based systems"},{"id":"http://arxiv.org/abs/2207.09060v1","updated":"2022-07-19T04:20:39Z","published":"2022-07-19T04:20:39Z","title":"Data Science and Machine Learning in Education","summary":"  The growing role of data science (DS) and machine learning (ML) in\nhigh-energy physics (HEP) is well established and pertinent given the complex\ndetectors, large data, sets and sophisticated analyses at the heart of HEP\nresearch. Moreover, exploiting symmetries inherent in physics data have\ninspired physics-informed ML as a vibrant sub-field of computer science\nresearch. HEP researchers benefit greatly from materials widely available\nmaterials for use in education, training and workforce development. They are\nalso contributing to these materials and providing software to DS/ML-related\nfields. Increasingly, physics departments are offering courses at the\nintersection of DS, ML and physics, often using curricula developed by HEP\nresearchers and involving open software and data used in HEP. In this white\npaper, we explore synergies between HEP research and DS/ML education, discuss\nopportunities and challenges at this intersection, and propose community\nactivities that will be mutually beneficial.\n","authors":["Gabriele Benelli","Thomas Y. Chen","Javier Duarte","Matthew Feickert","Matthew Graham","Lindsey Gray","Dan Hackett","Phil Harris","Shih-Chieh Hsu","Gregor Kasieczka","Elham E. Khoda","Matthias Komm","Mia Liu","Mark S. Neubauer","Scarlet Norberg","Alexx Perloff","Marcel Rieger","Claire Savard","Kazuhiro Terao","Savannah Thais","Avik Roy","Jean-Roch Vlimant","Grigorios Chachamis"],"pdf_url":"https://arxiv.org/pdf/2207.09060v1.pdf","comment":"Contribution to Snowmass 2021"},{"id":"http://arxiv.org/abs/2206.05263v2","updated":"2022-07-19T04:12:34Z","published":"2022-06-10T17:59:11Z","title":"Causal Balancing for Domain Generalization","summary":"  While machine learning models rapidly advance the state-of-the-art on various\nreal-world tasks, out-of-domain (OOD) generalization remains a challenging\nproblem given the vulnerability of these models to spurious correlations. We\npropose a causally-motivated balanced mini-batch sampling strategy to transform\nthe observed train distribution to a balanced distribution that is free of\nspurious correlations. We argue that the Bayes optimal classifier trained on\nsuch balanced distribution is minimax optimal across a diverse enough\nenvironment space. We also provide an identifiability guarantee of the latent\nvariable model of the proposed underlying data generation process with\ninvariant causal mechanisms, by utilizing enough number of train environments.\nExperiments are conducted on three domain generalization datasets,\ndemonstrating empirically that our balanced mini-batch sampling strategy\nimproves the performance of four different established domain generalization\nmodel baselines compared to the random mini-batch sampling strategy.\n","authors":["Xinyi Wang","Michael Saxon","Jiachen Li","Hongyang Zhang","Kun Zhang","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2206.05263v2.pdf","comment":"16 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2207.09052v1","updated":"2022-07-19T03:48:59Z","published":"2022-07-19T03:48:59Z","title":"Balanced Contrastive Learning for Long-Tailed Visual Recognition","summary":"  Real-world data typically follow a long-tailed distribution, where a few\nmajority categories occupy most of the data while most minority categories\ncontain a limited number of samples. Classification models minimizing\ncross-entropy struggle to represent and classify the tail classes. Although the\nproblem of learning unbiased classifiers has been well studied, methods for\nrepresenting imbalanced data are under-explored. In this paper, we focus on\nrepresentation learning for imbalanced data. Recently, supervised contrastive\nlearning has shown promising performance on balanced data recently. However,\nthrough our theoretical analysis, we find that for long-tailed data, it fails\nto form a regular simplex which is an ideal geometric configuration for\nrepresentation learning. To correct the optimization behavior of SCL and\nfurther improve the performance of long-tailed visual recognition, we propose a\nnovel loss for balanced contrastive learning (BCL). Compared with SCL, we have\ntwo improvements in BCL: class-averaging, which balances the gradient\ncontribution of negative classes; class-complement, which allows all classes to\nappear in every mini-batch. The proposed balanced contrastive learning (BCL)\nmethod satisfies the condition of forming a regular simplex and assists the\noptimization of cross-entropy. Equipped with BCL, the proposed two-branch\nframework can obtain a stronger feature representation and achieve competitive\nperformance on long-tailed benchmark datasets such as CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist2018. Our code is available at\n\\href{https://github.com/FlamieZhu/BCL}{this URL}.\n","authors":[" Jianggang"," Zhu"," Zheng"," Wang"," Jingjing"," Chen","Yi-Ping Phoebe"," Chen"," Yu-Gang"," Jiang"],"pdf_url":"https://arxiv.org/pdf/2207.09052v1.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/2207.09050v1","updated":"2022-07-19T03:38:43Z","published":"2022-07-19T03:38:43Z","title":"Don't Forget to Buy Milk: Contextually Aware Grocery Reminder Household\n  Robot","summary":"  Assistive robots operating in household environments would require items to\nbe available in the house to perform assistive tasks. However, when these items\nrun out, the assistive robot must remind its user to buy the missing items. In\nthis paper, we present a computational architecture that can allow a robot to\nlearn personalized contextual knowledge of a household through interactions\nwith its user. The architecture can then use the learned knowledge to make\npredictions about missing items from the household over a long period of time.\nThe architecture integrates state-of-the-art perceptual learning algorithms,\ncognitive models of memory encoding and learning, a reasoning module for\npredicting missing items from the household, and a graphical user interface\n(GUI) to interact with the user. The architecture is integrated with the Fetch\nmobile manipulator robot and validated in a large indoor environment with\nmultiple contexts and objects. Our experimental results show that the robot can\nadapt to an environment by learning contextual knowledge through interactions\nwith its user. The robot can also use the learned knowledge to correctly\npredict missing items over multiple weeks and it is robust against sensory and\nperceptual errors.\n","authors":["Ali Ayub","Chrystopher L. Nehaniv","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2207.09050v1.pdf","comment":"Accepted at IEEE ICDL 2022"},{"id":"http://arxiv.org/abs/2207.09049v1","updated":"2022-07-19T03:38:34Z","published":"2022-07-19T03:38:34Z","title":"RepBNN: towards a precise Binary Neural Network with Enhanced Feature\n  Map via Repeating","summary":"  Binary neural network (BNN) is an extreme quantization version of\nconvolutional neural networks (CNNs) with all features and weights mapped to\njust 1-bit. Although BNN saves a lot of memory and computation demand to make\nCNN applicable on edge or mobile devices, BNN suffers the drop of network\nperformance due to the reduced representation capability after binarization. In\nthis paper, we propose a new replaceable and easy-to-use convolution module\nRepConv, which enhances feature maps through replicating input or output along\nchannel dimension by $\\beta$ times without extra cost on the number of\nparameters and convolutional computation. We also define a set of RepTran rules\nto use RepConv throughout BNN modules like binary convolution, fully connected\nlayer and batch normalization. Experiments demonstrate that after the RepTran\ntransformation, a set of highly cited BNNs have achieved universally better\nperformance than the original BNN versions. For example, the Top-1 accuracy of\nRep-ReCU-ResNet-20, i.e., a RepBconv enhanced ReCU-ResNet-20, reaches 88.97% on\nCIFAR-10, which is 1.47% higher than that of the original network. And\nRep-AdamBNN-ReActNet-A achieves 71.342% Top-1 accuracy on ImageNet, a fresh\nstate-of-the-art result of BNNs. Code and models are available\nat:https://github.com/imfinethanks/Rep_AdamBNN.\n","authors":["Xulong Shi","Zhi Qi","Jiaxuan Cai","Keqi Fu","Yaru Zhao","Zan Li","Xuanyu Liu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2207.09049v1.pdf","comment":"This paper has absolutely nothing to do with repvgg, rep means\n  repeating"},{"id":"http://arxiv.org/abs/2001.06988v2","updated":"2022-07-19T03:12:56Z","published":"2020-01-20T05:56:32Z","title":"Deep learning generates custom-made logistic regression models for\n  explaining how breast cancer subtypes are classified","summary":"  Differentiating the intrinsic subtypes of breast cancer is crucial for\ndeciding the best treatment strategy. Deep learning can predict the subtypes\nfrom genetic information more accurately than conventional statistical methods,\nbut to date, deep learning has not been directly utilized to examine which\ngenes are associated with which subtypes. To clarify the mechanisms embedded in\nthe intrinsic subtypes, we developed an explainable deep learning model called\na point-wise linear (PWL) model that generates a custom-made logistic\nregression for each patient. Logistic regression, which is familiar to both\nphysicians and medical informatics researchers, allows us to analyze the\nimportance of the feature variables, and the PWL model harnesses these\npractical abilities of logistic regression. In this study, we show that\nanalyzing breast cancer subtypes is clinically beneficial for patients and one\nof the best ways to validate the capability of the PWL model. First, we trained\nthe PWL model with RNA-seq data to predict PAM50 intrinsic subtypes and applied\nit to the 41/50 genes of PAM50 through the subtype prediction task. Second, we\ndeveloped a deep enrichment analysis method to reveal the relationships between\nthe PAM50 subtypes and the copy numbers of breast cancer. Our findings showed\nthat the PWL model utilized genes relevant to the cell cycle-related pathways.\nThese preliminary successes in breast cancer subtype analysis demonstrate the\npotential of our analysis strategy to clarify the mechanisms underlying breast\ncancer and improve overall clinical outcomes.\n","authors":["Takuma Shibahara","Chisa Wada","Yasuho Yamashita","Kazuhiro Fujita","Masamichi Sato","Junichi Kuwata","Atsushi Okamoto","Yoshimasa Ono"],"pdf_url":"https://arxiv.org/pdf/2001.06988v2.pdf","comment":"25 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.08629v2","updated":"2022-07-19T02:53:45Z","published":"2022-07-18T14:23:31Z","title":"Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) tend to suffer from high computation costs due\nto the exponentially increasing scale of graph data and the number of model\nparameters, which restricts their utility in practical applications. To this\nend, some recent works focus on sparsifying GNNs with the lottery ticket\nhypothesis (LTH) to reduce inference costs while maintaining performance\nlevels. However, the LTH-based methods suffer from two major drawbacks: 1) they\nrequire exhaustive and iterative training of dense models, resulting in an\nextremely large training computation cost, and 2) they only trim graph\nstructures and model parameters but ignore the node feature dimension, where\nsignificant redundancy exists. To overcome the above limitations, we propose a\ncomprehensive graph gradual pruning framework termed CGP. This is achieved by\ndesigning a during-training graph pruning paradigm to dynamically prune GNNs\nwithin one training process. Unlike LTH-based methods, the proposed CGP\napproach requires no re-training, which significantly reduces the computation\ncosts. Furthermore, we design a co-sparsifying strategy to comprehensively trim\nall three core elements of GNNs: graph structures, node features, and model\nparameters. Meanwhile, aiming at refining the pruning operation, we introduce a\nregrowth process into our CGP framework, in order to re-establish the pruned\nbut important connections. The proposed CGP is evaluated by using a node\nclassification task across 6 GNN architectures, including shallow models (GCN\nand GAT), shallow-but-deep-propagation models (SGC and APPNP), and deep models\n(GCNII and ResGCN), on a total of 14 real-world graph datasets, including\nlarge-scale graph datasets from the challenging Open Graph Benchmark.\nExperiments reveal that our proposed strategy greatly improves both training\nand inference efficiency while matching or even exceeding the accuracy of\nexisting methods.\n","authors":["Chuang Liu","Xueqi Ma","Yibing Zhan","Liang Ding","Dapeng Tao","Bo Du","Wenbin Hu","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2207.08629v2.pdf","comment":"29 pages, 27 figures, submitting to IEEE TNNLS"},{"id":"http://arxiv.org/abs/2207.09031v1","updated":"2022-07-19T02:36:36Z","published":"2022-07-19T02:36:36Z","title":"Decorrelative Network Architecture for Robust Electrocardiogram\n  Classification","summary":"  Artificial intelligence has made great progresses in medical data analysis,\nbut the lack of robustness and interpretability has kept these methods from\nbeing widely deployed. In particular, data-driven models are vulnerable to\nadversarial attacks, which are small, targeted perturbations that dramatically\ndegrade model performance. As a recent example, while deep learning has shown\nimpressive performance in electrocardiogram (ECG) classification, Han et al.\ncrafted realistic perturbations that fooled the network 74% of the time [2020].\nCurrent adversarial defense paradigms are computationally intensive and\nimpractical for many high dimensional problems. Previous research indicates\nthat a network vulnerability is related to the features learned during\ntraining. We propose a novel approach based on ensemble decorrelation and\nFourier partitioning for training parallel network arms into a decorrelated\narchitecture to learn complementary features, significantly reducing the chance\nof a perturbation fooling all arms of the deep learning model. We test our\napproach in ECG classification, demonstrating a much-improved 77.2% chance of\nat least one correct network arm on the strongest adversarial attack tested, in\ncontrast to a 21.7% chance from a comparable ensemble. Our approach does not\nrequire expensive optimization with adversarial samples, and thus can be scaled\nto large problems. These methods can easily be applied to other tasks for\nimproved network robustness.\n","authors":["Christopher Wiedeman","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09031v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2207.07931v2","updated":"2022-07-19T02:36:17Z","published":"2022-07-16T12:53:52Z","title":"Learnable Mixed-precision and Dimension Reduction Co-design for\n  Low-storage Activation","summary":"  Recently, deep convolutional neural networks (CNNs) have achieved many\neye-catching results. However, deploying CNNs on resource-constrained edge\ndevices is constrained by limited memory bandwidth for transmitting large\nintermediated data during inference, i.e., activation. Existing research\nutilizes mixed-precision and dimension reduction to reduce computational\ncomplexity but pays less attention to its application for activation\ncompression. To further exploit the redundancy in activation, we propose a\nlearnable mixed-precision and dimension reduction co-design system, which\nseparates channels into groups and allocates specific compression policies\naccording to their importance. In addition, the proposed dynamic searching\ntechnique enlarges search space and finds out the optimal bit-width allocation\nautomatically. Our experimental results show that the proposed methods improve\n3.54%/1.27% in accuracy and save 0.18/2.02 bits per value over existing\nmixed-precision methods on ResNet18 and MobileNetv2, respectively.\n","authors":["Yu-Shan Tai","Cheng-Yang Chang","Chieh-Fang Teng"," AnYeu"," Wu"],"pdf_url":"https://arxiv.org/pdf/2207.07931v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.09320v1","updated":"2022-07-19T15:19:03Z","published":"2022-07-19T15:19:03Z","title":"Group Validation in Recommender Systems: Framework for Multi-layer\n  Performance Evaluation","summary":"  Interpreting the performance results of models that attempt to realize user\nbehavior in platforms that employ recommenders is a big challenge that\nresearchers and practitioners continue to face. Although current evaluation\ntools possess the capacity to provide solid general overview of a system's\nperformance, they still lack consistency and effectiveness in their use as\nevident in most recent studies on the topic. Current traditional assessment\ntechniques tend to fail to detect variations that could occur on smaller\nsubsets of the data and lack the ability to explain how such variations affect\nthe overall performance. In this article, we focus on the concept of data\nclustering for evaluation in recommenders and apply a neighborhood assessment\nmethod for the datasets of recommender system applications. This new method,\nnamed neighborhood-based evaluation, aids in better understanding critical\nperformance variations in more compact subsets of the system to help spot\nweaknesses where such variations generally go unnoticed with conventional\nmetrics and are typically averaged out. This new modular evaluation layer\ncomplements the existing assessment mechanisms and provides the possibility of\nseveral applications to the recommender ecosystem such as model evolution\ntests, fraud/attack detection and a possibility for hosting a hybrid model\nsetup.\n","authors":["Wissam Al Jurdi","Jacques Bou Abdo","Jacques Demerjian","Abdallah Makhoul"],"pdf_url":"https://arxiv.org/pdf/2207.09320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.12876v2","updated":"2022-07-19T07:20:58Z","published":"2021-05-26T23:10:51Z","title":"A Hybrid Recommender System for Recommending Smartphones to Prospective\n  Customers","summary":"  Recommender Systems are a subclass of machine learning systems that employ\nsophisticated information filtering strategies to reduce the search time and\nsuggest the most relevant items to any particular user. Hybrid recommender\nsystems combine multiple recommendation strategies in different ways to benefit\nfrom their complementary advantages. Some hybrid recommender systems have\ncombined collaborative filtering and content-based approaches to build systems\nthat are more robust. In this paper, we propose a hybrid recommender system,\nwhich combines Alternating Least Squares (ALS) based collaborative filtering\nwith deep learning to enhance recommendation performance as well as overcome\nthe limitations associated with the collaborative filtering approach,\nespecially concerning its cold start problem. In essence, we use the outputs\nfrom ALS (collaborative filtering) to influence the recommendations from a Deep\nNeural Network (DNN), which combines characteristic, contextual, structural and\nsequential information, in a big data processing framework. We have conducted\nseveral experiments in testing the efficacy of the proposed hybrid architecture\nin recommending smartphones to prospective customers and compared its\nperformance with other open-source recommenders. The results have shown that\nthe proposed system has outperformed several existing hybrid recommender\nsystems.\n","authors":["Pratik K. Biswas","Songlin Liu"],"pdf_url":"https://arxiv.org/pdf/2105.12876v2.pdf","comment":"Expert Systems With Applications, 2022"},{"id":"http://arxiv.org/abs/2207.09051v1","updated":"2022-07-19T03:45:38Z","published":"2022-07-19T03:45:38Z","title":"HICF: Hyperbolic Informative Collaborative Filtering","summary":"  Considering the prevalence of the power-law distribution in user-item\nnetworks, hyperbolic space has attracted considerable attention and achieved\nimpressive performance in the recommender system recently. The advantage of\nhyperbolic recommendation lies in that its exponentially increasing capacity is\nwell-suited to describe the power-law distributed user-item network whereas the\nEuclidean equivalent is deficient. Nonetheless, it remains unclear which kinds\nof items can be effectively recommended by the hyperbolic model and which\ncannot. To address the above concerns, we take the most basic recommendation\ntechnique, collaborative filtering, as a medium, to investigate the behaviors\nof hyperbolic and Euclidean recommendation models. The results reveal that (1)\ntail items get more emphasis in hyperbolic space than that in Euclidean space,\nbut there is still ample room for improvement; (2) head items receive modest\nattention in hyperbolic space, which could be considerably improved; (3) and\nnonetheless, the hyperbolic models show more competitive performance than\nEuclidean models. Driven by the above observations, we design a novel learning\nmethod, named hyperbolic informative collaborative filtering (HICF), aiming to\ncompensate for the recommendation effectiveness of the head item while at the\nsame time improving the performance of the tail item. The main idea is to adapt\nthe hyperbolic margin ranking learning, making its pull and push procedure\ngeometric-aware, and providing informative guidance for the learning of both\nhead and tail items. Extensive experiments back up the analytic findings and\nalso show the effectiveness of the proposed method. The work is valuable for\npersonalized recommendations since it reveals that the hyperbolic space\nfacilitates modeling the tail item, which often represents user-customized\npreferences or new products.\n","authors":["Menglin Yang","Zhihao Li","Min Zhou","Jiahong Liu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2207.09051v1.pdf","comment":"Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining (KDD '22)"},{"id":"http://arxiv.org/abs/2207.08087v2","updated":"2022-07-19T03:40:45Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various NLP and IR\ndownstream applications have benefited from ESE due to its ability to discover\nknowledge. Although existing bootstrapping methods have achieved great\nprogress, most of them still rely on manually pre-defined context patterns. A\nnon-negligible shortcoming of the pre-defined context patterns is that they\ncannot be flexibly generalized to all kinds of semantic classes, and we call\nthis phenomenon as \"semantic sensitivity\". To address this problem, we devise a\ncontext pattern generation module that utilizes autoregressive language models\n(e.g., GPT-2) to automatically generate high-quality context patterns for\nentities. In addition, we propose the GAPA, a novel ESE framework that\nleverages the aforementioned GenerAted PAtterns to expand target entities.\nExtensive experiments and detailed analyses on three widely used datasets\ndemonstrate the effectiveness of our method. All the codes of our experiments\nwill be available for reproducibility.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2207.07331v2","updated":"2022-07-19T01:39:59Z","published":"2022-07-15T08:03:37Z","title":"Modeling Multi-interest News Sequence for News Recommendation","summary":"  A session-based news recommender system recommends the next news to a user by\nmodeling the potential interests embedded in a sequence of news read/clicked by\nher/him in a session. Generally, a user's interests are diverse, namely there\nare multiple interests corresponding to different types of news, e.g., news of\ndistinct topics, within a session. %Modeling such multiple interests is\ncritical for precise news recommendation. However, most of existing methods\ntypically overlook such important characteristic and thus fail to distinguish\nand model the potential multiple interests of a user, impeding accurate\nrecommendation of the next piece of news. Therefore, this paper proposes\nmulti-interest news sequence (MINS) model for news recommendation. In MINS, a\nnews encoder based on self-attention is devised on learn an informative\nembedding for each piece of news, and then a novel parallel interest network is\ndevised to extract the potential multiple interests embedded in the news\nsequence in preparation for the subsequent next-news recommendations. The\nexperimental results on a real-world dataset demonstrate that our model can\nachieve better performance than the state-of-the-art compared models.\n","authors":["Rongyao Wang","Wenpeng Lu"],"pdf_url":"https://arxiv.org/pdf/2207.07331v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.09192v1","updated":"2022-07-19T10:54:54Z","published":"2022-07-19T10:54:54Z","title":"A self-contained and self-explanatory DNA storage system","summary":"  Current research on DNA storage usually focuses on the improvement of storage\ndensity by developing effective encoding and decoding schemes while lacking the\nconsideration on the uncertainty in ultra-long-term data storage and retention.\nConsequently, the current DNA storage systems are often not self-contained,\nimplying that they have to resort to external tools for the restoration of the\nstored DNA data. This may result in high risks in data loss since the required\ntools might not be available due to the high uncertainty in far future. To\naddress this issue, we propose in this paper a self-contained DNA storage\nsystem that can bring self-explanatory to its stored data without relying on\nany external tool. To this end, we design a specific DNA file format whereby a\nseparate storage scheme is developed to reduce the data redundancy while an\neffective indexing is designed for random read operations to the stored data\nfile. We verified through experimental data that the proposed self-contained\nand self-explanatory method can not only get rid of the reliance on external\ntools for data restoration but also minimise the data redundancy brought about\nwhen the amount of data to be stored reaches a certain scale.\n","authors":["Min Li","Jiashu Wu","Junbiao Dai","Qingshan Jiang","Qiang Qu","Xiaoluo Huang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16007v3","updated":"2022-07-19T01:24:54Z","published":"2022-03-30T02:30:20Z","title":"Multi-target Filter and Detector for Unknown-number Speaker Diarization","summary":"  A strong representation of a target speaker can aid in extracting important\ninformation regarding the speaker and detecting the corresponding temporal\nregions in a multi-speaker conversation. In this study, we propose a neural\narchitecture that simultaneously extracts speaker representations that are\nconsistent with the speaker diarization objective and detects the presence of\neach speaker frame by frame, regardless of the number of speakers in the\nconversation. A speaker representation (known as a z-vector) extractor and\nframe-speaker contextualizer, which is realized by a residual network and\nprocessing data in both the temporal and speaker dimensions, are integrated\ninto a unified framework. Testing on the CALLHOME corpus reveals that our model\noutperforms most methods presented to date. An evaluation in a more challenging\ncase of concurrent speakers ranging from two to seven demonstrates that our\nmodel also achieves relative diarization error rate reductions of 26.35% and\n6.4% over two typical baselines, namely the traditional x-vector clustering\nmodel and attention-based model, respectively.\n","authors":["Chin-Yi Cheng","Hung-Shin Lee","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2203.16007v3.pdf","comment":"Submitted to IEEE Signal Processing Letters"}]},"2022-07-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.05203v2","updated":"2022-07-20T16:04:15Z","published":"2022-03-10T07:26:15Z","title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes","summary":"  3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations contained in point clouds. Existing methods only treat such relations\nas by-products of object feature learning in graphs without specifically\nencoding them, which leads to sub-optimal results. In this paper, aiming at\nimproving 3D dense captioning via capturing and utilizing the complex relations\nin the 3D scene, we propose MORE, a Multi-Order RElation mining model, to\nsupport generating more descriptive and comprehensive captions. Technically,\nour MORE encodes object relations in a progressive manner since complex\nrelations can be deduced from a limited number of basic ones. We first devise a\nnovel Spatial Layout Graph Convolution (SLGC), which semantically encodes\nseveral first-order relations as edges of a graph constructed over 3D object\nproposals. Next, from the resulting graph, we further extract multiple triplets\nwhich encapsulate basic first-order relations as the basic unit, and construct\nseveral Object-centric Triplet Attention Graphs (OTAG) to infer multi-order\nrelations for every target object. The updated node features from OTAG are\naggregated and fed into the caption decoder to provide abundant relational\ncues, so that captions including diverse relations with context objects can be\ngenerated. Extensive experiments on the Scan2Cap dataset prove the\neffectiveness of our proposed MORE and its components, and we also outperform\nthe current state-of-the-art method. Our code is available at\nhttps://github.com/SxJyJay/MORE.\n","authors":["Yang Jiao","Shaoxiang Chen","Zequn Jie","Jingjing Chen","Lin Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.05203v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09980v1","updated":"2022-07-20T15:39:30Z","published":"2022-07-20T15:39:30Z","title":"REFACTOR GNNS: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective","summary":"  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing REFACTOR GNNS. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our REFACTOR GNNS. Across\na multitude of well-established KGC benchmarks, our REFACTOR GNNS achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n","authors":["Yihong Chen","Pushkar Mishra","Luca Franceschi","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2207.09980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12886v6","updated":"2022-07-20T15:04:49Z","published":"2022-03-24T07:15:24Z","title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool\n  Children","summary":"  Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying an Automatic Speech Recognition(ASR) system is useless since they are\npre-trained on voices that are different from children's voices in terms of\nfrequency and amplitude. We constructed an ASR for our cognitive test system to\nsolve this issue using the Wav2Vec 2.0 model with a new pre-training objective\ncalled Random Frequency Pitch(RFP). In addition, we used our new dataset to\nfine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)\ntests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian\nsection of the CommonVoice dataset. Furthermore, our novel methodology produces\npositive outcomes in zero- and few-shot scenarios.\n","authors":["Amirhossein Abaskohi","Fatemeh Mortazavi","Hadi Moradi"],"pdf_url":"https://arxiv.org/pdf/2203.12886v6.pdf","comment":"8 pages, 5 figures, 4 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2207.09889v1","updated":"2022-07-20T13:33:41Z","published":"2022-07-20T13:33:41Z","title":"When Is TTS Augmentation Through a Pivot Language Useful?","summary":"  Developing Automatic Speech Recognition (ASR) for low-resource languages is a\nchallenge due to the small amount of transcribed audio data. For many such\nlanguages, audio and text are available separately, but not audio with\ntranscriptions. Using text, speech can be synthetically produced via\ntext-to-speech (TTS) systems. However, many low-resource languages do not have\nquality TTS systems either. We propose an alternative: produce synthetic audio\nby running text from the target language through a trained TTS system for a\nhigher-resource pivot language. We investigate when and how this technique is\nmost effective in low-resource settings. In our experiments, using several\nthousand synthetic TTS text-speech pairs and duplicating authentic data to\nbalance yields optimal results. Our findings suggest that searching over a set\nof candidate pivot languages can lead to marginal improvements and that,\nsurprisingly, ASR performance can by harmed by increases in measured TTS\nquality. Application of these findings improves ASR by 64.5\\% and 45.0\\%\ncharacter error reduction rate (CERR) respectively for two low-resource\nlanguages: Guaran\\'i and Suba.\n","authors":["Nathaniel Robinson","Perez Ogayo","Swetha Gangu","David R. Mortensen","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2207.09889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09732v1","updated":"2022-07-20T08:19:54Z","published":"2022-07-20T08:19:54Z","title":"Introducing Auxiliary Text Query-modifier to Content-based Audio\n  Retrieval","summary":"  The amount of audio data available on public websites is growing rapidly, and\nan efficient mechanism for accessing the desired data is necessary. We propose\na content-based audio retrieval method that can retrieve a target audio that is\nsimilar to but slightly different from the query audio by introducing auxiliary\ntextual information which describes the difference between the query and target\naudio. While the range of conventional content-based audio retrieval is limited\nto audio that is similar to the query audio, the proposed method can adjust the\nretrieval range by adding an embedding of the auxiliary text query-modifier to\nthe embedding of the query sample audio in a shared latent space. To evaluate\nour method, we built a dataset comprising two different audio clips and the\ntext that describes the difference. The experimental results show that the\nproposed method retrieves the paired audio more accurately than the baseline.\nWe also confirmed based on visualization that the proposed method obtains the\nshared latent space in which the audio difference and the corresponding text\nare represented as similar embedding vectors.\n","authors":["Daiki Takeuchi","Yasunori Ohishi","Daisuke Niizumi","Noboru Harada","Kunio Kashino"],"pdf_url":"https://arxiv.org/pdf/2207.09732v1.pdf","comment":"Accepted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2207.08141v2","updated":"2022-07-20T07:55:59Z","published":"2022-07-17T11:20:58Z","title":"ELECTRA is a Zero-Shot Learner, Too","summary":"  Recently, for few-shot or even zero-shot learning, the new paradigm\n\"pre-train, prompt, and predict\" has achieved remarkable achievements compared\nwith the \"pre-train, fine-tune\" paradigm. After the success of prompt-based\nGPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)\nprompt learning methods became popular and widely used. However, another\nefficient pre-trained discriminative model, ELECTRA, has probably been\nneglected. In this paper, we attempt to accomplish several NLP tasks in the\nzero-shot scenario using a novel our proposed replaced token detection\n(RTD)-based prompt learning method. Experimental results show that ELECTRA\nmodel based on RTD-prompt learning achieves surprisingly state-of-the-art\nzero-shot performance. Numerically, compared to MLM-RoBERTa-large and\nMLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%\nimprovement on all 15 tasks. Especially on the SST-2 task, our\nRTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training\ndata. Overall, compared to the pre-trained masked language models, the\npre-trained replaced token detection model performs better in zero-shot\nlearning. The source code is available at:\nhttps://github.com/nishiwen1214/RTD-ELECTRA.\n","authors":["Shiwen Ni","Hung-Yu Kao"],"pdf_url":"https://arxiv.org/pdf/2207.08141v2.pdf","comment":"The source code is available at:\n  https://github.com/nishiwen1214/RTD-ELECTRA"},{"id":"http://arxiv.org/abs/2207.09674v1","updated":"2022-07-20T06:07:26Z","published":"2022-07-20T06:07:26Z","title":"Improving Data Driven Inverse Text Normalization using Data Augmentation","summary":"  Inverse text normalization (ITN) is used to convert the spoken form output of\nan automatic speech recognition (ASR) system to a written form. Traditional\nhandcrafted ITN rules can be complex to transcribe and maintain. Meanwhile\nneural modeling approaches require quality large-scale spoken-written pair\nexamples in the same or similar domain as the ASR system (in-domain data), to\ntrain. Both these approaches require costly and complex annotations. In this\npaper, we present a data augmentation technique that effectively generates rich\nspoken-written numeric pairs from out-of-domain textual data with minimal human\nannotation. We empirically demonstrate that ITN model trained using our data\naugmentation technique consistently outperform ITN model trained using only\nin-domain data across all numeric surfaces like cardinal, currency, and\nfraction, by an overall accuracy of 14.44%.\n","authors":["Laxmi Pandey","Debjyoti Paul","Pooja Chitkara","Yutong Pang","Xuedong Zhang","Kjell Schubert","Mark Chou","Shu Liu","Yatharth Saraf"],"pdf_url":"https://arxiv.org/pdf/2207.09674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09666v1","updated":"2022-07-20T05:49:01Z","published":"2022-07-20T05:49:01Z","title":"GRIT: Faster and Better Image captioning Transformer Using Dual Visual\n  Features","summary":"  Current state-of-the-art methods for image captioning employ region-based\nfeatures, as they provide object-level information that is essential to\ndescribe the content of images; they are usually extracted by an object\ndetector such as Faster R-CNN. However, they have several issues, such as lack\nof contextual information, the risk of inaccurate detection, and the high\ncomputational cost. The first two could be resolved by additionally using\ngrid-based features. However, how to extract and fuse these two types of\nfeatures is uncharted. This paper proposes a Transformer-only neural\narchitecture, dubbed GRIT (Grid- and Region-based Image captioning\nTransformer), that effectively utilizes the two visual features to generate\nbetter captions. GRIT replaces the CNN-based detector employed in previous\nmethods with a DETR-based one, making it computationally faster. Moreover, its\nmonolithic design consisting only of Transformers enables end-to-end training\nof the model. This innovative design and the integration of the dual visual\nfeatures bring about significant performance improvement. The experimental\nresults on several image captioning benchmarks show that GRIT outperforms\nprevious methods in inference accuracy and speed.\n","authors":["Van-Quang Nguyen","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2207.09666v1.pdf","comment":"Accepted to ECCV 2022; 14 pages with appendix; Code:\n  https://github.com/davidnvq/grit"},{"id":"http://arxiv.org/abs/2110.08352v2","updated":"2022-07-20T05:14:37Z","published":"2021-10-15T20:28:27Z","title":"Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming\n  E2E ASR via Supernet","summary":"  From wearables to powerful smart devices, modern automatic speech recognition\n(ASR) models run on a variety of edge devices with different computational\nbudgets. To navigate the Pareto front of model accuracy vs model size,\nresearchers are trapped in a dilemma of optimizing model accuracy by training\nand fine-tuning models for each individual edge device while keeping the\ntraining GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,\nwhere a single neural network can be pruned to generate optimized model for a\nlarge range of model sizes. We develop training strategies for Omni-sparsity\nDNN that allows it to find models along the Pareto front of word-error-rate\n(WER) vs model size while keeping the training GPU-hours to no more than that\nof training one singular model. We demonstrate the Omni-sparsity DNN with\nstreaming E2E ASR models. Our results show great saving on training time and\nresources with similar or better accuracy on LibriSpeech compared to\nindividually pruned sparse models: 2%-6.6% better WER on Test-other.\n","authors":["Haichuan Yang","Yuan Shangguan","Dilin Wang","Meng Li","Pierce Chuang","Xiaohui Zhang","Ganesh Venkatesh","Ozlem Kalinli","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2110.08352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.13062v2","updated":"2022-07-20T04:54:28Z","published":"2020-10-25T08:13:34Z","title":"Transgender Community Sentiment Analysis from Social Media Data: A\n  Natural Language Processing Approach","summary":"  Transgender community is experiencing a huge disparity in mental health\nconditions compared with the general population. Interpreting the social medial\ndata posted by transgender people may help us understand the sentiments of\nthese sexual minority groups better and apply early interventions. In this\nstudy, we manually categorize 300 social media comments posted by transgender\npeople to the sentiment of negative, positive, and neutral. 5 machine learning\nalgorithms and 2 deep neural networks are adopted to build sentiment analysis\nclassifiers based on the annotated data. Results show that our annotations are\nreliable with a high Cohen's Kappa score over 0.8 across all three classes.\nLSTM model yields an optimal performance of accuracy over 0.85 and AUC of\n0.876. Our next step will focus on using advanced natural language processing\nalgorithms on a larger annotated dataset.\n","authors":["Yuqiao Liu","Yudan Wang","Ying Zhao","Zhixiang Li"],"pdf_url":"https://arxiv.org/pdf/2010.13062v2.pdf","comment":"5 pages, 1 figures"},{"id":"http://arxiv.org/abs/2207.09643v1","updated":"2022-07-20T04:20:46Z","published":"2022-07-20T04:20:46Z","title":"Integrating Linguistic Theory and Neural Language Models","summary":"  Transformer-based language models have recently achieved remarkable results\nin many natural language tasks. However, performance on leaderboards is\ngenerally achieved by leveraging massive amounts of training data, and rarely\nby encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language\nprocessing. In this dissertation, I present several case studies to illustrate\nhow theoretical linguistics and neural language models are still relevant to\neach other. First, language models are useful to linguists by providing an\nobjective tool to measure semantic distance, which is difficult to do using\ntraditional methods. On the other hand, linguistic theory contributes to\nlanguage modelling research by providing frameworks and sources of data to\nprobe our language models for specific aspects of language understanding.\n  This thesis contributes three studies that explore different aspects of the\nsyntax-semantics interface in language models. In the first part of my thesis,\nI apply language models to the problem of word class flexibility. Using mBERT\nas a source of semantic distance measurements, I present evidence in favour of\nanalyzing word class flexibility as a directional process. In the second part\nof my thesis, I propose a method to measure surprisal at intermediate layers of\nlanguage models. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and\ncommonsense anomalies. Finally, in the third part of my thesis, I adapt several\npsycholinguistic studies to show that language models contain knowledge of\nargument structure constructions. In summary, my thesis develops new\nconnections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of\nlanguage models.\n","authors":["Bai Li"],"pdf_url":"https://arxiv.org/pdf/2207.09643v1.pdf","comment":"PhD dissertation"},{"id":"http://arxiv.org/abs/2207.09068v2","updated":"2022-07-20T03:52:56Z","published":"2022-07-19T04:45:41Z","title":"PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic\n  Search","summary":"  Since BERT (Devlin et al., 2018), learning contextualized word embeddings has\nbeen a de-facto standard in NLP. However, the progress of learning\ncontextualized phrase embeddings is hindered by the lack of a human-annotated,\nphrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of\n~28K of noun phrases accompanied by their contextual Wikipedia pages and a\nsuite of three tasks of increasing difficulty for evaluating the quality of\nphrase embeddings. We find that training on our dataset improves ranking\nmodels' accuracy and remarkably pushes Question Answering (QA) models to\nnear-human accuracy which is 95% Exact Match (EM) on semantic search given a\nquery phrase and a passage. Interestingly, we find evidence that such\nimpressive performance is because the QA models learn to better capture the\ncommon meaning of a phrase regardless of its actual context. That is, on our\nPhrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially\n(60% EM), failing to differentiate between two different senses of the same\nphrase under two different contexts. Further results on our 3-task PiC\nbenchmark reveal that learning contextualized phrase embeddings remains an\ninteresting, open challenge.\n","authors":["Thang M. Pham","Seunghyun Yoon","Trung Bui","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2207.09068v2.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09638v1","updated":"2022-07-20T03:37:37Z","published":"2022-07-20T03:37:37Z","title":"Doge Tickets: Uncovering Domain-general Language Models by Playing\n  Lottery Tickets","summary":"  Over-parameterized models, typically pre-trained language models (LMs), have\nshown an appealing expressive power due to their small learning bias. However,\nthe huge learning capacity of LMs can also lead to large learning variance. In\na pilot study, we find that, when faced with multiple domains, a critical\nportion of parameters behave unexpectedly in a domain-specific manner while\nothers behave in a domain-general one. Motivated by this phenomenon, we for the\nfirst time posit that domain-general parameters can underpin a domain-general\nLM that can be derived from the original LM. To uncover the domain-general LM,\nwe propose to identify domain-general parameters by playing lottery tickets\n(dubbed doge tickets). In order to intervene the lottery, we propose a\ndomain-general score, which depicts how domain-invariant a parameter is by\nassociating it with the variance. Comprehensive experiments are conducted on\nthe Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets\nobtains an improved out-of-domain generalization in comparison with a range of\ncompetitive baselines. Analysis results further hint the existence of\ndomain-general parameters and the performance consistency of doge tickets.\n","authors":["Yi Yang","Chen Zhang","Benyou Wang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2207.09638v1.pdf","comment":"Accepted to NLPCC 2022. Code is available at\n  https://github.com/Ylily1015/DogeTickets"},{"id":"http://arxiv.org/abs/2109.05698v2","updated":"2022-07-20T03:33:00Z","published":"2021-09-13T04:17:58Z","title":"Detecting Textual Adversarial Examples through Randomized Substitution\n  and Vote","summary":"  A line of work has shown that natural text processing models are vulnerable\nto adversarial examples. Correspondingly, various defense methods are proposed\nto mitigate the threat of textual adversarial examples, eg, adversarial\ntraining, input transformations, detection, etc. In this work, we treat the\noptimization process for synonym substitution based textual adversarial attacks\nas a specific sequence of word replacement, in which each word mutually\ninfluences other words. We identify that we could destroy such mutual\ninteraction and eliminate the adversarial perturbation by randomly substituting\na word with its synonyms. Based on this observation, we propose a novel textual\nadversarial example detection method, termed Randomized Substitution and Vote\n(RS&V), which votes the prediction label by accumulating the logits of k\nsamples generated by randomly substituting the words in the input text with\nsynonyms. The proposed RS&V is generally applicable to any existing neural\nnetworks without modification on the architecture or extra training, and it is\northogonal to prior work on making the classification network itself more\nrobust. Empirical evaluations on three benchmark datasets demonstrate that our\nRS&V could detect the textual adversarial examples more successfully than the\nexisting detection methods while maintaining the high classification accuracy\non benign samples.\n","authors":["Xiaosen Wang","Yifeng Xiong","Kun He"],"pdf_url":"https://arxiv.org/pdf/2109.05698v2.pdf","comment":"Accepted by UAI 2022, code is avaliable at\n  https://github.com/JHL-HUST/RSV"},{"id":"http://arxiv.org/abs/2204.13749v2","updated":"2022-07-20T23:44:01Z","published":"2022-04-28T19:41:08Z","title":"Learning to Split for Automatic Bias Detection","summary":"  Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split cannot generalize to the testing\nsplit. This performance gap suggests that the testing split is\nunder-represented in the dataset, which is a signal of potential bias.\nIdentifying non-generalizable splits is challenging since we have no\nannotations about the bias. In this work, we show that the prediction\ncorrectness of each example in the testing split can be used as a source of\nweak supervision: generalization performance will drop if we move examples that\nare predicted correctly away from the testing split, leaving only those that\nare mis-predicted. ls is task-agnostic and can be applied to any supervised\nlearning problem, ranging from natural language understanding and image\nclassification to molecular property prediction. Empirical results show that ls\nis able to generate astonishingly challenging splits that correlate with\nhuman-identified biases. Moreover, we demonstrate that combining robust\nlearning algorithms (such as group DRO) with splits identified by ls enables\nautomatic de-biasing. Compared to previous state-of-the-art, we substantially\nimprove the worst-group performance (23.4% on average) when the source of\nbiases is unknown during training and validation.\n","authors":["Yujia Bao","Regina Barzilay"],"pdf_url":"https://arxiv.org/pdf/2204.13749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05195v2","updated":"2022-07-20T20:47:15Z","published":"2022-06-10T15:53:55Z","title":"Nominal Metaphor Generation with Multitask Learning","summary":"  Metaphor generation is a challenging task which can impact many downstream\ntasks such as improving user satisfaction with dialogue systems and story\ngeneration. This paper tackles the problem of Chinese nominal metaphor\ngeneration by introducing a multitask metaphor generation framework with\nself-training and metaphor identification mechanisms. Self-training addresses\nthe data scarcity issue of metaphor datasets. That is, instead of solely\nrelying on labelled metaphor datasets which are usually small in size,\nself-training helps identify potential metaphors from a large-scale unlabelled\ncorpus for metaphor generation. The metaphor weighting mechanism enables our\nmodel to focus on the metaphor-related parts of the input (e.g., the comparison\nof the metaphor and comparator) during model learning and thus improves the\nmetaphoricity of the generated metaphors. Our model is trained on an annotated\ncorpus consisting of 6.3k sentences that contain diverse metaphorical\nexpressions. Experimental results show that our model is able to generate\nmetaphors with better readability and creativity compared to the baseline\nmodels, even in the situation where training data is insufficient.\n","authors":["Yucheng Li","Chenghua Lin","Frank Geurin"],"pdf_url":"https://arxiv.org/pdf/2206.05195v2.pdf","comment":"INLG 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2207.10077v1","updated":"2022-07-20T17:59:51Z","published":"2022-07-20T17:59:51Z","title":"Discover and Mitigate Unknown Biases with Debiasing Alternate Networks","summary":"  Deep image classifiers have been found to learn biases from datasets. To\nmitigate the biases, most previous methods require labels of protected\nattributes (e.g., age, skin tone) as full-supervision, which has two\nlimitations: 1) it is infeasible when the labels are unavailable; 2) they are\nincapable of mitigating unknown biases -- biases that humans do not\npreconceive. To resolve those problems, we propose Debiasing Alternate Networks\n(DebiAN), which comprises two networks -- a Discoverer and a Classifier. By\ntraining in an alternate manner, the discoverer tries to find multiple unknown\nbiases of the classifier without any annotations of biases, and the classifier\naims at unlearning the biases identified by the discoverer. While previous\nworks evaluate debiasing results in terms of a single bias, we create\nMulti-Color MNIST dataset to better benchmark mitigation of multiple biases in\na multi-bias setting, which not only reveals the problems in previous methods\nbut also demonstrates the advantage of DebiAN in identifying and mitigating\nmultiple biases simultaneously. We further conduct extensive experiments on\nreal-world datasets, showing that the discoverer in DebiAN can identify unknown\nbiases that may be hard to be found by humans. Regarding debiasing, DebiAN\nachieves strong bias mitigation performance.\n","authors":["Zhiheng Li","Anthony Hoogs","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2207.10077v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10075v1","updated":"2022-07-20T17:59:44Z","published":"2022-07-20T17:59:44Z","title":"Is an Object-Centric Video Representation Beneficial for Transfer?","summary":"  The objective of this work is to learn an object-centric video\nrepresentation, with the aim of improving transferability to novel tasks, i.e.,\ntasks different from the pre-training task of action classification. To this\nend, we introduce a new object-centric video recognition model based on a\ntransformer architecture. The model learns a set of object-centric summary\nvectors for the video, and uses these vectors to fuse the visual and\nspatio-temporal trajectory `modalities' of the video clip. We also introduce a\nnovel trajectory contrast loss to further enhance objectness in these summary\nvectors. With experiments on four datasets -- SomethingSomething-V2,\nSomethingElse, Action Genome and EpicKitchens -- we show that the\nobject-centric model outperforms prior video representations (both\nobject-agnostic and object-aware), when: (1) classifying actions on unseen\nobjects and unseen environments; (2) low-shot learning to novel classes; (3)\nlinear probe to other downstream tasks; as well as (4) for standard action\nclassification.\n","authors":["Chuhan Zhang","Ankush Gupta","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2207.10075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10074v1","updated":"2022-07-20T17:58:10Z","published":"2022-07-20T17:58:10Z","title":"Semantic uncertainty intervals for disentangled latent spaces","summary":"  Meaningful uncertainty quantification in computer vision requires reasoning\nabout semantic information -- say, the hair color of the person in a photo or\nthe location of a car on the street. To this end, recent breakthroughs in\ngenerative modeling allow us to represent semantic information in disentangled\nlatent spaces, but providing uncertainties on the semantic latent variables has\nremained challenging. In this work, we provide principled uncertainty intervals\nthat are guaranteed to contain the true semantic factors for any underlying\ngenerative model. The method does the following: (1) it uses quantile\nregression to output a heuristic uncertainty interval for each element in the\nlatent space (2) calibrates these uncertainties such that they contain the true\nvalue of the latent for a new, unseen input. The endpoints of these calibrated\nintervals can then be propagated through the generator to produce interpretable\nuncertainty visualizations for each semantic factor. This technique reliably\ncommunicates semantically meaningful, principled, and instance-adaptive\nuncertainty in inverse problems like image super-resolution and image\ncompletion.\n","authors":["Swami Sankaranarayanan","Anastasios N. Angelopoulos","Stephen Bates","Yaniv Romano","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2207.10074v1.pdf","comment":"Code: https://github.com/swamiviv/generative_semantic_uncertainty"},{"id":"http://arxiv.org/abs/2203.16482v2","updated":"2022-07-20T17:49:27Z","published":"2022-03-30T17:18:11Z","title":"RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point\n  Clouds","summary":"  Object reconstruction from 3D point clouds has achieved impressive progress\nin the computer vision and computer graphics research field. However,\nreconstruction from time-varying point clouds (a.k.a. 4D point clouds) is\ngenerally overlooked. In this paper, we propose a new network architecture,\nnamely RFNet-4D, that jointly reconstruct objects and their motion flows from\n4D point clouds. The key insight is that simultaneously performing both tasks\nvia learning spatial and temporal features from a sequence of point clouds can\nleverage individual tasks, leading to improved overall performance. To prove\nthis ability, we design a temporal vector field learning module using\nunsupervised learning approach for flow estimation, leveraged by supervised\nlearning of spatial structures for object reconstruction. Extensive experiments\nand analyses on benchmark dataset validated the effectiveness and efficiency of\nour method. As shown in experimental results, our method achieves\nstate-of-the-art performance on both flow estimation and object reconstruction\nwhile performing much faster than existing methods in both training and\ninference. Our code and data are available at\nhttps://github.com/hkust-vgd/RFNet-4D\n","authors":["Tuan-Anh Vu","Duc Thanh Nguyen","Binh-Son Hua","Quang-Hieu Pham","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2203.16482v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10061v1","updated":"2022-07-20T17:47:22Z","published":"2022-07-20T17:47:22Z","title":"Monocular 3D Object Reconstruction with GAN Inversion","summary":"  Recovering a textured 3D mesh from a monocular image is highly challenging,\nparticularly for in-the-wild objects that lack 3D ground truths. In this work,\nwe present MeshInversion, a novel framework to improve the reconstruction by\nexploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh\nsynthesis. Reconstruction is achieved by searching for a latent space in the 3D\nGAN that best resembles the target mesh in accordance with the single view\nobservation. Since the pre-trained GAN encapsulates rich 3D semantics in terms\nof mesh geometry and texture, searching within the GAN manifold thus naturally\nregularizes the realness and fidelity of the reconstruction. Importantly, such\nregularization is directly applied in the 3D space, providing crucial guidance\nof mesh parts that are unobserved in the 2D space. Experiments on standard\nbenchmarks show that our framework obtains faithful 3D reconstructions with\nconsistent geometry and texture across both observed and unobserved parts.\nMoreover, it generalizes well to meshes that are less commonly seen, such as\nthe extended articulation of deformable objects. Code is released at\nhttps://github.com/junzhezhang/mesh-inversion\n","authors":["Junzhe Zhang","Daxuan Ren","Zhongang Cai","Chai Kiat Yeo","Bo Dai","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2207.10061v1.pdf","comment":"ECCV 2022. Project page:\n  https://www.mmlab-ntu.com/project/meshinversion/"},{"id":"http://arxiv.org/abs/2203.06451v3","updated":"2022-07-20T17:39:21Z","published":"2022-03-12T14:57:49Z","title":"Bringing Rolling Shutter Images Alive with Dual Reversed Distortion","summary":"  Rolling shutter (RS) distortion can be interpreted as the result of picking a\nrow of pixels from instant global shutter (GS) frames over time during the\nexposure of the RS camera. This means that the information of each instant GS\nframe is partially, yet sequentially, embedded into the row-dependent\ndistortion. Inspired by this fact, we address the challenging task of reversing\nthis process, i.e., extracting undistorted GS frames from images suffering from\nRS distortion. However, since RS distortion is coupled with other factors such\nas readout settings and the relative velocity of scene elements to the camera,\nmodels that only exploit the geometric correlation between temporally adjacent\nimages suffer from poor generality in processing data with different readout\nsettings and dynamic scenes with both camera motion and object motion. In this\npaper, instead of two consecutive frames, we propose to exploit a pair of\nimages captured by dual RS cameras with reversed RS directions for this highly\nchallenging task. Grounded on the symmetric and complementary nature of dual\nreversed distortion, we develop a novel end-to-end model, IFED, to generate\ndual optical flow sequence through iterative learning of the velocity field\nduring the RS time. Extensive experimental results demonstrate that IFED is\nsuperior to naive cascade schemes, as well as the state-of-the-art which\nutilizes adjacent RS images. Most importantly, although it is trained on a\nsynthetic dataset, IFED is shown to be effective at retrieving GS frame\nsequences from real-world RS distorted images of dynamic scenes. Code is\navailable at https://github.com/zzh-tech/Dual-Reversed-RS.\n","authors":["Zhihang Zhong","Mingdeng Cao","Xiao Sun","Zhirong Wu","Zhongyi Zhou","Yinqiang Zheng","Stephen Lin","Imari Sato"],"pdf_url":"https://arxiv.org/pdf/2203.06451v3.pdf","comment":"ECCV2022 Oral"},{"id":"http://arxiv.org/abs/2207.10053v1","updated":"2022-07-20T17:33:19Z","published":"2022-07-20T17:33:19Z","title":"3D Clothed Human Reconstruction in the Wild","summary":"  Although much progress has been made in 3D clothed human reconstruction, most\nof the existing methods fail to produce robust results from in-the-wild images,\nwhich contain diverse human poses and appearances. This is mainly due to the\nlarge domain gap between training datasets and in-the-wild datasets. The\ntraining datasets are usually synthetic ones, which contain rendered images\nfrom GT 3D scans. However, such datasets contain simple human poses and less\nnatural image appearances compared to those of real in-the-wild datasets, which\nmakes generalization of it to in-the-wild images extremely challenging. To\nresolve this issue, in this work, we propose ClothWild, a 3D clothed human\nreconstruction framework that firstly addresses the robustness on in-thewild\nimages. First, for the robustness to the domain gap, we propose a weakly\nsupervised pipeline that is trainable with 2D supervision targets of\nin-the-wild datasets. Second, we design a DensePose-based loss function to\nreduce ambiguities of the weak supervision. Extensive empirical tests on\nseveral public in-the-wild datasets demonstrate that our proposed ClothWild\nproduces much more accurate and robust results than the state-of-the-art\nmethods. The codes are available in here:\nhttps://github.com/hygenie1228/ClothWild_RELEASE.\n","authors":["Gyeongsik Moon","Hyeongjin Nam","Takaaki Shiratori","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2207.10053v1.pdf","comment":"Accepted to ECCV 2022, 25 pages including the supplementary material"},{"id":"http://arxiv.org/abs/2207.10049v1","updated":"2022-07-20T17:27:50Z","published":"2022-07-20T17:27:50Z","title":"Pretraining a Neural Network before Knowing Its Architecture","summary":"  Training large neural networks is possible by training a smaller hypernetwork\nthat predicts parameters for the large ones. A recently released Graph\nHyperNetwork (GHN) trained this way on one million smaller ImageNet\narchitectures is able to predict parameters for large unseen networks such as\nResNet-50. While networks with predicted parameters lose performance on the\nsource task, the predicted parameters have been found useful for fine-tuning on\nother tasks. We study if fine-tuning based on the same GHN is still useful on\nnovel strong architectures that were published after the GHN had been trained.\nWe found that for recent architectures such as ConvNeXt, GHN initialization\nbecomes less useful than for ResNet-50. One potential reason is the increased\ndistribution shift of novel architectures from those used to train the GHN. We\nalso found that the predicted parameters lack the diversity necessary to\nsuccessfully fine-tune parameters with gradient descent. We alleviate this\nlimitation by applying simple post-processing techniques to predicted\nparameters before fine-tuning them on a target task and improve fine-tuning of\nResNet-50 and ConvNeXt.\n","authors":["Boris Knyazev"],"pdf_url":"https://arxiv.org/pdf/2207.10049v1.pdf","comment":"Accepted at ICML 2022 Workshop on Pre-training: Perspectives,\n  Pitfalls, and Paths Forward, source code is available at\n  https://github.com/facebookresearch/ppuda"},{"id":"http://arxiv.org/abs/2207.10047v1","updated":"2022-07-20T17:24:22Z","published":"2022-07-20T17:24:22Z","title":"Densely Constrained Depth Estimator for Monocular 3D Object Detection","summary":"  Estimating accurate 3D locations of objects from monocular images is a\nchallenging problem because of lacking depth. Previous work shows that\nutilizing the object's keypoint projection constraints to estimate multiple\ndepth candidates boosts the detection performance. However, the existing\nmethods can only utilize vertical edges as projection constraints for depth\nestimation. So these methods only use a small number of projection constraints\nand produce insufficient depth candidates, leading to inaccurate depth\nestimation. In this paper, we propose a method that utilizes dense projection\nconstraints from edges of any direction. In this way, we employ much more\nprojection constraints and produce considerable depth candidates. Besides, we\npresent a graph matching weighting module to merge the depth candidates. The\nproposed method DCD (Densely Constrained Detector) achieves state-of-the-art\nperformance on the KITTI and WOD benchmarks. Code is released at\nhttps://github.com/BraveGroup/DCD.\n","authors":["Yingyan Li","Yunchao Chen","Jiawei He","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.10047v1.pdf","comment":"Accept by ECCV22"},{"id":"http://arxiv.org/abs/2207.10040v1","updated":"2022-07-20T17:09:16Z","published":"2022-07-20T17:09:16Z","title":"Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A\n  New Physics-Inspired Transformer Model","summary":"  Image restoration algorithms for atmospheric turbulence are known to be much\nmore challenging to design than traditional ones such as blur or noise because\nthe distortion caused by the turbulence is an entanglement of spatially varying\nblur, geometric distortion, and sensor noise. Existing CNN-based restoration\nmethods built upon convolutional kernels with static weights are insufficient\nto handle the spatially dynamical atmospheric turbulence effect. To address\nthis problem, in this paper, we propose a physics-inspired transformer model\nfor imaging through atmospheric turbulence. The proposed network utilizes the\npower of transformer blocks to jointly extract a dynamical turbulence\ndistortion map and restore a turbulence-free image. In addition, recognizing\nthe lack of a comprehensive dataset, we collect and present two new real-world\nturbulence datasets that allow for evaluation with both classical objective\nmetrics (e.g., PSNR and SSIM) and a new task-driven metric using text\nrecognition accuracy. Both real testing sets and all related code will be made\npublicly available.\n","authors":["Zhiyuan Mao","Ajay Jaiswal","Zhangyang Wang","Stanley H. Chan"],"pdf_url":"https://arxiv.org/pdf/2207.10040v1.pdf","comment":"This paper is accepted as a poster at ECCV 2022"},{"id":"http://arxiv.org/abs/2204.00486v3","updated":"2022-07-20T17:02:56Z","published":"2022-04-01T14:45:30Z","title":"GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and\n  Retrieval","summary":"  Cognitive science has shown that humans perceive videos in terms of events\nseparated by the state changes of dominant subjects. State changes trigger new\nevents and are one of the most useful among the large amount of redundant\ninformation perceived. However, previous research focuses on the overall\nunderstanding of segments without evaluating the fine-grained status changes\ninside. In this paper, we introduce a new dataset called Kinetic-GEB+. The\ndataset consists of over 170k boundaries associated with captions describing\nstatus changes in the generic events in 12K videos. Upon this new dataset, we\npropose three tasks supporting the development of a more fine-grained, robust,\nand human-like understanding of videos through status changes. We evaluate many\nrepresentative baselines in our dataset, where we also design a new TPD\n(Temporal-based Pairwise Difference) Modeling method for visual difference and\nachieve significant performance improvements. Besides, the results show there\nare still formidable challenges for current methods in the utilization of\ndifferent granularities, representation of visual difference, and the accurate\nlocalization of status changes. Further analysis shows that our dataset can\ndrive developing more powerful methods to understand status changes and thus\nimprove video level comprehension. The dataset is available at\nhttps://github.com/Yuxuan-W/GEB-Plus\n","authors":["Yuxuan Wang","Difei Gao","Licheng Yu","Stan Weixian Lei","Matt Feiszli","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2204.00486v3.pdf","comment":"In Proceedings of the European Conference on Computer Vision 2022"},{"id":"http://arxiv.org/abs/2207.10035v1","updated":"2022-07-20T17:01:33Z","published":"2022-07-20T17:01:33Z","title":"Fully Sparse 3D Object Detection","summary":"  As the perception range of LiDAR increases, LiDAR-based 3D object detection\nbecomes a dominant task in the long-range perception task of autonomous\ndriving. The mainstream 3D object detectors usually build dense feature maps in\nthe network backbone and prediction head. However, the computational and\nspatial costs on the dense feature map are quadratic to the perception range,\nwhich makes them hardly scale up to the long-range setting. To enable efficient\nlong-range LiDAR-based object detection, we build a fully sparse 3D object\ndetector (FSD). The computational and spatial cost of FSD is roughly linear to\nthe number of points and independent of the perception range. FSD is built upon\nthe general sparse voxel encoder and a novel sparse instance recognition (SIR)\nmodule. SIR first groups the points into instances and then applies\ninstance-wise feature extraction and prediction. In this way, SIR resolves the\nissue of center feature missing, which hinders the design of the fully sparse\narchitecture for all center-based or anchor-based detectors. Moreover, SIR\navoids the time-consuming neighbor queries in previous point-based methods by\ngrouping points into instances. We conduct extensive experiments on the\nlarge-scale Waymo Open Dataset to reveal the working mechanism of FSD, and\nstate-of-the-art performance is reported. To demonstrate the superiority of FSD\nin long-range detection, we also conduct experiments on Argoverse 2 Dataset,\nwhich has a much larger perception range ($200m$) than Waymo Open Dataset\n($75m$). On such a large perception range, FSD achieves state-of-the-art\nperformance and is 2.4$\\times$ faster than the dense counterpart.Codes will be\nreleased at https://github.com/TuSimple/SST.\n","authors":["Lue Fan","Feng Wang","Naiyan Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.10035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10031v1","updated":"2022-07-20T16:46:02Z","published":"2022-07-20T16:46:02Z","title":"MOTCOM: The Multi-Object Tracking Dataset Complexity Metric","summary":"  There exists no comprehensive metric for describing the complexity of\nMulti-Object Tracking (MOT) sequences. This lack of metrics decreases\nexplainability, complicates comparison of datasets, and reduces the\nconversation on tracker performance to a matter of leader board position. As a\nremedy, we present the novel MOT dataset complexity metric (MOTCOM), which is a\ncombination of three sub-metrics inspired by key problems in MOT: occlusion,\nerratic motion, and visual similarity. The insights of MOTCOM can open nuanced\ndiscussions on tracker performance and may lead to a wider acknowledgement of\nnovel contributions developed for either less known datasets or those aimed at\nsolving sub-problems. We evaluate MOTCOM on the comprehensive MOT17, MOT20, and\nMOTSynth datasets and show that MOTCOM is far better at describing the\ncomplexity of MOT sequences compared to the conventional density and number of\ntracks. Project page at https://vap.aau.dk/motcom\n","authors":["Malte Pedersen","Joakim Bruslund Haurum","Patrick Dendorfer","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2207.10031v1.pdf","comment":"ECCV 2022. Project webpage https://vap.aau.dk/motcom"},{"id":"http://arxiv.org/abs/2207.10026v1","updated":"2022-07-20T16:41:41Z","published":"2022-07-20T16:41:41Z","title":"Locality Guidance for Improving Vision Transformers on Tiny Datasets","summary":"  While the Vision Transformer (VT) architecture is becoming trendy in computer\nvision, pure VT models perform poorly on tiny datasets. To address this issue,\nthis paper proposes the locality guidance for improving the performance of VTs\non tiny datasets. We first analyze that the local information, which is of\ngreat importance for understanding images, is hard to be learned with limited\ndata due to the high flexibility and intrinsic globality of the self-attention\nmechanism in VTs. To facilitate local information, we realize the locality\nguidance for VTs by imitating the features of an already trained convolutional\nneural network (CNN), inspired by the built-in local-to-global hierarchy of\nCNN. Under our dual-task learning paradigm, the locality guidance provided by a\nlightweight CNN trained on low-resolution images is adequate to accelerate the\nconvergence and improve the performance of VTs to a large extent. Therefore,\nour locality guidance approach is very simple and efficient, and can serve as a\nbasic performance enhancement method for VTs on tiny datasets. Extensive\nexperiments demonstrate that our method can significantly improve VTs when\ntraining from scratch on tiny datasets and is compatible with different kinds\nof VTs and datasets. For example, our proposed method can boost the performance\nof various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85%\nfor PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing\nthe potential of VTs on tiny datasets. The code is available at\nhttps://github.com/lkhl/tiny-transformers.\n","authors":["Kehan Li","Runyi Yu","Zhennan Wang","Li Yuan","Guoli Song","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2207.10026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10025v1","updated":"2022-07-20T16:41:37Z","published":"2022-07-20T16:41:37Z","title":"Learning from Synthetic Data: Facial Expression Classification based on\n  Ensemble of Multi-task Networks","summary":"  Facial expression in-the-wild is essential for various interactive computing\ndomains. Especially, \"Learning from Synthetic Data\" (LSD) is an important topic\nin the facial expression recognition task. In this paper, we propose a\nmulti-task learning-based facial expression recognition approach which consists\nof emotion and appearance learning branches that can share all face\ninformation, and present preliminary results for the LSD challenge introduced\nin the 4th affective behavior analysis in-the-wild (ABAW) competition. Our\nmethod achieved the mean F1 score of 0.71.\n","authors":["Jae-Yeop Jeong","Yeong-Gi Hong","JiYeon Oh","Sumin Hong","Jin-Woo Jeong","Yuchul Jung"],"pdf_url":"https://arxiv.org/pdf/2207.10025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10024v1","updated":"2022-07-20T16:41:36Z","published":"2022-07-20T16:41:36Z","title":"Difficulty-Aware Simulator for Open Set Recognition","summary":"  Open set recognition (OSR) assumes unknown instances appear out of the blue\nat the inference time. The main challenge of OSR is that the response of models\nfor unknowns is totally unpredictable. Furthermore, the diversity of open set\nmakes it harder since instances have different difficulty levels. Therefore, we\npresent a novel framework, DIfficulty-Aware Simulator (DIAS), that generates\nfakes with diverse difficulty levels to simulate the real world. We first\ninvestigate fakes from generative adversarial network (GAN) in the classifier's\nviewpoint and observe that these are not severely challenging. This leads us to\ndefine the criteria for difficulty by regarding samples generated with GANs\nhaving moderate-difficulty. To produce hard-difficulty examples, we introduce\nCopycat, imitating the behavior of the classifier. Furthermore, moderate- and\neasy-difficulty samples are also yielded by our modified GAN and Copycat,\nrespectively. As a result, DIAS outperforms state-of-the-art methods with both\nmetrics of AUROC and F-score. Our code is available at\nhttps://github.com/wjun0830/Difficulty-Aware-Simulator.\n","authors":["WonJun Moon","Junho Park","Hyun Seok Seong","Cheol-Ho Cho","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2207.10024v1.pdf","comment":"Accepted to ECCV 2022. Code is available at\n  github.com/wjun0830/Difficulty-Aware-Simulator"},{"id":"http://arxiv.org/abs/2207.10023v1","updated":"2022-07-20T16:41:14Z","published":"2022-07-20T16:41:14Z","title":"Tailoring Self-Supervision for Supervised Learning","summary":"  Recently, it is shown that deploying a proper self-supervision is a\nprospective way to enhance the performance of supervised learning. Yet, the\nbenefits of self-supervision are not fully exploited as previous pretext tasks\nare specialized for unsupervised representation learning. To this end, we begin\nby presenting three desirable properties for such auxiliary tasks to assist the\nsupervised objective. First, the tasks need to guide the model to learn rich\nfeatures. Second, the transformations involved in the self-supervision should\nnot significantly alter the training distribution. Third, the tasks are\npreferred to be light and generic for high applicability to prior arts.\nSubsequently, to show how existing pretext tasks can fulfill these and be\ntailored for supervised learning, we propose a simple auxiliary\nself-supervision task, predicting localizable rotation (LoRot). Our exhaustive\nexperiments validate the merits of LoRot as a pretext task tailored for\nsupervised learning in terms of robustness and generalization capability. Our\ncode is available at https://github.com/wjun0830/Localizable-Rotation.\n","authors":["WonJun Moon","Ji-Hwan Kim","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2207.10023v1.pdf","comment":"Accepted to ECCV 2022. Code is available at\n  github.com/wjun0830/Localizable-Rotation"},{"id":"http://arxiv.org/abs/2207.10022v1","updated":"2022-07-20T16:40:38Z","published":"2022-07-20T16:40:38Z","title":"Secrets of Event-Based Optical Flow","summary":"  Event cameras respond to scene dynamics and offer advantages to estimate\nmotion. Following recent image-based deep-learning achievements, optical flow\nestimation methods for event cameras have rushed to combine those image-based\nmethods with event data. However, it requires several adaptations (data\nconversion, loss function, etc.) as they have very different properties. We\ndevelop a principled method to extend the Contrast Maximization framework to\nestimate optical flow from events alone. We investigate key elements: how to\ndesign the objective function to prevent overfitting, how to warp events to\ndeal better with occlusions, and how to improve convergence with multi-scale\nraw events. With these key elements, our method ranks first among unsupervised\nmethods on the MVSEC benchmark, and is competitive on the DSEC benchmark.\nMoreover, our method allows us to expose the issues of the ground truth flow in\nthose benchmarks, and produces remarkable results when it is transferred to\nunsupervised learning settings. Our code is available at\nhttps://github.com/tub-rip/event_based_optical_flow\n","authors":["Shintaro Shiba","Yoshimitsu Aoki","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2207.10022v1.pdf","comment":"23 pages, 11 figures, 7 tables,\n  https://github.com/tub-rip/event_based_optical_flow"},{"id":"http://arxiv.org/abs/2106.09703v2","updated":"2022-07-20T16:38:23Z","published":"2021-06-17T17:57:11Z","title":"MaCLR: Motion-aware Contrastive Learning of Representations for Videos","summary":"  We present MaCLR, a novel method to explicitly perform cross-modal\nself-supervised video representations learning from visual and motion\nmodalities. Compared to previous video representation learning methods that\nmostly focus on learning motion cues implicitly from RGB inputs, MaCLR enriches\nstandard contrastive learning objectives for RGB video clips with a cross-modal\nlearning objective between a Motion pathway and a Visual pathway. We show that\nthe representation learned with our MaCLR method focuses more on foreground\nmotion regions and thus generalizes better to downstream tasks. To demonstrate\nthis, we evaluate MaCLR on five datasets for both action recognition and action\ndetection, and demonstrate state-of-the-art self-supervised performance on all\ndatasets. Furthermore, we show that MaCLR representation can be as effective as\nrepresentations learned with full supervision on UCF101 and HMDB51 action\nrecognition, and even outperform the supervised representation for action\nrecognition on VidSitu and SSv2, and action detection on AVA.\n","authors":["Fanyi Xiao","Joseph Tighe","Davide Modolo"],"pdf_url":"https://arxiv.org/pdf/2106.09703v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10015v1","updated":"2022-07-20T16:24:57Z","published":"2022-07-20T16:24:57Z","title":"Generative Domain Adaptation for Face Anti-Spoofing","summary":"  Face anti-spoofing (FAS) approaches based on unsupervised domain adaption\n(UDA) have drawn growing attention due to promising performances for target\nscenarios. Most existing UDA FAS methods typically fit the trained models to\nthe target domain via aligning the distribution of semantic high-level\nfeatures. However, insufficient supervision of unlabeled target domains and\nneglect of low-level feature alignment degrade the performances of existing\nmethods. To address these issues, we propose a novel perspective of UDA FAS\nthat directly fits the target data to the models, i.e., stylizes the target\ndata to the source-domain style via image translation, and further feeds the\nstylized data into the well-trained source model for classification. The\nproposed Generative Domain Adaptation (GDA) framework combines two carefully\ndesigned consistency constraints: 1) Inter-domain neural statistic consistency\nguides the generator in narrowing the inter-domain gap. 2) Dual-level semantic\nconsistency ensures the semantic quality of stylized images. Besides, we\npropose intra-domain spectrum mixup to further expand target data distributions\nto ensure generalization and reduce the intra-domain gap. Extensive experiments\nand visualizations demonstrate the effectiveness of our method against the\nstate-of-the-art methods.\n","authors":["Qianyu Zhou","Ke-Yue Zhang","Taiping Yao","Ran Yi","Kekai Sheng","Shouhong Ding","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2207.10015v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10008v1","updated":"2022-07-20T16:11:48Z","published":"2022-07-20T16:11:48Z","title":"E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs","summary":"  Minimal solutions for relative rotation and translation estimation tasks have\nbeen explored in different scenarios, typically relying on the so-called\nco-visibility graph. However, how to build direct rotation relationships\nbetween two frames without overlap is still an open topic, which, if solved,\ncould greatly improve the accuracy of visual odometry.\n  In this paper, a new minimal solution is proposed to solve relative rotation\nestimation between two images without overlapping areas by exploiting a new\ngraph structure, which we call Extensibility Graph (E-Graph). Differently from\na co-visibility graph, high-level landmarks, including vanishing directions and\nplane normals, are stored in our E-Graph, which are geometrically extensible.\nBased on E-Graph, the rotation estimation problem becomes simpler and more\nelegant, as it can deal with pure rotational motion and requires fewer\nassumptions, e.g. Manhattan/Atlanta World, planar/vertical motion. Finally, we\nembed our rotation estimation strategy into a complete camera tracking and\nmapping system which obtains 6-DoF camera poses and a dense 3D mesh model.\n  Extensive experiments on public benchmarks demonstrate that the proposed\nmethod achieves state-of-the-art tracking performance.\n","authors":["Yanyan Li","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2207.10008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10003v1","updated":"2022-07-20T16:05:56Z","published":"2022-07-20T16:05:56Z","title":"BYEL : Bootstrap on Your Emotion Latent","summary":"  According to the problem of dataset construction cost for training in deep\nlearning and the development of generative models, more and more researches are\nbeing conducted to train with synthetic data and to inference using real data.\nWe propose emotion aware Self-Supervised Learning using ABAW's Learning\nSynthetic Data (LSD) dataset. We pre-train our method to LSD dataset as a\nself-supervised learning and then use the same LSD dataset to do downstream\ntraining on the emotion classification task as a supervised learning. As a\nresult, a higher result(0.63) than baseline(0.5) was obtained.\n","authors":["Hyungjun Lee","Hwangyu Lim","Sejoon Lim"],"pdf_url":"https://arxiv.org/pdf/2207.10003v1.pdf","comment":"ABAW4th competition"},{"id":"http://arxiv.org/abs/2207.10002v1","updated":"2022-07-20T16:05:32Z","published":"2022-07-20T16:05:32Z","title":"Overcoming Shortcut Learning in a Target Domain by Generalizing Basic\n  Visual Factors from a Source Domain","summary":"  Shortcut learning occurs when a deep neural network overly relies on spurious\ncorrelations in the training dataset in order to solve downstream tasks. Prior\nworks have shown how this impairs the compositional generalization capability\nof deep learning models. To address this problem, we propose a novel approach\nto mitigate shortcut learning in uncontrolled target domains. Our approach\nextends the training set with an additional dataset (the source domain), which\nis specifically designed to facilitate learning independent representations of\nbasic visual factors. We benchmark our idea on synthetic target domains where\nwe explicitly control shortcut opportunities as well as real-world target\ndomains. Furthermore, we analyze the effect of different specifications of the\nsource domain and the network architecture on compositional generalization. Our\nmain finding is that leveraging data from a source domain is an effective way\nto mitigate shortcut learning. By promoting independence across different\nfactors of variation in the learned representations, networks can learn to\nconsider only predictive factors and ignore potential shortcut factors during\ninference.\n","authors":["Piyapat Saranrittichai","Chaithanya Kumar Mummadi","Claudia Blaiotta","Mauricio Munoz","Volker Fischer"],"pdf_url":"https://arxiv.org/pdf/2207.10002v1.pdf","comment":"Accepted for publication at European Conference on Computer Vision\n  (ECCV) 2022"},{"id":"http://arxiv.org/abs/2203.05203v2","updated":"2022-07-20T16:04:15Z","published":"2022-03-10T07:26:15Z","title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes","summary":"  3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations contained in point clouds. Existing methods only treat such relations\nas by-products of object feature learning in graphs without specifically\nencoding them, which leads to sub-optimal results. In this paper, aiming at\nimproving 3D dense captioning via capturing and utilizing the complex relations\nin the 3D scene, we propose MORE, a Multi-Order RElation mining model, to\nsupport generating more descriptive and comprehensive captions. Technically,\nour MORE encodes object relations in a progressive manner since complex\nrelations can be deduced from a limited number of basic ones. We first devise a\nnovel Spatial Layout Graph Convolution (SLGC), which semantically encodes\nseveral first-order relations as edges of a graph constructed over 3D object\nproposals. Next, from the resulting graph, we further extract multiple triplets\nwhich encapsulate basic first-order relations as the basic unit, and construct\nseveral Object-centric Triplet Attention Graphs (OTAG) to infer multi-order\nrelations for every target object. The updated node features from OTAG are\naggregated and fed into the caption decoder to provide abundant relational\ncues, so that captions including diverse relations with context objects can be\ngenerated. Extensive experiments on the Scan2Cap dataset prove the\neffectiveness of our proposed MORE and its components, and we also outperform\nthe current state-of-the-art method. Our code is available at\nhttps://github.com/SxJyJay/MORE.\n","authors":["Yang Jiao","Shaoxiang Chen","Zequn Jie","Jingjing Chen","Lin Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.05203v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2203.11191v2","updated":"2022-07-20T15:59:52Z","published":"2022-03-21T17:59:19Z","title":"Robust Visual Tracking by Segmentation","summary":"  Estimating the target extent poses a fundamental challenge in visual object\ntracking. Typically, trackers are box-centric and fully rely on a bounding box\nto define the target in the scene. In practice, objects often have complex\nshapes and are not aligned with the image axis. In these cases, bounding boxes\ndo not provide an accurate description of the target and often contain a\nmajority of background pixels. We propose a segmentation-centric tracking\npipeline that not only produces a highly accurate segmentation mask, but also\ninternally works with segmentation masks instead of bounding boxes. Thus, our\ntracker is able to better learn a target representation that clearly\ndifferentiates the target in the scene from background content. In order to\nachieve the necessary robustness for the challenging tracking scenario, we\npropose a separate instance localization component that is used to condition\nthe segmentation decoder when producing the output mask. We infer a bounding\nbox from the segmentation mask, validate our tracker on challenging tracking\ndatasets and achieve the new state of the art on LaSOT with a success AUC score\nof 69.7%. Since most tracking datasets do not contain mask annotations, we\ncannot use them to evaluate predicted segmentation masks. Instead, we validate\nour segmentation quality on two popular video object segmentation datasets.\n","authors":["Matthieu Paul","Martin Danelljan","Christoph Mayer","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.11191v2.pdf","comment":"Accepted at ECCV 2022. Code and trained models are available at:\n  https://github.com/visionml/pytracking"},{"id":"http://arxiv.org/abs/2207.01375v2","updated":"2022-07-20T15:56:15Z","published":"2022-07-04T12:52:54Z","title":"GraphVid: It Only Takes a Few Nodes to Understand a Video","summary":"  We propose a concise representation of videos that encode perceptually\nmeaningful features into graphs. With this representation, we aim to leverage\nthe large amount of redundancies in videos and save computations. First, we\nconstruct superpixel-based graph representations of videos by considering\nsuperpixels as graph nodes and create spatial and temporal connections between\nadjacent superpixels. Then, we leverage Graph Convolutional Networks to process\nthis representation and predict the desired output. As a result, we are able to\ntrain models with much fewer parameters, which translates into short training\nperiods and a reduction in computation resource requirements. A comprehensive\nexperimental study on the publicly available datasets Kinetics-400 and Charades\nshows that the proposed method is highly cost-effective and uses limited\ncommodity hardware during training and inference. It reduces the computational\nrequirements 10-fold while achieving results that are comparable to\nstate-of-the-art methods. We believe that the proposed approach is a promising\ndirection that could open the door to solving video understanding more\nefficiently and enable more resource limited users to thrive in this research\nfield.\n","authors":["Eitan Kosman","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2207.01375v2.pdf","comment":"Accepted to ECCV2022 (Oral)"},{"id":"http://arxiv.org/abs/2204.00570v3","updated":"2022-07-20T15:51:41Z","published":"2022-04-01T16:56:26Z","title":"Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised\n  Domain Adaptation","summary":"  We consider unsupervised domain adaptation (UDA), where labeled data from a\nsource domain (e.g., photographs) and unlabeled data from a target domain\n(e.g., sketches) are used to learn a classifier for the target domain.\nConventional UDA methods (e.g., domain adversarial training) learn\ndomain-invariant features to improve generalization to the target domain. In\nthis paper, we show that contrastive pre-training, which learns features on\nunlabeled source and target data and then fine-tunes on labeled source data, is\ncompetitive with strong UDA methods. However, we find that contrastive\npre-training does not learn domain-invariant features, diverging from\nconventional UDA intuitions. We show theoretically that contrastive\npre-training can learn features that vary subtantially across domains but still\ngeneralize to the target domain, by disentangling domain and class information.\nOur results suggest that domain invariance is not necessary for UDA. We\nempirically validate our theory on benchmark vision datasets.\n","authors":["Kendrick Shen","Robbie Jones","Ananya Kumar","Sang Michael Xie","Jeff Z. HaoChen","Tengyu Ma","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2204.00570v3.pdf","comment":"Accepted to ICML 2022 (Long Talk)"},{"id":"http://arxiv.org/abs/2207.09988v1","updated":"2022-07-20T15:47:34Z","published":"2022-07-20T15:47:34Z","title":"DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation","summary":"  Unsupervised domain adaptation in semantic segmentation has been raised to\nalleviate the reliance on expensive pixel-wise annotations. It leverages a\nlabeled source domain dataset as well as unlabeled target domain images to\nlearn a segmentation network. In this paper, we observe two main issues of the\nexisting domain-invariant learning framework. (1) Being distracted by the\nfeature distribution alignment, the network cannot focus on the segmentation\ntask. (2) Fitting source domain data well would compromise the target domain\nperformance. To address these issues, we propose DecoupleNet that alleviates\nsource domain overfitting and enables the final model to focus more on the\nsegmentation task. Furthermore, we put forward Self-Discrimination (SD) and\nintroduce an auxiliary classifier to learn more discriminative target domain\nfeatures with pseudo labels. Finally, we propose Online Enhanced Self-Training\n(OEST) to contextually enhance the quality of pseudo labels in an online\nmanner. Experiments show our method outperforms existing state-of-the-art\nmethods, and extensive ablation studies verify the effectiveness of each\ncomponent. Code is available at https://github.com/dvlab-research/DecoupleNet.\n","authors":["Xin Lai","Zhuotao Tian","Xiaogang Xu","Yingcong Chen","Shu Liu","Hengshuang Zhao","Liwei Wang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2207.09988v1.pdf","comment":"Accepted to ECCV 2022. Code is available at\n  https://github.com/dvlab-research/DecoupleNet"},{"id":"http://arxiv.org/abs/2203.12119v2","updated":"2022-07-20T15:47:22Z","published":"2022-03-23T01:17:16Z","title":"Visual Prompt Tuning","summary":"  The current modus operandi in adapting pre-trained models involves updating\nall the backbone parameters, ie, full fine-tuning. This paper introduces Visual\nPrompt Tuning (VPT) as an efficient and effective alternative to full\nfine-tuning for large-scale Transformer models in vision. Taking inspiration\nfrom recent advances in efficiently tuning large language models, VPT\nintroduces only a small amount (less than 1% of model parameters) of trainable\nparameters in the input space while keeping the model backbone frozen. Via\nextensive experiments on a wide variety of downstream recognition tasks, we\nshow that VPT achieves significant performance gains compared to other\nparameter efficient tuning protocols. Most importantly, VPT even outperforms\nfull fine-tuning in many cases across model capacities and training data\nscales, while reducing per-task storage cost.\n","authors":["Menglin Jia","Luming Tang","Bor-Chun Chen","Claire Cardie","Serge Belongie","Bharath Hariharan","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2203.12119v2.pdf","comment":"ECCV2022"},{"id":"http://arxiv.org/abs/2203.04203v5","updated":"2022-07-20T15:45:22Z","published":"2022-03-08T17:07:09Z","title":"AssistQ: Affordance-centric Question-driven Task Completion for\n  Egocentric Assistant","summary":"  A long-standing goal of intelligent assistants such as AR glasses/robots has\nbeen to assist users in affordance-centric real-world scenarios, such as \"how\ncan I run the microwave for 1 minute?\". However, there is still no clear task\ndefinition and suitable benchmarks. In this paper, we define a new task called\nAffordance-centric Question-driven Task Completion, where the AI assistant\nshould learn from instructional videos to provide step-by-step help in the\nuser's view. To support the task, we constructed AssistQ, a new dataset\ncomprising 531 question-answer samples from 100 newly filmed instructional\nvideos. We also developed a novel Question-to-Actions (Q2A) model to address\nthe AQTC task and validate it on the AssistQ dataset. The results show that our\nmodel significantly outperforms several VQA-related baselines while still\nhaving large room for improvement. We expect our task and dataset to advance\nEgocentric AI Assistant's development. Our project page is available at:\nhttps://showlab.github.io/assistq/.\n","authors":["Benita Wong","Joya Chen","You Wu","Stan Weixian Lei","Dongxing Mao","Difei Gao","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2203.04203v5.pdf","comment":"Accepted by ECCV 2022. Equal contribution: Benita Wong, Joya Chen,\n  You Wu; Corresponding author: Mike Zheng Shou"},{"id":"http://arxiv.org/abs/2111.11187v5","updated":"2022-07-20T15:37:39Z","published":"2021-11-22T13:25:54Z","title":"PointMixer: MLP-Mixer for Point Cloud Understanding","summary":"  MLP-Mixer has newly appeared as a new challenger against the realm of CNNs\nand transformer. Despite its simplicity compared to transformer, the concept of\nchannel-mixing MLPs and token-mixing MLPs achieves noticeable performance in\nvisual recognition tasks. Unlike images, point clouds are inherently sparse,\nunordered and irregular, which limits the direct use of MLP-Mixer for point\ncloud understanding. In this paper, we propose PointMixer, a universal point\nset operator that facilitates information sharing among unstructured 3D points.\nBy simply replacing token-mixing MLPs with a softmax function, PointMixer can\n\"mix\" features within/between point sets. By doing so, PointMixer can be\nbroadly used in the network as inter-set mixing, intra-set mixing, and pyramid\nmixing. Extensive experiments show the competitive or superior performance of\nPointMixer in semantic segmentation, classification, and point reconstruction\nagainst transformer-based methods.\n","authors":["Jaesung Choe","Chunghyun Park","Francois Rameau","Jaesik Park","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2111.11187v5.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09978v1","updated":"2022-07-20T15:37:32Z","published":"2022-07-20T15:37:32Z","title":"NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation\n  on Point Clouds","summary":"  We introduce a method for instance proposal generation for 3D point clouds.\nExisting techniques typically directly regress proposals in a single\nfeed-forward step, leading to inaccurate estimation. We show that this serves\nas a critical bottleneck, and propose a method based on iterative bilateral\nfiltering with learned kernels. Following the spirit of bilateral filtering, we\nconsider both the deep feature embeddings of each point, as well as their\nlocations in the 3D space. We show via synthetic experiments that our method\nbrings drastic improvements when generating instance proposals for a given\npoint of interest. We further validate our method on the challenging ScanNet\nbenchmark, achieving the best instance segmentation performance amongst the\nsub-category of top-down methods.\n","authors":["Weiwei Sun","Daniel Rebain","Renjie Liao","Vladimir Tankovich","Soroosh Yazdani","Kwang Moo Yi","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2207.09978v1.pdf","comment":"Project website: https://neuralbf.github.io"},{"id":"http://arxiv.org/abs/2205.15781v3","updated":"2022-07-20T15:35:05Z","published":"2022-05-31T13:30:36Z","title":"Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation\n  Models","summary":"  Semantic image segmentation is a central and challenging task in autonomous\ndriving, addressed by training deep models. Since this training draws to a\ncurse of human-based image labeling, using synthetic images with automatically\ngenerated labels together with unlabeled real-world images is a promising\nalternative. This implies to address an unsupervised domain adaptation (UDA)\nproblem. In this paper, we propose a new co-training procedure for\nsynth-to-real UDA of semantic segmentation models. It consists of a\nself-training stage, which provides two domain-adapted models, and a model\ncollaboration loop for the mutual improvement of these two models. These models\nare then used to provide the final semantic segmentation labels (pseudo-labels)\nfor the real-world images. The overall procedure treats the deep models as\nblack boxes and drives their collaboration at the level of pseudo-labeled\ntarget images, i.e., neither modifying loss functions is required, nor explicit\nfeature alignment. We test our proposal on standard synthetic and real-world\ndatasets for on-board semantic segmentation. Our procedure shows improvements\nranging from ~13 to ~26 mIoU points over baselines, so establishing new\nstate-of-the-art results.\n","authors":["Jose L. Gómez","Gabriel Villalonga","Antonio M. López"],"pdf_url":"https://arxiv.org/pdf/2205.15781v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09966v1","updated":"2022-07-20T15:19:30Z","published":"2022-07-20T15:19:30Z","title":"Temporal and cross-modal attention for audio-visual zero-shot learning","summary":"  Audio-visual generalised zero-shot learning for video classification requires\nunderstanding the relations between the audio and visual information in order\nto be able to recognise samples from novel, previously unseen classes at test\ntime. The natural semantic and temporal alignment between audio and visual data\nin video data can be exploited to learn powerful representations that\ngeneralise to unseen classes at test time. We propose a multi-modal and\nTemporal Cross-attention Framework (\\modelName) for audio-visual generalised\nzero-shot learning. Its inputs are temporally aligned audio and visual features\nthat are obtained from pre-trained networks. Encouraging the framework to focus\non cross-modal correspondence across time instead of self-attention within the\nmodalities boosts the performance significantly. We show that our proposed\nframework that ingests temporal features yields state-of-the-art performance on\nthe \\ucf, \\vgg, and \\activity benchmarks for (generalised) zero-shot learning.\nCode for reproducing all results is available at\n\\url{https://github.com/ExplainableML/TCAF-GZSL}.\n","authors":["Otniel-Bogdan Mercea","Thomas Hummel","A. Sophia Koepke","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2207.09966v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09965v1","updated":"2022-07-20T15:18:43Z","published":"2022-07-20T15:18:43Z","title":"M2-Net: Multi-stages Specular Highlight Detection and Removal in\n  Multi-scenes","summary":"  In this paper, we propose a novel uniformity framework for highlight\ndetection and removal in multi-scenes, including synthetic images, face images,\nnatural images, and text images. The framework consists of three main\ncomponents, highlight feature extractor module, highlight coarse removal\nmodule, and highlight refine removal module. Firstly, the highlight feature\nextractor module can directly separate the highlight feature and non-highlight\nfeature from the original highlight image. Then highlight removal image is\nobtained using a coarse highlight removal network. To further improve the\nhighlight removal effect, the refined highlight removal image is finally\nobtained using refine highlight removal module based on contextual highlight\nattention mechanisms. Extensive experimental results in multiple scenes\nindicate that the proposed framework can obtain excellent visual effects of\nhighlight removal and achieve state-of-the-art results in several quantitative\nevaluation metrics. Our algorithm is applied for the first time in video\nhighlight removal with promising results.\n","authors":["Zhaoyangfan Huang","Kun Hu","Xingjun Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09963v1","updated":"2022-07-20T15:13:48Z","published":"2022-07-20T15:13:48Z","title":"Rethinking Few-Shot Class-Incremental Learning with Open-Set Hypothesis\n  in Hyperbolic Geometry","summary":"  Few-Shot Class-Incremental Learning (FSCIL) aims at incrementally learning\nnovel classes from a few labeled samples by avoiding the overfitting and\ncatastrophic forgetting simultaneously. The current protocol of FSCIL is built\nby mimicking the general class-incremental learning setting, while it is not\ntotally appropriate due to the different data configuration, i.e., novel\nclasses are all in the limited data regime. In this paper, we rethink the\nconfiguration of FSCIL with the open-set hypothesis by reserving the\npossibility in the first session for incoming categories. To assign better\nperformances on both close-set and open-set recognition to the model,\nHyperbolic Reciprocal Point Learning module (Hyper-RPL) is built on Reciprocal\nPoint Learning (RPL) with hyperbolic neural networks. Besides, for learning\nnovel categories from limited labeled data, we incorporate a hyperbolic metric\nlearning (Hyper-Metric) module into the distillation-based framework to\nalleviate the overfitting issue and better handle the trade-off issue between\nthe preservation of old knowledge and the acquisition of new knowledge. The\ncomprehensive assessments of the proposed configuration and modules on three\nbenchmark datasets are executed to validate the effectiveness concerning three\nevaluation indicators.\n","authors":["Yawen Cui","Zitong Yu","Wei Peng","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2207.09963v1.pdf","comment":"submitted to IEEE Transactions on Cybernetics"},{"id":"http://arxiv.org/abs/2203.04694v2","updated":"2022-07-20T15:11:57Z","published":"2022-03-09T13:13:55Z","title":"Align-Deform-Subtract: An Interventional Framework for Explaining Object\n  Differences","summary":"  Given two object images, how can we explain their differences in terms of the\nunderlying object properties? To address this question, we propose\nAlign-Deform-Subtract (ADS) -- an interventional framework for explaining\nobject differences. By leveraging semantic alignments in image-space as\ncounterfactual interventions on the underlying object properties, ADS\niteratively quantifies and removes differences in object properties. The result\nis a set of \"disentangled\" error measures which explain object differences in\nterms of the underlying properties. Experiments on real and synthetic data\nillustrate the efficacy of the framework.\n","authors":["Cian Eastwood","Li Nanbo","Christopher K. I. Williams"],"pdf_url":"https://arxiv.org/pdf/2203.04694v2.pdf","comment":"ICLR 2022 Workshop on Objects, Structure and Causality"},{"id":"http://arxiv.org/abs/2207.09957v1","updated":"2022-07-20T15:04:32Z","published":"2022-07-20T15:04:32Z","title":"Estimating Model Performance under Domain Shifts with Class-Specific\n  Confidence Scores","summary":"  Machine learning models are typically deployed in a test setting that differs\nfrom the training setting, potentially leading to decreased model performance\nbecause of domain shift. If we could estimate the performance that a\npre-trained model would achieve on data from a specific deployment setting, for\nexample a certain clinic, we could judge whether the model could safely be\ndeployed or if its performance degrades unacceptably on the specific data.\nExisting approaches estimate this based on the confidence of predictions made\non unlabeled test data from the deployment's domain. We find existing methods\nstruggle with data that present class imbalance, because the methods used to\ncalibrate confidence do not account for bias induced by class imbalance,\nconsequently failing to estimate class-wise accuracy. Here, we introduce\nclass-wise calibration within the framework of performance estimation for\nimbalanced datasets. Specifically, we derive class-specific modifications of\nstate-of-the-art confidence-based model evaluation methods including\ntemperature scaling (TS), difference of confidences (DoC), and average\nthresholded confidence (ATC). We also extend the methods to estimate Dice\nsimilarity coefficient (DSC) in image segmentation. We conduct experiments on\nfour tasks and find the proposed modifications consistently improve the\nestimation accuracy for imbalanced datasets. Our methods improve accuracy\nestimation by 18\\% in classification under natural domain shifts, and double\nthe estimation accuracy on segmentation tasks, when compared with prior\nmethods.\n","authors":["Zeju Li","Konstantinos Kamnitsas","Mobarakol Islam","Chen Chen","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2207.09957v1.pdf","comment":"Accepted at MICCAI 2022"},{"id":"http://arxiv.org/abs/2207.09956v1","updated":"2022-07-20T15:02:55Z","published":"2022-07-20T15:02:55Z","title":"Telepresence Video Quality Assessment","summary":"  Video conferencing, which includes both video and audio content, has\ncontributed to dramatic increases in Internet traffic, as the COVID-19 pandemic\nforced millions of people to work and learn from home. Global Internet traffic\nof video conferencing has dramatically increased Because of this, efficient and\naccurate video quality tools are needed to monitor and perceptually optimize\ntelepresence traffic streamed via Zoom, Webex, Meet, etc. However, existing\nmodels are limited in their prediction capabilities on multi-modal, live\nstreaming telepresence content. Here we address the significant challenges of\nTelepresence Video Quality Assessment (TVQA) in several ways. First, we\nmitigated the dearth of subjectively labeled data by collecting ~2k\ntelepresence videos from different countries, on which we crowdsourced ~80k\nsubjective quality labels. Using this new resource, we created a\nfirst-of-a-kind online video quality prediction framework for live streaming,\nusing a multi-modal learning framework with separate pathways to compute visual\nand audio quality predictions. Our all-in-one model is able to provide accurate\nquality predictions at the patch, frame, clip, and audiovisual levels. Our\nmodel achieves state-of-the-art performance on both existing quality databases\nand our new TVQA database, at a considerably lower computational expense,\nmaking it an attractive solution for mobile and embedded systems.\n","authors":["Zhenqiang Ying","Deepti Ghadiyaram","Alan Bovik"],"pdf_url":"https://arxiv.org/pdf/2207.09956v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09953v1","updated":"2022-07-20T14:58:13Z","published":"2022-07-20T14:58:13Z","title":"Learning Pedestrian Group Representations for Multi-modal Trajectory\n  Prediction","summary":"  Modeling the dynamics of people walking is a problem of long-standing\ninterest in computer vision. Many previous works involving pedestrian\ntrajectory prediction define a particular set of individual actions to\nimplicitly model group actions. In this paper, we present a novel architecture\nnamed GP-Graph which has collective group representations for effective\npedestrian trajectory prediction in crowded environments, and is compatible\nwith all types of existing approaches. A key idea of GP-Graph is to model both\nindividual-wise and group-wise relations as graph representations. To do this,\nGP-Graph first learns to assign each pedestrian into the most likely behavior\ngroup. Using this assignment information, GP-Graph then forms both intra- and\ninter-group interactions as graphs, accounting for human-human relations within\na group and group-group relations, respectively. To be specific, for the\nintra-group interaction, we mask pedestrian graph edges out of an associated\ngroup. We also propose group pooling&unpooling operations to represent a group\nwith multiple pedestrians as one graph node. Lastly, GP-Graph infers a\nprobability map for socially-acceptable future trajectories from the integrated\nfeatures of both group interactions. Moreover, we introduce a group-level\nlatent vector sampling to ensure collective inferences over a set of possible\nfuture trajectories. Extensive experiments are conducted to validate the\neffectiveness of our architecture, which demonstrates consistent performance\nimprovements with publicly available benchmarks. Code is publicly available at\nhttps://github.com/inhwanbae/GPGraph.\n","authors":["Inhwan Bae","Jin-Hwi Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2207.09953v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2203.01325v2","updated":"2022-07-20T14:49:54Z","published":"2022-03-02T13:30:56Z","title":"Self-Supervised Learning for Real-World Super-Resolution from Dual\n  Zoomed Observations","summary":"  In this paper, we consider two challenging issues in reference-based\nsuper-resolution (RefSR), (i) how to choose a proper reference image, and (ii)\nhow to learn real-world RefSR in a self-supervised manner. Particularly, we\npresent a novel self-supervised learning approach for real-world image SR from\nobservations at dual camera zooms (SelfDZSR). Considering the popularity of\nmultiple cameras in modern smartphones, the more zoomed (telephoto) image can\nbe naturally leveraged as the reference to guide the SR of the lesser zoomed\n(short-focus) image. Furthermore, SelfDZSR learns a deep network to obtain the\nSR result of short-focus image to have the same resolution as the telephoto\nimage. For this purpose, we take the telephoto image instead of an additional\nhigh-resolution image as the supervision information and select a center patch\nfrom it as the reference to super-resolve the corresponding short-focus image\npatch. To mitigate the effect of the misalignment between short-focus\nlow-resolution (LR) image and telephoto ground-truth (GT) image, we design an\nauxiliary-LR generator and map the GT to an auxiliary-LR while keeping the\nspatial position unchanged. Then the auxiliary-LR can be utilized to deform the\nLR features by the proposed adaptive spatial transformer networks (AdaSTN), and\nmatch the Ref features to GT. During testing, SelfDZSR can be directly deployed\nto super-solve the whole short-focus image with the reference of telephoto\nimage. Experiments show that our method achieves better quantitative and\nqualitative performance against state-of-the-arts. Codes are available at\nhttps://github.com/cszhilu1998/SelfDZSR.\n","authors":["Zhilu Zhang","Ruohao Wang","Hongzhi Zhang","Yunjin Chen","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2203.01325v2.pdf","comment":"ECCV 2022 camera ready"},{"id":"http://arxiv.org/abs/2207.09949v1","updated":"2022-07-20T14:47:28Z","published":"2022-07-20T14:47:28Z","title":"VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual\n  Data","summary":"  While monocular 3D pose estimation seems to have achieved very accurate\nresults on the public datasets, their generalization ability is largely\noverlooked. In this work, we perform a systematic evaluation of the existing\nmethods and find that they get notably larger errors when tested on different\ncameras, human poses and appearance. To address the problem, we introduce\nVirtualPose, a two-stage learning framework to exploit the hidden \"free lunch\"\nspecific to this task, i.e. generating infinite number of poses and cameras for\ntraining models at no cost. To that end, the first stage transforms images to\nabstract geometry representations (AGR), and then the second maps them to 3D\nposes. It addresses the generalization issue from two aspects: (1) the first\nstage can be trained on diverse 2D datasets to reduce the risk of over-fitting\nto limited appearance; (2) the second stage can be trained on diverse AGR\nsynthesized from a large number of virtual cameras and poses. It outperforms\nthe SOTA methods without using any paired images and 3D poses from the\nbenchmarks, which paves the way for practical applications. Code is available\nat https://github.com/wkom/VirtualPose.\n","authors":["Jiajun Su","Chunyu Wang","Xiaoxuan Ma","Wenjun Zeng","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09944v1","updated":"2022-07-20T14:41:09Z","published":"2022-07-20T14:41:09Z","title":"Probable Domain Generalization via Quantile Risk Minimization","summary":"  Domain generalization (DG) seeks predictors which perform well on unseen test\ndistributions by leveraging labeled training data from multiple related\ndistributions or domains. To achieve this, the standard formulation optimizes\nfor worst-case performance over the set of all possible domains. However, with\nworst-case shifts very unlikely in practice, this generally leads to\noverly-conservative solutions. In fact, a recent study found that no DG\nalgorithm outperformed empirical risk minimization in terms of average\nperformance. In this work, we argue that DG is neither a worst-case problem nor\nan average-case problem, but rather a probabilistic one. To this end, we\npropose a probabilistic framework for DG, which we call Probable Domain\nGeneralization, wherein our key idea is that distribution shifts seen during\ntraining should inform us of probable shifts at test time. To realize this, we\nexplicitly relate training and test domains as draws from the same underlying\nmeta-distribution, and propose a new optimization problem -- Quantile Risk\nMinimization (QRM) -- which requires that predictors generalize with high\nprobability. We then prove that QRM: (i) produces predictors that generalize to\nnew domains with a desired probability, given sufficiently many domains and\nsamples; and (ii) recovers the causal predictor as the desired probability of\ngeneralization approaches one. In our experiments, we introduce a more holistic\nquantile-focused evaluation protocol for DG, and show that our algorithms\noutperform state-of-the-art baselines on real and synthetic data.\n","authors":["Cian Eastwood","Alexander Robey","Shashank Singh","Julius von Kügelgen","Hamed Hassani","George J. Pappas","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2207.09944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09554v2","updated":"2022-07-20T14:26:15Z","published":"2022-03-17T18:36:11Z","title":"CoGS: Controllable Generation and Search from Sketch and Style","summary":"  We present CoGS, a novel method for the style-conditioned, sketch-driven\nsynthesis of images. CoGS enables exploration of diverse appearance\npossibilities for a given sketched object, enabling decoupled control over the\nstructure and the appearance of the output. Coarse-grained control over object\nstructure and appearance are enabled via an input sketch and an exemplar\n\"style\" conditioning image to a transformer-based sketch and style encoder to\ngenerate a discrete codebook representation. We map the codebook representation\ninto a metric space, enabling fine-grained control over selection and\ninterpolation between multiple synthesis options before generating the image\nvia a vector quantized GAN (VQGAN) decoder. Our framework thereby unifies\nsearch and synthesis tasks, in that a sketch and style pair may be used to run\nan initial synthesis which may be refined via combination with similar results\nin a search corpus to produce an image more closely matching the user's intent.\nWe show that our model, trained on the 125 object classes of our newly created\nPseudosketches dataset, is capable of producing a diverse gamut of semantic\ncontent and appearance styles.\n","authors":["Cusuh Ham","Gemma Canet Tarres","Tu Bui","James Hays","Zhe Lin","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2203.09554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09935v1","updated":"2022-07-20T14:20:52Z","published":"2022-07-20T14:20:52Z","title":"Towards Efficient and Scale-Robust Ultra-High-Definition Image\n  Demoireing","summary":"  With the rapid development of mobile devices, modern widely-used mobile\nphones typically allow users to capture 4K resolution (i.e.,\nultra-high-definition) images. However, for image demoireing, a challenging\ntask in low-level vision, existing works are generally carried out on\nlow-resolution or synthetic images. Hence, the effectiveness of these methods\non 4K resolution images is still unknown. In this paper, we explore moire\npattern removal for ultra-high-definition images. To this end, we propose the\nfirst ultra-high-definition demoireing dataset (UHDM), which contains 5,000\nreal-world 4K resolution image pairs, and conduct a benchmark study on current\nstate-of-the-art methods. Further, we present an efficient baseline model\nESDNet for tackling 4K moire images, wherein we build a semantic-aligned\nscale-aware module to address the scale variation of moire patterns. Extensive\nexperiments manifest the effectiveness of our approach, which outperforms\nstate-of-the-art methods by a large margin while being much more lightweight.\nCode and dataset are available at https://xinyu-andy.github.io/uhdm-page.\n","authors":["Xin Yu","Peng Dai","Wenbo Li","Lan Ma","Jiajun Shen","Jia Li","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2207.09935v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09934v1","updated":"2022-07-20T14:20:35Z","published":"2022-07-20T14:20:35Z","title":"DeepIPC: Deeply Integrated Perception and Control for Mobile Robot in\n  Real Environments","summary":"  We propose DeepIPC, an end-to-end multi-task model that handles both\nperception and control tasks in driving a mobile robot autonomously. The model\nconsists of two main parts, perception and controller modules. The perception\nmodule takes RGB image and depth map to perform semantic segmentation and\nbird's eye view (BEV) semantic mapping along with providing their encoded\nfeatures. Meanwhile, the controller module processes these features with the\nmeasurement of GNSS locations and angular speed to estimate waypoints that come\nwith latent features. Then, two different agents are used to translate\nwaypoints and latent features into a set of navigational controls to drive the\nrobot. The model is evaluated by predicting driving records and performing\nautomated driving under various conditions in the real environment. Based on\nthe experimental results, DeepIPC achieves the best drivability and multi-task\nperformance even with fewer parameters compared to the other models.\n","authors":["Oskar Natan","Jun Miura"],"pdf_url":"https://arxiv.org/pdf/2207.09934v1.pdf","comment":"To be submitted to a journal or conference proceedings"},{"id":"http://arxiv.org/abs/2207.09933v1","updated":"2022-07-20T14:20:03Z","published":"2022-07-20T14:20:03Z","title":"Robust Landmark-based Stent Tracking in X-ray Fluoroscopy","summary":"  In clinical procedures of angioplasty (i.e., open clogged coronary arteries),\ndevices such as balloons and stents need to be placed and expanded in arteries\nunder the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,\nthe resulting images are often noisy. To check the correct placement of these\ndevices, typically multiple motion-compensated frames are averaged to enhance\nthe view. Therefore, device tracking is a necessary procedure for this purpose.\nEven though angioplasty devices are designed to have radiopaque markers for the\nease of tracking, current methods struggle to deliver satisfactory results due\nto the small marker size and complex scenes in angioplasty. In this paper, we\npropose an end-to-end deep learning framework for single stent tracking, which\nconsists of three hierarchical modules: U-Net based landmark detection, ResNet\nbased stent proposal and feature extraction, and graph convolutional neural\nnetwork (GCN) based stent tracking that temporally aggregates both spatial\ninformation and appearance features. The experiments show that our method\nperforms significantly better in detection compared with the state-of-the-art\npoint-based tracking models. In addition, its fast inference speed satisfies\nclinical requirements.\n","authors":["Luojie Huang","Yikang Liu","Li Chen","Eric Z Chen","Xiao Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2207.09933v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09927v1","updated":"2022-07-20T14:12:05Z","published":"2022-07-20T14:12:05Z","title":"ViGAT: Bottom-up event recognition and explanation in video using\n  factorized graph attention network","summary":"  In this paper a pure-attention bottom-up approach, called ViGAT, that\nutilizes an object detector together with a Vision Transformer (ViT) backbone\nnetwork to derive object and frame features, and a head network to process\nthese features for the task of event recognition and explanation in video, is\nproposed. The ViGAT head consists of graph attention network (GAT) blocks\nfactorized along the spatial and temporal dimensions in order to capture\neffectively both local and long-term dependencies between objects or frames.\nMoreover, using the weighted in-degrees (WiDs) derived from the adjacency\nmatrices at the various GAT blocks, we show that the proposed architecture can\nidentify the most salient objects and frames that explain the decision of the\nnetwork. A comprehensive evaluation study is performed, demonstrating that the\nproposed approach provides state-of-the-art results on three large, publicly\navailable video datasets (FCVID, Mini-Kinetics, ActivityNet).\n","authors":["Nikolaos Gkalelis","Dimitrios Daskalakis","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2207.09927v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2207.09925v1","updated":"2022-07-20T14:08:37Z","published":"2022-07-20T14:08:37Z","title":"An Efficient Framework for Few-shot Skeleton-based Temporal Action\n  Segmentation","summary":"  Temporal action segmentation (TAS) aims to classify and locate actions in the\nlong untrimmed action sequence. With the success of deep learning, many deep\nmodels for action segmentation have emerged. However, few-shot TAS is still a\nchallenging problem. This study proposes an efficient framework for the\nfew-shot skeleton-based TAS, including a data augmentation method and an\nimproved model. The data augmentation approach based on motion interpolation is\npresented here to solve the problem of insufficient data, and can increase the\nnumber of samples significantly by synthesizing action sequences. Besides, we\nconcatenate a Connectionist Temporal Classification (CTC) layer with a network\ndesigned for skeleton-based TAS to obtain an optimized model. Leveraging CTC\ncan enhance the temporal alignment between prediction and ground truth and\nfurther improve the segment-wise metrics of segmentation results. Extensive\nexperiments on both public and self-constructed datasets, including two\nsmall-scale datasets and one large-scale dataset, show the effectiveness of two\nproposed methods in improving the performance of the few-shot skeleton-based\nTAS task.\n","authors":["Leiyang Xu","Qiang Wang","Xiaotian Lin","Lin Yuan"],"pdf_url":"https://arxiv.org/pdf/2207.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09915v1","updated":"2022-07-20T14:02:57Z","published":"2022-07-20T14:02:57Z","title":"A note on the variation of geometric functionals","summary":"  Calculus of Variation combined with Differential Geometry as tools of\nmodelling and solving problems in image processing and computer vision were\nintroduced in the late 80's and the 90s of the 20th century. The beginning of\nan extensive work in these directions was marked by works such as Geodesic\nActive Contours (GAC), the Beltrami framework, level set method of Osher and\nSethian the works of Charpiat et al. and the works by Chan and Vese to name\njust a few. In many cases the optimization of these functional are done by the\ngradient descent method via the calculation of the Euler-Lagrange equations.\nStraightforward use of the resulted EL equations in the gradient descent scheme\nleads to non-geometric and in some cases non sensical equations. It is\ncostumary to modify these EL equations or even the functional itself in order\nto obtain geometric and/or sensical equations. The aim of this note is to point\nto the correct way to derive the EL and the gradient descent equations such\nthat the resulted gradient descent equation is geometric and makes sense.\n","authors":["Nir Sochen"],"pdf_url":"https://arxiv.org/pdf/2207.09915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09899v1","updated":"2022-07-20T13:51:39Z","published":"2022-07-20T13:51:39Z","title":"Labeling instructions matter in biomedical image analysis","summary":"  Biomedical image analysis algorithm validation depends on high-quality\nannotation of reference datasets, for which labeling instructions are key.\nDespite their importance, their optimization remains largely unexplored. Here,\nwe present the first systematic study of labeling instructions and their impact\non annotation quality in the field. Through comprehensive examination of\nprofessional practice and international competitions registered at the MICCAI\nSociety, we uncovered a discrepancy between annotators' needs for labeling\ninstructions and their current quality and availability. Based on an analysis\nof 14,040 images annotated by 156 annotators from four professional companies\nand 708 Amazon Mechanical Turk (MTurk) crowdworkers using instructions with\ndifferent information density levels, we further found that including exemplary\nimages significantly boosts annotation performance compared to text-only\ndescriptions, while solely extending text descriptions does not. Finally,\nprofessional annotators constantly outperform MTurk crowdworkers. Our study\nraises awareness for the need of quality standards in biomedical image analysis\nlabeling instructions.\n","authors":["Tim Rädsch","Annika Reinke","Vivienn Weru","Minu D. Tizabi","Nicholas Schreck","A. Emre Kavur","Bünyamin Pekdemir","Tobias Roß","Annette Kopp-Schneider","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2207.09899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16317v2","updated":"2022-07-20T13:37:26Z","published":"2022-03-30T13:59:22Z","title":"PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised\n  Object Detection","summary":"  In this paper, we delve into two key techniques in Semi-Supervised Object\nDetection (SSOD), namely pseudo labeling and consistency training. We observe\nthat these two techniques currently neglect some important properties of object\ndetection, hindering efficient learning on unlabeled data. Specifically, for\npseudo labeling, existing works only focus on the classification score yet fail\nto guarantee the localization precision of pseudo boxes; For consistency\ntraining, the widely adopted random-resize training only considers the\nlabel-level consistency but misses the feature-level one, which also plays an\nimportant role in ensuring the scale invariance. To address the problems\nincurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that\nincludes Prediction-guided Label Assignment (PLA) and Positive-proposal\nConsistency Voting (PCV). PLA relies on model predictions to assign labels and\nmakes it robust to even coarse pseudo boxes; while PCV leverages the regression\nconsistency of positive proposals to reflect the localization quality of pseudo\nboxes. Furthermore, in consistency training, we propose Multi-view\nScale-invariant Learning (MSL) that includes mechanisms of both label- and\nfeature-level consistency, where feature consistency is achieved by aligning\nshifted feature pyramids between two images with identical content but varied\nscales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency\ntraining (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points\nunder 1%, 5%, and 10% labelling ratios, respectively. It also significantly\nimproves the learning efficiency for SSOD, e.g., PseCo halves the training time\nof the SOTA approach but achieves even better performance. Code is available at\nhttps://github.com/ligang-cs/PseCo.\n","authors":["Gang Li","Xiang Li","Yujie Wang","Yichao Wu","Ding Liang","Shanshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.16317v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09884v1","updated":"2022-07-20T13:30:16Z","published":"2022-07-20T13:30:16Z","title":"Negative Samples are at Large: Leveraging Hard-distance Elastic Loss for\n  Re-identification","summary":"  We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n","authors":["Hyungtae Lee","Sungmin Eum","Heesung Kwon"],"pdf_url":"https://arxiv.org/pdf/2207.09884v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2203.04187v2","updated":"2022-07-20T13:22:39Z","published":"2022-03-08T16:25:30Z","title":"RankSeg: Adaptive Pixel Classification with Image Category Ranking for\n  Segmentation","summary":"  The segmentation task has traditionally been formulated as a complete-label\npixel classification task to predict a class for each pixel from a fixed number\nof predefined semantic categories shared by all images or videos. Yet,\nfollowing this formulation, standard architectures will inevitably encounter\nvarious challenges under more realistic settings where the scope of categories\nscales up (e.g., beyond the level of 1k). On the other hand, in a typical image\nor video, only a few categories, i.e., a small subset of the complete label are\npresent. Motivated by this intuition, in this paper, we propose to decompose\nsegmentation into two sub-problems: (i) image-level or video-level multi-label\nclassification and (ii) pixel-level rank-adaptive selected-label\nclassification. Given an input image or video, our framework first conducts\nmulti-label classification over the complete label, then sorts the complete\nlabel and selects a small subset according to their class confidence scores. We\nthen use a rank-adaptive pixel classifier to perform the pixel-wise\nclassification over only the selected labels, which uses a set of rank-oriented\nlearnable temperature parameters to adjust the pixel classifications scores.\nOur approach is conceptually general and can be used to improve various\nexisting segmentation frameworks by simply using a lightweight multi-label\nclassification head and rank-adaptive pixel classifier. We demonstrate the\neffectiveness of our framework with competitive experimental results across\nfour tasks, including image semantic segmentation, image panoptic segmentation,\nvideo instance segmentation, and video semantic segmentation. Especially, with\nour RankSeg, Mask2Former gains +0.8%/+0.7%/+0.7% on ADE20K panoptic\nsegmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic\nsegmentation benchmarks respectively.\n","authors":["Haodi He","Yuhui Yuan","Xiangyu Yue","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2203.04187v2.pdf","comment":"Accepted at ECCV 2022, Code will be available at:\n  https://github.com/openseg-group/RankSeg"},{"id":"http://arxiv.org/abs/2207.09869v1","updated":"2022-07-20T13:04:08Z","published":"2022-07-20T13:04:08Z","title":"A Novel Neural Network Training Method for Autonomous Driving Using\n  Semi-Pseudo-Labels and 3D Data Augmentations","summary":"  Training neural networks to perform 3D object detection for autonomous\ndriving requires a large amount of diverse annotated data. However, obtaining\ntraining data with sufficient quality and quantity is expensive and sometimes\nimpossible due to human and sensor constraints. Therefore, a novel solution is\nneeded for extending current training methods to overcome this limitation and\nenable accurate 3D object detection. Our solution for the above-mentioned\nproblem combines semi-pseudo-labeling and novel 3D augmentations. For\ndemonstrating the applicability of the proposed method, we have designed a\nconvolutional neural network for 3D object detection which can significantly\nincrease the detection range in comparison with the training data distribution.\n","authors":["Tamas Matuszka","Daniel Kozma"],"pdf_url":"https://arxiv.org/pdf/2207.09869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07458v2","updated":"2022-07-20T13:03:18Z","published":"2022-06-15T11:29:58Z","title":"VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via\n  Speech-Visage Feature Selection","summary":"  The goal of this work is to reconstruct speech from a silent talking face\nvideo. Recent studies have shown impressive performance on synthesizing speech\nfrom silent talking face videos. However, they have not explicitly considered\non varying identity characteristics of different speakers, which place a\nchallenge in the video-to-speech synthesis, and this becomes more critical in\nunseen-speaker settings. Our approach is to separate the speech content and the\nvisage-style from a given silent talking face video. By guiding the model to\nindependently focus on modeling the two representations, we can obtain the\nspeech of high intelligibility from the model even when the input video of an\nunseen subject is given. To this end, we introduce speech-visage selection that\nseparates the speech content and the speaker identity from the visual features\nof the input video. The disentangled representations are jointly incorporated\nto synthesize speech through visage-style based synthesizer which generates\nspeech by coating the visage-styles while maintaining the speech content. Thus,\nthe proposed framework brings the advantage of synthesizing the speech\ncontaining the right content even with the silent talking face video of an\nunseen subject. We validate the effectiveness of the proposed framework on the\nGRID, TCD-TIMIT volunteer, and LRW datasets.\n","authors":["Joanna Hong","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2206.07458v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09868v1","updated":"2022-07-20T13:02:51Z","published":"2022-07-20T13:02:51Z","title":"Adaptive Mixture of Experts Learning for Generalizable Face\n  Anti-Spoofing","summary":"  With various face presentation attacks emerging continually, face\nanti-spoofing (FAS) approaches based on domain generalization (DG) have drawn\ngrowing attention. Existing DG-based FAS approaches always capture the\ndomain-invariant features for generalizing on the various unseen domains.\nHowever, they neglect individual source domains' discriminative characteristics\nand diverse domain-specific information of the unseen domains, and the trained\nmodel is not sufficient to be adapted to various unseen domains. To address\nthis issue, we propose an Adaptive Mixture of Experts Learning (AMEL)\nframework, which exploits the domain-specific information to adaptively\nestablish the link among the seen source domains and unseen target domains to\nfurther improve the generalization. Concretely, Domain-Specific Experts (DSE)\nare designed to investigate discriminative and unique domain-specific features\nas a complement to common domain-invariant features. Moreover, Dynamic Expert\nAggregation (DEA) is proposed to adaptively aggregate the complementary\ninformation of each source expert based on the domain relevance to the unseen\ntarget domain. And combined with meta-learning, these modules work\ncollaboratively to adaptively aggregate meaningful domain-specific information\nfor the various unseen target domains. Extensive experiments and visualizations\ndemonstrate the effectiveness of our method against the state-of-the-art\ncompetitors.\n","authors":["Qianyu Zhou","Ke-Yue Zhang","Taiping Yao","Ran Yi","Shouhong Ding","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2207.09868v1.pdf","comment":"Accepted to ACM MM 2022"},{"id":"http://arxiv.org/abs/2207.09865v1","updated":"2022-07-20T12:54:23Z","published":"2022-07-20T12:54:23Z","title":"Discrete-Constrained Regression for Local Counting Models","summary":"  Local counts, or the number of objects in a local area, is a continuous value\nby nature. Yet recent state-of-the-art methods show that formulating counting\nas a classification task performs better than regression. Through a series of\nexperiments on carefully controlled synthetic data, we show that this\ncounter-intuitive result is caused by imprecise ground truth local counts.\nFactors such as biased dot annotations and incorrectly matched Gaussian kernels\nused to generate ground truth counts introduce deviations from the true local\ncounts. Standard continuous regression is highly sensitive to these errors,\nexplaining the performance gap between classification and regression. To\nmitigate the sensitivity, we loosen the regression formulation from a\ncontinuous scale to a discrete ordering and propose a novel\ndiscrete-constrained (DC) regression. Applied to crowd counting, DC-regression\nis more accurate than both classification and standard regression on three\npublic benchmarks. A similar advantage also holds for the age estimation task,\nverifying the overall effectiveness of DC-regression.\n","authors":["Haipeng Xiong","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2207.09865v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09856v1","updated":"2022-07-20T12:44:13Z","published":"2022-07-20T12:44:13Z","title":"Evaluating the Stability of Deep Image Quality Assessment With Respect\n  to Image Scaling","summary":"  Image quality assessment (IQA) is a fundamental metric for image processing\ntasks (e.g., compression). With full-reference IQAs, traditional IQAs, such as\nPSNR and SSIM, have been used. Recently, IQAs based on deep neural networks\n(deep IQAs), such as LPIPS and DISTS, have also been used. It is known that\nimage scaling is inconsistent among deep IQAs, as some perform down-scaling as\npre-processing, whereas others instead use the original image size. In this\npaper, we show that the image scale is an influential factor that affects deep\nIQA performance. We comprehensively evaluate four deep IQAs on the same five\ndatasets, and the experimental results show that image scale significantly\ninfluences IQA performance. We found that the most appropriate image scale is\noften neither the default nor the original size, and the choice differs\ndepending on the methods and datasets used. We visualized the stability and\nfound that PieAPP is the most stable among the four deep IQAs.\n","authors":["Koki Tsubota","Hiroaki Akutsu","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2207.09856v1.pdf","comment":"IEICE Transactions on Information and Systems (Letter)"},{"id":"http://arxiv.org/abs/2207.09855v1","updated":"2022-07-20T12:40:32Z","published":"2022-07-20T12:40:32Z","title":"Everything is There in Latent Space: Attribute Editing and Attribute\n  Style Manipulation by StyleGAN Latent Space Exploration","summary":"  Unconstrained Image generation with high realism is now possible using recent\nGenerative Adversarial Networks (GANs). However, it is quite challenging to\ngenerate images with a given set of attributes. Recent methods use style-based\nGAN models to perform image editing by leveraging the semantic hierarchy\npresent in the layers of the generator. We present Few-shot Latent-based\nAttribute Manipulation and Editing (FLAME), a simple yet effective framework to\nperform highly controlled image editing by latent space manipulation.\nSpecifically, we estimate linear directions in the latent space (of a\npre-trained StyleGAN) that controls semantic attributes in the generated image.\nIn contrast to previous methods that either rely on large-scale attribute\nlabeled datasets or attribute classifiers, FLAME uses minimal supervision of a\nfew curated image pairs to estimate disentangled edit directions. FLAME can\nperform both individual and sequential edits with high precision on a diverse\nset of images while preserving identity. Further, we propose a novel task of\nAttribute Style Manipulation to generate diverse styles for attributes such as\neyeglass and hair. We first encode a set of synthetic images of the same\nidentity but having different attribute styles in the latent space to estimate\nan attribute style manifold. Sampling a new latent from this manifold will\nresult in a new attribute style in the generated image. We propose a novel\nsampling method to sample latent from the manifold, enabling us to generate a\ndiverse set of attribute styles beyond the styles present in the training set.\nFLAME can generate diverse attribute styles in a disentangled manner. We\nillustrate the superior performance of FLAME against previous image editing\nmethods by extensive qualitative and quantitative comparisons. FLAME also\ngeneralizes well on multiple datasets such as cars and churches.\n","authors":["Rishubh Parihar","Ankit Dhiman","Tejan Karmali","R. Venkatesh Babu"],"pdf_url":"https://arxiv.org/pdf/2207.09855v1.pdf","comment":"Project page: https://sites.google.com/view/flamelatentediting"},{"id":"http://arxiv.org/abs/2207.08782v2","updated":"2022-07-20T12:39:36Z","published":"2022-07-18T17:38:40Z","title":"Instance-Aware Observer Network for Out-of-Distribution Object\n  Segmentation","summary":"  Recent work on Observer Network has shown promising results on\nOut-Of-Distribution (OOD) detection for semantic segmentation. These methods\nhave difficulty in precisely locating the point of interest in the image, i.e,\nthe anomaly. This limitation is due to the difficulty of fine-grained\nprediction at the pixel level. To address this issue, we provide instance\nknowledge to the observer. We extend the approach of ObsNet by harnessing an\ninstance-wise mask prediction. We use an additional, class agnostic, object\ndetector to filter and aggregate observer predictions. Finally, we predict an\nunique anomaly score for each instance in the image. We show that our proposed\nmethod accurately disentangle in-distribution objects from Out-Of-Distribution\nobjects on three datasets.\n","authors":["Victor Besnier","Andrei Bursuc","David Picard","Alexandre Briot"],"pdf_url":"https://arxiv.org/pdf/2207.08782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09851v1","updated":"2022-07-20T12:35:38Z","published":"2022-07-20T12:35:38Z","title":"An Embedded Monocular Vision Approach for Ground-Aware Objects Detection\n  and Position Estimation","summary":"  In the RoboCup Small Size League (SSL), teams are encouraged to propose\nsolutions for executing basic soccer tasks inside the SSL field using only\nembedded sensing information. Thus, this work proposes an embedded monocular\nvision approach for detecting objects and estimating relative positions inside\nthe soccer field. Prior knowledge from the environment is exploited by assuming\nobjects lay on the ground, and the onboard camera has its position fixed on the\nrobot. We implemented the proposed method on an NVIDIA Jetson Nano and employed\nSSD MobileNet v2 for 2D Object Detection with TensorRT optimization, detecting\nballs, robots, and goals with distances up to 3.5 meters. Ball localization\nevaluation shows that the proposed solution overcomes the currently used SSL\nvision system for positions closer than 1 meter to the onboard camera with a\nRoot Mean Square Error of 14.37 millimeters. In addition, the proposed method\nachieves real-time performance with an average processing speed of 30 frames\nper second.\n","authors":["João G. Melo","Edna Barros"],"pdf_url":"https://arxiv.org/pdf/2207.09851v1.pdf","comment":"12 pages, 5 figures, submitted to RoboCup Symposium 2022"},{"id":"http://arxiv.org/abs/2204.00559v4","updated":"2022-07-20T12:29:45Z","published":"2022-04-01T16:39:16Z","title":"DFNet: Enhance Absolute Pose Regression with Direct Feature Matching","summary":"  We introduce a camera relocalization pipeline that combines absolute pose\nregression (APR) and direct feature matching. By incorporating\nexposure-adaptive novel view synthesis, our method successfully addresses\nphotometric distortions in outdoor environments that existing photometric-based\nmethods fail to handle. With domain-invariant feature matching, our solution\nimproves pose regression accuracy using semi-supervised learning on unlabeled\ndata. In particular, the pipeline consists of two components: Novel View\nSynthesizer and DFNet. The former synthesizes novel views compensating for\nchanges in exposure and the latter regresses camera poses and extracts robust\nfeatures that close the domain gap between real images and synthetic ones.\nFurthermore, we introduce an online synthetic data generation scheme. We show\nthat these approaches effectively enhance camera pose estimation both in indoor\nand outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by\noutperforming existing single-image APR methods by as much as 56%, comparable\nto 3D structure-based methods.\n","authors":["Shuai Chen","Xinghui Li","Zirui Wang","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2204.00559v4.pdf","comment":"ECCV 2022. Code released at https://github.com/ActiveVisionLab/DFNet"},{"id":"http://arxiv.org/abs/2112.08906v2","updated":"2022-07-20T12:26:00Z","published":"2021-12-16T14:24:17Z","title":"On the Uncertain Single-View Depths in Colonoscopies","summary":"  Estimating depth information from endoscopic images is a prerequisite for a\nwide set of AI-assisted technologies, such as accurate localization and\nmeasurement of tumors, or identification of non-inspected areas. As the domain\nspecificity of colonoscopies -- deformable low-texture environments with\nfluids, poor lighting conditions and abrupt sensor motions -- pose challenges\nto multi-view 3D reconstructions, single-view depth learning stands out as a\npromising line of research. Depth learning can be extended in a Bayesian\nsetting, which enables continual learning, improves decision making and can be\nused to compute confidence intervals or quantify uncertainty for in-body\nmeasurements. In this paper, we explore for the first time Bayesian deep\nnetworks for single-view depth estimation in colonoscopies. Our specific\ncontribution is two-fold: 1) an exhaustive analysis of scalable Bayesian\nnetworks for depth learning in different datasets, highlighting challenges and\nconclusions regarding synthetic-to-real domain changes and supervised vs.\nself-supervised methods; and 2) a novel teacher-student approach to deep depth\nlearning that takes into account the teacher uncertainty.\n","authors":["Javier Rodríguez-Puigvert","David Recasens","Javier Civera","Rubén Martínez-Cantín"],"pdf_url":"https://arxiv.org/pdf/2112.08906v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2201.07412v2","updated":"2022-07-20T12:25:18Z","published":"2022-01-19T04:31:57Z","title":"Poseur: Direct Human Pose Regression with Transformers","summary":"  We propose a direct, regression-based approach to 2D human pose estimation\nfrom single images. We formulate the problem as a sequence prediction task,\nwhich we solve using a Transformer network. This network directly learns a\nregression mapping from images to the keypoint coordinates, without resorting\nto intermediate representations such as heatmaps. This approach avoids much of\nthe complexity associated with heatmap-based approaches. To overcome the\nfeature misalignment issues of previous regression-based methods, we propose an\nattention mechanism that adaptively attends to the features that are most\nrelevant to the target keypoints, considerably improving the accuracy.\nImportantly, our framework is end-to-end differentiable, and naturally learns\nto exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,\ntwo predominant pose-estimation datasets, demonstrate that our method\nsignificantly improves upon the state-of-the-art in regression-based pose\nestimation. More notably, ours is the first regression-based approach to\nperform favorably compared to the best heatmap-based pose estimation methods.\n","authors":["Weian Mao","Yongtao Ge","Chunhua Shen","Zhi Tian","Xinlong Wang","Zhibin Wang","Anton van den Hengel"],"pdf_url":"https://arxiv.org/pdf/2201.07412v2.pdf","comment":"Accepted to Proc. Eur. Conf. Comp. Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2205.06230v2","updated":"2022-07-20T12:24:25Z","published":"2022-05-12T17:20:36Z","title":"Simple Open-Vocabulary Object Detection with Vision Transformers","summary":"  Combining simple architectures with large-scale pre-training has led to\nmassive improvements in image classification. For object detection,\npre-training and scaling approaches are less well established, especially in\nthe long-tailed and open-vocabulary setting, where training data is relatively\nscarce. In this paper, we propose a strong recipe for transferring image-text\nmodels to open-vocabulary object detection. We use a standard Vision\nTransformer architecture with minimal modifications, contrastive image-text\npre-training, and end-to-end detection fine-tuning. Our analysis of the scaling\nproperties of this setup shows that increasing image-level pre-training and\nmodel size yield consistent improvements on the downstream detection task. We\nprovide the adaptation strategies and regularizations needed to attain very\nstrong performance on zero-shot text-conditioned and one-shot image-conditioned\nobject detection. Code and models are available on GitHub.\n","authors":["Matthias Minderer","Alexey Gritsenko","Austin Stone","Maxim Neumann","Dirk Weissenborn","Alexey Dosovitskiy","Aravindh Mahendran","Anurag Arnab","Mostafa Dehghani","Zhuoran Shen","Xiao Wang","Xiaohua Zhai","Thomas Kipf","Neil Houlsby"],"pdf_url":"https://arxiv.org/pdf/2205.06230v2.pdf","comment":"ECCV 2022 camera-ready version"},{"id":"http://arxiv.org/abs/2103.16554v2","updated":"2022-07-20T11:53:15Z","published":"2021-03-30T17:57:25Z","title":"Pre-training strategies and datasets for facial representation learning","summary":"  What is the best way to learn a universal face representation? Recent work on\nDeep Learning in the area of face analysis has focused on supervised learning\nfor specific tasks of interest (e.g. face recognition, facial landmark\nlocalization etc.) but has overlooked the overarching question of how to find a\nfacial representation that can be readily adapted to several facial analysis\ntasks and datasets. To this end, we make the following 4 contributions: (a) we\nintroduce, for the first time, a comprehensive evaluation benchmark for facial\nrepresentation learning consisting of 5 important face analysis tasks. (b) We\nsystematically investigate two ways of large-scale representation learning\napplied to faces: supervised and unsupervised pre-training. Importantly, we\nfocus our evaluations on the case of few-shot facial learning. (c) We\ninvestigate important properties of the training datasets including their size\nand quality (labelled, unlabelled or even uncurated). (d) To draw our\nconclusions, we conducted a very large number of experiments. Our main two\nfindings are: (1) Unsupervised pre-training on completely in-the-wild,\nuncurated data provides consistent and, in some cases, significant accuracy\nimprovements for all facial tasks considered. (2) Many existing facial video\ndatasets seem to have a large amount of redundancy. We will release code, and\npre-trained models to facilitate future research.\n","authors":["Adrian Bulat","Shiyang Cheng","Jing Yang","Andrew Garbett","Enrique Sanchez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2103.16554v2.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09840v1","updated":"2022-07-20T11:52:07Z","published":"2022-07-20T11:52:07Z","title":"EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer","summary":"  Most existing methods view makeup transfer as transferring color\ndistributions of different facial regions and ignore details such as eye\nshadows and blushes. Besides, they only achieve controllable transfer within\npredefined fixed regions. This paper emphasizes the transfer of makeup details\nand steps towards more flexible controls. To this end, we propose Exquisite and\nlocally editable GAN for makeup transfer (EleGANt). It encodes facial\nattributes into pyramidal feature maps to preserves high-frequency information.\nIt uses attention to extract makeup features from the reference and adapt them\nto the source face, and we introduce a novel Sow-Attention Module that applies\nattention within shifted overlapped windows to reduce the computational cost.\nMoreover, EleGANt is the first to achieve customized local editing within\narbitrary areas by corresponding editing on the feature maps. Extensive\nexperiments demonstrate that EleGANt generates realistic makeup faces with\nexquisite details and achieves state-of-the-art performance. The code is\navailable at https://github.com/Chenyu-Yang-2000/EleGANt.\n","authors":["Chenyu Yang","Wanrong He","Yingqing Xu","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2207.09840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.02452v2","updated":"2022-07-20T11:48:23Z","published":"2022-03-04T17:29:32Z","title":"Contextformer: A Transformer with Spatio-Channel Attention for Context\n  Modeling in Learned Image Compression","summary":"  Entropy modeling is a key component for high-performance image compression\nalgorithms. Recent developments in autoregressive context modeling helped\nlearning-based methods to surpass their classical counterparts. However, the\nperformance of those models can be further improved due to the underexploited\nspatio-channel dependencies in latent space, and the suboptimal implementation\nof context adaptivity. Inspired by the adaptive characteristics of the\ntransformers, we propose a transformer-based context model, named\nContextformer, which generalizes the de facto standard attention mechanism to\nspatio-channel attention. We replace the context model of a modern compression\nframework with the Contextformer and test it on the widely used Kodak,\nCLIC2020, and Tecnick image datasets. Our experimental results show that the\nproposed model provides up to 11% rate savings compared to the standard\nVersatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various\nlearning-based models in terms of PSNR and MS-SSIM.\n","authors":["A. Burakhan Koyuncu","Han Gao","Atanas Boev","Georgii Gaikov","Elena Alshina","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2203.02452v2.pdf","comment":"Accepted at ECCV 2022; 31 pages (14 main paper + References + 13\n  Appendix)"},{"id":"http://arxiv.org/abs/2207.09835v1","updated":"2022-07-20T11:41:29Z","published":"2022-07-20T11:41:29Z","title":"UNIF: United Neural Implicit Functions for Clothed Human Reconstruction\n  and Animation","summary":"  We propose united implicit functions (UNIF), a part-based method for clothed\nhuman reconstruction and animation with raw scans and skeletons as the input.\nPrevious part-based methods for human reconstruction rely on ground-truth part\nlabels from SMPL and thus are limited to minimal-clothed humans. In contrast,\nour method learns to separate parts from body motions instead of part\nsupervision, thus can be extended to clothed humans and other articulated\nobjects. Our Partition-from-Motion is achieved by a bone-centered\ninitialization, a bone limit loss, and a section normal loss that ensure stable\npart division even when the training poses are limited. We also present a\nminimal perimeter loss for SDF to suppress extra surfaces and part overlapping.\nAnother core of our method is an adjacent part seaming algorithm that produces\nnon-rigid deformations to maintain the connection between parts which\nsignificantly relieves the part-based artifacts. Under this algorithm, we\nfurther propose \"Competing Parts\", a method that defines blending weights by\nthe relative position of a point to bones instead of the absolute position,\navoiding the generalization problem of neural implicit functions with inverse\nLBS (linear blend skinning). We demonstrate the effectiveness of our method by\nclothed human body reconstruction and animation on the CAPE and the ClothSeq\ndatasets.\n","authors":["Shenhan Qian","Jiale Xu","Ziwei Liu","Liqian Ma","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2207.09835v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2205.12796v2","updated":"2022-07-20T11:38:29Z","published":"2022-05-25T14:10:33Z","title":"Non-rigid Point Cloud Registration with Neural Deformation Pyramid","summary":"  Non-rigid point cloud registration is a key component in many computer vision\nand computer graphics applications. The high complexity of the unknown\nnon-rigid motion make this task a challenging problem. In this paper, we break\ndown this problem via hierarchical motion decomposition. Our method called\nNeural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid\narchitecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),\ntakes as input a sinusoidally encoded 3D point and outputs its motion\nincrements from the previous level. The sinusoidal function starts with a low\ninput frequency and gradually increases when the pyramid level goes down. This\nallows a multi-level rigid to nonrigid motion decomposition and also speeds up\nthe solving by 50 times compared to the existing MLP-based approach. Our method\nachieves advanced partialto-partial non-rigid point cloud registration results\non the 4DMatch/4DLoMatch benchmark under both no-learned and supervised\nsettings.\n","authors":["Yang Li","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2205.12796v2.pdf","comment":"Code: https://github.com/rabbityl/DeformationPyramid"},{"id":"http://arxiv.org/abs/2204.04662v2","updated":"2022-07-20T11:37:42Z","published":"2022-04-10T11:38:33Z","title":"FOSTER: Feature Boosting and Compression for Class-Incremental Learning","summary":"  The ability to learn new concepts continually is necessary in this\never-changing world. However, deep neural networks suffer from catastrophic\nforgetting when learning new categories. Many works have been proposed to\nalleviate this phenomenon, whereas most of them either fall into the\nstability-plasticity dilemma or take too much computation or storage overhead.\nInspired by the gradient boosting algorithm to gradually fit the residuals\nbetween the target model and the previous ensemble model, we propose a novel\ntwo-stage learning paradigm FOSTER, empowering the model to learn new\ncategories adaptively. Specifically, we first dynamically expand new modules to\nfit the residuals between the target and the output of the original model.\nNext, we remove redundant parameters and feature dimensions through an\neffective distillation strategy to maintain the single backbone model. We\nvalidate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different\nsettings. Experimental results show that our method achieves state-of-the-art\nperformance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.\n","authors":["Fu-Yun Wang","Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2204.04662v2.pdf","comment":"Accepted to ECCV 2022. Code is available at:\n  https://github.com/G-U-N/ECCV22-FOSTER"},{"id":"http://arxiv.org/abs/2207.09814v1","updated":"2022-07-20T10:55:55Z","published":"2022-07-20T10:55:55Z","title":"NUWA-Infinity: Autoregressive over Autoregressive Generation for\n  Infinite Visual Synthesis","summary":"  In this paper, we present NUWA-Infinity, a generative model for infinite\nvisual synthesis, which is defined as the task of generating arbitrarily-sized\nhigh-resolution images or long-duration videos. An autoregressive over\nautoregressive generation mechanism is proposed to deal with this variable-size\ngeneration task, where a global patch-level autoregressive model considers the\ndependencies between patches, and a local token-level autoregressive model\nconsiders dependencies between visual tokens within each patch. A Nearby\nContext Pool (NCP) is introduced to cache-related patches already generated as\nthe context for the current patch being generated, which can significantly save\ncomputation costs without sacrificing patch-level dependency modeling. An\nArbitrary Direction Controller (ADC) is used to decide suitable generation\norders for different visual synthesis tasks and learn order-aware positional\nembeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate\nhigh-resolution images with arbitrary sizes and support long-duration video\ngeneration additionally. Compared to NUWA, which also covers images and videos,\nNUWA-Infinity has superior visual synthesis capabilities in terms of resolution\nand variable-size generation. The GitHub link is\nhttps://github.com/microsoft/NUWA. The homepage link is\nhttps://nuwa-infinity.microsoft.com.\n","authors":["Chenfei Wu","Jian Liang","Xiaowei Hu","Zhe Gan","Jianfeng Wang","Lijuan Wang","Zicheng Liu","Yuejian Fang","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2207.09814v1.pdf","comment":"24 pages, 19 figures"},{"id":"http://arxiv.org/abs/2112.13548v3","updated":"2022-07-20T10:54:23Z","published":"2021-12-27T07:18:50Z","title":"Responsive Listening Head Generation: A Benchmark Dataset and Baseline","summary":"  We present a new listening head generation benchmark, for synthesizing\nresponsive feedbacks of a listener (e.g., nod, smile) during a face-to-face\nconversation. As the indispensable complement to talking heads generation,\nlistening head generation has seldomly been studied in literature.\nAutomatically synthesizing listening behavior that actively responds to a\ntalking head, is critical to applications such as digital human, virtual agents\nand social robots. In this work, we propose a novel dataset \"ViCo\",\nhighlighting the listening head generation during a face-to-face conversation.\nA total number of 92 identities (67 speakers and 76 listeners) are involved in\nViCo, featuring 483 clips in a paired \"speaking-listening\" pattern, where\nlisteners show three listening styles based on their attitudes: positive,\nneutral, negative. Different from traditional speech-to-gesture or talking-head\ngeneration, listening head generation takes as input both the audio and visual\nsignals from the speaker, and gives non-verbal feedbacks (e.g., head motions,\nfacial expressions) in a real-time manner. Our dataset supports a wide range of\napplications such as human-to-human interaction, video-to-video translation,\ncross-modal understanding and generation. To encourage further research, we\nalso release a listening head generation baseline, conditioning on different\nlistening attitudes. Code & ViCo dataset: https://project.mhzhou.com/vico.\n","authors":["Mohan Zhou","Yalong Bai","Wei Zhang","Ting Yao","Tiejun Zhao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2112.13548v3.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09812v1","updated":"2022-07-20T10:53:48Z","published":"2022-07-20T10:53:48Z","title":"The Anatomy of Video Editing: A Dataset and Benchmark Suite for\n  AI-Assisted Video Editing","summary":"  Machine learning is transforming the video editing industry. Recent advances\nin computer vision have leveled-up video editing tasks such as intelligent\nreframing, rotoscoping, color grading, or applying digital makeups. However,\nmost of the solutions have focused on video manipulation and VFX. This work\nintroduces the Anatomy of Video Editing, a dataset, and benchmark, to foster\nresearch in AI-assisted video editing. Our benchmark suite focuses on video\nediting tasks, beyond visual effects, such as automatic footage organization\nand assisted video assembling. To enable research on these fronts, we annotate\nmore than 1.5M tags, with relevant concepts to cinematography, from 196176\nshots sampled from movie scenes. We establish competitive baseline methods and\ndetailed analyses for each of the tasks. We hope our work sparks innovative\nresearch towards underexplored areas of AI-assisted video editing.\n","authors":["Dawit Mureja Argaw","Fabian Caba Heilbron","Joon-Young Lee","Markus Woodson","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2207.09812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12385v2","updated":"2022-07-20T10:39:16Z","published":"2021-11-24T10:10:04Z","title":"Space-Partitioning RANSAC","summary":"  A new algorithm is proposed to accelerate RANSAC model quality calculations.\nThe method is based on partitioning the joint correspondence space, e.g., 2D-2D\npoint correspondences, into a pair of regular grids. The grid cells are mapped\nby minimal sample models, estimated within RANSAC, to reject correspondences\nthat are inconsistent with the model parameters early. The proposed technique\nis general. It works with arbitrary transformations even if a point is mapped\nto a point set, e.g., as a fundamental matrix maps to epipolar lines. The\nmethod is tested on thousands of image pairs from publicly available datasets\non fundamental and essential matrix, homography and radially distorted\nhomography estimation. On average, it reduces the RANSAC run-time by 41% with\nprovably no deterioration in the accuracy. It can be straightforwardly plugged\ninto state-of-the-art RANSAC frameworks, e.g. VSAC.\n","authors":["Daniel Barath","Gabor Valasek"],"pdf_url":"https://arxiv.org/pdf/2111.12385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09805v1","updated":"2022-07-20T10:38:29Z","published":"2022-07-20T10:38:29Z","title":"Multimodal Transformer for Automatic 3D Annotation and Object Detection","summary":"  Despite a growing number of datasets being collected for training 3D object\ndetection models, significant human effort is still required to annotate 3D\nboxes on LiDAR scans. To automate the annotation and facilitate the production\nof various customized datasets, we propose an end-to-end multimodal transformer\n(MTrans) autolabeler, which leverages both LiDAR scans and images to generate\nprecise 3D box annotations from weak 2D bounding boxes. To alleviate the\npervasive sparsity problem that hinders existing autolabelers, MTrans densifies\nthe sparse point clouds by generating new 3D points based on 2D image\ninformation. With a multi-task design, MTrans segments the\nforeground/background, densifies LiDAR point clouds, and regresses 3D boxes\nsimultaneously. Experimental results verify the effectiveness of the MTrans for\nimproving the quality of the generated labels. By enriching the sparse point\nclouds, our method achieves 4.48\\% and 4.03\\% better 3D AP on KITTI moderate\nand hard samples, respectively, versus the state-of-the-art autolabeler. MTrans\ncan also be extended to improve the accuracy for 3D object detection, resulting\nin a remarkable 89.45\\% AP on KITTI hard samples. Codes are at\n\\url{https://github.com/Cliu2/MTrans}.\n","authors":["Chang Liu","Xiaoyan Qian","Binxiao Huang","Xiaojuan Qi","Edmund Lam","Siew-Chong Tan","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2207.09805v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2103.14373v4","updated":"2022-07-20T10:29:12Z","published":"2021-03-26T10:20:28Z","title":"D2C-SR: A Divergence to Convergence Approach for Real-World Image\n  Super-Resolution","summary":"  In this paper, we present D2C-SR, a novel framework for the task of\nreal-world image super-resolution. As an ill-posed problem, the key challenge\nin super-resolution related tasks is there can be multiple predictions for a\ngiven low-resolution input. Most classical deep learning based approaches\nignored the fundamental fact and lack explicit modeling of the underlying\nhigh-frequency distribution which leads to blurred results. Recently, some\nmethods of GAN-based or learning super-resolution space can generate simulated\ntextures but do not promise the accuracy of the textures which have low\nquantitative performance. Rethinking both, we learn the distribution of\nunderlying high-frequency details in a discrete form and propose a two-stage\npipeline: divergence stage to convergence stage. At divergence stage, we\npropose a tree-based structure deep network as our divergence backbone.\nDivergence loss is proposed to encourage the generated results from the\ntree-based network to diverge into possible high-frequency representations,\nwhich is our way of discretely modeling the underlying high-frequency\ndistribution. At convergence stage, we assign spatial weights to fuse these\ndivergent predictions to obtain the final output with more accurate details.\nOur approach provides a convenient end-to-end manner to inference. We conduct\nevaluations on several real-world benchmarks, including a new proposed\nD2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that\nD2C-SR achieves better accuracy and visual improvements against\nstate-of-the-art methods, with a significantly less parameters number and our\nD2C structure can also be applied as a generalized structure to some other\nmethods to obtain improvement. Our codes and dataset are available at\nhttps://github.com/megvii-research/D2C-SR\n","authors":["Youwei Li","Haibin Huang","Lanpeng Jia","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2103.14373v4.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09796v1","updated":"2022-07-20T10:26:18Z","published":"2022-07-20T10:26:18Z","title":"EASNet: Searching Elastic and Accurate Network Architecture for Stereo\n  Matching","summary":"  Recent advanced studies have spent considerable human efforts on optimizing\nnetwork architectures for stereo matching but hardly achieved both high\naccuracy and fast inference speed. To ease the workload in network design,\nneural architecture search (NAS) has been applied with great success to various\nsparse prediction tasks, such as image classification and object detection.\nHowever, existing NAS studies on the dense prediction task, especially stereo\nmatching, still cannot be efficiently and effectively deployed on devices of\ndifferent computing capabilities. To this end, we propose to train an elastic\nand accurate network for stereo matching (EASNet) that supports various 3D\narchitectural settings on devices with different computing capabilities. Given\nthe deployment latency constraint on the target device, we can quickly extract\na sub-network from the full EASNet without additional training while the\naccuracy of the sub-network can still be maintained. Extensive experiments show\nthat our EASNet outperforms both state-of-the-art human-designed and NAS-based\narchitectures on Scene Flow and MPI Sintel datasets in terms of model accuracy\nand inference speed. Particularly, deployed on an inference GPU, EASNet\nachieves a new SOTA 0.73 EPE on the Scene Flow dataset with 100 ms, which is\n4.5$\\times$ faster than LEAStereo with a better quality model.\n","authors":["Qiang Wang","Shaohuai Shi","Kaiyong Zhao","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2207.09796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13681v3","updated":"2022-07-20T10:26:06Z","published":"2021-11-26T18:59:58Z","title":"ManiFest: Manifold Deformation for Few-shot Image Translation","summary":"  Most image-to-image translation methods require a large number of training\nimages, which restricts their applicability. We instead propose ManiFest: a\nframework for few-shot image translation that learns a context-aware\nrepresentation of a target domain from a few images only. To enforce feature\nconsistency, our framework learns a style manifold between source and proxy\nanchor domains (assumed to be composed of large numbers of images). The learned\nmanifold is interpolated and deformed towards the few-shot target domain via\npatch-based adversarial and feature statistics alignment losses. All of these\ncomponents are trained simultaneously during a single end-to-end loop. In\naddition to the general few-shot translation task, our approach can\nalternatively be conditioned on a single exemplar image to reproduce its\nspecific style. Extensive experiments demonstrate the efficacy of ManiFest on\nmultiple tasks, outperforming the state-of-the-art on all metrics and in both\nthe general- and exemplar-based scenarios. Our code is available at\nhttps://github.com/cv-rits/Manifest .\n","authors":["Fabio Pizzati","Jean-François Lalonde","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2111.13681v3.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09792v1","updated":"2022-07-20T10:09:53Z","published":"2022-07-20T10:09:53Z","title":"Unsupervised Industrial Anomaly Detection via Pattern Generative and\n  Contrastive Networks","summary":"  It is hard to collect enough flaw images for training deep learning network\nin industrial production. Therefore, existing industrial anomaly detection\nmethods prefer to use CNN-based unsupervised detection and localization network\nto achieve this task. However, these methods always fail when there are\nvarieties happened in new signals since traditional end-to-end networks suffer\nbarriers of fitting nonlinear model in high-dimensional space. Moreover, they\nhave a memory library by clustering the feature of normal images essentially,\nwhich cause it is not robust to texture change. To this end, we propose the\nVision Transformer based (VIT-based) unsupervised anomaly detection network. It\nutilizes a hierarchical task learning and human experience to enhance its\ninterpretability. Our network consists of pattern generation and comparison\nnetworks. Pattern generation network uses two VIT-based encoder modules to\nextract the feature of two consecutive image patches, then uses VIT-based\ndecoder module to learn the human designed style of these features and predict\nthe third image patch. After this, we use the Siamese-based network to compute\nthe similarity of the generation image patch and original image patch. Finally,\nwe refine the anomaly localization by the bi-directional inference strategy.\nComparison experiments on public dataset MVTec dataset show our method achieves\n99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we\ngive a qualitative illustration on our own leather and cloth datasets. The\naccurate segment results strongly prove the accuracy of our method in anomaly\ndetection.\n","authors":["Jianfeng Huang","Chenyang Li","Yimin Lin","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2207.09792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09790v1","updated":"2022-07-20T10:08:34Z","published":"2022-07-20T10:08:34Z","title":"FaceFormer: Scale-aware Blind Face Restoration with Transformers","summary":"  Blind face restoration usually encounters with diverse scale face inputs,\nespecially in the real world. However, most of the current works support\nspecific scale faces, which limits its application ability in real-world\nscenarios. In this work, we propose a novel scale-aware blind face restoration\nframework, named FaceFormer, which formulates facial feature restoration as\nscale-aware transformation. The proposed Facial Feature Up-sampling (FFUP)\nmodule dynamically generates upsampling filters based on the original\nscale-factor priors, which facilitate our network to adapt to arbitrary face\nscales. Moreover, we further propose the facial feature embedding (FFE) module\nwhich leverages transformer to hierarchically extract diversity and robustness\nof facial latent. Thus, our FaceFormer achieves fidelity and robustness\nrestored faces, which possess realistic and symmetrical details of facial\ncomponents. Extensive experiments demonstrate that our proposed method trained\nwith synthetic dataset generalizes better to a natural low quality images than\ncurrent state-of-the-arts.\n","authors":["Aijin Li","Gen Li","Lei Sun","Xintao Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01322v2","updated":"2022-07-20T09:42:46Z","published":"2022-07-04T10:59:33Z","title":"Harmonizer: Learning to Perform White-Box Image and Video Harmonization","summary":"  Recent works on image harmonization solve the problem as a pixel-wise image\ntranslation task via large autoencoders. They have unsatisfactory performances\nand slow inference speeds when dealing with high-resolution images. In this\nwork, we observe that adjusting the input arguments of basic image filters,\ne.g., brightness and contrast, is sufficient for humans to produce realistic\nimages from the composite ones. Hence, we frame image harmonization as an\nimage-level regression problem to learn the arguments of the filters that\nhumans use for the task. We present a Harmonizer framework for image\nharmonization. Unlike prior methods that are based on black-box autoencoders,\nHarmonizer contains a neural network for filter argument prediction and several\nwhite-box filters (based on the predicted arguments) for image harmonization.\nWe also introduce a cascade regressor and a dynamic loss strategy for\nHarmonizer to learn filter arguments more stably and precisely. Since our\nnetwork only outputs image-level arguments and the filters we used are\nefficient, Harmonizer is much lighter and faster than existing methods.\nComprehensive experiments demonstrate that Harmonizer surpasses existing\nmethods notably, especially with high-resolution inputs. Finally, we apply\nHarmonizer to video harmonization, which achieves consistent results across\nframes and 56 fps at 1080P resolution. Code and models are available at:\nhttps://github.com/ZHKKKe/Harmonizer.\n","authors":["Zhanghan Ke","Chunyi Sun","Lei Zhu","Ke Xu","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2207.01322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09778v1","updated":"2022-07-20T09:33:42Z","published":"2022-07-20T09:33:42Z","title":"CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR\n  Segmentation","summary":"  3D LiDAR semantic segmentation is fundamental for autonomous driving. Several\nUnsupervised Domain Adaptation (UDA) methods for point cloud data have been\nrecently proposed to improve model generalization for different sensors and\nenvironments. Researchers working on UDA problems in the image domain have\nshown that sample mixing can mitigate domain shift. We propose a new approach\nof sample mixing for point cloud UDA, namely Compositional Semantic Mix\n(CoSMix), the first UDA approach for point cloud segmentation based on sample\nmixing. CoSMix consists of a two-branch symmetric network that can process\nlabelled synthetic data (source) and real-world unlabelled point clouds\n(target) concurrently. Each branch operates on one domain by mixing selected\npieces of data from the other one, and by using the semantic information\nderived from source labels and target pseudo-labels. We evaluate CoSMix on two\nlarge-scale datasets, showing that it outperforms state-of-the-art methods by a\nlarge margin. Our code is available at\nhttps://github.com/saltoricristiano/cosmix-uda.\n","authors":["Cristiano Saltori","Fabio Galasso","Giuseppe Fiameni","Nicu Sebe","Elisa Ricci","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2207.09778v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09777v1","updated":"2022-07-20T09:33:39Z","published":"2022-07-20T09:33:39Z","title":"AU-Supervised Convolutional Vision Transformers for Synthetic Facial\n  Expression Recognition","summary":"  The paper describes our proposed methodology for the six basic expression\nclassification track of Affective Behavior Analysis in-the-wild (ABAW)\nCompetition 2022. In Learing from Synthetic Data(LSD) task, facial expression\nrecognition (FER) methods aim to learn the representation of expression from\nthe artificially generated data and generalise to real data. Because of the\nambiguous of the synthetic data and the objectivity of the facial Action Unit\n(AU), we resort to the AU information for performance boosting, and make\ncontributions as follows. First, to adapt the model to synthetic scenarios, we\nuse the knowledge from pre-trained large-scale face recognition data. Second,\nwe propose a conceptually-new framework, termed as AU-Supervised Convolutional\nVision Transformers (AU-CVT), which clearly improves the performance of FER by\njointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT\nachieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The\nsource code of our work is publicly available online:\nhttps://github.com/msy1412/ABAW4\n","authors":["Shuyi Mao","Xinpeng Li","Junyao Chen","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2207.09777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09440v3","updated":"2022-07-20T09:29:02Z","published":"2022-03-17T17:00:55Z","title":"TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes","summary":"  Many basic indoor activities such as eating or writing are always conducted\nupon different tabletops (e.g., coffee tables, writing desks). It is\nindispensable to understanding tabletop scenes in 3D indoor scene parsing\napplications. Unfortunately, it is hard to meet this demand by directly\ndeploying data-driven algorithms, since 3D tabletop scenes are rarely available\nin current datasets. To remedy this defect, we introduce TO-Scene, a\nlarge-scale dataset focusing on tabletop scenes, which contains 20,740 scenes\nwith three variants. To acquire the data, we design an efficient and scalable\nframework, where a crowdsourcing UI is developed to transfer CAD objects from\nModelNet and ShapeNet onto tables from ScanNet, then the output tabletop scenes\nare simulated into real scans and annotated automatically.\n  Further, a tabletop-aware learning strategy is proposed for better perceiving\nthe small-sized tabletop instances. Notably, we also provide a real scanned\ntest set TO-Real to verify the practical value of TO-Scene. Experiments show\nthat the algorithms trained on TO-Scene indeed work on the realistic test data,\nand our proposed tabletop-aware learning strategy greatly improves the\nstate-of-the-art results on both 3D semantic segmentation and object detection\ntasks. Dataset and code are available at\nhttps://github.com/GAP-LAB-CUHK-SZ/TO-Scene.\n","authors":["Mutian Xu","Pei Chen","Haolin Liu","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2203.09440v3.pdf","comment":"ECCV 2022 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2207.09775v1","updated":"2022-07-20T09:28:51Z","published":"2022-07-20T09:28:51Z","title":"More Practical Scenario of Open-set Object Detection: Open at Category\n  Level and Closed at Super-category Level","summary":"  Open-set object detection (OSOD) has recently attracted considerable\nattention. It is to detect unknown objects while correctly\ndetecting/classifying known objects. We first point out that the scenario of\nOSOD considered in recent studies, which considers an unlimited variety of\nunknown objects similar to open-set recognition (OSR), has a fundamental issue.\nThat is, we cannot determine what to detect and what not for such unlimited\nunknown objects, which is necessary for detection tasks. This issue leads to\ndifficulty with the evaluation of methods' performance on unknown object\ndetection. We then introduce a novel scenario of OSOD, which deals with only\nunknown objects that share the super-category with known objects. It has many\nreal-world applications, e.g., detecting an increasing number of fine-grained\nobjects. This new setting is free from the above issue and evaluation\ndifficulty. Moreover, it makes detecting unknown objects more realistic owing\nto the visual similarity between known and unknown objects. We show through\nexperimental results that a simple method based on the uncertainty of class\nprediction from standard detectors outperforms the current state-of-the-art\nOSOD methods tested in the previous setting.\n","authors":["Yusuke Hosoya","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2207.09775v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.09774v1","updated":"2022-07-20T09:28:16Z","published":"2022-07-20T09:28:16Z","title":"Drivable Volumetric Avatars using Texel-Aligned Features","summary":"  Photorealistic telepresence requires both high-fidelity body modeling and\nfaithful driving to enable dynamically synthesized appearance that is\nindistinguishable from reality. In this work, we propose an end-to-end\nframework that addresses two core challenges in modeling and driving full-body\navatars of real people. One challenge is driving an avatar while staying\nfaithful to details and dynamics that cannot be captured by a global\nlow-dimensional parameterization such as body pose. Our approach supports\ndriving of clothed avatars with wrinkles and motion that a real driving\nperformer exhibits beyond the training corpus. Unlike existing global state\nrepresentations or non-parametric screen-space approaches, we introduce\ntexel-aligned features -- a localised representation which can leverage both\nthe structural prior of a skeleton-based parametric model and observed sparse\nimage signals at the same time. Another challenge is modeling a temporally\ncoherent clothed avatar, which typically requires precise surface tracking. To\ncircumvent this, we propose a novel volumetric avatar representation by\nextending mixtures of volumetric primitives to articulated objects. By\nexplicitly incorporating articulation, our approach naturally generalizes to\nunseen poses. We also introduce a localized viewpoint conditioning, which leads\nto a large improvement in generalization of view-dependent appearance. The\nproposed volumetric representation does not require high-quality mesh tracking\nas a prerequisite and brings significant quality improvements compared to\nmesh-based counterparts. In our experiments, we carefully examine our design\nchoices and demonstrate the efficacy of our approach, outperforming the\nstate-of-the-art methods on challenging driving scenarios.\n","authors":["Edoardo Remelli","Timur Bagautdinov","Shunsuke Saito","Tomas Simon","Chenglei Wu","Shih-En Wei","Kaiwen Guo","Zhe Cao","Fabian Prada","Jason Saragih","Yaser Sheikh"],"pdf_url":"https://arxiv.org/pdf/2207.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.14180v2","updated":"2022-07-20T09:26:42Z","published":"2022-06-28T17:47:53Z","title":"High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled\n  Conditions","summary":"  Image-based virtual try-on aims to synthesize an image of a person wearing a\ngiven clothing item. To solve the task, the existing methods warp the clothing\nitem to fit the person's body and generate the segmentation map of the person\nwearing the item before fusing the item with the person. However, when the\nwarping and the segmentation generation stages operate individually without\ninformation exchange, the misalignment between the warped clothes and the\nsegmentation map occurs, which leads to the artifacts in the final image. The\ninformation disconnection also causes excessive warping near the clothing\nregions occluded by the body parts, so-called pixel-squeezing artifacts. To\nsettle the issues, we propose a novel try-on condition generator as a unified\nmodule of the two stages (i.e., warping and segmentation generation stages). A\nnewly proposed feature fusion block in the condition generator implements the\ninformation exchange, and the condition generator does not create any\nmisalignment or pixel-squeezing artifacts. We also introduce discriminator\nrejection that filters out the incorrect segmentation map predictions and\nassures the performance of virtual try-on frameworks. Experiments on a\nhigh-resolution dataset demonstrate that our model successfully handles the\nmisalignment and occlusion, and significantly outperforms the baselines. Code\nis available at https://github.com/sangyun884/HR-VITON.\n","authors":["Sangyun Lee","Gyojung Gu","Sunghyun Park","Seunghwan Choi","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2206.14180v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09771v1","updated":"2022-07-20T09:26:29Z","published":"2022-07-20T09:26:29Z","title":"Localization supervision of chest x-ray classifiers using label-specific\n  eye-tracking annotation","summary":"  Convolutional neural networks (CNNs) have been successfully applied to chest\nx-ray (CXR) images. Moreover, annotated bounding boxes have been shown to\nimprove the interpretability of a CNN in terms of localizing abnormalities.\nHowever, only a few relatively small CXR datasets containing bounding boxes are\navailable, and collecting them is very costly. Opportunely, eye-tracking (ET)\ndata can be collected in a non-intrusive way during the clinical workflow of a\nradiologist. We use ET data recorded from radiologists while dictating CXR\nreports to train CNNs. We extract snippets from the ET data by associating them\nwith the dictation of keywords and use them to supervise the localization of\nabnormalities. We show that this method improves a model's interpretability\nwithout impacting its image-level classification.\n","authors":["Ricardo Bigolin Lanfredi","Joyce D. Schroeder","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2207.09771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09769v1","updated":"2022-07-20T09:25:57Z","published":"2022-07-20T09:25:57Z","title":"A Hybrid Convolutional Neural Network with Meta Feature Learning for\n  Abnormality Detection in Wireless Capsule Endoscopy Images","summary":"  Wireless Capsule Endoscopy is one of the most advanced non-invasive methods\nfor the examination of gastrointestinal tracts. An intelligent computer-aided\ndiagnostic system for detecting gastrointestinal abnormalities like polyp,\nbleeding, inflammation, etc. is highly exigent in wireless capsule endoscopy\nimage analysis. Abnormalities greatly differ in their shape, size, color, and\ntexture, and some appear to be visually similar to normal regions. This poses a\nchallenge in designing a binary classifier due to intra-class variations. In\nthis study, a hybrid convolutional neural network is proposed for abnormality\ndetection that extracts a rich pool of meaningful features from wireless\ncapsule endoscopy images using a variety of convolution operations. It consists\nof three parallel convolutional neural networks, each with a distinctive\nfeature learning capability. The first network utilizes depthwise separable\nconvolution, while the second employs cosine normalized convolution operation.\nA novel meta-feature extraction mechanism is introduced in the third network,\nto extract patterns from the statistical information drawn over the features\ngenerated from the first and second networks and its own previous layer. The\nnetwork trio effectively handles intra-class variance and efficiently detects\ngastrointestinal abnormalities. The proposed hybrid convolutional neural\nnetwork model is trained and tested on two widely used publicly available\ndatasets. The test results demonstrate that the proposed model outperforms six\nstate-of-the-art methods with 97\\% and 98\\% classification accuracy on KID and\nKvasir-Capsule datasets respectively. Cross dataset evaluation results also\ndemonstrate the generalization performance of the proposed model.\n","authors":["Samir Jain","Ayan Seal","Aparajita Ojha"],"pdf_url":"https://arxiv.org/pdf/2207.09769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09767v1","updated":"2022-07-20T09:18:57Z","published":"2022-07-20T09:18:57Z","title":"Collaborating Domain-shared and Target-specific Feature Clustering for\n  Cross-domain 3D Action Recognition","summary":"  In this work, we consider the problem of cross-domain 3D action recognition\nin the open-set setting, which has been rarely explored before. Specifically,\nthere is a source domain and a target domain that contain the skeleton\nsequences with different styles and categories, and our purpose is to cluster\nthe target data by utilizing the labeled source data and unlabeled target data.\nFor such a challenging task, this paper presents a novel approach dubbed CoDT\nto collaboratively cluster the domain-shared features and target-specific\nfeatures. CoDT consists of two parallel branches. One branch aims to learn\ndomain-shared features with supervised learning in the source domain, while the\nother is to learn target-specific features using contrastive learning in the\ntarget domain. To cluster the features, we propose an online clustering\nalgorithm that enables simultaneous promotion of robust pseudo label generation\nand feature clustering. Furthermore, to leverage the complementarity of\ndomain-shared features and target-specific features, we propose a novel\ncollaborative clustering strategy to enforce pair-wise relationship consistency\nbetween the two branches. We conduct extensive experiments on multiple\ncross-domain 3D action recognition datasets, and the results demonstrate the\neffectiveness of our method.\n","authors":["Qinying Liu","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09767v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2111.13844v3","updated":"2022-07-20T09:18:37Z","published":"2021-11-27T08:15:44Z","title":"Adaptive Image Transformations for Transfer-based Adversarial Attack","summary":"  Adversarial attacks provide a good way to study the robustness of deep\nlearning models. One category of methods in transfer-based black-box attack\nutilizes several image transformation operations to improve the transferability\nof adversarial examples, which is effective, but fails to take the specific\ncharacteristic of the input image into consideration. In this work, we propose\na novel architecture, called Adaptive Image Transformation Learner (AITL),\nwhich incorporates different image transformation operations into a unified\nframework to further improve the transferability of adversarial examples.\nUnlike the fixed combinational transformations used in existing works, our\nelaborately designed transformation learner adaptively selects the most\neffective combination of image transformations specific to the input image.\nExtensive experiments on ImageNet demonstrate that our method significantly\nimproves the attack success rates on both normally trained models and defense\nmodels under various settings.\n","authors":["Zheng Yuan","Jie Zhang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2111.13844v3.pdf","comment":"34 pages, 7 figures, 11 tables. Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.00974v2","updated":"2022-07-20T09:10:39Z","published":"2022-07-03T07:54:05Z","title":"NARRATE: A Normal Assisted Free-View Portrait Stylizer","summary":"  In this work, we propose NARRATE, a novel pipeline that enables\nsimultaneously editing portrait lighting and perspective in a photorealistic\nmanner. As a hybrid neural-physical face model, NARRATE leverages complementary\nbenefits of geometry-aware generative approaches and normal-assisted physical\nface models. In a nutshell, NARRATE first inverts the input portrait to a\ncoarse geometry and employs neural rendering to generate images resembling the\ninput, as well as producing convincing pose changes. However, inversion step\nintroduces mismatch, bringing low-quality images with less facial details. As\nsuch, we further estimate portrait normal to enhance the coarse geometry,\ncreating a high-fidelity physical face model. In particular, we fuse the neural\nand physical renderings to compensate for the imperfect inversion, resulting in\nboth realistic and view-consistent novel perspective images. In relighting\nstage, previous works focus on single view portrait relighting but ignoring\nconsistency between different perspectives as well, leading unstable and\ninconsistent lighting effects for view changes. We extend Total Relighting to\nfix this problem by unifying its multi-view input normal maps with the physical\nface model. NARRATE conducts relighting with consistent normal maps, imposing\ncross-view constraints and exhibiting stable and coherent illumination effects.\nWe experimentally demonstrate that NARRATE achieves more photorealistic,\nreliable results over prior works. We further bridge NARRATE with animation and\nstyle transfer tools, supporting pose change, light change, facial animation,\nand style transfer, either separately or in combination, all at a photographic\nquality. We showcase vivid free-view facial animations as well as 3D-aware\nrelightable stylization, which help facilitate various AR/VR applications like\nvirtual cinematography, 3D video conferencing, and post-production.\n","authors":["Youjia Wang","Teng Xu","Yiwen Wu","Minzhang Li","Wenzheng Chen","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2207.00974v2.pdf","comment":"14 pages,13 figures https://youtu.be/mP4FV3evmyw"},{"id":"http://arxiv.org/abs/2207.09763v1","updated":"2022-07-20T09:06:07Z","published":"2022-07-20T09:06:07Z","title":"GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D\n  LiDAR Segmentation","summary":"  3D point cloud semantic segmentation is fundamental for autonomous driving.\nMost approaches in the literature neglect an important aspect, i.e., how to\ndeal with domain shift when handling dynamic scenes. This can significantly\nhinder the navigation capabilities of self-driving vehicles. This paper\nadvances the state of the art in this research field. Our first contribution\nconsists in analysing a new unexplored scenario in point cloud segmentation,\nnamely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We\nexperimentally show that state-of-the-art methods have a rather limited ability\nto adapt pre-trained deep network models to unseen domains in an online manner.\nOur second contribution is an approach that relies on adaptive self-training\nand geometric-feature propagation to adapt a pre-trained source model online\nwithout requiring either source data or target labels. Our third contribution\nis to study SF-OUDA in a challenging setup where source data is synthetic and\ntarget data is point clouds captured in the real world. We use the recent\nSynLiDAR dataset as a synthetic source and introduce two new synthetic (source)\ndatasets, which can stimulate future synthetic-to-real autonomous driving\nresearch. Our experiments show the effectiveness of our segmentation approach\non thousands of real-world point clouds. Code and synthetic datasets are\navailable at https://github.com/saltoricristiano/gipso-sfouda.\n","authors":["Cristiano Saltori","Evgeny Krivosheev","Stéphane Lathuilière","Nicu Sebe","Fabio Galasso","Giuseppe Fiameni","Elisa Ricci","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2207.09763v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09759v1","updated":"2022-07-20T09:04:12Z","published":"2022-07-20T09:04:12Z","title":"Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action\n  Recognition","summary":"  A primary challenge faced in few-shot action recognition is inadequate video\ndata for training. To address this issue, current methods in this field mainly\nfocus on devising algorithms at the feature level while little attention is\npaid to processing input video data. Moreover, existing frame sampling\nstrategies may omit critical action information in temporal and spatial\ndimensions, which further impacts video utilization efficiency. In this paper,\nwe propose a novel video frame sampler for few-shot action recognition to\naddress this issue, where task-specific spatial-temporal frame sampling is\nachieved via a temporal selector (TS) and a spatial amplifier (SA).\nSpecifically, our sampler first scans the whole video at a small computational\ncost to obtain a global perception of video frames. The TS plays its role in\nselecting top-T frames that contribute most significantly and subsequently. The\nSA emphasizes the discriminative information of each frame by amplifying\ncritical regions with the guidance of saliency maps. We further adopt\ntask-adaptive learning to dynamically adjust the sampling strategy according to\nthe episode task at hand. Both the implementations of TS and SA are\ndifferentiable for end-to-end optimization, facilitating seamless integration\nof our proposed sampler with most few-shot action recognition methods.\nExtensive experiments show a significant boost in the performances on various\nbenchmarks including long-term videos.\n","authors":["Huabin Liu","Weixian Lv","John See","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2207.09759v1.pdf","comment":"Accepted by ACM MM 2022"},{"id":"http://arxiv.org/abs/2207.09748v1","updated":"2022-07-20T08:46:18Z","published":"2022-07-20T08:46:18Z","title":"Facial Affect Analysis: Learning from Synthetic Data & Multi-Task\n  Learning Challenges","summary":"  Facial affect analysis remains a challenging task with its setting\ntransitioned from lab-controlled to in-the-wild situations. In this paper, we\npresent novel frameworks to handle the two challenges in the 4th Affective\nBehavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL)\nChallenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL\nchallenge, we adopt the SMM-EmotionNet with a better ensemble strategy of\nfeature vectors. For LSD challenge, we propose respective methods to combat the\nproblems of single labels, imbalanced distribution, fine-tuning limitations,\nand choice of model architectures. Experimental results on the official\nvalidation sets from the competition demonstrated that our proposed approaches\noutperformed baselines by a large margin. The code is available at\nhttps://github.com/sylyoung/ABAW4-HUST-ANT.\n","authors":["Siyang Li","Yifan Xu","Huanyu Wu","Dongrui Wu","Yingjie Yin","Jiajiong Cao","Jingting Ding"],"pdf_url":"https://arxiv.org/pdf/2207.09748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09740v1","updated":"2022-07-20T08:34:50Z","published":"2022-07-20T08:34:50Z","title":"Interpreting Latent Spaces of Generative Models for Medical Images using\n  Unsupervised Methods","summary":"  Generative models such as Generative Adversarial Networks (GANs) and\nVariational Autoencoders (VAEs) play an increasingly important role in medical\nimage analysis. The latent spaces of these models often show semantically\nmeaningful directions corresponding to human-interpretable image\ntransformations. However, until now, their exploration for medical images has\nbeen limited due to the requirement of supervised data. Several methods for\nunsupervised discovery of interpretable directions in GAN latent spaces have\nshown interesting results on natural images. This work explores the potential\nof applying these techniques on medical images by training a GAN and a VAE on\nthoracic CT scans and using an unsupervised method to discover interpretable\ndirections in the resulting latent space. We find several directions\ncorresponding to non-trivial image transformations, such as rotation or breast\nsize. Furthermore, the directions show that the generative models capture 3D\nstructure despite being presented only with 2D data. The results show that\nunsupervised methods to discover interpretable directions in GANs generalize to\nVAEs and can be applied to medical images. This opens a wide array of future\nwork using these methods in medical image analysis.\n","authors":["Julian Schön","Raghavendra Selvan","Jens Petersen"],"pdf_url":"https://arxiv.org/pdf/2207.09740v1.pdf","comment":"Accepted for presentation at DGM4MICCAI 2022"},{"id":"http://arxiv.org/abs/2204.01341v2","updated":"2022-07-20T08:32:13Z","published":"2022-04-04T09:31:16Z","title":"An application of Pixel Interval Down-sampling (PID) for dense tiny\n  microorganism counting on environmental microorganism images","summary":"  This paper proposes a novel pixel interval down-sampling network (PID-Net)\nfor dense tiny object (yeast cells) counting tasks with higher accuracy. The\nPID-Net is an end-to-end convolutional neural network (CNN) model with an\nencoder--decoder architecture. The pixel interval down-sampling operations are\nconcatenated with max-pooling operations to combine the sparse and dense\nfeatures. This addresses the limitation of contour conglutination of dense\nobjects while counting. The evaluation was conducted using classical\nsegmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as\ncounting metrics. The experimental results show that the proposed PID-Net had\nthe best performance and potential for dense tiny object counting tasks, which\nachieved 96.97\\% counting accuracy on the dataset with 2448 yeast cell images.\nBy comparing with the state-of-the-art approaches, such as Attention U-Net,\nSwin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects\nwith clearer boundaries and fewer incorrect debris, which shows the great\npotential of PID-Net in the task of accurate counting.\n","authors":["Jiawei Zhang","Ning Xu","Chen Li","Md Mamunur Rahaman","Yu-Dong Yao","Yu-Hao Lin","Jinghua Zhang","Tao Jiang","Wenjun Qin","Marcin Grzegorzek"],"pdf_url":"https://arxiv.org/pdf/2204.01341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11654v2","updated":"2022-07-20T08:25:56Z","published":"2022-03-22T12:26:56Z","title":"Fine-Grained Scene Graph Generation with Data Transfer","summary":"  Scene graph generation (SGG) is designed to extract (subject, predicate,\nobject) triplets in images. Recent works have made a steady progress on SGG,\nand provide useful tools for high-level vision and language understanding.\nHowever, due to the data distribution problems including long-tail distribution\nand semantic ambiguity, the predictions of current SGG models tend to collapse\nto several frequent but uninformative predicates (e.g., on, at), which limits\npractical application of these models in downstream tasks. To deal with the\nproblems above, we propose a novel Internal and External Data Transfer\n(IETrans) method, which can be applied in a plug-and-play fashion and expanded\nto large SGG with 1,807 predicate classes. Our IETrans tries to relieve the\ndata distribution problem by automatically creating an enhanced dataset that\nprovides more sufficient and coherent annotations for all predicates. By\ntraining on the enhanced dataset, a Neural Motif model doubles the macro\nperformance while maintaining competitive micro performance. The code and data\nare publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.\n","authors":["Ao Zhang","Yuan Yao","Qianyu Chen","Wei Ji","Zhiyuan Liu","Maosong Sun","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2203.11654v2.pdf","comment":"ECCV 2022 (Oral)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2207.10077v1","updated":"2022-07-20T17:59:51Z","published":"2022-07-20T17:59:51Z","title":"Discover and Mitigate Unknown Biases with Debiasing Alternate Networks","summary":"  Deep image classifiers have been found to learn biases from datasets. To\nmitigate the biases, most previous methods require labels of protected\nattributes (e.g., age, skin tone) as full-supervision, which has two\nlimitations: 1) it is infeasible when the labels are unavailable; 2) they are\nincapable of mitigating unknown biases -- biases that humans do not\npreconceive. To resolve those problems, we propose Debiasing Alternate Networks\n(DebiAN), which comprises two networks -- a Discoverer and a Classifier. By\ntraining in an alternate manner, the discoverer tries to find multiple unknown\nbiases of the classifier without any annotations of biases, and the classifier\naims at unlearning the biases identified by the discoverer. While previous\nworks evaluate debiasing results in terms of a single bias, we create\nMulti-Color MNIST dataset to better benchmark mitigation of multiple biases in\na multi-bias setting, which not only reveals the problems in previous methods\nbut also demonstrates the advantage of DebiAN in identifying and mitigating\nmultiple biases simultaneously. We further conduct extensive experiments on\nreal-world datasets, showing that the discoverer in DebiAN can identify unknown\nbiases that may be hard to be found by humans. Regarding debiasing, DebiAN\nachieves strong bias mitigation performance.\n","authors":["Zhiheng Li","Anthony Hoogs","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2207.10077v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10074v1","updated":"2022-07-20T17:58:10Z","published":"2022-07-20T17:58:10Z","title":"Semantic uncertainty intervals for disentangled latent spaces","summary":"  Meaningful uncertainty quantification in computer vision requires reasoning\nabout semantic information -- say, the hair color of the person in a photo or\nthe location of a car on the street. To this end, recent breakthroughs in\ngenerative modeling allow us to represent semantic information in disentangled\nlatent spaces, but providing uncertainties on the semantic latent variables has\nremained challenging. In this work, we provide principled uncertainty intervals\nthat are guaranteed to contain the true semantic factors for any underlying\ngenerative model. The method does the following: (1) it uses quantile\nregression to output a heuristic uncertainty interval for each element in the\nlatent space (2) calibrates these uncertainties such that they contain the true\nvalue of the latent for a new, unseen input. The endpoints of these calibrated\nintervals can then be propagated through the generator to produce interpretable\nuncertainty visualizations for each semantic factor. This technique reliably\ncommunicates semantically meaningful, principled, and instance-adaptive\nuncertainty in inverse problems like image super-resolution and image\ncompletion.\n","authors":["Swami Sankaranarayanan","Anastasios N. Angelopoulos","Stephen Bates","Yaniv Romano","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2207.10074v1.pdf","comment":"Code: https://github.com/swamiviv/generative_semantic_uncertainty"},{"id":"http://arxiv.org/abs/2207.10062v1","updated":"2022-07-20T17:47:54Z","published":"2022-07-20T17:47:54Z","title":"DataPerf: Benchmarks for Data-Centric AI Development","summary":"  Machine learning (ML) research has generally focused on models, while the\nmost prominent datasets have been employed for everyday ML tasks without regard\nfor the breadth, difficulty, and faithfulness of these datasets to the\nunderlying problem. Neglecting the fundamental importance of datasets has\ncaused major problems involving data cascades in real-world applications and\nsaturation of dataset-driven criteria for model quality, hindering research\ngrowth. To solve this problem, we present DataPerf, a benchmark package for\nevaluating ML datasets and dataset-working algorithms. We intend it to enable\nthe \"data ratchet,\" in which training sets will aid in evaluating test sets on\nthe same problems, and vice versa. Such a feedback-driven strategy will\ngenerate a virtuous loop that will accelerate development of data-centric AI.\nThe MLCommons Association will maintain DataPerf.\n","authors":["Mark Mazumder","Colby Banbury","Xiaozhe Yao","Bojan Karlaš","William Gaviria Rojas","Sudnya Diamos","Greg Diamos","Lynn He","Douwe Kiela","David Jurado","David Kanter","Rafael Mosquera","Juan Ciro","Lora Aroyo","Bilge Acun","Sabri Eyuboglu","Amirata Ghorbani","Emmett Goodman","Tariq Kane","Christine R. Kirkpatrick","Tzu-Sheng Kuo","Jonas Mueller","Tristan Thrush","Joaquin Vanschoren","Margaret Warren","Adina Williams","Serena Yeung","Newsha Ardalani","Praveen Paritosh","Ce Zhang","James Zou","Carole-Jean Wu","Cody Coleman","Andrew Ng","Peter Mattson","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2207.10062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10077v2","updated":"2022-07-20T17:44:47Z","published":"2021-10-08T04:17:59Z","title":"Deep Learning to Estimate Permeability using Geophysical Data","summary":"  Time-lapse electrical resistivity tomography (ERT) is a popular geophysical\nmethod to estimate three-dimensional (3D) permeability fields from electrical\npotential difference measurements. Traditional inversion and data assimilation\nmethods are used to ingest this ERT data into hydrogeophysical models to\nestimate permeability. Due to ill-posedness and the curse of dimensionality,\nexisting inversion strategies provide poor estimates and low resolution of the\n3D permeability field. Recent advances in deep learning provide us with\npowerful algorithms to overcome this challenge. This paper presents a deep\nlearning (DL) framework to estimate the 3D subsurface permeability from\ntime-lapse ERT data. To test the feasibility of the proposed framework, we\ntrain DL-enabled inverse models on simulation data. Subsurface process models\nbased on hydrogeophysics are used to generate this synthetic data for deep\nlearning analyses. Results show that proposed weak supervised learning can\ncapture salient spatial features in the 3D permeability field. Quantitatively,\nthe average mean squared error (in terms of the natural log) on the strongly\nlabeled training, validation, and test datasets is less than 0.5. The R2-score\n(global metric) is greater than 0.75, and the percent error in each cell (local\nmetric) is less than 10%. Finally, an added benefit in terms of computational\ncost is that the proposed DL-based inverse model is at least O(104) times\nfaster than running a forward model. Note that traditional inversion may\nrequire multiple forward model simulations (e.g., in the order of 10 to 1000),\nwhich are very expensive. This computational savings (O(105) - O(107)) makes\nthe proposed DL-based inverse model attractive for subsurface imaging and\nreal-time ERT monitoring applications due to fast and yet reasonably accurate\nestimations of the permeability field.\n","authors":["M. K. Mudunuru","E. L. D. Cromwell","H. Wang","X. Chen"],"pdf_url":"https://arxiv.org/pdf/2110.10077v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2207.10050v1","updated":"2022-07-20T17:29:04Z","published":"2022-07-20T17:29:04Z","title":"Discriminator-Weighted Offline Imitation Learning from Suboptimal\n  Demonstrations","summary":"  We study the problem of offline Imitation Learning (IL) where an agent aims\nto learn an optimal expert behavior policy without additional online\nenvironment interactions. Instead, the agent is provided with a supplementary\noffline dataset from suboptimal behaviors. Prior works that address this\nproblem either require that expert data occupies the majority proportion of the\noffline dataset, or need to learn a reward function and perform offline\nreinforcement learning (RL) afterwards. In this paper, we aim to address the\nproblem without additional steps of reward learning and offline RL training for\nthe case when demonstrations contain a large proportion of suboptimal data.\nBuilt upon behavioral cloning (BC), we introduce an additional discriminator to\ndistinguish expert and non-expert data. We propose a cooperation framework to\nboost the learning of both tasks, Based on this framework, we design a new IL\nalgorithm, where the outputs of discriminator serve as the weights of the BC\nloss. Experimental results show that our proposed algorithm achieves higher\nreturns and faster training speed compared to baseline algorithms.\n","authors":["Haoran Xu","Xianyuan Zhan","Honglei Yin","Huiling Qin"],"pdf_url":"https://arxiv.org/pdf/2207.10050v1.pdf","comment":"ICML 2022, code at https://github.com/ryanxhr/DWBC"},{"id":"http://arxiv.org/abs/2207.10049v1","updated":"2022-07-20T17:27:50Z","published":"2022-07-20T17:27:50Z","title":"Pretraining a Neural Network before Knowing Its Architecture","summary":"  Training large neural networks is possible by training a smaller hypernetwork\nthat predicts parameters for the large ones. A recently released Graph\nHyperNetwork (GHN) trained this way on one million smaller ImageNet\narchitectures is able to predict parameters for large unseen networks such as\nResNet-50. While networks with predicted parameters lose performance on the\nsource task, the predicted parameters have been found useful for fine-tuning on\nother tasks. We study if fine-tuning based on the same GHN is still useful on\nnovel strong architectures that were published after the GHN had been trained.\nWe found that for recent architectures such as ConvNeXt, GHN initialization\nbecomes less useful than for ResNet-50. One potential reason is the increased\ndistribution shift of novel architectures from those used to train the GHN. We\nalso found that the predicted parameters lack the diversity necessary to\nsuccessfully fine-tune parameters with gradient descent. We alleviate this\nlimitation by applying simple post-processing techniques to predicted\nparameters before fine-tuning them on a target task and improve fine-tuning of\nResNet-50 and ConvNeXt.\n","authors":["Boris Knyazev"],"pdf_url":"https://arxiv.org/pdf/2207.10049v1.pdf","comment":"Accepted at ICML 2022 Workshop on Pre-training: Perspectives,\n  Pitfalls, and Paths Forward, source code is available at\n  https://github.com/facebookresearch/ppuda"},{"id":"http://arxiv.org/abs/2207.10046v1","updated":"2022-07-20T17:20:58Z","published":"2022-07-20T17:20:58Z","title":"Adaptive Step-Size Methods for Compressed SGD","summary":"  Compressed Stochastic Gradient Descent (SGD) algorithms have been recently\nproposed to address the communication bottleneck in distributed and\ndecentralized optimization problems, such as those that arise in federated\nmachine learning. Existing compressed SGD algorithms assume the use of\nnon-adaptive step-sizes(constant or diminishing) to provide theoretical\nconvergence guarantees. Typically, the step-sizes are fine-tuned in practice to\nthe dataset and the learning algorithm to provide good empirical performance.\nSuch fine-tuning might be impractical in many learning scenarios, and it is\ntherefore of interest to study compressed SGD using adaptive step-sizes.\nMotivated by prior work on adaptive step-size methods for SGD to train neural\nnetworks efficiently in the uncompressed setting, we develop an adaptive\nstep-size method for compressed SGD. In particular, we introduce a scaling\ntechnique for the descent step in compressed SGD, which we use to establish\norder-optimal convergence rates for convex-smooth and strong convex-smooth\nobjectives under an interpolation condition and for non-convex objectives under\na strong growth condition. We also show through simulation examples that\nwithout this scaling, the algorithm can fail to converge. We present\nexperimental results on deep neural networks for real-world datasets, and\ncompare the performance of our proposed algorithm with previously proposed\ncompressed SGD methods in literature, and demonstrate improved performance on\nResNet-18, ResNet-34 and DenseNet architectures for CIFAR-100 and CIFAR-10\ndatasets at various levels of compression.\n","authors":["Adarsh M. Subramaniam","Akshayaa Magesh","Venugopal V. Veeravalli"],"pdf_url":"https://arxiv.org/pdf/2207.10046v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2207.10020v1","updated":"2022-07-20T16:36:20Z","published":"2022-07-20T16:36:20Z","title":"MANI-Rank: Multiple Attribute and Intersectional Group Fairness for\n  Consensus Ranking","summary":"  Combining the preferences of many rankers into one single consensus ranking\nis critical for consequential applications from hiring and admissions to\nlending. While group fairness has been extensively studied for classification,\ngroup fairness in rankings and in particular rank aggregation remains in its\ninfancy. Recent work introduced the concept of fair rank aggregation for\ncombining rankings but restricted to the case when candidates have a single\nbinary protected attribute, i.e., they fall into two groups only. Yet it\nremains an open problem how to create a consensus ranking that represents the\npreferences of all rankers while ensuring fair treatment for candidates with\nmultiple protected attributes such as gender, race, and nationality. In this\nwork, we are the first to define and solve this open Multi-attribute Fair\nConsensus Ranking (MFCR) problem. As a foundation, we design novel group\nfairness criteria for rankings, called MANI-RANK, ensuring fair treatment of\ngroups defined by individual protected attributes and their intersection.\nLeveraging the MANI-RANK criteria, we develop a series of algorithms that for\nthe first time tackle the MFCR problem. Our experimental study with a rich\nvariety of consensus scenarios demonstrates our MFCR methodology is the only\napproach to achieve both intersectional and protected attribute fairness while\nalso representing the preferences expressed through many base rankings. Our\nreal-world case study on merit scholarships illustrates the effectiveness of\nour MFCR methods to mitigate bias across multiple protected attributes and\ntheir intersections. This is an extended version of \"MANI-Rank: Multiple\nAttribute and Intersectional Group Fairness for Consensus Ranking\", to appear\nin ICDE 2022.\n","authors":["Kathleen Cachel","Elke Rundensteiner","Lane Harrison"],"pdf_url":"https://arxiv.org/pdf/2207.10020v1.pdf","comment":"This paper has been accepted by IEEE ICDE 2022. 15 pages, and 7\n  figures"},{"id":"http://arxiv.org/abs/2207.10018v1","updated":"2022-07-20T16:31:19Z","published":"2022-07-20T16:31:19Z","title":"Mitigating Algorithmic Bias with Limited Annotations","summary":"  Existing work on fairness modeling commonly assumes that sensitive attributes\nfor all instances are fully available, which may not be true in many real-world\napplications due to the high cost of acquiring sensitive information. When\nsensitive attributes are not disclosed or available, it is needed to manually\nannotate a small part of the training data to mitigate bias. However, the\nskewed distribution across different sensitive groups preserves the skewness of\nthe original dataset in the annotated subset, which leads to non-optimal bias\nmitigation. To tackle this challenge, we propose Active Penalization Of\nDiscrimination (APOD), an interactive framework to guide the limited\nannotations towards maximally eliminating the effect of algorithmic bias. The\nproposed APOD integrates discrimination penalization with active instance\nselection to efficiently utilize the limited annotation budget, and it is\ntheoretically proved to be capable of bounding the algorithmic bias. According\nto the evaluation on five benchmark datasets, APOD outperforms the\nstate-of-the-arts baseline methods under the limited annotation budget, and\nshows comparable performance to fully annotated bias mitigation, which\ndemonstrates that APOD could benefit real-world applications when sensitive\ninformation is limited.\n","authors":["Guanchu Wang","Mengnan Du","Ninghao Liu","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2207.10018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10017v1","updated":"2022-07-20T16:30:47Z","published":"2022-07-20T16:30:47Z","title":"Predictive Object-Centric Process Monitoring","summary":"  The automation and digitalization of business processes has resulted in large\namounts of data captured in information systems, which can aid businesses in\nunderstanding their processes better, improve workflows, or provide operational\nsupport. By making predictions about ongoing processes, bottlenecks can be\nidentified and resources reallocated, as well as insights gained into the state\nof a process instance (case). Traditionally, data is extracted from systems in\nthe form of an event log with a single identifying case notion, such as an\norder id for an Order to Cash (O2C) process. However, real processes often have\nmultiple object types, for example, order, item, and package, so a format that\nforces the use of a single case notion does not reflect the underlying\nrelations in the data. The Object-Centric Event Log (OCEL) format was\nintroduced to correctly capture this information. The state-of-the-art\npredictive methods have been tailored to only traditional event logs. This\nthesis shows that a prediction method utilizing Generative Adversarial Networks\n(GAN), Long Short-Term Memory (LSTM) architectures, and Sequence to Sequence\nmodels (Seq2seq), can be augmented with the rich data contained in OCEL.\nObjects in OCEL can have attributes that are useful in predicting the next\nevent and timestamp, such as a priority class attribute for an object type\npackage indicating slower or faster processing. In the metrics of sequence\nsimilarity of predicted remaining events and mean absolute error (MAE) of the\ntimestamp, the approach in this thesis matches or exceeds previous research,\ndepending on whether selected object attributes are useful features for the\nmodel. Additionally, this thesis provides a web interface to predict the next\nsequence of activities from user input.\n","authors":["Timo Rohrer","Anahita Farhang Ghahfarokhi","Mohamed Behery","Gerhard Lakemeyer","Wil M. P. van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2207.10017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10003v1","updated":"2022-07-20T16:05:56Z","published":"2022-07-20T16:05:56Z","title":"BYEL : Bootstrap on Your Emotion Latent","summary":"  According to the problem of dataset construction cost for training in deep\nlearning and the development of generative models, more and more researches are\nbeing conducted to train with synthetic data and to inference using real data.\nWe propose emotion aware Self-Supervised Learning using ABAW's Learning\nSynthetic Data (LSD) dataset. We pre-train our method to LSD dataset as a\nself-supervised learning and then use the same LSD dataset to do downstream\ntraining on the emotion classification task as a supervised learning. As a\nresult, a higher result(0.63) than baseline(0.5) was obtained.\n","authors":["Hyungjun Lee","Hwangyu Lim","Sejoon Lim"],"pdf_url":"https://arxiv.org/pdf/2207.10003v1.pdf","comment":"ABAW4th competition"},{"id":"http://arxiv.org/abs/2207.09999v1","updated":"2022-07-20T16:03:10Z","published":"2022-07-20T16:03:10Z","title":"Digital Twin-based Intrusion Detection for Industrial Control Systems","summary":"  Digital twins have recently gained significant interest in simulation,\noptimization, and predictive maintenance of Industrial Control Systems (ICS).\nRecent studies discuss the possibility of using digital twins for intrusion\ndetection in industrial systems. Accordingly, this study contributes to a\ndigital twin-based security framework for industrial control systems, extending\nits capabilities for simulation of attacks and defense mechanisms. Four types\nof process-aware attack scenarios are implemented on a standalone open-source\ndigital twin of an industrial filling plant: command injection, network Denial\nof Service (DoS), calculated measurement modification, and naive measurement\nmodification. A stacked ensemble classifier is proposed as the real-time\nintrusion detection, based on the offline evaluation of eight supervised\nmachine learning algorithms. The designed stacked model outperforms previous\nmethods in terms of F1-Score and accuracy, by combining the predictions of\nvarious algorithms, while it can detect and classify intrusions in near\nreal-time (0.1 seconds). This study also discusses the practicality and\nbenefits of the proposed digital twin-based security framework.\n","authors":["Seba Anna Varghese","Alireza Dehlaghi Ghadim","Ali Balador","Zahra Alimadadi","Panos Papadimitratos"],"pdf_url":"https://arxiv.org/pdf/2207.09999v1.pdf","comment":"7 pages, 7 figures, 3 tables, workshop paper"},{"id":"http://arxiv.org/abs/2204.00570v3","updated":"2022-07-20T15:51:41Z","published":"2022-04-01T16:56:26Z","title":"Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised\n  Domain Adaptation","summary":"  We consider unsupervised domain adaptation (UDA), where labeled data from a\nsource domain (e.g., photographs) and unlabeled data from a target domain\n(e.g., sketches) are used to learn a classifier for the target domain.\nConventional UDA methods (e.g., domain adversarial training) learn\ndomain-invariant features to improve generalization to the target domain. In\nthis paper, we show that contrastive pre-training, which learns features on\nunlabeled source and target data and then fine-tunes on labeled source data, is\ncompetitive with strong UDA methods. However, we find that contrastive\npre-training does not learn domain-invariant features, diverging from\nconventional UDA intuitions. We show theoretically that contrastive\npre-training can learn features that vary subtantially across domains but still\ngeneralize to the target domain, by disentangling domain and class information.\nOur results suggest that domain invariance is not necessary for UDA. We\nempirically validate our theory on benchmark vision datasets.\n","authors":["Kendrick Shen","Robbie Jones","Ananya Kumar","Sang Michael Xie","Jeff Z. HaoChen","Tengyu Ma","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2204.00570v3.pdf","comment":"Accepted to ICML 2022 (Long Talk)"},{"id":"http://arxiv.org/abs/2110.06206v2","updated":"2022-07-20T15:49:57Z","published":"2021-10-12T17:55:30Z","title":"StARformer: Transformer with State-Action-Reward Representations for\n  Visual Reinforcement Learning","summary":"  Reinforcement Learning (RL) can be considered as a sequence modeling task:\ngiven a sequence of past state-action-reward experiences, an agent predicts a\nsequence of next actions. In this work, we propose State-Action-Reward\nTransformer (StARformer) for visual RL, which explicitly models short-term\nstate-action-reward representations (StAR-representations), essentially\nintroducing a Markovian-like inductive bias to improve long-term modeling. Our\napproach first extracts StAR-representations by self-attending image state\npatches, action, and reward tokens within a short temporal window. These are\nthen combined with pure image state representations -- extracted as\nconvolutional features, to perform self-attention over the whole sequence. Our\nexperiments show that StARformer outperforms the state-of-the-art\nTransformer-based method on image-based Atari and DeepMind Control Suite\nbenchmarks, in both offline-RL and imitation learning settings. StARformer is\nalso more compliant with longer sequences of inputs. Our code is available at\nhttps://github.com/elicassion/StARformer.\n","authors":["Jinghuan Shang","Kumara Kahatapitiya","Xiang Li","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2110.06206v2.pdf","comment":"Accepted to ECCV 2022. Our code is available at\n  https://github.com/elicassion/StARformer"},{"id":"http://arxiv.org/abs/2207.09980v1","updated":"2022-07-20T15:39:30Z","published":"2022-07-20T15:39:30Z","title":"REFACTOR GNNS: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective","summary":"  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing REFACTOR GNNS. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our REFACTOR GNNS. Across\na multitude of well-established KGC benchmarks, our REFACTOR GNNS achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n","authors":["Yihong Chen","Pushkar Mishra","Luca Franceschi","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2207.09980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09971v1","updated":"2022-07-20T15:29:45Z","published":"2022-07-20T15:29:45Z","title":"NeuralNEB -- Neural Networks can find Reaction Paths Fast","summary":"  Machine Learning (ML) models have, in contrast to their usefulness in\nmolecular dynamics studies, had limited success as surrogate potentials for\nreaction barrier search. It is due to the scarcity of training data in relevant\ntransition state regions of chemical space. Currently, available datasets for\ntraining ML models on small molecular systems almost exclusively contain\nconfigurations at or near equilibrium. In this work, we present the dataset\nTransition1x containing 9.6 million Density Functional Theory (DFT)\ncalculations of forces and energies of molecular configurations on and around\nreaction pathways at the wB97x/6-31G(d) level of theory. The data was generated\nby running Nudged Elastic Band (NEB) calculations with DFT on 10k reactions\nwhile saving intermediate calculations. We train state-of-the-art equivariant\ngraph message-passing neural network models on Transition1x and cross-validate\non the popular ANI1x and QM9 datasets. We show that ML models cannot learn\nfeatures in transition-state regions solely by training on hitherto popular\nbenchmark datasets. Transition1x is a new challenging benchmark that will\nprovide an important step towards developing next-generation ML force fields\nthat also work far away from equilibrium configurations and reactive systems.\n","authors":["Mathias Schreiner","Arghya Bhowmik","Tejs Vegge","Ole Winther"],"pdf_url":"https://arxiv.org/pdf/2207.09971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.04694v2","updated":"2022-07-20T15:11:57Z","published":"2022-03-09T13:13:55Z","title":"Align-Deform-Subtract: An Interventional Framework for Explaining Object\n  Differences","summary":"  Given two object images, how can we explain their differences in terms of the\nunderlying object properties? To address this question, we propose\nAlign-Deform-Subtract (ADS) -- an interventional framework for explaining\nobject differences. By leveraging semantic alignments in image-space as\ncounterfactual interventions on the underlying object properties, ADS\niteratively quantifies and removes differences in object properties. The result\nis a set of \"disentangled\" error measures which explain object differences in\nterms of the underlying properties. Experiments on real and synthetic data\nillustrate the efficacy of the framework.\n","authors":["Cian Eastwood","Li Nanbo","Christopher K. I. Williams"],"pdf_url":"https://arxiv.org/pdf/2203.04694v2.pdf","comment":"ICLR 2022 Workshop on Objects, Structure and Causality"},{"id":"http://arxiv.org/abs/2207.09960v1","updated":"2022-07-20T15:10:02Z","published":"2022-07-20T15:10:02Z","title":"Measuring and signing fairness as performance under multiple stakeholder\n  distributions","summary":"  As learning machines increase their influence on decisions concerning human\nlives, analyzing their fairness properties becomes a subject of central\nimportance. Yet, our best tools for measuring the fairness of learning systems\nare rigid fairness metrics encapsulated as mathematical one-liners, offer\nlimited power to the stakeholders involved in the prediction task, and are easy\nto manipulate when we exhort excessive pressure to optimize them. To advance\nthese issues, we propose to shift focus from shaping fairness metrics to\ncurating the distributions of examples under which these are computed. In\nparticular, we posit that every claim about fairness should be immediately\nfollowed by the tagline \"Fair under what examples, and collected by whom?\". By\nhighlighting connections to the literature in domain generalization, we propose\nto measure fairness as the ability of the system to generalize under multiple\nstress tests -- distributions of examples with social relevance. We encourage\neach stakeholder to curate one or multiple stress tests containing examples\nreflecting their (possibly conflicting) interests. The machine passes or fails\neach stress test by falling short of or exceeding a pre-defined metric value.\nThe test results involve all stakeholders in a discussion about how to improve\nthe learning system, and provide flexible assessments of fairness dependent on\ncontext and based on interpretable data. We provide full implementation\nguidelines for stress testing, illustrate both the benefits and shortcomings of\nthis framework, and introduce a cryptographic scheme to enable a degree of\nprediction accountability from system providers.\n","authors":["David Lopez-Paz","Diane Bouchacourt","Levent Sagun","Nicolas Usunier"],"pdf_url":"https://arxiv.org/pdf/2207.09960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09959v1","updated":"2022-07-20T15:09:16Z","published":"2022-07-20T15:09:16Z","title":"Exploration of Parameter Spaces Assisted by Machine Learning","summary":"  We showcase a variety of functions and classes that implement sampling\nprocedures with improved exploration of the parameter space assisted by machine\nlearning. Special attention is paid to setting sane defaults with the objective\nthat adjustments required by different problems remain minimal. This collection\nof routines can be employed for different types of analysis, from finding\nbounds on the parameter space to accumulating samples in areas of interest. In\nparticular, we discuss two methods assisted by incorporating different machine\nlearning models: regression and classification. We show that a machine learning\nclassifier can provide higher efficiency for exploring the parameter space.\nAlso, we introduce a boosting technique to improve the slow convergence at the\nstart of the process. The use of these routines is better explained with the\nhelp of a few examples that illustrate the type of results one can obtain. We\nalso include examples of the code used to obtain the examples as well as\ndescriptions of the adjustments that can be made to adapt the calculation to\nother problems. We finalize by showing the impact of these techniques when\nexploring the parameter space of the two Higgs doublet model that matches the\nmeasured Higgs Boson signal strength. The code used for this paper and\ninstructions on how to use it are available on the web.\n","authors":["A. Hammad","Myeonghun Park","Raymundo Ramos","Pankaj Saha"],"pdf_url":"https://arxiv.org/pdf/2207.09959v1.pdf","comment":"15 pages, 5 figures. Code and instructions are available on\n  https://github.com/AHamamd150/MLscanner"},{"id":"http://arxiv.org/abs/2207.09957v1","updated":"2022-07-20T15:04:32Z","published":"2022-07-20T15:04:32Z","title":"Estimating Model Performance under Domain Shifts with Class-Specific\n  Confidence Scores","summary":"  Machine learning models are typically deployed in a test setting that differs\nfrom the training setting, potentially leading to decreased model performance\nbecause of domain shift. If we could estimate the performance that a\npre-trained model would achieve on data from a specific deployment setting, for\nexample a certain clinic, we could judge whether the model could safely be\ndeployed or if its performance degrades unacceptably on the specific data.\nExisting approaches estimate this based on the confidence of predictions made\non unlabeled test data from the deployment's domain. We find existing methods\nstruggle with data that present class imbalance, because the methods used to\ncalibrate confidence do not account for bias induced by class imbalance,\nconsequently failing to estimate class-wise accuracy. Here, we introduce\nclass-wise calibration within the framework of performance estimation for\nimbalanced datasets. Specifically, we derive class-specific modifications of\nstate-of-the-art confidence-based model evaluation methods including\ntemperature scaling (TS), difference of confidences (DoC), and average\nthresholded confidence (ATC). We also extend the methods to estimate Dice\nsimilarity coefficient (DSC) in image segmentation. We conduct experiments on\nfour tasks and find the proposed modifications consistently improve the\nestimation accuracy for imbalanced datasets. Our methods improve accuracy\nestimation by 18\\% in classification under natural domain shifts, and double\nthe estimation accuracy on segmentation tasks, when compared with prior\nmethods.\n","authors":["Zeju Li","Konstantinos Kamnitsas","Mobarakol Islam","Chen Chen","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2207.09957v1.pdf","comment":"Accepted at MICCAI 2022"},{"id":"http://arxiv.org/abs/2207.09955v1","updated":"2022-07-20T15:01:12Z","published":"2022-07-20T15:01:12Z","title":"Operation-Level Performance Benchmarking of Graph Neural Networks for\n  Scientific Applications","summary":"  As Graph Neural Networks (GNNs) increase in popularity for scientific machine\nlearning, their training and inference efficiency is becoming increasingly\ncritical. Additionally, the deep learning field as a whole is trending towards\nwider and deeper networks, and ever increasing data sizes, to the point where\nhard hardware bottlenecks are often encountered. Emerging specialty hardware\nplatforms provide an exciting solution to this problem. In this paper, we\nsystematically profile and select low-level operations pertinent to GNNs for\nscientific computing implemented in the Pytorch Geometric software framework.\nThese are then rigorously benchmarked on NVIDIA A100 GPUs for several various\ncombinations of input values, including tensor sparsity. We then analyze these\nresults for each operation. At a high level, we conclude that on NVIDIA\nsystems: (1) confounding bottlenecks such as memory inefficiency often dominate\nruntime costs moreso than data sparsity alone, (2) native Pytorch operations\nare often as or more competitive than their Pytorch Geometric equivalents,\nespecially at low to moderate levels of input data sparsity, and (3) many\noperations central to state-of-the-art GNN architectures have little to no\noptimization for sparsity. We hope that these results serve as a baseline for\nthose developing these operations on specialized hardware and that our\nsubsequent analysis helps to facilitate future software and hardware based\noptimizations of these operations and thus scalable GNN performance as a whole.\n","authors":["Ryien Hosseini","Filippo Simini","Venkatram Vishwanath"],"pdf_url":"https://arxiv.org/pdf/2207.09955v1.pdf","comment":"Published as workshop paper at MLSys 2022 (MLBench)"},{"id":"http://arxiv.org/abs/2207.09953v1","updated":"2022-07-20T14:58:13Z","published":"2022-07-20T14:58:13Z","title":"Learning Pedestrian Group Representations for Multi-modal Trajectory\n  Prediction","summary":"  Modeling the dynamics of people walking is a problem of long-standing\ninterest in computer vision. Many previous works involving pedestrian\ntrajectory prediction define a particular set of individual actions to\nimplicitly model group actions. In this paper, we present a novel architecture\nnamed GP-Graph which has collective group representations for effective\npedestrian trajectory prediction in crowded environments, and is compatible\nwith all types of existing approaches. A key idea of GP-Graph is to model both\nindividual-wise and group-wise relations as graph representations. To do this,\nGP-Graph first learns to assign each pedestrian into the most likely behavior\ngroup. Using this assignment information, GP-Graph then forms both intra- and\ninter-group interactions as graphs, accounting for human-human relations within\na group and group-group relations, respectively. To be specific, for the\nintra-group interaction, we mask pedestrian graph edges out of an associated\ngroup. We also propose group pooling&unpooling operations to represent a group\nwith multiple pedestrians as one graph node. Lastly, GP-Graph infers a\nprobability map for socially-acceptable future trajectories from the integrated\nfeatures of both group interactions. Moreover, we introduce a group-level\nlatent vector sampling to ensure collective inferences over a set of possible\nfuture trajectories. Extensive experiments are conducted to validate the\neffectiveness of our architecture, which demonstrates consistent performance\nimprovements with publicly available benchmarks. Code is publicly available at\nhttps://github.com/inhwanbae/GPGraph.\n","authors":["Inhwan Bae","Jin-Hwi Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2207.09953v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09951v1","updated":"2022-07-20T14:54:40Z","published":"2022-07-20T14:54:40Z","title":"Deep Reinforcement Learning for Market Making Under a Hawkes\n  Process-Based Limit Order Book Model","summary":"  The stochastic control problem of optimal market making is among the central\nproblems in quantitative finance. In this paper, a deep reinforcement\nlearning-based controller is trained on a weakly consistent, multivariate\nHawkes process-based limit order book simulator to obtain market making\ncontrols. The proposed approach leverages the advantages of Monte Carlo\nbacktesting and contributes to the line of research on market making under\nweakly consistent limit order book models. The ensuing deep reinforcement\nlearning controller is compared to multiple market making benchmarks, with the\nresults indicating its superior performance with respect to various risk-reward\nmetrics, even under significant transaction costs.\n","authors":["Bruno Gašperov","Zvonko Kostanjčar"],"pdf_url":"https://arxiv.org/pdf/2207.09951v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.09947v1","updated":"2022-07-20T14:43:45Z","published":"2022-07-20T14:43:45Z","title":"Fixed Points of Cone Mapping with the Application to Neural Networks","summary":"  We derive conditions for the existence of fixed points of cone mappings\nwithout assuming scalability of functions. Monotonicity and scalability are\noften inseparable in the literature in the context of searching for fixed\npoints of interference mappings. In applications, such mappings are\napproximated by non-negative neural networks. It turns out, however, that the\nprocess of training non-negative networks requires imposing an artificial\nconstraint on the weights of the model. However, in the case of specific\nnon-negative data, it cannot be said that if the mapping is non-negative, it\nhas only non-negative weights. Therefore, we considered the problem of the\nexistence of fixed points for general neural networks, assuming the conditions\nof tangency conditions with respect to specific cones. This does not relax the\nphysical assumptions, because even assuming that the input and output are to be\nnon-negative, the weights can have (small, but) less than zero values. Such\nproperties (often found in papers on the interpretability of weights of neural\nnetworks) lead to the weakening of the assumptions about the monotonicity or\nscalability of the mapping associated with the neural network. To the best of\nour knowledge, this paper is the first to study this phenomenon.\n","authors":["Grzegorz Gabor","Krzysztof Rykaczewski"],"pdf_url":"https://arxiv.org/pdf/2207.09947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08824v2","updated":"2022-07-20T14:41:17Z","published":"2022-07-18T16:26:24Z","title":"3D Equivariant Molecular Graph Pretraining","summary":"  Pretraining molecular representation models without labels is fundamental to\nvarious applications. Conventional methods mainly process 2D molecular graphs\nand focus solely on 2D tasks, making their pretrained models incapable of\ncharacterizing 3D geometry and thus defective for downstream 3D tasks. In this\nwork, we tackle 3D molecular pretraining in a complete and novel sense. In\nparticular, we first propose to adopt an equivariant energy-based model as the\nbackbone for pretraining, which enjoys the merit of fulfilling the symmetry of\n3D space. Then we develop a node-level pretraining loss for force prediction,\nwhere we further exploit the Riemann-Gaussian distribution to ensure the loss\nto be E(3)-invariant, enabling more robustness. Moreover, a graph-level noise\nscale prediction task is also leveraged to further promote the eventual\nperformance. We evaluate our model pretrained from a large-scale 3D dataset\nGEOM-QM9 on two challenging 3D benchmarks: MD17 and QM9. The experimental\nresults support the better efficacy of our method against current\nstate-of-the-art pretraining approaches, and verify the validity of our design\nfor each proposed component.\n","authors":["Rui Jiao","Jiaqi Han","Wenbing Huang","Yu Rong","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2207.08824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09944v1","updated":"2022-07-20T14:41:09Z","published":"2022-07-20T14:41:09Z","title":"Probable Domain Generalization via Quantile Risk Minimization","summary":"  Domain generalization (DG) seeks predictors which perform well on unseen test\ndistributions by leveraging labeled training data from multiple related\ndistributions or domains. To achieve this, the standard formulation optimizes\nfor worst-case performance over the set of all possible domains. However, with\nworst-case shifts very unlikely in practice, this generally leads to\noverly-conservative solutions. In fact, a recent study found that no DG\nalgorithm outperformed empirical risk minimization in terms of average\nperformance. In this work, we argue that DG is neither a worst-case problem nor\nan average-case problem, but rather a probabilistic one. To this end, we\npropose a probabilistic framework for DG, which we call Probable Domain\nGeneralization, wherein our key idea is that distribution shifts seen during\ntraining should inform us of probable shifts at test time. To realize this, we\nexplicitly relate training and test domains as draws from the same underlying\nmeta-distribution, and propose a new optimization problem -- Quantile Risk\nMinimization (QRM) -- which requires that predictors generalize with high\nprobability. We then prove that QRM: (i) produces predictors that generalize to\nnew domains with a desired probability, given sufficiently many domains and\nsamples; and (ii) recovers the causal predictor as the desired probability of\ngeneralization approaches one. In our experiments, we introduce a more holistic\nquantile-focused evaluation protocol for DG, and show that our algorithms\noutperform state-of-the-art baselines on real and synthetic data.\n","authors":["Cian Eastwood","Alexander Robey","Shashank Singh","Julius von Kügelgen","Hamed Hassani","George J. Pappas","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2207.09944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09071v2","updated":"2022-07-20T14:30:50Z","published":"2022-07-19T04:58:06Z","title":"Learning Action Translator for Meta Reinforcement Learning on\n  Sparse-Reward Tasks","summary":"  Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of\ntraining tasks simultaneously and quickly adapting to new tasks. It requires\nmassive amounts of data drawn from training tasks to infer the common structure\nshared among tasks. Without heavy reward engineering, the sparse rewards in\nlong-horizon tasks exacerbate the problem of sample efficiency in meta-RL.\nAnother challenge in meta-RL is the discrepancy of difficulty level among\ntasks, which might cause one easy task dominating learning of the shared policy\nand thus preclude policy adaptation to new tasks. This work introduces a novel\nobjective function to learn an action translator among training tasks. We\ntheoretically verify that the value of the transferred policy with the action\ntranslator can be close to the value of the source policy and our objective\nfunction (approximately) upper bounds the value difference. We propose to\ncombine the action translator with context-based meta-RL algorithms for better\ndata collection and more efficient exploration during meta-training. Our\napproach empirically improves the sample efficiency and performance of meta-RL\nalgorithms on sparse-reward tasks.\n","authors":["Yijie Guo","Qiucheng Wu","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2207.09071v2.pdf","comment":"Published in AAAI 2022"},{"id":"http://arxiv.org/abs/2207.09938v1","updated":"2022-07-20T14:25:32Z","published":"2022-07-20T14:25:32Z","title":"Deep Preconditioners and their application to seismic wavefield\n  processing","summary":"  Seismic data processing heavily relies on the solution of physics-driven\ninverse problems. In the presence of unfavourable data acquisition conditions\n(e.g., regular or irregular coarse sampling of sources and/or receivers), the\nunderlying inverse problem becomes very ill-posed and prior information is\nrequired to obtain a satisfactory solution. Sparsity-promoting inversion,\ncoupled with fixed-basis sparsifying transforms, represent the go-to approach\nfor many processing tasks due to its simplicity of implementation and proven\nsuccessful application in a variety of acquisition scenarios. Leveraging the\nability of deep neural networks to find compact representations of complex,\nmulti-dimensional vector spaces, we propose to train an AutoEncoder network to\nlearn a direct mapping between the input seismic data and a representative\nlatent manifold. The trained decoder is subsequently used as a nonlinear\npreconditioner for the physics-driven inverse problem at hand. Synthetic and\nfield data are presented for a variety of seismic processing tasks and the\nproposed nonlinear, learned transformations are shown to outperform fixed-basis\ntransforms and convergence faster to the sought solution.\n","authors":["Matteo Ravasi"],"pdf_url":"https://arxiv.org/pdf/2207.09938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09933v1","updated":"2022-07-20T14:20:03Z","published":"2022-07-20T14:20:03Z","title":"Robust Landmark-based Stent Tracking in X-ray Fluoroscopy","summary":"  In clinical procedures of angioplasty (i.e., open clogged coronary arteries),\ndevices such as balloons and stents need to be placed and expanded in arteries\nunder the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,\nthe resulting images are often noisy. To check the correct placement of these\ndevices, typically multiple motion-compensated frames are averaged to enhance\nthe view. Therefore, device tracking is a necessary procedure for this purpose.\nEven though angioplasty devices are designed to have radiopaque markers for the\nease of tracking, current methods struggle to deliver satisfactory results due\nto the small marker size and complex scenes in angioplasty. In this paper, we\npropose an end-to-end deep learning framework for single stent tracking, which\nconsists of three hierarchical modules: U-Net based landmark detection, ResNet\nbased stent proposal and feature extraction, and graph convolutional neural\nnetwork (GCN) based stent tracking that temporally aggregates both spatial\ninformation and appearance features. The experiments show that our method\nperforms significantly better in detection compared with the state-of-the-art\npoint-based tracking models. In addition, its fast inference speed satisfies\nclinical requirements.\n","authors":["Luojie Huang","Yikang Liu","Li Chen","Eric Z Chen","Xiao Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2207.09933v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09927v1","updated":"2022-07-20T14:12:05Z","published":"2022-07-20T14:12:05Z","title":"ViGAT: Bottom-up event recognition and explanation in video using\n  factorized graph attention network","summary":"  In this paper a pure-attention bottom-up approach, called ViGAT, that\nutilizes an object detector together with a Vision Transformer (ViT) backbone\nnetwork to derive object and frame features, and a head network to process\nthese features for the task of event recognition and explanation in video, is\nproposed. The ViGAT head consists of graph attention network (GAT) blocks\nfactorized along the spatial and temporal dimensions in order to capture\neffectively both local and long-term dependencies between objects or frames.\nMoreover, using the weighted in-degrees (WiDs) derived from the adjacency\nmatrices at the various GAT blocks, we show that the proposed architecture can\nidentify the most salient objects and frames that explain the decision of the\nnetwork. A comprehensive evaluation study is performed, demonstrating that the\nproposed approach provides state-of-the-art results on three large, publicly\navailable video datasets (FCVID, Mini-Kinetics, ActivityNet).\n","authors":["Nikolaos Gkalelis","Dimitrios Daskalakis","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2207.09927v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2207.09918v1","updated":"2022-07-20T14:03:57Z","published":"2022-07-20T14:03:57Z","title":"Large Scale Radio Frequency Signal Classification","summary":"  Existing datasets used to train deep learning models for narrowband radio\nfrequency (RF) signal classification lack enough diversity in signal types and\nchannel impairments to sufficiently assess model performance in the real world.\nWe introduce the Sig53 dataset consisting of 5 million synthetically-generated\nsamples from 53 different signal classes and expertly chosen impairments. We\nalso introduce TorchSig, a signals processing machine learning toolkit that can\nbe used to generate this dataset. TorchSig incorporates data handling\nprinciples that are common to the vision domain, and it is meant to serve as an\nopen-source foundation for future signals machine learning research. Initial\nexperiments using the Sig53 dataset are conducted using state of the art (SoTA)\nconvolutional neural networks (ConvNets) and Transformers. These experiments\nreveal Transformers outperform ConvNets without the need for additional\nregularization or a ConvNet teacher, which is contrary to results from the\nvision domain. Additional experiments demonstrate that TorchSig's\ndomain-specific data augmentations facilitate model training, which ultimately\nbenefits model performance. Finally, TorchSig supports on-the-fly synthetic\ndata creation at training time, thus enabling massive scale training sessions\nwith virtually unlimited datasets.\n","authors":["Luke Boegner","Manbir Gulati","Garrett Vanhoy","Phillip Vallance","Bradley Comar","Silvija Kokalj-Filipovic","Craig Lennon","Robert D. Miller"],"pdf_url":"https://arxiv.org/pdf/2207.09918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.06212v2","updated":"2022-07-20T13:58:24Z","published":"2022-05-12T16:52:48Z","title":"Contingency-constrained economic dispatch with safe reinforcement\n  learning","summary":"  Future power systems will rely heavily on micro grids with a high share of\ndecentralised renewable energy sources and energy storage systems. The high\ncomplexity and uncertainty in this context might make conventional power\ndispatch strategies infeasible. Reinforcement-learning based (RL) controllers\ncan address this challenge, however, cannot themselves provide safety\nguarantees, preventing their deployment in practice. To overcome this\nlimitation, we propose a formally validated RL controller for economic\ndispatch. We extend conventional constraints by a time-dependent constraint\nencoding the islanding contingency. The contingency constraint is computed\nusing set-based backwards reachability analysis and actions of the RL agent are\nverified through a safety layer. Unsafe actions are projected into the safe\naction space while leveraging constrained zonotope set representations for\ncomputational efficiency. The developed approach is demonstrated on a\nresidential use case using real-world measurements.\n","authors":["Michael Eichelbeck","Hannah Markgraf","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2205.06212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.09571v2","updated":"2022-07-20T13:28:56Z","published":"2022-06-20T04:58:09Z","title":"Deep Random Vortex Method for Simulation and Inference of Navier-Stokes\n  Equations","summary":"  Navier-Stokes equations are significant partial differential equations that\ndescribe the motion of fluids such as liquids and air. Due to the importance of\nNavier-Stokes equations, the development on efficient numerical schemes is\nimportant for both science and engineer. Recently, with the development of AI\ntechniques, several approaches have been designed to integrate deep neural\nnetworks in simulating and inferring the fluid dynamics governed by\nincompressible Navier-Stokes equations, which can accelerate the simulation or\ninferring process in a mesh-free and differentiable way. In this paper, we\npoint out that the capability of existing deep Navier-Stokes informed methods\nis limited to handle non-smooth or fractional equations, which are two critical\nsituations in reality. To this end, we propose the \\emph{Deep Random Vortex\nMethod} (DRVM), which combines the neural network with a random vortex dynamics\nsystem equivalent to the Navier-Stokes equation. Specifically, the random\nvortex dynamics motivates a Monte Carlo based loss function for training the\nneural network, which avoids the calculation of derivatives through\nauto-differentiation. Therefore, DRVM not only can efficiently solve\nNavier-Stokes equations involving rough path, non-differentiable initial\nconditions and fractional operators, but also inherits the mesh-free and\ndifferentiable benefits of the deep-learning-based solver. We conduct\nexperiments on the Cauchy problem, parametric solver learning, and the inverse\nproblem of both 2-d and 3-d incompressible Navier-Stokes equations. The\nproposed method achieves accurate results for simulation and inference of\nNavier-Stokes equations. Especially for the cases that include singular initial\nconditions, DRVM significantly outperforms existing PINN method.\n","authors":["Rui Zhang","Peiyan Hu","Qi Meng","Yue Wang","Rongchan Zhu","Bingguang Chen","Zhi-Ming Ma","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2206.09571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09874v1","updated":"2022-07-20T13:15:23Z","published":"2022-07-20T13:15:23Z","title":"Stream-based active learning with linear models","summary":"  The proliferation of automated data collection schemes and the advances in\nsensorics are increasing the amount of data we are able to monitor in\nreal-time. However, given the high annotation costs and the time required by\nquality inspections, data is often available in an unlabeled form. This is\nfostering the use of active learning for the development of soft sensors and\npredictive models. In production, instead of performing random inspections to\nobtain product information, labels are collected by evaluating the information\ncontent of the unlabeled data. Several query strategy frameworks for regression\nhave been proposed in the literature but most of the focus has been dedicated\nto the static pool-based scenario. In this work, we propose a new strategy for\nthe stream-based scenario, where instances are sequentially offered to the\nlearner, which must instantaneously decide whether to perform the quality check\nto obtain the label or discard the instance. The approach is inspired by the\noptimal experimental design theory and the iterative aspect of the\ndecision-making process is tackled by setting a threshold on the\ninformativeness of the unlabeled data points. The proposed approach is\nevaluated using numerical simulations and the Tennessee Eastman Process\nsimulator. The results confirm that selecting the examples suggested by the\nproposed algorithm allows for a faster reduction in the prediction error.\n","authors":["Davide Cacciarelli","Murat Kulahci","John Sølve Tyssedal"],"pdf_url":"https://arxiv.org/pdf/2207.09874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09869v1","updated":"2022-07-20T13:04:08Z","published":"2022-07-20T13:04:08Z","title":"A Novel Neural Network Training Method for Autonomous Driving Using\n  Semi-Pseudo-Labels and 3D Data Augmentations","summary":"  Training neural networks to perform 3D object detection for autonomous\ndriving requires a large amount of diverse annotated data. However, obtaining\ntraining data with sufficient quality and quantity is expensive and sometimes\nimpossible due to human and sensor constraints. Therefore, a novel solution is\nneeded for extending current training methods to overcome this limitation and\nenable accurate 3D object detection. Our solution for the above-mentioned\nproblem combines semi-pseudo-labeling and novel 3D augmentations. For\ndemonstrating the applicability of the proposed method, we have designed a\nconvolutional neural network for 3D object detection which can significantly\nincrease the detection range in comparison with the training data distribution.\n","authors":["Tamas Matuszka","Daniel Kozma"],"pdf_url":"https://arxiv.org/pdf/2207.09869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07458v2","updated":"2022-07-20T13:03:18Z","published":"2022-06-15T11:29:58Z","title":"VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via\n  Speech-Visage Feature Selection","summary":"  The goal of this work is to reconstruct speech from a silent talking face\nvideo. Recent studies have shown impressive performance on synthesizing speech\nfrom silent talking face videos. However, they have not explicitly considered\non varying identity characteristics of different speakers, which place a\nchallenge in the video-to-speech synthesis, and this becomes more critical in\nunseen-speaker settings. Our approach is to separate the speech content and the\nvisage-style from a given silent talking face video. By guiding the model to\nindependently focus on modeling the two representations, we can obtain the\nspeech of high intelligibility from the model even when the input video of an\nunseen subject is given. To this end, we introduce speech-visage selection that\nseparates the speech content and the speaker identity from the visual features\nof the input video. The disentangled representations are jointly incorporated\nto synthesize speech through visage-style based synthesizer which generates\nspeech by coating the visage-styles while maintaining the speech content. Thus,\nthe proposed framework brings the advantage of synthesizing the speech\ncontaining the right content even with the silent talking face video of an\nunseen subject. We validate the effectiveness of the proposed framework on the\nGRID, TCD-TIMIT volunteer, and LRW datasets.\n","authors":["Joanna Hong","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2206.07458v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.08435v2","updated":"2022-07-20T13:02:19Z","published":"2022-07-18T08:41:00Z","title":"Robust Simulation-Based Inference in Cosmology with Bayesian Neural\n  Networks","summary":"  Simulation-based inference (SBI) is rapidly establishing itself as a standard\nmachine learning technique for analyzing data in cosmological surveys. Despite\ncontinual improvements to the quality of density estimation by learned models,\napplications of such techniques to real data are entirely reliant on the\ngeneralization power of neural networks far outside the training distribution,\nwhich is mostly unconstrained. Due to the imperfections in scientist-created\nsimulations, and the large computational expense of generating all possible\nparameter combinations, SBI methods in cosmology are vulnerable to such\ngeneralization issues. Here, we discuss the effects of both issues, and show\nhow using a Bayesian neural network framework for training SBI can mitigate\nbiases, and result in more reliable inference outside the training set. We\nintroduce cosmoSWAG, the first application of Stochastic Weight Averaging to\ncosmology, and apply it to SBI trained for inference on the cosmic microwave\nbackground.\n","authors":["Pablo Lemos","Miles Cranmer","Muntazir Abidi","ChangHoon Hahn","Michael Eickenberg","Elena Massara","David Yallup","Shirley Ho"],"pdf_url":"https://arxiv.org/pdf/2207.08435v2.pdf","comment":"5 pages, 3 figures. Accepted at the ML4Astro Machine Learning for\n  Astrophysics Workshop at the Thirty-ninth International Conference on Machine\n  Learning (ICML 2022)"},{"id":"http://arxiv.org/abs/2207.09860v1","updated":"2022-07-20T12:51:06Z","published":"2022-07-20T12:51:06Z","title":"Learning to Solve Soft-Constrained Vehicle Routing Problems with\n  Lagrangian Relaxation","summary":"  Vehicle Routing Problems (VRPs) in real-world applications often come with\nvarious constraints, therefore bring additional computational challenges to\nexact solution methods or heuristic search approaches. The recent idea to learn\nheuristic move patterns from sample data has become increasingly promising to\nreduce solution developing costs. However, using learning-based approaches to\naddress more types of constrained VRP remains a challenge. The difficulty lies\nin controlling for constraint violations while searching for optimal solutions.\nTo overcome this challenge, we propose a Reinforcement Learning based method to\nsolve soft-constrained VRPs by incorporating the Lagrangian relaxation\ntechnique and using constrained policy optimization. We apply the method on\nthree common types of VRPs, the Travelling Salesman Problem with Time Windows\n(TSPTW), the Capacitated VRP (CVRP) and the Capacitated VRP with Time Windows\n(CVRPTW), to show the generalizability of the proposed method. After comparing\nto existing RL-based methods and open-source heuristic solvers, we demonstrate\nits competitive performance in finding solutions with a good balance in travel\ndistance, constraint violations and inference speed.\n","authors":["Qiaoyue Tang","Yangzhe Kong","Lemeng Pan","Choonmeng Lee"],"pdf_url":"https://arxiv.org/pdf/2207.09860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09858v1","updated":"2022-07-20T12:46:26Z","published":"2022-07-20T12:46:26Z","title":"UniHPF : Universal Healthcare Predictive Framework with Zero Domain\n  Knowledge","summary":"  Despite the abundance of Electronic Healthcare Records (EHR), its\nheterogeneity restricts the utilization of medical data in building predictive\nmodels. To address this challenge, we propose Universal Healthcare Predictive\nFramework (UniHPF), which requires no medical domain knowledge and minimal\npre-processing for multiple prediction tasks. Experimental results demonstrate\nthat UniHPF is capable of building large-scale EHR models that can process any\nform of medical data from distinct EHR systems. Our framework significantly\noutperforms baseline models in multi-source learning tasks, including transfer\nand pooled learning, while also showing comparable results when trained on a\nsingle medical dataset. To empirically demonstrate the efficacy of our work, we\nconducted extensive experiments using various datasets, model structures, and\ntasks. We believe that our findings can provide helpful insights for further\nresearch on the multi-source learning of EHRs.\n","authors":["Kyunghoon Hur","Jungwoo Oh","Junu Kim","Min Jae Lee","Eunbyeol Choi","Jiyoun Kim","Seong-Eun Moon","Young-Hak Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2207.09858v1.pdf","comment":"Main paper (11pages)"},{"id":"http://arxiv.org/abs/2207.09849v1","updated":"2022-07-20T12:27:22Z","published":"2022-07-20T12:27:22Z","title":"Automated machine learning for borehole resistivity measurements","summary":"  Deep neural networks (DNNs) offer a real-time solution for the inversion of\nborehole resistivity measurements to approximate forward and inverse operators.\nIt is possible to use extremely large DNNs to approximate the operators, but it\ndemands a considerable training time. Moreover, evaluating the network after\ntraining also requires a significant amount of memory and processing power. In\naddition, we may overfit the model. In this work, we propose a scoring function\nthat accounts for the accuracy and size of the DNNs compared to a reference DNN\nthat provides a good approximation for the operators. Using this scoring\nfunction, we use DNN architecture search algorithms to obtain a quasi-optimal\nDNN smaller than the reference network; hence, it requires less computational\neffort during training and evaluation. The quasi-optimal DNN delivers\ncomparable accuracy to the original large DNN.\n","authors":["M. Shahriari","D. Pardo","S. Kargaran","T. Teijeiro"],"pdf_url":"https://arxiv.org/pdf/2207.09849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09845v1","updated":"2022-07-20T12:17:02Z","published":"2022-07-20T12:17:02Z","title":"Quantifying the Effect of Feedback Frequency in Interactive\n  Reinforcement Learning for Robotic Tasks","summary":"  Reinforcement learning (RL) has become widely adopted in robot control.\nDespite many successes, one major persisting problem can be very low data\nefficiency. One solution is interactive feedback, which has been shown to speed\nup RL considerably. As a result, there is an abundance of different strategies,\nwhich are, however, primarily tested on discrete grid-world and small scale\noptimal control scenarios. In the literature, there is no consensus about which\nfeedback frequency is optimal or at which time the feedback is most beneficial.\nTo resolve these discrepancies we isolate and quantify the effect of feedback\nfrequency in robotic tasks with continuous state and action spaces. The\nexperiments encompass inverse kinematics learning for robotic manipulator arms\nof different complexity. We show that seemingly contradictory reported\nphenomena occur at different complexity levels. Furthermore, our results\nsuggest that no single ideal feedback frequency exists. Rather that feedback\nfrequency should be changed as the agent's proficiency in the task increases.\n","authors":["Daniel Harnack","Julie Pivin-Bachler","Nicolás Navarro-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2207.09845v1.pdf","comment":"Neural Computing and Applications. Special Issue on Human-aligned\n  Reinforcement Learning for Autonomous Agents and Robots"},{"id":"http://arxiv.org/abs/2206.06182v2","updated":"2022-07-20T12:15:12Z","published":"2022-06-13T14:13:15Z","title":"AI-based Data Preparation and Data Analytics in Healthcare: The Case of\n  Diabetes","summary":"  The Associazione Medici Diabetologi (AMD) collects and manages one of the\nlargest worldwide-available collections of diabetic patient records, also known\nas the AMD database. This paper presents the initial results of an ongoing\nproject whose focus is the application of Artificial Intelligence and Machine\nLearning techniques for conceptualizing, cleaning, and analyzing such an\nimportant and valuable dataset, with the goal of providing predictive insights\nto better support diabetologists in their diagnostic and therapeutic choices.\n","authors":["Marianna Maranghi","Aris Anagnostopoulos","Irene Cannistraci","Ioannis Chatzigiannakis","Federico Croce","Giulia Di Teodoro","Michele Gentile","Giorgio Grani","Maurizio Lenzerini","Stefano Leonardi","Andrea Mastropietro","Laura Palagi","Massimiliano Pappa","Riccardo Rosati","Riccardo Valentini","Paola Velardi"],"pdf_url":"https://arxiv.org/pdf/2206.06182v2.pdf","comment":"The work has been presented at the conference Ital-IA 2022\n  (https://www.ital-ia2022.it/)"},{"id":"http://arxiv.org/abs/2012.07065v2","updated":"2022-07-20T12:02:52Z","published":"2020-12-13T13:59:48Z","title":"LSCALE: Latent Space Clustering-Based Active Learning for Node\n  Classification","summary":"  Node classification on graphs is an important task in many practical domains.\nIt usually requires labels for training, which can be difficult or expensive to\nobtain in practice. Given a budget for labelling, active learning aims to\nimprove performance by carefully choosing which nodes to label. Previous graph\nactive learning methods learn representations using labelled nodes and select\nsome unlabelled nodes for label acquisition. However, they do not fully utilize\nthe representation power present in unlabelled nodes. We argue that the\nrepresentation power in unlabelled nodes can be useful for active learning and\nfor further improving performance of active learning for node classification.\nIn this paper, we propose a latent space clustering-based active learning\nframework for node classification (LSCALE), where we fully utilize the\nrepresentation power in both labelled and unlabelled nodes. Specifically, to\nselect nodes for labelling, our framework uses the K-Medoids clustering\nalgorithm on a latent space based on a dynamic combination of both unsupervised\nfeatures and supervised features. In addition, we design an incremental\nclustering module to avoid redundancy between nodes selected at different\nsteps. Extensive experiments on five datasets show that our proposed framework\nLSCALE consistently and significantly outperforms the stateof-the-art\napproaches by a large margin.\n","authors":["Juncheng Liu","Yiwei Wang","Bryan Hooi","Renchi Yang","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2012.07065v2.pdf","comment":"ECML-PKDD 2022"},{"id":"http://arxiv.org/abs/2103.16554v2","updated":"2022-07-20T11:53:15Z","published":"2021-03-30T17:57:25Z","title":"Pre-training strategies and datasets for facial representation learning","summary":"  What is the best way to learn a universal face representation? Recent work on\nDeep Learning in the area of face analysis has focused on supervised learning\nfor specific tasks of interest (e.g. face recognition, facial landmark\nlocalization etc.) but has overlooked the overarching question of how to find a\nfacial representation that can be readily adapted to several facial analysis\ntasks and datasets. To this end, we make the following 4 contributions: (a) we\nintroduce, for the first time, a comprehensive evaluation benchmark for facial\nrepresentation learning consisting of 5 important face analysis tasks. (b) We\nsystematically investigate two ways of large-scale representation learning\napplied to faces: supervised and unsupervised pre-training. Importantly, we\nfocus our evaluations on the case of few-shot facial learning. (c) We\ninvestigate important properties of the training datasets including their size\nand quality (labelled, unlabelled or even uncurated). (d) To draw our\nconclusions, we conducted a very large number of experiments. Our main two\nfindings are: (1) Unsupervised pre-training on completely in-the-wild,\nuncurated data provides consistent and, in some cases, significant accuracy\nimprovements for all facial tasks considered. (2) Many existing facial video\ndatasets seem to have a large amount of redundancy. We will release code, and\npre-trained models to facilitate future research.\n","authors":["Adrian Bulat","Shiyang Cheng","Jing Yang","Andrew Garbett","Enrique Sanchez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2103.16554v2.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2203.02452v2","updated":"2022-07-20T11:48:23Z","published":"2022-03-04T17:29:32Z","title":"Contextformer: A Transformer with Spatio-Channel Attention for Context\n  Modeling in Learned Image Compression","summary":"  Entropy modeling is a key component for high-performance image compression\nalgorithms. Recent developments in autoregressive context modeling helped\nlearning-based methods to surpass their classical counterparts. However, the\nperformance of those models can be further improved due to the underexploited\nspatio-channel dependencies in latent space, and the suboptimal implementation\nof context adaptivity. Inspired by the adaptive characteristics of the\ntransformers, we propose a transformer-based context model, named\nContextformer, which generalizes the de facto standard attention mechanism to\nspatio-channel attention. We replace the context model of a modern compression\nframework with the Contextformer and test it on the widely used Kodak,\nCLIC2020, and Tecnick image datasets. Our experimental results show that the\nproposed model provides up to 11% rate savings compared to the standard\nVersatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various\nlearning-based models in terms of PSNR and MS-SSIM.\n","authors":["A. Burakhan Koyuncu","Han Gao","Atanas Boev","Georgii Gaikov","Elena Alshina","Eckehard Steinbach"],"pdf_url":"https://arxiv.org/pdf/2203.02452v2.pdf","comment":"Accepted at ECCV 2022; 31 pages (14 main paper + References + 13\n  Appendix)"},{"id":"http://arxiv.org/abs/2207.09833v1","updated":"2022-07-20T11:37:46Z","published":"2022-07-20T11:37:46Z","title":"AI Fairness: from Principles to Practice","summary":"  This paper summarizes and evaluates various approaches, methods, and\ntechniques for pursuing fairness in artificial intelligence (AI) systems. It\nexamines the merits and shortcomings of these measures and proposes practical\nguidelines for defining, measuring, and preventing bias in AI. In particular,\nit cautions against some of the simplistic, yet common, methods for evaluating\nbias in AI systems, and offers more sophisticated and effective alternatives.\nThe paper also addresses widespread controversies and confusions in the field\nby providing a common language among different stakeholders of high-impact AI\nsystems. It describes various trade-offs involving AI fairness, and provides\npractical recommendations for balancing them. It offers techniques for\nevaluating the costs and benefits of fairness targets, and defines the role of\nhuman judgment in setting these targets. This paper provides discussions and\nguidelines for AI practitioners, organization leaders, and policymakers, as\nwell as various links to additional materials for a more technical audience.\nNumerous real-world examples are provided to clarify the concepts, challenges,\nand recommendations from a practical perspective.\n","authors":["Arash Bateni","Matthew C. Chan","Ray Eitel-Porter"],"pdf_url":"https://arxiv.org/pdf/2207.09833v1.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2204.04662v2","updated":"2022-07-20T11:37:42Z","published":"2022-04-10T11:38:33Z","title":"FOSTER: Feature Boosting and Compression for Class-Incremental Learning","summary":"  The ability to learn new concepts continually is necessary in this\never-changing world. However, deep neural networks suffer from catastrophic\nforgetting when learning new categories. Many works have been proposed to\nalleviate this phenomenon, whereas most of them either fall into the\nstability-plasticity dilemma or take too much computation or storage overhead.\nInspired by the gradient boosting algorithm to gradually fit the residuals\nbetween the target model and the previous ensemble model, we propose a novel\ntwo-stage learning paradigm FOSTER, empowering the model to learn new\ncategories adaptively. Specifically, we first dynamically expand new modules to\nfit the residuals between the target and the output of the original model.\nNext, we remove redundant parameters and feature dimensions through an\neffective distillation strategy to maintain the single backbone model. We\nvalidate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different\nsettings. Experimental results show that our method achieves state-of-the-art\nperformance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.\n","authors":["Fu-Yun Wang","Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2204.04662v2.pdf","comment":"Accepted to ECCV 2022. Code is available at:\n  https://github.com/G-U-N/ECCV22-FOSTER"},{"id":"http://arxiv.org/abs/2106.05587v2","updated":"2022-07-20T11:33:28Z","published":"2021-06-10T08:40:30Z","title":"A Discontinuity Capturing Shallow Neural Network for Elliptic Interface\n  Problems","summary":"  In this paper, a new Discontinuity Capturing Shallow Neural Network (DCSNN)\nfor approximating $d$-dimensional piecewise continuous functions and for\nsolving elliptic interface problems is developed. There are three novel\nfeatures in the present network; namely, (i) jump discontinuities are\naccurately captured, (ii) it is completely shallow, comprising only one hidden\nlayer, (iii) it is completely mesh-free for solving partial differential\nequations. The crucial idea here is that a $d$-dimensional piecewise continuous\nfunction can be extended to a continuous function defined in\n$(d+1)$-dimensional space, where the augmented coordinate variable labels the\npieces of each sub-domain. We then construct a shallow neural network to\nexpress this new function. Since only one hidden layer is employed, the number\nof training parameters (weights and biases) scales linearly with the dimension\nand the neurons used in the hidden layer. For solving elliptic interface\nproblems, the network is trained by minimizing the mean square error loss that\nconsists of the residual of the governing equation, boundary condition, and the\ninterface jump conditions. We perform a series of numerical tests to\ndemonstrate the accuracy of the present network. Our DCSNN model is efficient\ndue to only a moderate number of parameters needed to be trained (a few hundred\nparameters used throughout all numerical examples), and the results indicate\ngood accuracy. Compared with the results obtained by the traditional grid-based\nimmersed interface method (IIM), which is designed particularly for elliptic\ninterface problems, our network model shows a better accuracy than IIM. We\nconclude by solving a six-dimensional problem to demonstrate the capability of\nthe present network for high-dimensional applications.\n","authors":["Wei-Fan Hu","Te-Sheng Lin","Ming-Chih Lai"],"pdf_url":"https://arxiv.org/pdf/2106.05587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.07743v3","updated":"2022-07-20T11:29:37Z","published":"2021-05-17T11:34:09Z","title":"Universal Regular Conditional Distributions","summary":"  We introduce a deep learning model which can generically approximate regular\nconditional distributions (RCDs). The proposed model operates in three phases:\nfirst linearizes inputs from a given metric space $\\mathcal{X}$ to\n$\\mathbb{R}^d$ via a feature map then, these linearized features are processed\nby a deep feedforward neural network, and the network's outputs are then\ntranslated to the $1$-Wasserstein space $\\mathcal{P}_1(\\mathbb{R}^D)$ via a\nprobabilistic extension of the attention mechanism introduced by Bahdanau et\nal. (2014). We find that the models built using our framework can approximate\nany continuous function from $\\mathbb{R}^d$ to $\\mathcal{P}_1(\\mathbb{R}^D)$\nuniformly on compact sets, quantitatively. We identify two ways of avoiding the\ncurse of dimensionality when approximating $\\mathcal{P}_1(\\mathbb{R}^D)$-valued\nfunctions. The first strategy describes functions in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be efficiently\napproximated on any compact subset of $\\mathbb{R}^d$. The second approach\ndescribes compact subsets of $\\mathbb{R}^d$, on which any most in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ can be efficiently approximated.\nThe results are verified experimentally.\n","authors":["Anastasis Kratsios"],"pdf_url":"https://arxiv.org/pdf/2105.07743v3.pdf","comment":"Regular Conditional Distributions, Geometric Deep Learning,\n  Computational Optimal Transport, Measure-Valued Neural Networks, Universal\n  Approximation, Transformers"},{"id":"http://arxiv.org/abs/2207.09821v1","updated":"2022-07-20T11:14:15Z","published":"2022-07-20T11:14:15Z","title":"Journal Impact Factor and Peer Review Thoroughness and Helpfulness: A\n  Supervised Machine Learning Study","summary":"  The journal impact factor (JIF) is often equated with journal quality and the\nquality of the peer review of the papers submitted to the journal. We examined\nthe association between the content of peer review and JIF by analysing 10,000\npeer review reports submitted to 1,644 medical and life sciences journals. Two\nresearchers hand-coded a random sample of 2,000 sentences. We then trained\nmachine learning models to classify all 187,240 sentences as contributing or\nnot contributing to content categories. We examined the association between ten\ngroups of journals defined by JIF deciles and the content of peer reviews using\nlinear mixed-effects models, adjusting for the length of the review. The JIF\nranged from 0.21 to 74.70. The length of peer reviews increased from the lowest\n(median number of words 185) to the JIF group (387 words). The proportion of\nsentences allocated to different content categories varied widely, even within\nJIF groups. For thoroughness, sentences on 'Materials and Methods' were more\ncommon in the highest JIF journals than in the lowest JIF group (difference of\n7.8 percentage points; 95% CI 4.9 to 10.7%). The trend for 'Presentation and\nReporting' went in the opposite direction, with the highest JIF journals giving\nless emphasis to such content (difference -8.9%; 95% CI -11.3 to -6.5%). For\nhelpfulness, reviews for higher JIF journals devoted less attention to\n'Suggestion and Solution' and provided fewer Examples than lower impact factor\njournals. No, or only small differences were evident for other content\ncategories. In conclusion, peer review in journals with higher JIF tends to be\nmore thorough in discussing the methods used but less helpful in terms of\nsuggesting solutions and providing examples. Differences were modest and\nvariability high, indicating that the JIF is a bad predictor for the quality of\npeer review of an individual manuscript.\n","authors":["Anna Severin","Michaela Strinzel","Matthias Egger","Tiago Barros","Alexander Sokolov","Julia Vilstrup Mouatt","Stefan Müller"],"pdf_url":"https://arxiv.org/pdf/2207.09821v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2207.09818v1","updated":"2022-07-20T11:07:46Z","published":"2022-07-20T11:07:46Z","title":"Operating Envelopes under Probabilistic Electricity Demand and Solar\n  Generation Forecasts","summary":"  The increasing penetration of distributed energy resources in low-voltage\nnetworks is turning end-users from consumers to prosumers. However, the\nincomplete smart meter rollout and paucity of smart meter data due to the\nregulatory separation between retail and network service provision make active\ndistribution network management difficult. Furthermore, distribution network\noperators oftentimes do not have access to real-time smart meter data, which\ncreates an additional challenge. For the lack of better solutions, they use\nblanket rooftop solar export limits, leading to suboptimal outcomes. To address\nthis, we designed a conditional generative adversarial network (CGAN)-based\nmodel to forecast household solar generation and electricity demand, which\nserves as an input to chance-constrained optimal power flow used to compute\nfair operating envelopes under uncertainty.\n","authors":["Yu Yi","Gregor Verbic"],"pdf_url":"https://arxiv.org/pdf/2207.09818v1.pdf","comment":"In proceedings of the 11th Bulk Power Systems Dynamics and Control\n  Symposium (IREP 2022), July 25-30, 2022, Banff, Canada"},{"id":"http://arxiv.org/abs/2111.13681v3","updated":"2022-07-20T10:26:06Z","published":"2021-11-26T18:59:58Z","title":"ManiFest: Manifold Deformation for Few-shot Image Translation","summary":"  Most image-to-image translation methods require a large number of training\nimages, which restricts their applicability. We instead propose ManiFest: a\nframework for few-shot image translation that learns a context-aware\nrepresentation of a target domain from a few images only. To enforce feature\nconsistency, our framework learns a style manifold between source and proxy\nanchor domains (assumed to be composed of large numbers of images). The learned\nmanifold is interpolated and deformed towards the few-shot target domain via\npatch-based adversarial and feature statistics alignment losses. All of these\ncomponents are trained simultaneously during a single end-to-end loop. In\naddition to the general few-shot translation task, our approach can\nalternatively be conditioned on a single exemplar image to reproduce its\nspecific style. Extensive experiments demonstrate the efficacy of ManiFest on\nmultiple tasks, outperforming the state-of-the-art on all metrics and in both\nthe general- and exemplar-based scenarios. Our code is available at\nhttps://github.com/cv-rits/Manifest .\n","authors":["Fabio Pizzati","Jean-François Lalonde","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2111.13681v3.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2204.05141v2","updated":"2022-07-20T10:22:14Z","published":"2022-04-11T14:19:04Z","title":"Learning Object-Centered Autotelic Behaviors with Graph Neural Networks","summary":"  Although humans live in an open-ended world and endlessly face new\nchallenges, they do not have to learn from scratch each time they face the next\none. Rather, they have access to a handful of previously learned skills, which\nthey rapidly adapt to new situations. In artificial intelligence, autotelic\nagents, which are intrinsically motivated to represent and set their own goals,\nexhibit promising skill adaptation capabilities. However, these capabilities\nare highly constrained by their policy and goal space representations. In this\npaper, we propose to investigate the impact of these representations on the\nlearning and transfer capabilities of autotelic agents. We study different\nimplementations of autotelic agents using four types of Graph Neural Networks\npolicy representations and two types of goal spaces, either geometric or\npredicate-based. By testing agents on unseen goals, we show that combining\nobject-centered architectures that are expressive enough with semantic\nrelational goals helps learning to reach more difficult goals. We also release\nour graph-based implementations to encourage further research in this\ndirection.\n","authors":["Ahmed Akakzia","Olivier Sigaud"],"pdf_url":"https://arxiv.org/pdf/2204.05141v2.pdf","comment":"15 pages, 10 figures, published at the Conference on Lifelong\n  Learning Agents COLLAS 2022"},{"id":"http://arxiv.org/abs/2111.12506v3","updated":"2022-07-20T10:01:57Z","published":"2021-11-24T14:04:32Z","title":"Generalized Normalizing Flows via Markov Chains","summary":"  Normalizing flows, diffusion normalizing flows and variational autoencoders\nare powerful generative models. This chapter provides a unified framework to\nhandle these approaches via Markov chains. We consider stochastic normalizing\nflows as a pair of Markov chains fulfilling some properties and show how many\nstate-of-the-art models for data generation fit into this framework. Indeed\nnumerical simulations show that including stochastic layers improves the\nexpressivity of the network and allows for generating multimodal distributions\nfrom unimodal ones. The Markov chains point of view enables us to couple both\ndeterministic layers as invertible neural networks and stochastic layers as\nMetropolis-Hasting layers, Langevin layers, variational autoencoders and\ndiffusion normalizing flows in a mathematically sound way. Our framework\nestablishes a useful mathematical tool to combine the various approaches.\n","authors":["Paul Hagemann","Johannes Hertrich","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2111.12506v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2109.11375"},{"id":"http://arxiv.org/abs/2203.01909v2","updated":"2022-07-20T10:01:24Z","published":"2022-03-03T18:39:50Z","title":"An Adaptive Human Driver Model for Realistic Race Car Simulations","summary":"  Engineering a high-performance race car requires a direct consideration of\nthe human driver using real-world tests or Human-Driver-in-the-Loop\nsimulations. Apart from that, offline simulations with human-like race driver\nmodels could make this vehicle development process more effective and efficient\nbut are hard to obtain due to various challenges. With this work, we intend to\nprovide a better understanding of race driver behavior and introduce an\nadaptive human race driver model based on imitation learning. Using existing\nfindings and an interview with a professional race engineer, we identify\nfundamental adaptation mechanisms and how drivers learn to optimize lap time on\na new track. Subsequently, we use these insights to develop generalization and\nadaptation techniques for a recently presented probabilistic driver modeling\napproach and evaluate it using data from professional race drivers and a\nstate-of-the-art race car simulator. We show that our framework can create\nrealistic driving line distributions on unseen race tracks with almost\nhuman-like performance. Moreover, our driver model optimizes its driving lap by\nlap, correcting driving errors from previous laps while achieving faster lap\ntimes. This work contributes to a better understanding and modeling of the\nhuman driver, aiming to expedite simulation methods in the modern vehicle\ndevelopment process and potentially supporting automated driving and racing\ntechnologies.\n","authors":["Stefan Löckel","Siwei Ju","Maximilian Schaller","Peter van Vliet","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2203.01909v2.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09786v1","updated":"2022-07-20T09:59:28Z","published":"2022-07-20T09:59:28Z","title":"Non-Uniform Diffusion Models","summary":"  Diffusion models have emerged as one of the most promising frameworks for\ndeep generative modeling. In this work, we explore the potential of non-uniform\ndiffusion models. We show that non-uniform diffusion leads to multi-scale\ndiffusion models which have similar structure to this of multi-scale\nnormalizing flows. We experimentally find that in the same or less training\ntime, the multi-scale diffusion model achieves better FID score than the\nstandard uniform diffusion model. More importantly, it generates samples $4.4$\ntimes faster in $128\\times 128$ resolution. The speed-up is expected to be\nhigher in higher resolutions where more scales are used. Moreover, we show that\nnon-uniform diffusion leads to a novel estimator for the conditional score\nfunction which achieves on par performance with the state-of-the-art\nconditional denoising estimator. Our theoretical and experimental findings are\naccompanied by an open source library MSDiff which can facilitate further\nresearch of non-uniform diffusion models.\n","authors":["Georgios Batzolis","Jan Stanczuk","Carola-Bibiane Schönlieb","Christian Etmann"],"pdf_url":"https://arxiv.org/pdf/2207.09786v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2111.13606"},{"id":"http://arxiv.org/abs/2207.09785v1","updated":"2022-07-20T09:50:23Z","published":"2022-07-20T09:50:23Z","title":"Unsupervised energy disaggregation via convolutional sparse coding","summary":"  In this work, a method for unsupervised energy disaggregation in private\nhouseholds equipped with smart meters is proposed. This method aims to classify\npower consumption as active or passive, granting the ability to report on the\nresidents' activity and presence without direct interaction. This lays the\nfoundation for applications like non-intrusive health monitoring of private\nhomes.\n  The proposed method is based on minimizing a suitable energy functional, for\nwhich the iPALM (inertial proximal alternating linearized minimization)\nalgorithm is employed, demonstrating that various conditions guaranteeing\nconvergence are satisfied.\n  In order to confirm feasibility of the proposed method, experiments on\nsemi-synthetic test data sets and a comparison to existing, supervised methods\nare provided.\n","authors":["Christian Aarset","Andreas Habring","Martin Holler","Mario Mitter"],"pdf_url":"https://arxiv.org/pdf/2207.09785v1.pdf","comment":"9 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2207.09783v1","updated":"2022-07-20T09:47:53Z","published":"2022-07-20T09:47:53Z","title":"Cancer Subtyping by Improved Transcriptomic Features Using Vector\n  Quantized Variational Autoencoder","summary":"  Defining and separating cancer subtypes is essential for facilitating\npersonalized therapy modality and prognosis of patients. The definition of\nsubtypes has been constantly recalibrated as a result of our deepened\nunderstanding. During this recalibration, researchers often rely on clustering\nof cancer data to provide an intuitive visual reference that could reveal the\nintrinsic characteristics of subtypes. The data being clustered are often omics\ndata such as transcriptomics that have strong correlations to the underlying\nbiological mechanism. However, while existing studies have shown promising\nresults, they suffer from issues associated with omics data: sample scarcity\nand high dimensionality. As such, existing methods often impose unrealistic\nassumptions to extract useful features from the data while avoiding overfitting\nto spurious correlations. In this paper, we propose to leverage a recent strong\ngenerative model, Vector Quantized Variational AutoEncoder (VQ-VAE), to tackle\nthe data issues and extract informative latent features that are crucial to the\nquality of subsequent clustering by retaining only information relevant to\nreconstructing the input. VQ-VAE does not impose strict assumptions and hence\nits latent features are better representations of the input, capable of\nyielding superior clustering performance with any mainstream clustering method.\nExtensive experiments and medical analysis on multiple datasets comprising 10\ndistinct cancers demonstrate the VQ-VAE clustering results can significantly\nand robustly improve prognosis over prevalent subtyping systems.\n","authors":["Zheng Chen","Ziwei Yang","Lingwei Zhu","Guang Shi","Kun Yue","Takashi Matsubara","Shigehiko Kanaya","MD Altaf-Ul-Amin"],"pdf_url":"https://arxiv.org/pdf/2207.09783v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2207.09778v1","updated":"2022-07-20T09:33:42Z","published":"2022-07-20T09:33:42Z","title":"CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR\n  Segmentation","summary":"  3D LiDAR semantic segmentation is fundamental for autonomous driving. Several\nUnsupervised Domain Adaptation (UDA) methods for point cloud data have been\nrecently proposed to improve model generalization for different sensors and\nenvironments. Researchers working on UDA problems in the image domain have\nshown that sample mixing can mitigate domain shift. We propose a new approach\nof sample mixing for point cloud UDA, namely Compositional Semantic Mix\n(CoSMix), the first UDA approach for point cloud segmentation based on sample\nmixing. CoSMix consists of a two-branch symmetric network that can process\nlabelled synthetic data (source) and real-world unlabelled point clouds\n(target) concurrently. Each branch operates on one domain by mixing selected\npieces of data from the other one, and by using the semantic information\nderived from source labels and target pseudo-labels. We evaluate CoSMix on two\nlarge-scale datasets, showing that it outperforms state-of-the-art methods by a\nlarge margin. Our code is available at\nhttps://github.com/saltoricristiano/cosmix-uda.\n","authors":["Cristiano Saltori","Fabio Galasso","Giuseppe Fiameni","Nicu Sebe","Elisa Ricci","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2207.09778v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2203.07574v3","updated":"2022-07-20T09:23:36Z","published":"2022-03-15T00:49:15Z","title":"Time-series image denoising of pressure-sensitive paint data by\n  projected multivariate singular spectrum analysis","summary":"  Time-series data, such as unsteady pressure-sensitive paint (PSP) measurement\ndata, may contain a significant amount of random noise. Thus, in this study, we\ninvestigated a noise-reduction method that combines multivariate singular\nspectrum analysis (MSSA) with low-dimensional data representation. MSSA is a\nstate-space reconstruction technique that utilizes time-delay embedding, and\nthe low-dimensional representation is achieved by projecting data onto the\nsingular value decomposition (SVD) basis. The noise-reduction performance of\nthe proposed method for unsteady PSP data, i.e., the projected MSSA, is\ncompared with that of the truncated SVD method, one of the most employed\nnoise-reduction methods. The result shows that the projected MSSA exhibits\nbetter performance in reducing random noise than the truncated SVD method.\nAdditionally, in contrast to that of the truncated SVD method, the performance\nof the projected MSSA is less sensitive to the truncation rank. Furthermore,\nthe projected MSSA achieves denoising effectively by extracting smooth\ntrajectories in a state space from noisy input data. Expectedly, the projected\nMSSA will be effective for reducing random noise in not only PSP measurement\ndata, but also various high-dimensional time-series data.\n","authors":["Yuya Ohmichi","Kohmi Takahashi","Kazuyuki Nakakita"],"pdf_url":"https://arxiv.org/pdf/2203.07574v3.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09768v1","updated":"2022-07-20T09:23:35Z","published":"2022-07-20T09:23:35Z","title":"Learning Counterfactually Invariant Predictors","summary":"  We propose a method to learn predictors that are invariant under\ncounterfactual changes of certain covariates. This method is useful when the\nprediction target is causally influenced by covariates that should not affect\nthe predictor output. For instance, an object recognition model may be\ninfluenced by position, orientation, or scale of the object itself. We address\nthe problem of training predictors that are explicitly counterfactually\ninvariant to changes of such covariates. We propose a model-agnostic\nregularization term based on conditional kernel mean embeddings, to enforce\ncounterfactual invariance during training. We prove the soundness of our\nmethod, which can handle mixed categorical and continuous multi-variate\nattributes. Empirical results on synthetic and real-world data demonstrate the\nefficacy of our method in a variety of settings.\n","authors":["Francesco Quinzan","Cecilia Casolo","Krikamol Muandet","Niki Kilbertus","Yucen Luo"],"pdf_url":"https://arxiv.org/pdf/2207.09768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09765v1","updated":"2022-07-20T09:16:12Z","published":"2022-07-20T09:16:12Z","title":"ApHMM: Accelerating Profile Hidden Markov Models for Fast and\n  Energy-Efficient Genome Analysis","summary":"  Profile hidden Markov models (pHMMs) are widely used in many bioinformatics\napplications to accurately identify similarities between biological sequences\n(e.g., DNA or protein sequences). PHMMs use a commonly-adopted and\nhighly-accurate method, called the Baum-Welch algorithm, to calculate these\nsimilarities. However, the Baum-Welch algorithm is computationally expensive,\nand existing works provide either software- or hardware-only solutions for a\nfixed pHMM design. When we analyze the state-of-the-art works, we find that\nthere is a pressing need for a flexible, high-performant, and energy-efficient\nhardware-software co-design to efficiently and effectively solve all the major\ninefficiencies in the Baum-Welch algorithm for pHMMs.\n  We propose ApHMM, the first flexible acceleration framework that can\nsignificantly reduce computational and energy overheads of the Baum-Welch\nalgorithm for pHMMs. ApHMM leverages hardware-software co-design to solve the\nmajor inefficiencies in the Baum-Welch algorithm by 1) designing a flexible\nhardware to support different pHMMs designs, 2) exploiting the predictable data\ndependency pattern in an on-chip memory with memoization techniques, 3) quickly\neliminating negligible computations with a hardware-based filter, and 4)\nminimizing the redundant computations. We implement our 1) hardware-software\noptimizations on a specialized hardware and 2) software optimizations for GPUs\nto provide the first flexible Baum-Welch accelerator for pHMMs. ApHMM provides\nsignificant speedups of 15.55x-260.03x, 1.83x-5.34x, and 27.97x compared to\nCPU, GPU, and FPGA implementations of the Baum-Welch algorithm, respectively.\nApHMM outperforms the state-of-the-art CPU implementations of three important\nbioinformatics applications, 1) error correction, 2) protein family search, and\n3) multiple sequence alignment, by 1.29x-59.94x, 1.03x-1.75x, and 1.03x-1.95x,\nrespectively.\n","authors":["Can Firtina","Kamlesh Pillai","Gurpreet S. Kalsi","Bharathwaj Suresh","Damla Senol Cali","Jeremie Kim","Taha Shahroodi","Meryem Banu Cavlak","Joel Lindegger","Mohammed Alser","Juan Gómez Luna","Sreenivas Subramoney","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2207.09765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09763v1","updated":"2022-07-20T09:06:07Z","published":"2022-07-20T09:06:07Z","title":"GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D\n  LiDAR Segmentation","summary":"  3D point cloud semantic segmentation is fundamental for autonomous driving.\nMost approaches in the literature neglect an important aspect, i.e., how to\ndeal with domain shift when handling dynamic scenes. This can significantly\nhinder the navigation capabilities of self-driving vehicles. This paper\nadvances the state of the art in this research field. Our first contribution\nconsists in analysing a new unexplored scenario in point cloud segmentation,\nnamely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We\nexperimentally show that state-of-the-art methods have a rather limited ability\nto adapt pre-trained deep network models to unseen domains in an online manner.\nOur second contribution is an approach that relies on adaptive self-training\nand geometric-feature propagation to adapt a pre-trained source model online\nwithout requiring either source data or target labels. Our third contribution\nis to study SF-OUDA in a challenging setup where source data is synthetic and\ntarget data is point clouds captured in the real world. We use the recent\nSynLiDAR dataset as a synthetic source and introduce two new synthetic (source)\ndatasets, which can stimulate future synthetic-to-real autonomous driving\nresearch. Our experiments show the effectiveness of our segmentation approach\non thousands of real-world point clouds. Code and synthetic datasets are\navailable at https://github.com/saltoricristiano/gipso-sfouda.\n","authors":["Cristiano Saltori","Evgeny Krivosheev","Stéphane Lathuilière","Nicu Sebe","Fabio Galasso","Giuseppe Fiameni","Elisa Ricci","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2207.09763v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2110.00201v3","updated":"2022-07-20T09:01:50Z","published":"2021-10-01T04:07:48Z","title":"Error-free approximation of explicit linear MPC through lattice\n  piecewise affine expression","summary":"  In this paper, the disjunctive and conjunctive lattice piecewise affine (PWA)\napproximations of explicit linear model predictive control (MPC) are proposed.\nThe training data are generated uniformly in the domain of interest, consisting\nof the state samples and corresponding affine control laws, based on which the\nlattice PWA approximations are constructed. Re-sampling of data is also\nproposed to guarantee that the lattice PWA approximations are identical to\nexplicit MPC control law in the unique order (UO) regions containing the sample\npoints as interior points. Additionally, under mild assumptions, the\nequivalence of the two lattice PWA approximations guarantees that the\napproximations are error-free in the domain of interest. The algorithms for\nderiving statistically error-free approximation to the explicit linear MPC are\nproposed and the complexity of the entire procedure is analyzed, which is\npolynomial with respect to the number of samples. The performance of the\nproposed approximation strategy is tested through two simulation examples, and\nthe result shows that with a moderate number of sample points, we can construct\nlattice PWA approximations that are equivalent to optimal control law of the\nexplicit linear MPC.\n","authors":["Jun Xu","Yunjiang Lou","Bart De Schutter","Zhenhua Xiong"],"pdf_url":"https://arxiv.org/pdf/2110.00201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09755v1","updated":"2022-07-20T08:57:53Z","published":"2022-07-20T08:57:53Z","title":"A Temporally and Spatially Local Spike-based Backpropagation Algorithm\n  to Enable Training in Hardware","summary":"  Spiking Neural Networks (SNNs) have emerged as a hardware efficient\narchitecture for classification tasks. The penalty of spikes-based encoding has\nbeen the lack of a universal training mechanism performed entirely using\nspikes. There have been several attempts to adopt the powerful backpropagation\n(BP) technique used in non-spiking artificial neural networks (ANN): (1) SNNs\ncan be trained by externally computed numerical gradients. (2) A major\nadvancement toward native spike-based learning has been the use of approximate\nBackpropagation using spike-time-dependent plasticity (STDP) with phased\nforward/backward passes. However, the transfer of information between such\nphases necessitates external memory and computational access. This is a\nchallenge for neuromorphic hardware implementations. In this paper, we propose\na stochastic SNN-based Back-Prop (SSNN-BP) algorithm that utilizes a composite\nneuron to simultaneously compute the forward pass activations and backward pass\ngradients explicitly with spikes. Although signed gradient values are a\nchallenge for spike-based representation, we tackle this by splitting the\ngradient signal into positive and negative streams. The composite neuron\nencodes information in the form of stochastic spike-trains and converts\nBackpropagation weight updates into temporally and spatially local discrete\nSTDP-like spike coincidence updates compatible with hardware-friendly Resistive\nProcessing Units (RPUs). Furthermore, our method approaches BP ANN baseline\nwith sufficiently long spike-trains. Finally, we show that softmax\ncross-entropy loss function can be implemented through inhibitory lateral\nconnections enforcing a Winner Take All (WTA) rule. Our SNN shows excellent\ngeneralization through comparable performance to ANNs on the MNIST,\nFashion-MNIST and Extended MNIST datasets. Thus, SSNN-BP enables BP compatible\nwith purely spike-based neuromorphic hardware.\n","authors":["Anmol Biswas","Vivek Saraswat","Udayan Ganguly"],"pdf_url":"https://arxiv.org/pdf/2207.09755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.05630v5","updated":"2022-07-20T08:57:44Z","published":"2020-06-10T03:11:40Z","title":"Distributionally Robust Batch Contextual Bandits","summary":"  Policy learning using historical observational data is an important problem\nthat has found widespread applications. Examples include selecting offers,\nprices, advertisements to send to customers, as well as selecting which\nmedication to prescribe to a patient. However, existing literature rests on the\ncrucial assumption that the future environment where the learned policy will be\ndeployed is the same as the past environment that has generated the data -- an\nassumption that is often false or too coarse an approximation. In this paper,\nwe lift this assumption and aim to learn a distributionally robust policy with\nincomplete observational data. We first present a policy evaluation procedure\nthat allows us to assess how well the policy does under the worst-case\nenvironment shift. We then establish a central limit theorem type guarantee for\nthis proposed policy evaluation scheme. Leveraging this evaluation scheme, we\nfurther propose a novel learning algorithm that is able to learn a policy that\nis robust to adversarial perturbations and unknown covariate shifts with a\nperformance guarantee based on the theory of uniform convergence. Finally, we\nempirically test the effectiveness of our proposed algorithm in synthetic\ndatasets and demonstrate that it provides the robustness that is missing using\nstandard policy learning algorithms. We conclude the paper by providing a\ncomprehensive application of our methods in the context of a real-world voting\ndataset.\n","authors":["Nian Si","Fan Zhang","Zhengyuan Zhou","Jose Blanchet"],"pdf_url":"https://arxiv.org/pdf/2006.05630v5.pdf","comment":"The short version has been accepted in ICML 2020"},{"id":"http://arxiv.org/abs/2207.09748v1","updated":"2022-07-20T08:46:18Z","published":"2022-07-20T08:46:18Z","title":"Facial Affect Analysis: Learning from Synthetic Data & Multi-Task\n  Learning Challenges","summary":"  Facial affect analysis remains a challenging task with its setting\ntransitioned from lab-controlled to in-the-wild situations. In this paper, we\npresent novel frameworks to handle the two challenges in the 4th Affective\nBehavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL)\nChallenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL\nchallenge, we adopt the SMM-EmotionNet with a better ensemble strategy of\nfeature vectors. For LSD challenge, we propose respective methods to combat the\nproblems of single labels, imbalanced distribution, fine-tuning limitations,\nand choice of model architectures. Experimental results on the official\nvalidation sets from the competition demonstrated that our proposed approaches\noutperformed baselines by a large margin. The code is available at\nhttps://github.com/sylyoung/ABAW4-HUST-ANT.\n","authors":["Siyang Li","Yifan Xu","Huanyu Wu","Dongrui Wu","Yingjie Yin","Jiajiong Cao","Jingting Ding"],"pdf_url":"https://arxiv.org/pdf/2207.09748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.09367v3","updated":"2022-07-20T08:45:05Z","published":"2021-09-20T08:37:03Z","title":"Network Clustering by Embedding of Attribute-augmented Graphs","summary":"  In this paper we propose a new approach to detect clusters in undirected\ngraphs with attributed vertices. The aim is to group vertices which are similar\nnot only in terms of structural connectivity but also in terms of attribute\nvalues. We incorporate structural and attribute similarities between the\nvertices in an augmented graph by creating additional vertices and edges as\nproposed in [6,38]. The augmented graph is then embedded in a Euclidean space\nassociated to its Laplacian where a modified K-means algorithm is applied to\nidentify clusters. The modified K-means relies on a vector distance measure\nwhere to each original vertex we assign a suitable vector-valued set of\ncoordinates depending on both structural connectivity and attribute\nsimilarities, so that each original graph vertex is thought as representative\nof $m+1$ vertices of the augmented graph, if $m$ is the number of vertex\nattributes. To define the coordinate vectors we employ our recently proposed\nalgorithm based on an adaptive AMG (Algebraic MultiGrid) method, which\nidentifies the coordinate directions in the embedding Euclidean space in terms\nof algebraically smooth vectors with respect to the augmented graph Laplacian,\nand thus extending our previous result for graphs without attributes.\n  We analyze the effectiveness of our proposed clustering method by comparison\nwith some well known methods, whose software implementation is freely\navailable, and also with results reported in the literature, on two different\ntypes of widely used synthetic graphs and on some real-world attributed graphs.\n","authors":["Pasqua D'Ambra","Panayot S. Vassilevski","Luisa Cutillo"],"pdf_url":"https://arxiv.org/pdf/2109.09367v3.pdf","comment":"31 pages, 12 figures, preprint"},{"id":"http://arxiv.org/abs/2207.09740v1","updated":"2022-07-20T08:34:50Z","published":"2022-07-20T08:34:50Z","title":"Interpreting Latent Spaces of Generative Models for Medical Images using\n  Unsupervised Methods","summary":"  Generative models such as Generative Adversarial Networks (GANs) and\nVariational Autoencoders (VAEs) play an increasingly important role in medical\nimage analysis. The latent spaces of these models often show semantically\nmeaningful directions corresponding to human-interpretable image\ntransformations. However, until now, their exploration for medical images has\nbeen limited due to the requirement of supervised data. Several methods for\nunsupervised discovery of interpretable directions in GAN latent spaces have\nshown interesting results on natural images. This work explores the potential\nof applying these techniques on medical images by training a GAN and a VAE on\nthoracic CT scans and using an unsupervised method to discover interpretable\ndirections in the resulting latent space. We find several directions\ncorresponding to non-trivial image transformations, such as rotation or breast\nsize. Furthermore, the directions show that the generative models capture 3D\nstructure despite being presented only with 2D data. The results show that\nunsupervised methods to discover interpretable directions in GANs generalize to\nVAEs and can be applied to medical images. This opens a wide array of future\nwork using these methods in medical image analysis.\n","authors":["Julian Schön","Raghavendra Selvan","Jens Petersen"],"pdf_url":"https://arxiv.org/pdf/2207.09740v1.pdf","comment":"Accepted for presentation at DGM4MICCAI 2022"},{"id":"http://arxiv.org/abs/2111.12990v2","updated":"2022-07-20T08:25:08Z","published":"2021-11-25T09:56:30Z","title":"Learning Algebraic Representation for Systematic Generalization in\n  Abstract Reasoning","summary":"  Is intelligence realized by connectionist or classicist? While connectionist\napproaches have achieved superhuman performance, there has been growing\nevidence that such task-specific superiority is particularly fragile in\nsystematic generalization. This observation lies in the central debate between\nconnectionist and classicist, wherein the latter continually advocates an\nalgebraic treatment in cognitive architectures. In this work, we follow the\nclassicist's call and propose a hybrid approach to improve systematic\ngeneralization in reasoning. Specifically, we showcase a prototype with\nalgebraic representation for the abstract spatial-temporal reasoning task of\nRaven's Progressive Matrices (RPM) and present the ALgebra-Aware\nNeuro-Semi-Symbolic (ALANS) learner. The ALANS learner is motivated by abstract\nalgebra and the representation theory. It consists of a neural visual\nperception frontend and an algebraic abstract reasoning backend: the frontend\nsummarizes the visual information from object-based representation, while the\nbackend transforms it into an algebraic structure and induces the hidden\noperator on the fly. The induced operator is later executed to predict the\nanswer's representation, and the choice most similar to the prediction is\nselected as the solution. Extensive experiments show that by incorporating an\nalgebraic treatment, the ALANS learner outperforms various pure connectionist\nmodels in domains requiring systematic generalization. We further show the\ngenerative nature of the learned algebraic representation; it can be decoded by\nisomorphism to generate an answer.\n","authors":["Chi Zhang","Sirui Xie","Baoxiong Jia","Ying Nian Wu","Song-Chun Zhu","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2111.12990v2.pdf","comment":"ECCV 2022 paper. Supplementary:\n  http://wellyzhang.github.io/attach/eccv22zhang_alans_supp.pdf Project:\n  http://wellyzhang.github.io/project/alans.html"},{"id":"http://arxiv.org/abs/2207.09732v1","updated":"2022-07-20T08:19:54Z","published":"2022-07-20T08:19:54Z","title":"Introducing Auxiliary Text Query-modifier to Content-based Audio\n  Retrieval","summary":"  The amount of audio data available on public websites is growing rapidly, and\nan efficient mechanism for accessing the desired data is necessary. We propose\na content-based audio retrieval method that can retrieve a target audio that is\nsimilar to but slightly different from the query audio by introducing auxiliary\ntextual information which describes the difference between the query and target\naudio. While the range of conventional content-based audio retrieval is limited\nto audio that is similar to the query audio, the proposed method can adjust the\nretrieval range by adding an embedding of the auxiliary text query-modifier to\nthe embedding of the query sample audio in a shared latent space. To evaluate\nour method, we built a dataset comprising two different audio clips and the\ntext that describes the difference. The experimental results show that the\nproposed method retrieves the paired audio more accurately than the baseline.\nWe also confirmed based on visualization that the proposed method obtains the\nshared latent space in which the audio difference and the corresponding text\nare represented as similar embedding vectors.\n","authors":["Daiki Takeuchi","Yasunori Ohishi","Daisuke Niizumi","Noboru Harada","Kunio Kashino"],"pdf_url":"https://arxiv.org/pdf/2207.09732v1.pdf","comment":"Accepted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2201.13019v2","updated":"2022-07-20T08:15:13Z","published":"2022-01-31T06:43:09Z","title":"On the Robustness of Quality Measures for GANs","summary":"  This work evaluates the robustness of quality measures of generative models\nsuch as Inception Score (IS) and Fr\\'echet Inception Distance (FID). Analogous\nto the vulnerability of deep models against a variety of adversarial attacks,\nwe show that such metrics can also be manipulated by additive pixel\nperturbations. Our experiments indicate that one can generate a distribution of\nimages with very high scores but low perceptual quality. Conversely, one can\noptimize for small imperceptible perturbations that, when added to real world\nimages, deteriorate their scores. We further extend our evaluation to\ngenerative models themselves, including the state of the art network\nStyleGANv2. We show the vulnerability of both the generative model and the FID\nagainst additive perturbations in the latent space. Finally, we show that the\nFID can be robustified by simply replacing the standard Inception with a robust\nInception. We validate the effectiveness of the robustified metric through\nextensive experiments, showing it is more robust against manipulation.\n","authors":["Motasem Alfarra","Juan C. Pérez","Anna Frühstück","Philip H. S. Torr","Peter Wonka","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2201.13019v2.pdf","comment":"Accepted at the European Conference in Computer Vision (ECCV 2022)"},{"id":"http://arxiv.org/abs/2207.09728v1","updated":"2022-07-20T08:13:08Z","published":"2022-07-20T08:13:08Z","title":"Revisiting data augmentation for subspace clustering","summary":"  Subspace clustering is the classical problem of clustering a collection of\ndata samples that approximately lie around several low-dimensional subspaces.\nThe current state-of-the-art approaches for this problem are based on the\nself-expressive model which represents the samples as linear combination of\nother samples. However, these approaches require sufficiently well-spread\nsamples for accurate representation which might not be necessarily accessible\nin many applications. In this paper, we shed light on this commonly neglected\nissue and argue that data distribution within each subspace plays a critical\nrole in the success of self-expressive models. Our proposed solution to tackle\nthis issue is motivated by the central role of data augmentation in the\ngeneralization power of deep neural networks. We propose two subspace\nclustering frameworks for both unsupervised and semi-supervised settings that\nuse augmented samples as an enlarged dictionary to improve the quality of the\nself-expressive representation. We present an automatic augmentation strategy\nusing a few labeled samples for the semi-supervised problem relying on the fact\nthat the data samples lie in the union of multiple linear subspaces.\nExperimental results confirm the effectiveness of data augmentation, as it\nsignificantly improves the performance of general self-expressive models.\n","authors":["Maryam Abdolali","Nicolas Gillis"],"pdf_url":"https://arxiv.org/pdf/2207.09728v1.pdf","comment":"38 pages (including 10 of supplementary material)"},{"id":"http://arxiv.org/abs/2207.09714v1","updated":"2022-07-20T07:32:02Z","published":"2022-07-20T07:32:02Z","title":"Differentiable Agent-based Epidemiology","summary":"  Mechanistic simulators are an indispensable tool for epidemiology to explore\nthe behavior of complex, dynamic infections under varying conditions and\nnavigate uncertain environments. ODE-based models are the dominant paradigm\nthat enable fast simulations and are tractable to gradient-based optimization,\nbut make simplifying assumptions about population homogeneity. Agent-based\nmodels (ABMs) are an increasingly popular alternative paradigm that can\nrepresent the heterogeneity of contact interactions with granular detail and\nagency of individual behavior. However, conventional ABM frameworks are not\ndifferentiable and present challenges in scalability; due to which it is\nnon-trivial to connect them to auxiliary data sources easily. In this paper we\nintroduce GradABM which is a new scalable, fast and differentiable design for\nABMs. GradABM runs simulations in few seconds on commodity hardware and enables\nfast forward and differentiable inverse simulations. This makes it amenable to\nbe merged with deep neural networks and seamlessly integrate heterogeneous data\nsources to help with calibration, forecasting and policy evaluation. We\ndemonstrate the efficacy of GradABM via extensive experiments with real\nCOVID-19 and influenza datasets. We are optimistic this work will bring ABM and\nAI communities closer together.\n","authors":["Ayush Chopra","Alexander Rodríguez","Jayakumar Subramanian","Balaji Krishnamurthy","B. Aditya Prakash","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2207.09714v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.09710v1","updated":"2022-07-20T07:26:15Z","published":"2022-07-20T07:26:15Z","title":"Learning Sequence Representations by Non-local Recurrent Neural Memory","summary":"  The key challenge of sequence representation learning is to capture the\nlong-range temporal dependencies. Typical methods for supervised sequence\nrepresentation learning are built upon recurrent neural networks to capture\ntemporal dependencies. One potential limitation of these methods is that they\nonly model one-order information interactions explicitly between adjacent time\nsteps in a sequence, hence the high-order interactions between nonadjacent time\nsteps are not fully exploited. It greatly limits the capability of modeling the\nlong-range temporal dependencies since the temporal features learned by\none-order interactions cannot be maintained for a long term due to temporal\ninformation dilution and gradient vanishing. To tackle this limitation, we\npropose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence\nrepresentation learning, which performs non-local operations \\MR{by means of\nself-attention mechanism} to learn full-order interactions within a sliding\ntemporal memory block and models global interactions between memory blocks in a\ngated recurrent manner. Consequently, our model is able to capture long-range\ndependencies. Besides, the latent high-level features contained in high-order\ninteractions can be distilled by our model. We validate the effectiveness and\ngeneralization of our NRNM on three types of sequence applications across\ndifferent modalities, including sequence classification, step-wise sequential\nprediction and sequence similarity learning. Our model compares favorably\nagainst other state-of-the-art methods specifically designed for each of these\nsequence applications.\n","authors":["Wenjie Pei","Xin Feng","Canmiao Fu","Qiong Cao","Guangming Lu","Yu-Wing Tai"],"pdf_url":"https://arxiv.org/pdf/2207.09710v1.pdf","comment":"To be appeared in International Journal of Computer Vision (IJCV).\n  arXiv admin note: substantial text overlap with arXiv:1908.09535"},{"id":"http://arxiv.org/abs/2207.06819v2","updated":"2022-07-20T07:08:57Z","published":"2022-07-14T10:59:39Z","title":"Anomal-E: A Self-Supervised Network Intrusion Detection System based on\n  Graph Neural Networks","summary":"  This paper investigates Graph Neural Networks (GNNs) application for\nself-supervised network intrusion and anomaly detection. GNNs are a deep\nlearning approach for graph-based data that incorporate graph structures into\nlearning to generalise graph representations and output embeddings. As network\nflows are naturally graph-based, GNNs are a suitable fit for analysing and\nlearning network behaviour. The majority of current implementations of\nGNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled\nnetwork traffic which can not only restrict the amount and structure of input\ntraffic, but also the NIDSs potential to adapt to unseen attacks. To overcome\nthese restrictions, we present Anomal-E, a GNN approach to intrusion and\nanomaly detection that leverages edge features and graph topological structure\nin a self-supervised process. This approach is, to the best our knowledge, the\nfirst successful and practical approach to network intrusion detection that\nutilises network flows in a self-supervised, edge leveraging GNN. Experimental\nresults on two modern benchmark NIDS datasets not only clearly display the\nimprovement of using Anomal-E embeddings rather than raw features, but also the\npotential Anomal-E has for detection on wild network traffic.\n","authors":["Evan Caville","Wai Weng Lo","Siamak Layeghy","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2207.06819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09693v1","updated":"2022-07-20T06:49:23Z","published":"2022-07-20T06:49:23Z","title":"Correntropy-Based Logistic Regression with Automatic Relevance\n  Determination for Robust Sparse Brain Activity Decoding","summary":"  Recent studies have utilized sparse classifications to predict categorical\nvariables from high-dimensional brain activity signals to expose human's\nintentions and mental states, selecting the relevant features automatically in\nthe model training process. However, existing sparse classification models will\nlikely be prone to the performance degradation which is caused by noise\ninherent in the brain recordings. To address this issue, we aim to propose a\nnew robust and sparse classification algorithm in this study. To this end, we\nintroduce the correntropy learning framework into the automatic relevance\ndetermination based sparse classification model, proposing a new\ncorrentropy-based robust sparse logistic regression algorithm. To demonstrate\nthe superior brain activity decoding performance of the proposed algorithm, we\nevaluate it on a synthetic dataset, an electroencephalogram (EEG) dataset, and\na functional magnetic resonance imaging (fMRI) dataset. The extensive\nexperimental results confirm that not only the proposed method can achieve\nhigher classification accuracy in a noisy and high-dimensional classification\ntask, but also it would select those more informative features for the decoding\nscenarios. Integrating the correntropy learning approach with the automatic\nrelevance determination technique will significantly improve the robustness\nwith respect to the noise, leading to more adequate robust sparse brain\ndecoding algorithm. It provides a more powerful approach in the real-world\nbrain activity decoding and the brain-computer interfaces.\n","authors":["Yuanhao Li","Badong Chen","Yuxi Shi","Natsue Yoshimura","Yasuharu Koike"],"pdf_url":"https://arxiv.org/pdf/2207.09693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03108v2","updated":"2022-07-20T06:48:04Z","published":"2021-12-06T15:21:52Z","title":"Flood Inflow Forecast Using L2-norm Ensemble Weighting Sea Surface\n  Feature","summary":"  It is important to forecast dam inflow for flood damage mitigation. The\nhydrograph provides critical information such as the start time, peak level,\nand volume. Particularly, dam management requires a 6-h lead time of the dam\ninflow forecast based on a future hydrograph. The authors propose novel target\ninflow weights to create an ocean feature vector extracted from the analyzed\nimages of the sea surface. We extracted 4,096 elements of the dimension vector\nin the fc6 layer of the pre-trained VGG16 network. Subsequently, we reduced it\nto three dimensions of t-SNE. Furthermore, we created the principal component\nof the sea temperature weights using PCA. We found that these weights\ncontribute to the stability of predictor importance by numerical experiments.\nAs base regression models, we calibrate the least squares with kernel\nexpansion, the quantile random forest minimized out-of bag error, and the\nsupport vector regression with a polynomial kernel. When we compute the\npredictor importance, we visualize the stability of each variable importance\nintroduced by our proposed weights, compared with other results without\nweights. We apply our method to a dam at Kanto region in Japan and focus on the\ntrained term from 2007 to 2018, with a limited flood term from June to October.\nWe test the accuracy over the 2019 flood term. Finally, we present the applied\nresults and further statistical learning for unknown flood forecast.\n","authors":["Takato Yasuno","Masazumi Amakata","Junichiro Fujii","Masahiro Okano","Riku Ogata"],"pdf_url":"https://arxiv.org/pdf/2112.03108v2.pdf","comment":"23 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2207.09688v1","updated":"2022-07-20T06:38:36Z","published":"2022-07-20T06:38:36Z","title":"Intrinsic dimension estimation for discrete metrics","summary":"  Real world-datasets characterized by discrete features are ubiquitous: from\ncategorical surveys to clinical questionnaires, from unweighted networks to DNA\nsequences. Nevertheless, the most common unsupervised dimensional reduction\nmethods are designed for continuous spaces, and their use for discrete spaces\ncan lead to errors and biases. In this letter we introduce an algorithm to\ninfer the intrinsic dimension (ID) of datasets embedded in discrete spaces. We\ndemonstrate its accuracy on benchmark datasets, and we apply it to analyze a\nmetagenomic dataset for species fingerprinting, finding a surprisingly small\nID, of order 2. This suggests that evolutive pressure acts on a low-dimensional\nmanifold despite the high-dimensionality of sequences' space.\n","authors":["Iuri Macocco","Aldo Glielmo","Jacopo Grilli","Alessandro Laio"],"pdf_url":"https://arxiv.org/pdf/2207.09688v1.pdf","comment":"RevTeX4.2, 12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2207.09682v1","updated":"2022-07-20T06:27:06Z","published":"2022-07-20T06:27:06Z","title":"Quantized Training of Gradient Boosting Decision Trees","summary":"  Recent years have witnessed significant success in Gradient Boosting Decision\nTrees (GBDT) for a wide range of machine learning applications. Generally, a\nconsensus about GBDT's training algorithms is gradients and statistics are\ncomputed based on high-precision floating points. In this paper, we investigate\nan essentially important question which has been largely ignored by the\nprevious literature: how many bits are needed for representing gradients in\ntraining GBDT? To solve this mystery, we propose to quantize all the\nhigh-precision gradients in a very simple yet effective way in the GBDT's\ntraining algorithm. Surprisingly, both our theoretical analysis and empirical\nstudies show that the necessary precisions of gradients without hurting any\nperformance can be quite low, e.g., 2 or 3 bits. With low-precision gradients,\nmost arithmetic operations in GBDT training can be replaced by integer\noperations of 8, 16, or 32 bits. Promisingly, these findings may pave the way\nfor much more efficient training of GBDT from several aspects: (1) speeding up\nthe computation of gradient statistics in histograms; (2) compressing the\ncommunication cost of high-precision statistical information during distributed\ntraining; (3) the inspiration of utilization and development of hardware\narchitectures which well support low-precision computation for GBDT training.\nBenchmarked on CPU, GPU, and distributed clusters, we observe up to 2$\\times$\nspeedup of our simple quantization strategy compared with SOTA GBDT systems on\nextensive datasets, demonstrating the effectiveness and potential of the\nlow-precision training of GBDT. The code will be released to the official\nrepository of LightGBM.\n","authors":["Yu Shi","Guolin Ke","Zhuoming Chen","Shuxin Zheng","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2207.09682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09674v1","updated":"2022-07-20T06:07:26Z","published":"2022-07-20T06:07:26Z","title":"Improving Data Driven Inverse Text Normalization using Data Augmentation","summary":"  Inverse text normalization (ITN) is used to convert the spoken form output of\nan automatic speech recognition (ASR) system to a written form. Traditional\nhandcrafted ITN rules can be complex to transcribe and maintain. Meanwhile\nneural modeling approaches require quality large-scale spoken-written pair\nexamples in the same or similar domain as the ASR system (in-domain data), to\ntrain. Both these approaches require costly and complex annotations. In this\npaper, we present a data augmentation technique that effectively generates rich\nspoken-written numeric pairs from out-of-domain textual data with minimal human\nannotation. We empirically demonstrate that ITN model trained using our data\naugmentation technique consistently outperform ITN model trained using only\nin-domain data across all numeric surfaces like cardinal, currency, and\nfraction, by an overall accuracy of 14.44%.\n","authors":["Laxmi Pandey","Debjyoti Paul","Pooja Chitkara","Yutong Pang","Xuedong Zhang","Kjell Schubert","Mark Chou","Shu Liu","Yatharth Saraf"],"pdf_url":"https://arxiv.org/pdf/2207.09674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.08685v3","updated":"2022-07-20T05:51:00Z","published":"2021-09-17T17:01:42Z","title":"Self-supervised learning methods and applications in medical imaging\n  analysis: A survey","summary":"  The scarcity of high-quality annotated medical imaging datasets is a major\nproblem that collides with machine learning applications in the field of\nmedical imaging analysis and impedes its advancement. Self-supervised learning\nis a recent training paradigm that enables learning robust representations\nwithout the need for human annotation which can be considered an effective\nsolution for the scarcity of annotated medical data. This article reviews the\nstate-of-the-art research directions in self-supervised learning approaches for\nimage data with a concentration on their applications in the field of medical\nimaging analysis. The article covers a set of the most recent self-supervised\nlearning methods from the computer vision field as they are applicable to the\nmedical imaging analysis and categorize them as predictive, generative, and\ncontrastive approaches. Moreover, the article covers 40 of the most recent\nresearch papers in the field of self-supervised learning in medical imaging\nanalysis aiming at shedding the light on the recent innovation in the field.\nFinally, the article concludes with possible future research directions in the\nfield.\n","authors":["Saeed Shurrab","Rehab Duwairi"],"pdf_url":"https://arxiv.org/pdf/2109.08685v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09667v1","updated":"2022-07-20T05:49:16Z","published":"2022-07-20T05:49:16Z","title":"Generalizable and Robust Deep Learning Algorithm for Atrial Fibrillation\n  Diagnosis Across Ethnicities, Ages and Sexes","summary":"  To drive health innovation that meets the needs of all and democratize\nhealthcare, there is a need to assess the generalization performance of deep\nlearning (DL) algorithms across various distribution shifts to ensure that\nthese algorithms are robust. This retrospective study is, to the best of our\nknowledge, the first to develop and assess the generalization performance of a\ndeep learning (DL) model for AF events detection from long term beat-to-beat\nintervals across ethnicities, ages and sexes. The new recurrent DL model,\ndenoted ArNet2, was developed on a large retrospective dataset of 2,147\npatients totaling 51,386 hours of continuous electrocardiogram (ECG). The\nmodels generalization was evaluated on manually annotated test sets from four\ncenters (USA, Israel, Japan and China) totaling 402 patients. The model was\nfurther validated on a retrospective dataset of 1,730 consecutives Holter\nrecordings from the Rambam Hospital Holter clinic, Haifa, Israel. The model\noutperformed benchmark state-of-the-art models and generalized well across\nethnicities, ages and sexes. Performance was higher for female than male and\nyoung adults (less than 60 years old) and showed some differences across\nethnicities. The main finding explaining these variations was an impairment in\nperformance in groups with a higher prevalence of atrial flutter (AFL). Our\nfindings on the relative performance of ArNet2 across groups may have clinical\nimplications on the choice of the preferred AF examination method to use\nrelative to the group of interest.\n","authors":["Shany Biton","Mohsin Aldhafeeri","Erez Marcusohn","Kenta Tsutsui","Tom Szwagier","Adi Elias","Julien Oster","Jean Marc Sellal","Mahmoud Suleiman","Joachim A. Behar"],"pdf_url":"https://arxiv.org/pdf/2207.09667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09665v1","updated":"2022-07-20T05:45:36Z","published":"2022-07-20T05:45:36Z","title":"ExoSGAN and ExoACGAN: Exoplanet Detection using Adversarial Training\n  Algorithms","summary":"  Exoplanet detection opens the door to the discovery of new habitable worlds\nand helps us understand how planets were formed. With the objective of finding\nearth-like habitable planets, NASA launched Kepler space telescope and its\nfollow up mission K2. The advancement of observation capabilities has increased\nthe range of fresh data available for research, and manually handling them is\nboth time-consuming and difficult. Machine learning and deep learning\ntechniques can greatly assist in lowering human efforts to process the vast\narray of data produced by the modern instruments of these exoplanet programs in\nan economical and unbiased manner. However, care should be taken to detect all\nthe exoplanets precisely while simultaneously minimizing the misclassification\nof non-exoplanet stars. In this paper, we utilize two variations of generative\nadversarial networks, namely semi-supervised generative adversarial networks\nand auxiliary classifier generative adversarial networks, to detect transiting\nexoplanets in K2 data. We find that the usage of these models can be helpful\nfor the classification of stars with exoplanets. Both of our techniques are\nable to categorize the light curves with a recall and precision of 1.00 on the\ntest data. Our semi-supervised technique is beneficial to solve the cumbersome\ntask of creating a labeled dataset.\n","authors":["Cicy K Agnes","Akthar Naveed V","Anitha Mary M O Chacko"],"pdf_url":"https://arxiv.org/pdf/2207.09665v1.pdf","comment":"26 pages total"},{"id":"http://arxiv.org/abs/2207.09657v1","updated":"2022-07-20T05:22:26Z","published":"2022-07-20T05:22:26Z","title":"Multigraph Topology Design for Cross-Silo Federated Learning","summary":"  Cross-silo federated learning utilizes a few hundred reliable data silos with\nhigh-speed access links to jointly train a model. While this approach becomes a\npopular setting in federated learning, designing a robust topology to reduce\nthe training time is still an open problem. In this paper, we present a new\nmultigraph topology for cross-silo federated learning. We first construct the\nmultigraph using the overlay graph. We then parse this multigraph into\ndifferent simple graphs with isolated nodes. The existence of isolated nodes\nallows us to perform model aggregation without waiting for other nodes, hence\nreducing the training time. We further propose a new distributed learning\nalgorithm to use with our multigraph topology. The intensive experiments on\npublic datasets show that our proposed method significantly reduces the\ntraining time compared with recent state-of-the-art topologies while ensuring\nconvergence and maintaining the model's accuracy.\n","authors":["Binh X. Nguyen","Tuong Do","Hien Nguyen","Vuong Pham","Toan Tran","Erman Tjiputra","Quang Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2207.09657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09370v2","updated":"2022-07-20T05:13:18Z","published":"2022-07-19T16:15:11Z","title":"Data-Centric Epidemic Forecasting: A Survey","summary":"  The COVID-19 pandemic has brought forth the importance of epidemic\nforecasting for decision makers in multiple domains, ranging from public health\nto the economy as a whole. While forecasting epidemic progression is frequently\nconceptualized as being analogous to weather forecasting, however it has some\nkey differences and remains a non-trivial task. The spread of diseases is\nsubject to multiple confounding factors spanning human behavior, pathogen\ndynamics, weather and environmental conditions. Research interest has been\nfueled by the increased availability of rich data sources capturing previously\nunobservable facets and also due to initiatives from government public health\nand funding agencies. This has resulted, in particular, in a spate of work on\n'data-centered' solutions which have shown potential in enhancing our\nforecasting capabilities by leveraging non-traditional data sources as well as\nrecent innovations in AI and machine learning. This survey delves into various\ndata-driven methodological and practical advancements and introduces a\nconceptual framework to navigate through them. First, we enumerate the large\nnumber of epidemiological datasets and novel data streams that are relevant to\nepidemic forecasting, capturing various factors like symptomatic online\nsurveys, retail and commerce, mobility, genomics data and more. Next, we\ndiscuss methods and modeling paradigms focusing on the recent data-driven\nstatistical and deep-learning based methods as well as on the novel class of\nhybrid models that combine domain knowledge of mechanistic models with the\neffectiveness and flexibility of statistical approaches. We also discuss\nexperiences and challenges that arise in real-world deployment of these\nforecasting systems including decision-making informed by forecasts. Finally,\nwe highlight some challenges and open problems found across the forecasting\npipeline.\n","authors":["Alexander Rodríguez","Harshavardhan Kamarthi","Pulak Agarwal","Javen Ho","Mira Patel","Suchet Sapre","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2207.09370v2.pdf","comment":"67 pages, 12 figures"},{"id":"http://arxiv.org/abs/2207.09653v1","updated":"2022-07-20T04:55:18Z","published":"2022-07-20T04:55:18Z","title":"FedDM: Iterative Distribution Matching for Communication-Efficient\n  Federated Learning","summary":"  Federated learning~(FL) has recently attracted increasing attention from\nacademia and industry, with the ultimate goal of achieving collaborative\ntraining under privacy and communication constraints. Existing iterative model\naveraging based FL algorithms require a large number of communication rounds to\nobtain a well-performed model due to extremely unbalanced and non-i.i.d data\npartitioning among different clients. Thus, we propose FedDM to build the\nglobal training objective from multiple local surrogate functions, which\nenables the server to gain a more global view of the loss landscape. In detail,\nwe construct synthetic sets of data on each client to locally match the loss\nlandscape from original data through distribution matching. FedDM reduces\ncommunication rounds and improves model quality by transmitting more\ninformative and smaller synthesized data compared with unwieldy model weights.\nWe conduct extensive experiments on three image classification datasets, and\nresults show that our method can outperform other FL counterparts in terms of\nefficiency and model performance. Moreover, we demonstrate that FedDM can be\nadapted to preserve differential privacy with Gaussian mechanism and train a\nbetter model under the same privacy budget.\n","authors":["Yuanhao Xiong","Ruochen Wang","Minhao Cheng","Felix Yu","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2207.09653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12387v2","updated":"2022-07-20T04:50:47Z","published":"2022-02-24T22:16:53Z","title":"Provable Stochastic Optimization for Global Contrastive Learning: Small\n  Batch Does Not Harm Performance","summary":"  In this paper, we study contrastive learning from an optimization\nperspective, aiming to analyze and address a fundamental issue of existing\ncontrastive learning methods that either rely on a large batch size or a large\ndictionary of feature vectors. We consider a global objective for contrastive\nlearning, which contrasts each positive pair with all negative pairs for an\nanchor point. From the optimization perspective, we explain why existing\nmethods such as SimCLR require a large batch size in order to achieve a\nsatisfactory result. In order to remove such requirement, we propose a\nmemory-efficient Stochastic Optimization algorithm for solving the Global\nobjective of Contrastive Learning of Representations, named SogCLR. We show\nthat its optimization error is negligible under a reasonable condition after a\nsufficient number of iterations or is diminishing for a slightly different\nglobal contrastive objective. Empirically, we demonstrate that SogCLR with\nsmall batch size (e.g., 256) can achieve similar performance as SimCLR with\nlarge batch size (e.g., 8192) on self-supervised learning task on ImageNet-1K.\nWe also attempt to show that the proposed optimization technique is generic and\ncan be applied to solving other contrastive losses, e.g., two-way contrastive\nlosses for bimodal contrastive learning. The proposed method is implemented in\nour open-sourced library LibAUC (www.libauc.org).\n","authors":["Zhuoning Yuan","Yuexin Wu","Zi-Hao Qiu","Xianzhi Du","Lijun Zhang","Denny Zhou","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2202.12387v2.pdf","comment":"Accepted by ICML2022"},{"id":"http://arxiv.org/abs/2104.14767v2","updated":"2022-07-20T04:35:14Z","published":"2021-04-30T05:51:07Z","title":"TREND: Truncated Generalized Normal Density Estimation of Inception\n  Embeddings for GAN Evaluation","summary":"  Evaluating image generation models such as generative adversarial networks\n(GANs) is a challenging problem. A common approach is to compare the\ndistributions of the set of ground truth images and the set of generated test\nimages. The Frech\\'et Inception distance is one of the most widely used metrics\nfor evaluation of GANs, which assumes that the features from a trained\nInception model for a set of images follow a normal distribution. In this\npaper, we argue that this is an over-simplified assumption, which may lead to\nunreliable evaluation results, and more accurate density estimation can be\nachieved using a truncated generalized normal distribution. Based on this, we\npropose a novel metric for accurate evaluation of GANs, named TREND (TRuncated\ngEneralized Normal Density estimation of inception embeddings). We demonstrate\nthat our approach significantly reduces errors of density estimation, which\nconsequently eliminates the risk of faulty evaluation results. Furthermore, we\nshow that the proposed metric significantly improves robustness of evaluation\nresults against variation of the number of image samples.\n","authors":["Junghyuk Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2104.14767v2.pdf","comment":"Accepted in ECCV 2022"},{"id":"http://arxiv.org/abs/2207.09640v1","updated":"2022-07-20T04:02:19Z","published":"2022-07-20T04:02:19Z","title":"Test-Time Adaptation via Conjugate Pseudo-labels","summary":"  Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.\n","authors":["Sachin Goyal","Mingjie Sun","Aditi Raghunathan","Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2207.09640v1.pdf","comment":"19 Pages, Under Review"},{"id":"http://arxiv.org/abs/2207.09639v1","updated":"2022-07-20T03:54:05Z","published":"2022-07-20T03:54:05Z","title":"DC-BENCH: Dataset Condensation Benchmark","summary":"  Dataset Condensation is a newly emerging technique aiming at learning a tiny\ndataset that captures the rich information encoded in the original dataset. As\nthe size of datasets contemporary machine learning models rely on becomes\nincreasingly large, condensation methods become a prominent direction for\naccelerating network training and reducing data storage. Despite numerous\nmethods have been proposed in this rapidly growing field, evaluating and\ncomparing different condensation methods is non-trivial and still remains an\nopen issue. The quality of condensed dataset are often shadowed by many\ncritical contributing factors to the end performance, such as data augmentation\nand model architectures. The lack of a systematic way to evaluate and compare\ncondensation methods not only hinders our understanding of existing techniques,\nbut also discourages practical usage of the synthesized datasets. This work\nprovides the first large-scale standardized benchmark on Dataset Condensation.\nIt consists of a suite of evaluations to comprehensively reflect the\ngenerability and effectiveness of condensation methods through the lens of\ntheir generated dataset. Leveraging this benchmark, we conduct a large-scale\nstudy of current condensation methods, and report many insightful findings that\nopen up new possibilities for future development. The benchmark library,\nincluding evaluators, baseline methods, and generated datasets, is open-sourced\nto facilitate future research and application.\n","authors":["Justin Cui","Ruochen Wang","Si Si","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2207.09639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.00207v2","updated":"2022-07-20T03:40:02Z","published":"2022-01-01T15:17:07Z","title":"AutoDES: AutoML Pipeline Generation of Classification with Dynamic\n  Ensemble Strategy Selection","summary":"  Automating machine learning has achieved remarkable technological\ndevelopments in recent years, and building an automated machine learning\npipeline is now an essential task. The model ensemble is the technique of\ncombining multiple models to get a better and more robust model. However,\nexisting automated machine learning tends to be simplistic in handling the\nmodel ensemble, where the ensemble strategy is fixed, such as stacked\ngeneralization. There have been many techniques on different ensemble methods,\nespecially ensemble selection, and the fixed ensemble strategy limits the upper\nlimit of the model's performance. In this article, we present a novel framework\nfor automated machine learning. Our framework incorporates advances in dynamic\nensemble selection, and to our best knowledge, our approach is the first in the\nfield of AutoML to search and optimize ensemble strategies. In the comparison\nexperiments, our method outperforms the state-of-the-art automated machine\nlearning frameworks with the same CPU time in 42 classification datasets from\nthe OpenML platform. Ablation experiments on our framework validate the\neffectiveness of our proposed method.\n","authors":["Yunpu Zhao","Rui Zhang","Xiaqing Li"],"pdf_url":"https://arxiv.org/pdf/2201.00207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09638v1","updated":"2022-07-20T03:37:37Z","published":"2022-07-20T03:37:37Z","title":"Doge Tickets: Uncovering Domain-general Language Models by Playing\n  Lottery Tickets","summary":"  Over-parameterized models, typically pre-trained language models (LMs), have\nshown an appealing expressive power due to their small learning bias. However,\nthe huge learning capacity of LMs can also lead to large learning variance. In\na pilot study, we find that, when faced with multiple domains, a critical\nportion of parameters behave unexpectedly in a domain-specific manner while\nothers behave in a domain-general one. Motivated by this phenomenon, we for the\nfirst time posit that domain-general parameters can underpin a domain-general\nLM that can be derived from the original LM. To uncover the domain-general LM,\nwe propose to identify domain-general parameters by playing lottery tickets\n(dubbed doge tickets). In order to intervene the lottery, we propose a\ndomain-general score, which depicts how domain-invariant a parameter is by\nassociating it with the variance. Comprehensive experiments are conducted on\nthe Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets\nobtains an improved out-of-domain generalization in comparison with a range of\ncompetitive baselines. Analysis results further hint the existence of\ndomain-general parameters and the performance consistency of doge tickets.\n","authors":["Yi Yang","Chen Zhang","Benyou Wang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2207.09638v1.pdf","comment":"Accepted to NLPCC 2022. Code is available at\n  https://github.com/Ylily1015/DogeTickets"},{"id":"http://arxiv.org/abs/2109.02353v2","updated":"2022-07-20T03:33:40Z","published":"2021-09-06T10:44:54Z","title":"Reconfigurable Intelligent Surface Empowered Over-the-Air Federated Edge\n  Learning","summary":"  Federated edge learning (FEEL) has emerged as a revolutionary paradigm to\ndevelop AI services at the edge of 6G wireless networks as it supports\ncollaborative model training at a massive number of mobile devices. However,\nmodel communication over wireless channels, especially in uplink model\nuploading of FEEL, has been widely recognized as a bottleneck that critically\nlimits the efficiency of FEEL. Although over-the-air computation can alleviate\nthe excessive cost of radio resources in FEEL model uploading, practical\nimplementations of over-the-air FEEL still suffer from several challenges,\nincluding strong straggler issues, large communication overheads, and potential\nprivacy leakage. In this article, we study these challenges in over-the-air\nFEEL and leverage reconfigurable intelligent surface (RIS), a key enabler of\nfuture wireless systems, to address these challenges. We study the\nstate-of-the-art solutions on RIS-empowered FEEL and explore the promising\nresearch opportunities for adopting RIS to enhance FEEL performance.\n","authors":["Hang Liu","Zehong Lin","Xiaojun Yuan","Ying-Jun Angela Zhang"],"pdf_url":"https://arxiv.org/pdf/2109.02353v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2109.05698v2","updated":"2022-07-20T03:33:00Z","published":"2021-09-13T04:17:58Z","title":"Detecting Textual Adversarial Examples through Randomized Substitution\n  and Vote","summary":"  A line of work has shown that natural text processing models are vulnerable\nto adversarial examples. Correspondingly, various defense methods are proposed\nto mitigate the threat of textual adversarial examples, eg, adversarial\ntraining, input transformations, detection, etc. In this work, we treat the\noptimization process for synonym substitution based textual adversarial attacks\nas a specific sequence of word replacement, in which each word mutually\ninfluences other words. We identify that we could destroy such mutual\ninteraction and eliminate the adversarial perturbation by randomly substituting\na word with its synonyms. Based on this observation, we propose a novel textual\nadversarial example detection method, termed Randomized Substitution and Vote\n(RS&V), which votes the prediction label by accumulating the logits of k\nsamples generated by randomly substituting the words in the input text with\nsynonyms. The proposed RS&V is generally applicable to any existing neural\nnetworks without modification on the architecture or extra training, and it is\northogonal to prior work on making the classification network itself more\nrobust. Empirical evaluations on three benchmark datasets demonstrate that our\nRS&V could detect the textual adversarial examples more successfully than the\nexisting detection methods while maintaining the high classification accuracy\non benign samples.\n","authors":["Xiaosen Wang","Yifeng Xiong","Kun He"],"pdf_url":"https://arxiv.org/pdf/2109.05698v2.pdf","comment":"Accepted by UAI 2022, code is avaliable at\n  https://github.com/JHL-HUST/RSV"},{"id":"http://arxiv.org/abs/2202.01719v2","updated":"2022-07-20T03:27:47Z","published":"2022-02-03T17:36:07Z","title":"FORML: Learning to Reweight Data for Fairness","summary":"  Machine learning models are trained to minimize the mean loss for a single\nmetric, and thus typically do not consider fairness and robustness. Neglecting\nsuch metrics in training can make these models prone to fairness violations\nwhen training data are imbalanced or test distributions differ. This work\nintroduces Fairness Optimized Reweighting via Meta-Learning (FORML), a training\nalgorithm that balances fairness and robustness with accuracy by jointly\nlearning training sample weights and neural network parameters. The approach\nincreases model fairness by learning to balance the contributions from both\nover- and under-represented sub-groups through dynamic reweighting of the data\nlearned from a user-specified held-out set representative of the distribution\nunder which fairness is desired. FORML improves equality of opportunity\nfairness criteria on image classification tasks, reduces bias of corrupted\nlabels, and facilitates building more fair datasets via data condensation.\nThese improvements are achieved without pre-processing data or post-processing\nmodel outputs, without learning an additional weighting function, without\nchanging model architecture, and while maintaining accuracy on the original\npredictive metric.\n","authors":["Bobby Yan","Skyler Seto","Nicholas Apostoloff"],"pdf_url":"https://arxiv.org/pdf/2202.01719v2.pdf","comment":"9 pages, 2 figures, Presented at ICML 2022 DataPerf Workshop"},{"id":"http://arxiv.org/abs/2207.09627v1","updated":"2022-07-20T02:58:46Z","published":"2022-07-20T02:58:46Z","title":"EVHA: Explainable Vision System for Hardware Testing and Assurance -- An\n  Overview","summary":"  Due to the ever-growing demands for electronic chips in different sectors the\nsemiconductor companies have been mandated to offshore their manufacturing\nprocesses. This unwanted matter has made security and trustworthiness of their\nfabricated chips concerning and caused creation of hardware attacks. In this\ncondition, different entities in the semiconductor supply chain can act\nmaliciously and execute an attack on the design computing layers, from devices\nto systems. Our attack is a hardware Trojan that is inserted during mask\ngeneration/fabrication in an untrusted foundry. The Trojan leaves a footprint\nin the fabricated through addition, deletion, or change of design cells. In\norder to tackle this problem, we propose Explainable Vision System for Hardware\nTesting and Assurance (EVHA) in this work that can detect the smallest possible\nchange to a design in a low-cost, accurate, and fast manner. The inputs to this\nsystem are Scanning Electron Microscopy (SEM) images acquired from the\nIntegrated Circuits (ICs) under examination. The system output is determination\nof IC status in terms of having any defect and/or hardware Trojan through\naddition, deletion, or change in the design cells at the cell-level. This\narticle provides an overview on the design, development, implementation, and\nanalysis of our defense system.\n","authors":["Md Mahfuz Al Hasan","Mohammad Tahsin Mostafiz","Thomas An Le","Jake Julia","Nidish Vashistha","Shayan Taheri","Navid Asadizanjani"],"pdf_url":"https://arxiv.org/pdf/2207.09627v1.pdf","comment":"Please contact Dr. Shayan Taheri for any questions and/or comments\n  regarding the paper arXiv submission at: \"www.shayan-taheri.com\". The Paper\n  Initial Submission: The ACM Journal on Emerging Technologies in Computing\n  Systems (JETC)"},{"id":"http://arxiv.org/abs/2204.04906v2","updated":"2022-07-20T02:47:35Z","published":"2022-04-11T07:11:45Z","title":"Application of QUBO solver using black-box optimization to structural\n  design for resonance avoidance","summary":"  Quadratic unconstrained binary optimization (QUBO) solvers can be applied to\ndesign an optimal structure to avoid resonance. QUBO algorithms that work on a\nclassical or quantum device have succeeded in some industrial applications.\nHowever, their applications are still limited due to the difficulty of\ntransforming from the original optimization problem to QUBO. Recently,\nblack-box optimization (BBO) methods have been proposed to tackle this issue\nusing a machine learning technique and a Bayesian treatment for combinatorial\noptimization. We employed the BBO methods to design a printed circuit board for\nresonance avoidance. This design problem is formulated to maximize natural\nfrequency and simultaneously minimize the number of mounting points. The\nnatural frequency, which is the bottleneck for the QUBO formulation, is\napproximated to a quadratic model in the BBO method. We demonstrated that BBO\nusing a factorization machine shows good performance in both the calculation\ntime and the success probability of finding the optimal solution. Our results\ncan open up QUBO solvers' potential for other applications in structural\ndesigns.\n","authors":["Tadayoshi Matsumori","Masato Taki","Tadashi Kadowaki"],"pdf_url":"https://arxiv.org/pdf/2204.04906v2.pdf","comment":"This is a preprint of an article published in Scientific Reports. The\n  final authenticated version is available online at:\n  10.1038/s41598-022-16149-8"},{"id":"http://arxiv.org/abs/2207.09624v1","updated":"2022-07-20T02:47:29Z","published":"2022-07-20T02:47:29Z","title":"Learning from few examples: Classifying sex from retinal images via deep\n  learning","summary":"  Deep learning has seen tremendous interest in medical imaging, particularly\nin the use of convolutional neural networks (CNNs) for developing automated\ndiagnostic tools. The facility of its non-invasive acquisition makes retinal\nfundus imaging amenable to such automated approaches. Recent work in analyzing\nfundus images using CNNs relies on access to massive data for training and\nvalidation - hundreds of thousands of images. However, data residency and data\nprivacy restrictions stymie the applicability of this approach in medical\nsettings where patient confidentiality is a mandate. Here, we showcase results\nfor the performance of DL on small datasets to classify patient sex from fundus\nimages - a trait thought not to be present or quantifiable in fundus images\nuntil recently. We fine-tune a Resnet-152 model whose last layer has been\nmodified for binary classification. In several experiments, we assess\nperformance in the small dataset context using one private (DOVS) and one\npublic (ODIR) data source. Our models, developed using approximately 2500\nfundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]).\nThis corresponds to a mere 25% decrease in performance despite a nearly\n1000-fold decrease in the dataset size compared to prior work in the\nliterature. Even with a hard task like sex categorization from retinal images,\nwe find that classification is possible with very small datasets. Additionally,\nwe perform domain adaptation experiments between DOVS and ODIR; explore the\neffect of data curation on training and generalizability; and investigate model\nensembling to maximize CNN classifier performance in the context of small\ndevelopment datasets.\n","authors":["Aaron Berk","Gulcenur Ozturan","Parsa Delavari","David Maberley","Özgür Yılmaz","Ipek Oruc"],"pdf_url":"https://arxiv.org/pdf/2207.09624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02192v3","updated":"2022-07-20T01:54:33Z","published":"2022-07-05T17:48:54Z","title":"CEN : Cooperatively Evolving Networks","summary":"  GANs have two competing modules: the generator module is trained to generate\nnew examples, and the discriminator module is trained to discriminate real\nexamples from generated examples. The training procedure of GAN is modeled as a\nfinitely repeated simultaneous game. Each module tries to increase its\nperformance at every repetition of the base game (at every batch of training\ndata) in a non-cooperative manner. We observed that each module can perform\nbetter if training is modeled as an infinitely repeated simultaneous game. At\nevery repetition of the base game (at every batch of training data) the\nstronger module (whose performance is increased or remains the same compared to\nthe previous batch of training data) cooperates with the weaker module (whose\nperformance is decreased compared to the previous batch of training data) and\nonly the weaker module is allowed to increase its performance.\n","authors":["Ch. Sobhan Babu","Ravindra Guravannavar"],"pdf_url":"https://arxiv.org/pdf/2207.02192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13749v2","updated":"2022-07-20T23:44:01Z","published":"2022-04-28T19:41:08Z","title":"Learning to Split for Automatic Bias Detection","summary":"  Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split cannot generalize to the testing\nsplit. This performance gap suggests that the testing split is\nunder-represented in the dataset, which is a signal of potential bias.\nIdentifying non-generalizable splits is challenging since we have no\nannotations about the bias. In this work, we show that the prediction\ncorrectness of each example in the testing split can be used as a source of\nweak supervision: generalization performance will drop if we move examples that\nare predicted correctly away from the testing split, leaving only those that\nare mis-predicted. ls is task-agnostic and can be applied to any supervised\nlearning problem, ranging from natural language understanding and image\nclassification to molecular property prediction. Empirical results show that ls\nis able to generate astonishingly challenging splits that correlate with\nhuman-identified biases. Moreover, we demonstrate that combining robust\nlearning algorithms (such as group DRO) with splits identified by ls enables\nautomatic de-biasing. Compared to previous state-of-the-art, we substantially\nimprove the worst-group performance (23.4% on average) when the source of\nbiases is unknown during training and validation.\n","authors":["Yujia Bao","Regina Barzilay"],"pdf_url":"https://arxiv.org/pdf/2204.13749v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2111.04476v4","updated":"2022-07-20T16:52:35Z","published":"2021-11-04T19:36:01Z","title":"Twitter Big Data as a Resource for Exoskeleton Research: A Large-Scale\n  Dataset of about 140,000 Tweets and 100 Research Questions","summary":"  The exoskeleton technology has been rapidly advancing in the recent past due\nto its multitude of applications and diverse use-cases in assisted living,\nmilitary, healthcare, firefighting, and industry 4.0. The exoskeleton market is\nprojected to increase by multiple times of its current value within the next\ntwo years. Therefore, it is crucial to study the degree and trends of user\ninterest, views, opinions, perspectives, attitudes, acceptance, feedback,\nengagement, buying behavior, and satisfaction, towards exoskeletons, for which\nthe availability of Big Data of conversations about exoskeletons is necessary.\nThe Internet of Everything style of today's living, characterized by people\nspending more time on the internet than ever before, with a specific focus on\nsocial media platforms, holds the potential for the development of such a\ndataset by the mining of relevant social media conversations. Twitter, one such\nsocial media platform, is highly popular amongst all age groups, where the\ntopics found in the conversation paradigms include emerging technologies such\nas exoskeletons. To address this research challenge, this work makes two\nscientific contributions to this field. First, it presents an open-access\ndataset of about 140,000 tweets about exoskeletons that were posted in a 5-year\nperiod from May 21, 2017, to May 21, 2022. Second, based on a comprehensive\nreview of the recent works in the fields of Big Data, Natural Language\nProcessing, Information Retrieval, Data Mining, Pattern Recognition, and\nArtificial Intelligence that may be applied to relevant Twitter data for\nadvancing research, innovation, and discovery in the field of exoskeleton\nresearch, a total of 100 Research Questions are presented for researchers to\nstudy, analyze, evaluate, ideate, and investigate based on this dataset.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2111.04476v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.12369v3","updated":"2022-07-20T11:00:45Z","published":"2021-02-24T15:52:24Z","title":"Neural content-aware collaborative filtering for cold-start music\n  recommendation","summary":"  State-of-the-art music recommender systems are based on collaborative\nfiltering, which builds upon learning similarities between users and songs from\nthe available listening data. These approaches inherently face the cold-start\nproblem, as they cannot recommend novel songs with no listening history.\nContent-aware recommendation addresses this issue by incorporating content\ninformation about the songs on top of collaborative filtering. However, methods\nfalling in this category rely on a shallow user/item interaction that\noriginates from a matrix factorization framework. In this work, we introduce\nneural content-aware collaborative filtering, a unified framework which\nalleviates these limits, and extends the recently introduced neural\ncollaborative filtering to its content-aware counterpart. We propose a\ngenerative model which leverages deep learning for both extracting content\ninformation from low-level acoustic features and for modeling the interaction\nbetween users and songs embeddings. The deep content feature extractor can\neither directly predict the item embedding, or serve as a regularization prior,\nyielding two variants (strict and relaxed) of our model. Experimental results\nshow that the proposed method reaches state-of-the-art results for a cold-start\nmusic recommendation task. We notably observe that exploiting deep neural\nnetworks for learning refined user/item interactions outperforms approaches\nusing a more simple interaction model in a content-aware framework.\n","authors":["Paul Magron","Cédric Févotte"],"pdf_url":"https://arxiv.org/pdf/2102.12369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09732v1","updated":"2022-07-20T08:19:54Z","published":"2022-07-20T08:19:54Z","title":"Introducing Auxiliary Text Query-modifier to Content-based Audio\n  Retrieval","summary":"  The amount of audio data available on public websites is growing rapidly, and\nan efficient mechanism for accessing the desired data is necessary. We propose\na content-based audio retrieval method that can retrieve a target audio that is\nsimilar to but slightly different from the query audio by introducing auxiliary\ntextual information which describes the difference between the query and target\naudio. While the range of conventional content-based audio retrieval is limited\nto audio that is similar to the query audio, the proposed method can adjust the\nretrieval range by adding an embedding of the auxiliary text query-modifier to\nthe embedding of the query sample audio in a shared latent space. To evaluate\nour method, we built a dataset comprising two different audio clips and the\ntext that describes the difference. The experimental results show that the\nproposed method retrieves the paired audio more accurately than the baseline.\nWe also confirmed based on visualization that the proposed method obtains the\nshared latent space in which the audio difference and the corresponding text\nare represented as similar embedding vectors.\n","authors":["Daiki Takeuchi","Yasunori Ohishi","Daisuke Niizumi","Noboru Harada","Kunio Kashino"],"pdf_url":"https://arxiv.org/pdf/2207.09732v1.pdf","comment":"Accepted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2207.09672v1","updated":"2022-07-20T06:02:11Z","published":"2022-07-20T06:02:11Z","title":"Duplicate Detection as a Service","summary":"  Completeness of a knowledge graph is an important quality dimension and\nfactor on how well an application that makes use of it performs. Completeness\ncan be improved by performing knowledge enrichment. Duplicate detection aims to\nfind identity links between the instances of knowledge graphs and is a\nfundamental subtask of knowledge enrichment. Current solutions to the problem\nrequire expert knowledge of the tool and the knowledge graph they are applied\nto. Users might not have this expert knowledge. We present our service-based\napproach to the duplicate detection task that provides an easy-to-use no-code\nsolution that is still competitive with the state-of-the-art and has recently\nbeen adopted in an industrial context. The evaluation will be based on several\nfrequently used test scenarios.\n","authors":["Juliette Opdenplatz","Umutcan Şimşek","Dieter Fensel"],"pdf_url":"https://arxiv.org/pdf/2207.09672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10192v1","updated":"2022-07-20T20:59:06Z","published":"2022-07-20T20:59:06Z","title":"Building Human Values into Recommender Systems: An Interdisciplinary\n  Synthesis","summary":"  Recommender systems are the algorithms which select, filter, and personalize\ncontent across many of the worlds largest platforms and apps. As such, their\npositive and negative effects on individuals and on societies have been\nextensively theorized and studied. Our overarching question is how to ensure\nthat recommender systems enact the values of the individuals and societies that\nthey serve. Addressing this question in a principled fashion requires technical\nknowledge of recommender design and operation, and also critically depends on\ninsights from diverse fields including social science, ethics, economics,\npsychology, policy and law. This paper is a multidisciplinary effort to\nsynthesize theory and practice from different perspectives, with the goal of\nproviding a shared language, articulating current design approaches, and\nidentifying open problems. It is not a comprehensive survey of this large\nspace, but a set of highlights identified by our diverse author cohort. We\ncollect a set of values that seem most relevant to recommender systems\noperating across different domains, then examine them from the perspectives of\ncurrent industry practice, measurement, product design, and policy approaches.\nImportant open problems include multi-stakeholder processes for defining values\nand resolving trade-offs, better values-driven measurements, recommender\ncontrols that people use, non-behavioral algorithmic feedback, optimization for\nlong-term outcomes, causal inference of recommender effects, academic-industry\nresearch collaborations, and interdisciplinary policy-making.\n","authors":["Jonathan Stray","Alon Halevy","Parisa Assar","Dylan Hadfield-Menell","Craig Boutilier","Amar Ashar","Lex Beattie","Michael Ekstrand","Claire Leibowicz","Connie Moon Sehat","Sara Johansen","Lianne Kerlin","David Vickrey","Spandana Singh","Sanne Vrijenhoek","Amy Zhang","McKane Andrus","Natali Helberger","Polina Proutskova","Tanushree Mitra","Nina Vasan"],"pdf_url":"https://arxiv.org/pdf/2207.10192v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.09927v1","updated":"2022-07-20T14:12:05Z","published":"2022-07-20T14:12:05Z","title":"ViGAT: Bottom-up event recognition and explanation in video using\n  factorized graph attention network","summary":"  In this paper a pure-attention bottom-up approach, called ViGAT, that\nutilizes an object detector together with a Vision Transformer (ViT) backbone\nnetwork to derive object and frame features, and a head network to process\nthese features for the task of event recognition and explanation in video, is\nproposed. The ViGAT head consists of graph attention network (GAT) blocks\nfactorized along the spatial and temporal dimensions in order to capture\neffectively both local and long-term dependencies between objects or frames.\nMoreover, using the weighted in-degrees (WiDs) derived from the adjacency\nmatrices at the various GAT blocks, we show that the proposed architecture can\nidentify the most salient objects and frames that explain the decision of the\nnetwork. A comprehensive evaluation study is performed, demonstrating that the\nproposed approach provides state-of-the-art results on three large, publicly\navailable video datasets (FCVID, Mini-Kinetics, ActivityNet).\n","authors":["Nikolaos Gkalelis","Dimitrios Daskalakis","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2207.09927v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2104.02618v2","updated":"2022-07-20T14:09:01Z","published":"2021-04-06T15:55:13Z","title":"Subjective Assessment Experiments That Recruit Few Observers With\n  Repetitions (FOWR)","summary":"  Recent studies have shown that it is possible to characterize subject bias\nand variance in subjective assessment tests. Apparent differences among\nsubjects can, for the most part, be explained by random factors. Building on\nthat theory, we propose a subjective test design where three to four team\nmembers each rate the stimuli multiple times. The results are comparable to a\nhigh performing objective metric. This provides a quick and simple way to\nanalyze new technologies and perform pre-tests for subjective assessment.\n","authors":["Pablo Perez","Lucjan Janowski","Narciso Garcia","Margaret Pinson"],"pdf_url":"https://arxiv.org/pdf/2104.02618v2.pdf","comment":"IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2207.10574v1","updated":"2022-07-20T13:37:57Z","published":"2022-07-20T13:37:57Z","title":"Face-to-Face Co-Located Human-Human Social Interaction Analysis using\n  Nonverbal Cues: A Survey","summary":"  This work presents a systematic review of recent efforts (since 2010) aimed\nat automatic analysis of nonverbal cues displayed in face-to-face co-located\nhuman-human social interactions. The main reason for focusing on nonverbal cues\nis that these are the physical, machine detectable traces of social and\npsychological phenomena. Therefore, detecting and understanding nonverbal cues\nmeans, at least to a certain extent, to detect and understand social and\npsychological phenomena. The covered topics are categorized into three as: a)\nmodeling social traits, such as leadership, dominance, personality traits, b)\nsocial role recognition and social relations detection and c) interaction\ndynamics analysis in terms of group cohesion, empathy, rapport and so forth. We\ntarget the co-located interactions, in which the interactants are always\nhumans. The survey covers a wide spectrum of settings and scenarios, including\nfree-standing interactions, meetings, indoor and outdoor social exchanges,\ndyadic conversations, and crowd dynamics. For each of them, the survey\nconsiders the three main elements of nonverbal cues analysis, namely data,\nsensing approaches and computational methodologies. The goal is to highlight\nthe main advances of the last decade, to point out existing limitations, and to\noutline future directions.\n","authors":["Cigdem Beyan","Alessandro Vinciarelli","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2207.10574v1.pdf","comment":"Submitted to ACM Computing and Surveys"}]},"2022-07-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2207.10617v1","updated":"2022-07-21T17:26:03Z","published":"2022-07-21T17:26:03Z","title":"Leveraging Natural Supervision for Language Representation Learning and\n  Generation","summary":"  Recent breakthroughs in Natural Language Processing (NLP) have been driven by\nlanguage models trained on a massive amount of plain text. While powerful,\nderiving supervision from textual resources is still an open question. For\nexample, language model pretraining often neglects the rich, freely-available\nstructures in textual data. In this thesis, we describe three lines of work\nthat seek to improve the training and evaluation of neural models using\nnaturally-occurring supervision.\n  We first investigate self-supervised training losses to help enhance the\nperformance of pretrained language models for various NLP tasks. Specifically,\nwe alter the sentence prediction loss to make it better suited to other\npretraining losses and more challenging to solve. We design an intermediate\nfinetuning step that uses self-supervised training to promote models' ability\nin cross-task generalization.\n  Then we describe methods to leverage the structures in Wikipedia and\nparaphrases. In particular, we propose training losses to exploit hyperlinks,\narticle structures, and article category graphs for entity-, discourse-,\nentailment-related knowledge. We propose a framework that uses paraphrase pairs\nto disentangle semantics and syntax in sentence representations. We extend the\nframework for a novel generation task that controls the syntax of output text\nwith a sentential exemplar.\n  Lastly, we discuss our work on tailoring textual resources for establishing\nchallenging evaluation tasks. We introduce three datasets by defining novel\ntasks using various fan-contributed websites, including a long-form\ndata-to-text generation dataset, a screenplay summarization dataset, and a\nlong-form story generation dataset. These datasets have unique characteristics\noffering challenges to future work in their respective task settings.\n","authors":["Mingda Chen"],"pdf_url":"https://arxiv.org/pdf/2207.10617v1.pdf","comment":"PhD Thesis"},{"id":"http://arxiv.org/abs/2111.05610v2","updated":"2022-07-21T17:19:19Z","published":"2021-11-10T10:05:11Z","title":"CLIP2TV: Align, Match and Distill for Video-Text Retrieval","summary":"  Modern video-text retrieval frameworks basically consist of three parts:\nvideo encoder, text encoder and the similarity head. With the success on both\nvisual and textual representation learning, transformer based encoders and\nfusion methods have also been adopted in the field of video-text retrieval. In\nthis report, we present CLIP2TV, aiming at exploring where the critical\nelements lie in transformer based methods. To achieve this, We first revisit\nsome recent works on multi-modal learning, then introduce some techniques into\nvideo-text retrieval, finally evaluate them through extensive experiments in\ndifferent configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,\noutperforming the previous SOTA result by 4.1%.\n","authors":["Zijian Gao","Jingyu Liu","Weiqi Sun","Sheng Chen","Dedan Chang","Lili Zhao"],"pdf_url":"https://arxiv.org/pdf/2111.05610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08323v2","updated":"2022-07-21T16:07:06Z","published":"2021-10-15T19:20:25Z","title":"On Learning the Transformer Kernel","summary":"  In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data\ndriven framework for learning the kernel function in Transformers. Our\nframework approximates the Transformer kernel as a dot product between spectral\nfeature maps and learns the kernel by learning the spectral distribution. This\nnot only helps in learning a generic kernel end-to-end, but also reduces the\ntime and space complexity of Transformers from quadratic to linear. We show\nthat KERNELIZED TRANSFORMERS achieve performance comparable to existing\nefficient Transformer architectures, both in terms of accuracy as well as\ncomputational efficiency. Our study also demonstrates that the choice of the\nkernel has a substantial impact on performance, and kernel learning variants\nare competitive alternatives to fixed kernel Transformers, both in long as well\nas short sequence tasks.\n","authors":["Sankalan Pal Chowdhury","Adamos Solomou","Avinava Dubey","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2110.08323v2.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2207.10551v1","updated":"2022-07-21T15:50:22Z","published":"2022-07-21T15:50:22Z","title":"Scaling Laws vs Model Architectures: How does Inductive Bias Influence\n  Scaling?","summary":"  There have been a lot of interest in the scaling properties of Transformer\nmodels. However, not much has been done on the front of investigating the\neffect of scaling properties of different inductive biases and model\narchitectures. Do model architectures scale differently? If so, how does\ninductive bias affect scaling behaviour? How does this influence upstream\n(pretraining) and downstream (transfer)? This paper conducts a systematic study\nof scaling behaviour of ten diverse model architectures such as Transformers,\nSwitch Transformers, Universal Transformers, Dynamic convolutions, Performers,\nand recently proposed MLP-Mixers. Via extensive experiments, we show that (1)\narchitecture is an indeed an important consideration when performing scaling\nand (2) the best performing model can fluctuate at different scales. We believe\nthat the findings outlined in this work has significant implications to how\nmodel architectures are currently evaluated in the community.\n","authors":["Yi Tay","Mostafa Dehghani","Samira Abnar","Hyung Won Chung","William Fedus","Jinfeng Rao","Sharan Narang","Vinh Q. Tran","Dani Yogatama","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2207.10551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10524v1","updated":"2022-07-21T15:05:42Z","published":"2022-07-21T15:05:42Z","title":"NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian\n  Languages","summary":"  At the center of the underlying issues that halt Indonesian natural language\nprocessing (NLP) research advancement, we find data scarcity. Resources in\nIndonesian languages, especially the local ones, are extremely scarce and\nunderrepresented. Many Indonesian researchers do not publish their dataset.\nFurthermore, the few public datasets that we have are scattered across\ndifferent platforms, thus makes performing reproducible and data-centric\nresearch in Indonesian NLP even more arduous. Rising to this challenge, we\ninitiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd\nstrives to provide the largest datasheets aggregation with standardized data\nloading for NLP tasks in all Indonesian languages. By enabling open and\ncentralized access to Indonesian NLP resources, we hope NusaCrowd can tackle\nthe data scarcity problem hindering NLP progress in Indonesia and bring NLP\npractitioners to move towards collaboration.\n","authors":["Samuel Cahyawijaya","Alham Fikri Aji","Holy Lovenia","Genta Indra Winata","Bryan Wilie","Rahmad Mahendra","Fajri Koto","David Moeljadi","Karissa Vincentio","Ade Romadhony","Ayu Purwarianti"],"pdf_url":"https://arxiv.org/pdf/2207.10524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.09817v4","updated":"2022-07-21T14:46:17Z","published":"2022-04-21T00:04:35Z","title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language\n  Processing","summary":"  Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n","authors":["Benedikt Boecking","Naoto Usuyama","Shruthi Bannur","Daniel C. Castro","Anton Schwaighofer","Stephanie Hyland","Maria Wetscherek","Tristan Naumann","Aditya Nori","Javier Alvarez-Valle","Hoifung Poon","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2204.09817v4.pdf","comment":"To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:\n  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook"},{"id":"http://arxiv.org/abs/2207.09980v2","updated":"2022-07-21T13:33:26Z","published":"2022-07-20T15:39:30Z","title":"ReFactorGNNs: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective","summary":"  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactorGNNs. Across a\nmultitude of well-established KGC benchmarks, our ReFactorGNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n","authors":["Yihong Chen","Pushkar Mishra","Luca Franceschi","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2207.09980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10397v1","updated":"2022-07-21T10:18:37Z","published":"2022-07-21T10:18:37Z","title":"CodeT: Code Generation with Generated Tests","summary":"  Given a programming problem, pre-trained language models such as Codex have\ndemonstrated the ability to generate multiple different code solutions via\nsampling. However, selecting a correct or best solution from those samples\nstill remains a challenge. While an easy way to verify the correctness of a\ncode solution is through executing test cases, producing high-quality test\ncases is prohibitively expensive. In this paper, we explore the use of\npre-trained language models to automatically generate test cases, calling our\nmethod CodeT: Code generation with generated Tests. CodeT executes the code\nsolutions using the generated test cases, and then chooses the best solution\nbased on a dual execution agreement with both the generated test cases and\nother generated solutions. We evaluate CodeT on five different pre-trained\nmodels with both HumanEval and MBPP benchmarks. Extensive experimental results\ndemonstrate CodeT can achieve significant, consistent, and surprising\nimprovements over previous methods. For example, CodeT improves the pass@1 on\nHumanEval to 65.8%, an increase of absolute 18.8% on the code-davinci-002\nmodel, and an absolute 20+% improvement over previous state-of-the-art results.\n","authors":["Bei Chen","Fengji Zhang","Anh Nguyen","Daoguang Zan","Zeqi Lin","Jian-Guang Lou","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2207.10397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.02874v2","updated":"2022-07-21T07:52:00Z","published":"2022-04-06T14:43:42Z","title":"ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound","summary":"  We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.\n","authors":["Yan-Bo Lin","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2204.02874v2.pdf","comment":"ECCV 2022 Oral project page: https://yanbo.ml/project_page/eclipse/"},{"id":"http://arxiv.org/abs/2111.02080v6","updated":"2022-07-21T07:44:13Z","published":"2021-11-03T09:12:33Z","title":"An Explanation of In-context Learning as Implicit Bayesian Inference","summary":"  Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.\n","authors":["Sang Michael Xie","Aditi Raghunathan","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2111.02080v6.pdf","comment":"ICLR 2022"},{"id":"http://arxiv.org/abs/2207.10342v1","updated":"2022-07-21T07:35:18Z","published":"2022-07-21T07:35:18Z","title":"Language Model Cascades","summary":"  Prompted models have demonstrated impressive few-shot learning abilities.\nRepeated interactions at test-time with a single model, or the composition of\nmultiple models together, further expands capabilities. These compositions are\nprobabilistic models, and may be expressed in the language of graphical models\nwith random variables whose values are complex data types such as strings.\nCases with control flow and dynamic structure require techniques from\nprobabilistic programming, which allow implementing disparate model structures\nand inference strategies in a unified language. We formalize several existing\ntechniques from this perspective, including scratchpads / chain of thought,\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\nprograms as language model cascades.\n","authors":["David Dohan","Winnie Xu","Aitor Lewkowycz","Jacob Austin","David Bieber","Raphael Gontijo Lopes","Yuhuai Wu","Henryk Michalewski","Rif A. Saurous","Jascha Sohl-dickstein","Kevin Murphy","Charles Sutton"],"pdf_url":"https://arxiv.org/pdf/2207.10342v1.pdf","comment":"Presented as spotlight at the Beyond Bases workshop at ICML 2022\n  (https://beyond-bayes.github.io)"},{"id":"http://arxiv.org/abs/2112.08879v5","updated":"2022-07-21T05:25:39Z","published":"2021-12-16T13:50:23Z","title":"Bottom Up Top Down Detection Transformers for Language Grounding in\n  Images and Point Clouds","summary":"  Most models tasked to ground referential utterances in 2D and 3D scenes learn\nto select the referred object from a pool of object proposals provided by a\npre-trained detector. This is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of the\nchair, or the tip of the front leg of the chair, which may be missed by the\ndetector. We propose a language grounding model that attends on the referential\nutterance and on the object proposal pool computed from a pre-trained detector\nto decode referenced objects with a detection head, without selecting them from\nthe pool. In this way, it is helped by powerful pre-trained object detectors\nwithout being restricted by their misses. We call our model Bottom Up Top Down\nDEtection TRansformers (BUTD-DETR) because it uses both language guidance (top\ndown) and objectness guidance (bottom-up) to ground referential utterances in\nimages and point clouds. Moreover, BUTD-DETR casts object detection as\nreferential grounding and uses object labels as language prompts to be grounded\nin the visual scene, augmenting supervision for the referential grounding task\nin this way. The proposed model sets a new state-of-the-art across popular 3D\nlanguage grounding benchmarks with significant performance gains over previous\n3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When\napplied in 2D images, it performs on par with the previous state of the art. We\nablate the design choices of our model and quantify their contribution to\nperformance. Our code and checkpoints can be found at the project website\nhttps://butd-detr.github.io.\n","authors":["Ayush Jain","Nikolaos Gkanatsios","Ishita Mediratta","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2112.08879v5.pdf","comment":"First two authors contributed equally | ECCV 2022 Camera Ready"},{"id":"http://arxiv.org/abs/2207.05223v2","updated":"2022-07-21T04:57:18Z","published":"2022-07-11T23:32:54Z","title":"Bootstrapping a User-Centered Task-Oriented Dialogue System","summary":"  We present TacoBot, a task-oriented dialogue system built for the inaugural\nAlexa Prize TaskBot Challenge, which assists users in completing multi-step\ncooking and home improvement tasks. TacoBot is designed with a user-centered\nprinciple and aspires to deliver a collaborative and accessible dialogue\nexperience. Towards that end, it is equipped with accurate language\nunderstanding, flexible dialogue management, and engaging response generation.\nFurthermore, TacoBot is backed by a strong search engine and an automated\nend-to-end test suite. In bootstrapping the development of TacoBot, we explore\na series of data augmentation strategies to train advanced neural language\nprocessing models and continuously improve the dialogue experience with\ncollected real conversations. At the end of the semifinals, TacoBot achieved an\naverage rating of 3.55/5.0.\n","authors":["Shijie Chen","Ziru Chen","Xiang Deng","Ashley Lewis","Lingbo Mo","Samuel Stevens","Zhen Wang","Xiang Yue","Tianshu Zhang","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2207.05223v2.pdf","comment":"Published in 1st Proceedings of Alexa Prize TaskBot (Alexa Prize\n  2021). TacoBot won 3rd place in the challenge. See project website\n  https://sunlab-osu.github.io/tacobot/ for details"},{"id":"http://arxiv.org/abs/2207.10285v1","updated":"2022-07-21T03:43:38Z","published":"2022-07-21T03:43:38Z","title":"Grounding Visual Representations with Texts for Domain Generalization","summary":"  Reducing the representational discrepancy between source and target domains\nis a key component to maximize the model generalization. In this work, we\nadvocate for leveraging natural language supervision for the domain\ngeneralization task. We introduce two modules to ground visual representations\nwith texts containing typical reasoning of humans: (1) Visual and Textual Joint\nEmbedder and (2) Textual Explanation Generator. The former learns the\nimage-text joint embedding space where we can ground high-level\nclass-discriminative information into the model. The latter leverages an\nexplainable model and generates explanations justifying the rationale behind\nits decision. To the best of our knowledge, this is the first work to leverage\nthe vision-and-language cross-modality approach for the domain generalization\ntask. Our experiments with a newly created CUB-DG benchmark dataset demonstrate\nthat cross-modality supervision can be successfully used to ground\ndomain-invariant visual representations and improve the model generalization.\nFurthermore, in the large-scale DomainBed benchmark, our proposed method\nachieves state-of-the-art results and ranks 1st in average performance for five\nmulti-domain datasets. The dataset and codes are available at\nhttps://github.com/mswzeus/GVRT.\n","authors":["Seonwoo Min","Nokyung Park","Siwon Kim","Seunghyun Park","Jinkyu Kim"],"pdf_url":"https://arxiv.org/pdf/2207.10285v1.pdf","comment":"25 pages (including Supplementary Materials), ECCV 2022 camera ready\n  version"},{"id":"http://arxiv.org/abs/2207.10284v1","updated":"2022-07-21T03:36:30Z","published":"2022-07-21T03:36:30Z","title":"Multi Resolution Analysis (MRA) for Approximate Self-Attention","summary":"  Transformers have emerged as a preferred model for many tasks in natural\nlangugage processing and vision. Recent efforts on training and deploying\nTransformers more efficiently have identified many strategies to approximate\nthe self-attention matrix, a key module in a Transformer architecture.\nEffective ideas include various prespecified sparsity patterns, low-rank basis\nexpansions and combinations thereof. In this paper, we revisit classical\nMultiresolution Analysis (MRA) concepts such as Wavelets, whose potential value\nin this setting remains underexplored thus far. We show that simple\napproximations based on empirical feedback and design choices informed by\nmodern hardware and implementation challenges, eventually yield a MRA-based\napproach for self-attention with an excellent performance profile across most\ncriteria of interest. We undertake an extensive set of experiments and\ndemonstrate that this multi-resolution scheme outperforms most efficient\nself-attention proposals and is favorable for both short and long sequences.\nCode is available at \\url{https://github.com/mlpen/mra-attention}.\n","authors":["Zhanpeng Zeng","Sourav Pal","Jeffery Kline","Glenn M Fung","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2207.10284v1.pdf","comment":"ICML2022"},{"id":"http://arxiv.org/abs/2009.06207v8","updated":"2022-07-21T02:36:18Z","published":"2020-09-14T05:29:24Z","title":"Contrastive Triple Extraction with Generative Transformer","summary":"  Triple extraction is an essential task in information extraction for natural\nlanguage processing and knowledge graph construction. In this paper, we revisit\nthe end-to-end triple extraction task for sequence generation. Since generative\ntriple extraction may struggle to capture long-term dependencies and generate\nunfaithful triples, we introduce a novel model, contrastive triple extraction\nwith a generative transformer. Specifically, we introduce a single shared\ntransformer module for encoder-decoder-based generation. To generate faithful\nresults, we propose a novel triplet contrastive training object. Moreover, we\nintroduce two mechanisms to further improve model performance (i.e., batch-wise\ndynamic attention-masking and triple-wise calibration). Experimental results on\nthree datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves\nbetter performance than that of baselines.\n","authors":["Hongbin Ye","Ningyu Zhang","Shumin Deng","Mosha Chen","Chuanqi Tan","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2009.06207v8.pdf","comment":"Accepted by AAAI 2021"},{"id":"http://arxiv.org/abs/2107.11020v3","updated":"2022-07-21T02:16:07Z","published":"2021-07-23T04:07:14Z","title":"Emotion analysis and detection during COVID-19","summary":"  Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~3K English tweets labeled\nwith emotions and temporally distributed across 18 months. Our analyses reveal\nthe emotional toll caused by COVID-19, and changes of the social narrative and\nassociated emotions over time. Motivated by the time-sensitive nature of crises\nand the cost of large-scale annotation efforts, we examine how well large\npre-trained language models generalize across domains and timeline in the task\nof perceived emotion prediction in the context of COVID-19. Our analyses\nsuggest that cross-domain information transfers occur, yet there are still\nsignificant gaps. We propose semi-supervised learning as a way to bridge this\ngap, obtaining significantly better performance using unlabeled data from the\ntarget domain.\n","authors":["Tiberiu Sosea","Chau Pham","Alexander Tekle","Cornelia Caragea","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2107.11020v3.pdf","comment":"LREC 2022"},{"id":"http://arxiv.org/abs/2207.10245v1","updated":"2022-07-21T00:59:04Z","published":"2022-07-21T00:59:04Z","title":"The Birth of Bias: A case study on the evolution of gender bias in an\n  English language model","summary":"  Detecting and mitigating harmful biases in modern language models are widely\nrecognized as crucial, open problems. In this paper, we take a step back and\ninvestigate how language models come to be biased in the first place. We use a\nrelatively small language model, using the LSTM architecture trained on an\nEnglish Wikipedia corpus. With full access to the data and to the model\nparameters as they change during every step while training, we can map in\ndetail how the representation of gender develops, what patterns in the dataset\ndrive this, and how the model's internal state relates to the bias in a\ndownstream task (semantic textual similarity). We find that the representation\nof gender is dynamic and identify different phases during training.\nFurthermore, we show that gender information is represented increasingly\nlocally in the input embeddings of the model and that, as a consequence,\ndebiasing these can be effective in reducing the downstream bias. Monitoring\nthe training dynamics, allows us to detect an asymmetry in how the female and\nmale gender are represented in the input embeddings. This is important, as it\nmay cause naive mitigation strategies to introduce new undesirable biases. We\ndiscuss the relevance of the findings for mitigation strategies more generally\nand the prospects of generalizing our methods to larger language models, the\nTransformer architecture, other languages and other undesirable biases.\n","authors":["Oskar van der Wal","Jaap Jumelet","Katrin Schulz","Willem Zuidema"],"pdf_url":"https://arxiv.org/pdf/2207.10245v1.pdf","comment":"Accepted at the 4th Workshop on Gender Bias in Natural Language\n  Processing (NAACL, 2022)"},{"id":"http://arxiv.org/abs/2202.02317v2","updated":"2022-07-21T00:45:49Z","published":"2022-02-04T18:58:36Z","title":"Webly Supervised Concept Expansion for General Purpose Vision Models","summary":"  General Purpose Vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from supervised datasets,\nlearn concepts from web image search, and leverage a key characteristic of\nGPVs: the ability to transfer visual knowledge across skills. We use a dataset\nof 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised\nconcept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5\nCOCO-based datasets (80 primary concepts), a newly curated series of 5 datasets\nbased on the OpenImages and VisualGenome repositories (~500 concepts), and the\nWeb-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2\nthat supports a variety of tasks -- from vision tasks like classification and\nlocalization to vision+language tasks like QA and captioning, to more niche\nones like human-object interaction detection. GPV-2 benefits hugely from web\ndata and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,\nand web demo are available at https://prior.allenai.org/projects/gpv2.\n","authors":["Amita Kamath","Christopher Clark","Tanmay Gupta","Eric Kolve","Derek Hoiem","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2202.02317v2.pdf","comment":"ECCV 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2207.10667v1","updated":"2022-07-21T17:59:59Z","published":"2022-07-21T17:59:59Z","title":"Online Domain Adaptation for Semantic Segmentation in Ever-Changing\n  Conditions","summary":"  Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between\ntraining and testing data and is, in most cases, carried out in offline manner.\nHowever, domain changes may occur continuously and unpredictably during\ndeployment (e.g. sudden weather changes). In such conditions, deep neural\nnetworks witness dramatic drops in accuracy and offline adaptation may not be\nenough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA)\nfor semantic segmentation. We design a pipeline that is robust to continuous\ndomain shifts, either gradual or sudden, and we evaluate it in the case of\nrainy and foggy scenarios. Our experiments show that our framework can\neffectively adapt to new domains during deployment, while not being affected by\ncatastrophic forgetting of the previous domains.\n","authors":["Theodoros Panagiotakopoulos","Pier Luigi Dovesi","Linus Härenstam-Nielsen","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2207.10667v1.pdf","comment":"ECCV 2022. Project page: https://theo2021.github.io/onda-web/"},{"id":"http://arxiv.org/abs/2207.10666v1","updated":"2022-07-21T17:59:56Z","published":"2022-07-21T17:59:56Z","title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers","summary":"  Vision transformer (ViT) recently has drawn great attention in computer\nvision due to its remarkable model capability. However, most prevailing ViT\nmodels suffer from huge number of parameters, restricting their applicability\non devices with limited resources. To alleviate this issue, we propose TinyViT,\na new family of tiny and efficient small vision transformers pretrained on\nlarge-scale datasets with our proposed fast distillation framework. The central\nidea is to transfer knowledge from large pretrained models to small ones, while\nenabling small models to get the dividends of massive pretraining data. More\nspecifically, we apply distillation during pretraining for knowledge transfer.\nThe logits of large teacher models are sparsified and stored in disk in advance\nto save the memory cost and computation overheads. The tiny student\ntransformers are automatically scaled down from a large pretrained model with\ncomputation and parameter constraints. Comprehensive experiments demonstrate\nthe efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k\nwith only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k\nwhile using 4.2 times fewer parameters. Moreover, increasing image resolutions,\nTinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using\nonly 11% parameters. Last but not the least, we demonstrate a good transfer\nability of TinyViT on various downstream tasks. Code and models are available\nat https://github.com/microsoft/Cream/tree/main/TinyViT.\n","authors":["Kan Wu","Jinnian Zhang","Houwen Peng","Mengchen Liu","Bin Xiao","Jianlong Fu","Lu Yuan"],"pdf_url":"https://arxiv.org/pdf/2207.10666v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2204.07159v2","updated":"2022-07-21T17:59:32Z","published":"2022-04-14T17:59:39Z","title":"A Level Set Theory for Neural Implicit Evolution under Explicit Flows","summary":"  Coordinate-based neural networks parameterizing implicit surfaces have\nemerged as efficient representations of geometry. They effectively act as\nparametric level sets with the zero-level set defining the surface of interest.\nWe present a framework that allows applying deformation operations defined for\ntriangle meshes onto such implicit surfaces. Several of these operations can be\nviewed as energy-minimization problems that induce an instantaneous flow field\non the explicit surface. Our method uses the flow field to deform parametric\nimplicit surfaces by extending the classical theory of level sets. We also\nderive a consolidated view for existing methods on differentiable surface\nextraction and rendering, by formalizing connections to the level-set theory.\nWe show that these methods drift from the theory and that our approach exhibits\nimprovements for applications like surface smoothing, mean-curvature flow,\ninverse rendering and user-defined editing on implicit geometry.\n","authors":["Ishit Mehta","Manmohan Chandraker","Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2204.07159v2.pdf","comment":"ECCV 2022 (Oral); Project Page at https://ishit.github.io/nie"},{"id":"http://arxiv.org/abs/2207.10664v1","updated":"2022-07-21T17:59:06Z","published":"2022-07-21T17:59:06Z","title":"Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset","summary":"  We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing\nresearch on audiovisual fine-grained categorization. While our community has\nmade great strides in fine-grained visual categorization on images, the\ncounterparts in audio and video fine-grained categorization are relatively\nunexplored. To encourage advancements in this space, we have carefully\nconstructed the SSW60 dataset to enable researchers to experiment with\nclassifying the same set of categories in three different modalities: images,\naudio, and video. The dataset covers 60 species of birds and is comprised of\nimages from existing datasets, and brand new, expert-curated audio and video\ndatasets. We thoroughly benchmark audiovisual classification performance and\nmodality fusion experiments through the use of state-of-the-art transformer\nmethods. Our findings show that performance of audiovisual fusion methods is\nbetter than using exclusively image or audio based methods for the task of\nvideo classification. We also present interesting modality transfer\nexperiments, enabled by the unique construction of SSW60 to encompass three\ndifferent modalities. We hope the SSW60 dataset and accompanying baselines spur\nresearch in this fascinating area.\n","authors":["Grant Van Horn","Rui Qian","Kimberly Wilber","Hartwig Adam","Oisin Mac Aodha","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2207.10664v1.pdf","comment":"ECCV 2022 Camera Ready"},{"id":"http://arxiv.org/abs/2207.10663v1","updated":"2022-07-21T17:58:02Z","published":"2022-07-21T17:58:02Z","title":"Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views","summary":"  We present Neural Pixel Composition (NPC), a novel approach for continuous\n3D-4D view synthesis given only a discrete set of multi-view observations as\ninput. Existing state-of-the-art approaches require dense multi-view\nsupervision and an extensive computational budget. The proposed formulation\nreliably operates on sparse and wide-baseline multi-view imagery and can be\ntrained efficiently within a few seconds to 10 minutes for hi-res (12MP)\ncontent, i.e., 200-400X faster convergence than existing methods. Crucial to\nour approach are two core novelties: 1) a representation of a pixel that\ncontains color and depth information accumulated from multi-views for a\nparticular location and time along a line of sight, and 2) a multi-layer\nperceptron (MLP) that enables the composition of this rich information provided\nfor a pixel location to obtain the final color output. We experiment with a\nlarge variety of multi-view sequences, compare to existing approaches, and\nachieve better results in diverse and challenging settings. Finally, our\napproach enables dense 3D reconstruction from sparse multi-views, where COLMAP,\na state-of-the-art 3D reconstruction approach, struggles.\n","authors":["Aayush Bansal","Michael Zollhoefer"],"pdf_url":"https://arxiv.org/pdf/2207.10663v1.pdf","comment":"A technical report on 3D-4D view synthesis (40 pages, 22 figures and\n  18 tables). High-resolution version of paper:\n  http://www.aayushbansal.xyz/npc/npc_hi-res.pdf. Project page (containing\n  video results): http://www.aayushbansal.xyz/npc/"},{"id":"http://arxiv.org/abs/2207.10662v1","updated":"2022-07-21T17:57:04Z","published":"2022-07-21T17:57:04Z","title":"Generalizable Patch-Based Neural Rendering","summary":"  Neural rendering has received tremendous attention since the advent of Neural\nRadiance Fields (NeRF), and has pushed the state-of-the-art on novel-view\nsynthesis considerably. The recent focus has been on models that overfit to a\nsingle scene, and the few attempts to learn models that can synthesize novel\nviews of unseen scenes mostly consist of combining deep convolutional features\nwith a NeRF-like model. We propose a different paradigm, where no deep features\nand no NeRF-like volume rendering are needed. Our method is capable of\npredicting the color of a target ray in a novel scene directly, just from a\ncollection of patches sampled from the scene. We first leverage epipolar\ngeometry to extract patches along the epipolar lines of each reference view.\nEach patch is linearly projected into a 1D feature vector and a sequence of\ntransformers process the collection. For positional encoding, we parameterize\nrays as in a light field representation, with the crucial difference that the\ncoordinates are canonicalized with respect to the target ray, which makes our\nmethod independent of the reference frame and improves generalization. We show\nthat our approach outperforms the state-of-the-art on novel view synthesis of\nunseen scenes even when being trained with considerably less data than prior\nwork.\n","authors":["Mohammed Suhail","Carlos Esteves","Leonid Sigal","Ameesh Makadia"],"pdf_url":"https://arxiv.org/pdf/2207.10662v1.pdf","comment":"Project Page with code and results at\n  https://mohammedsuhail.net/gen_patch_neural_rendering/"},{"id":"http://arxiv.org/abs/2207.10661v1","updated":"2022-07-21T17:56:54Z","published":"2022-07-21T17:56:54Z","title":"In Defense of Online Models for Video Instance Segmentation","summary":"  In recent years, video instance segmentation (VIS) has been largely advanced\nby offline models, while online models gradually attracted less attention\npossibly due to their inferior performance. However, online methods have their\ninherent advantage in handling long video sequences and ongoing videos while\noffline models fail due to the limit of computational resources. Therefore, it\nwould be highly desirable if online models can achieve comparable or even\nbetter performance than offline models. By dissecting current online models and\noffline models, we demonstrate that the main cause of the performance gap is\nthe error-prone association between frames caused by the similar appearance\namong different instances in the feature space. Observing this, we propose an\nonline framework based on contrastive learning that is able to learn more\ndiscriminative instance embeddings for association and fully exploit history\ninformation for stability. Despite its simplicity, our method outperforms all\nonline and offline methods on three benchmarks. Specifically, we achieve 49.5\nAP on YouTube-VIS 2019, a significant improvement of 13.2 AP and 2.1 AP over\nthe prior online and offline art, respectively. Moreover, we achieve 30.2 AP on\nOVIS, a more challenging dataset with significant crowding and occlusions,\nsurpassing the prior art by 14.8 AP. The proposed method won first place in the\nvideo instance segmentation track of the 4th Large-scale Video Object\nSegmentation Challenge (CVPR2022). We hope the simplicity and effectiveness of\nour method, as well as our insight into current methods, could shed light on\nthe exploration of VIS models.\n","authors":["Junfeng Wu","Qihao Liu","Yi Jiang","Song Bai","Alan Yuille","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2207.10661v1.pdf","comment":"ECCV 2022, Oral"},{"id":"http://arxiv.org/abs/2207.10660v1","updated":"2022-07-21T17:56:22Z","published":"2022-07-21T17:56:22Z","title":"Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild","summary":"  Recognizing scenes and objects in 3D from a single image is a longstanding\ngoal of computer vision with applications in robotics and AR/VR. For 2D\nrecognition, large datasets and scalable solutions have led to unprecedented\nadvances. In 3D, existing benchmarks are small in size and approaches\nspecialize in few object categories and specific domains, e.g. urban driving\nscenes. Motivated by the success of 2D recognition, we revisit the task of 3D\nobject detection by introducing a large benchmark, called Omni3D. Omni3D\nre-purposes and combines existing datasets resulting in 234k images annotated\nwith more than 3 million instances and 97 categories.3D detection at such scale\nis challenging due to variations in camera intrinsics and the rich diversity of\nscene and object types. We propose a model, called Cube R-CNN, designed to\ngeneralize across camera and scene types with a unified approach. We show that\nCube R-CNN outperforms prior works on the larger Omni3D and existing\nbenchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D object\nrecognition, show that it improves single-dataset performance and can\naccelerate learning on new smaller datasets via pre-training.\n","authors":["Garrick Brazil","Julian Straub","Nikhila Ravi","Justin Johnson","Georgia Gkioxari"],"pdf_url":"https://arxiv.org/pdf/2207.10660v1.pdf","comment":"Project website: https://garrickbrazil.com/omni3d"},{"id":"http://arxiv.org/abs/2207.10659v1","updated":"2022-07-21T17:54:36Z","published":"2022-07-21T17:54:36Z","title":"Novel Class Discovery without Forgetting","summary":"  Humans possess an innate ability to identify and differentiate instances that\nthey are not familiar with, by leveraging and adapting the knowledge that they\nhave acquired so far. Importantly, they achieve this without deteriorating the\nperformance on their earlier learning. Inspired by this, we identify and\nformulate a new, pragmatic problem setting of NCDwF: Novel Class Discovery\nwithout Forgetting, which tasks a machine learning model to incrementally\ndiscover novel categories of instances from unlabeled data, while maintaining\nits performance on the previously seen categories. We propose 1) a method to\ngenerate pseudo-latent representations which act as a proxy for (no longer\navailable) labeled data, thereby alleviating forgetting, 2) a\nmutual-information based regularizer which enhances unsupervised discovery of\nnovel classes, and 3) a simple Known Class Identifier which aids generalized\ninference when the testing data contains instances form both seen and unseen\ncategories. We introduce experimental protocols based on CIFAR-10, CIFAR-100\nand ImageNet-1000 to measure the trade-off between knowledge retention and\nnovel class discovery. Our extensive evaluations reveal that existing models\ncatastrophically forget previously seen categories while identifying novel\ncategories, while our method is able to effectively balance between the\ncompeting objectives. We hope our work will attract further research into this\nnewly identified pragmatic problem setting.\n","authors":["K J Joseph","Sujoy Paul","Gaurav Aggarwal","Soma Biswas","Piyush Rai","Kai Han","Vineeth N Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2207.10659v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10642v1","updated":"2022-07-21T17:50:16Z","published":"2022-07-21T17:50:16Z","title":"Generative Multiplane Images: Making a 2D GAN 3D-Aware","summary":"  What is really needed to make an existing 2D GAN 3D-aware? To answer this\nquestion, we modify a classical GAN, i.e., StyleGANv2, as little as possible.\nWe find that only two modifications are absolutely necessary: 1) a multiplane\nimage style generator branch which produces a set of alpha maps conditioned on\ntheir depth; 2) a pose-conditioned discriminator. We refer to the generated\noutput as a 'generative multiplane image' (GMPI) and emphasize that its\nrenderings are not only high-quality but also guaranteed to be view-consistent,\nwhich makes GMPIs different from many prior works. Importantly, the number of\nalpha maps can be dynamically adjusted and can differ between training and\ninference, alleviating memory concerns and enabling fast training of GMPIs in\nless than half a day at a resolution of $1024^2$. Our findings are consistent\nacross three challenging and common high-resolution datasets, including FFHQ,\nAFHQv2, and MetFaces.\n","authors":["Xiaoming Zhao","Fangchang Ma","David Güera","Zhile Ren","Alexander G. Schwing","Alex Colburn"],"pdf_url":"https://arxiv.org/pdf/2207.10642v1.pdf","comment":"ECCV2022; Project Page:\n  https://xiaoming-zhao.github.io/projects/gmpi/"},{"id":"http://arxiv.org/abs/2207.10625v1","updated":"2022-07-21T17:31:57Z","published":"2022-07-21T17:31:57Z","title":"A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery","summary":"  In this paper we present a new dynamical systems algorithm for clustering in\nhyperspectral images. The main idea of the algorithm is that data points are\n\\`pushed\\' in the direction of increasing density and groups of pixels that end\nup in the same dense regions belong to the same class. This is essentially a\nnumerical solution of the differential equation defined by the gradient of the\ndensity of data points on the data manifold. The number of classes is automated\nand the resulting clustering can be extremely accurate. In addition to\nproviding a accurate clustering, this algorithm presents a new tool for\nunderstanding hyperspectral data in high dimensions. We evaluate the algorithm\non the Urban (Available at www.tec.ary.mil/Hypercube/) scene comparing\nperformance against the k-means algorithm using pre-identified classes of\nmaterials as ground truth.\n","authors":["William F. Basener","Alexey Castrodad","David Messinger","Jennifer Mahle","Paul Prue"],"pdf_url":"https://arxiv.org/pdf/2207.10625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10623v1","updated":"2022-07-21T17:30:37Z","published":"2022-07-21T17:30:37Z","title":"MetaComp: Learning to Adapt for Online Depth Completion","summary":"  Relying on deep supervised or self-supervised learning, previous methods for\ndepth completion from paired single image and sparse depth data have achieved\nimpressive performance in recent years. However, facing a new environment where\nthe test data occurs online and differs from the training data in the RGB image\ncontent and depth sparsity, the trained model might suffer severe performance\ndrop. To encourage the trained model to work well in such conditions, we expect\nit to be capable of adapting to the new environment continuously and\neffectively. To achieve this, we propose MetaComp. It utilizes the\nmeta-learning technique to simulate adaptation policies during the training\nphase, and then adapts the model to new environments in a self-supervised\nmanner in testing. Considering that the input is multi-modal data, it would be\nchallenging to adapt a model to variations in two modalities simultaneously,\ndue to significant differences in structure and form of the two modal data.\nTherefore, we further propose to disentangle the adaptation procedure in the\nbasic meta-learning training into two steps, the first one focusing on the\ndepth sparsity while the second attending to the image content. During testing,\nwe take the same strategy to adapt the model online to new multi-modal data.\nExperimental results and comprehensive ablations show that our MetaComp is\ncapable of adapting to the depth completion in a new environment effectively\nand robust to changes in different modalities.\n","authors":["Yang Chen","Shanshan Zhao","Wei Ji","Mingming Gong","Liping Xie"],"pdf_url":"https://arxiv.org/pdf/2207.10623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.08275v2","updated":"2022-07-21T17:28:33Z","published":"2021-12-15T17:09:18Z","title":"SeqFormer: Sequential Transformer for Video Instance Segmentation","summary":"  In this work, we present SeqFormer for video instance segmentation. SeqFormer\nfollows the principle of vision transformer that models instance relationships\namong video frames. Nevertheless, we observe that a stand-alone instance query\nsuffices for capturing a time sequence of instances in a video, but attention\nmechanisms shall be done with each frame independently. To achieve this,\nSeqFormer locates an instance in each frame and aggregates temporal information\nto learn a powerful representation of a video-level instance, which is used to\npredict the mask sequences on each frame dynamically. Instance tracking is\nachieved naturally without tracking branches or post-processing. On\nYouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP\nwith a ResNet-101 backbone without bells and whistles. Such achievement\nsignificantly exceeds the previous state-of-the-art performance by 4.6 and 4.4,\nrespectively. In addition, integrated with the recently-proposed Swin\ntransformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer\ncould be a strong baseline that fosters future research in video instance\nsegmentation, and in the meantime, advances this field with a more robust,\naccurate, neat model. The code is available at\nhttps://github.com/wjf5203/SeqFormer.\n","authors":["Junfeng Wu","Yi Jiang","Song Bai","Wenqing Zhang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2112.08275v2.pdf","comment":"ECCV 2022, Oral"},{"id":"http://arxiv.org/abs/2207.10022v2","updated":"2022-07-21T17:26:51Z","published":"2022-07-20T16:40:38Z","title":"Secrets of Event-Based Optical Flow","summary":"  Event cameras respond to scene dynamics and offer advantages to estimate\nmotion. Following recent image-based deep-learning achievements, optical flow\nestimation methods for event cameras have rushed to combine those image-based\nmethods with event data. However, it requires several adaptations (data\nconversion, loss function, etc.) as they have very different properties. We\ndevelop a principled method to extend the Contrast Maximization framework to\nestimate optical flow from events alone. We investigate key elements: how to\ndesign the objective function to prevent overfitting, how to warp events to\ndeal better with occlusions, and how to improve convergence with multi-scale\nraw events. With these key elements, our method ranks first among unsupervised\nmethods on the MVSEC benchmark, and is competitive on the DSEC benchmark.\nMoreover, our method allows us to expose the issues of the ground truth flow in\nthose benchmarks, and produces remarkable results when it is transferred to\nunsupervised learning settings. Our code is available at\nhttps://github.com/tub-rip/event_based_optical_flow\n","authors":["Shintaro Shiba","Yoshimitsu Aoki","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2207.10022v2.pdf","comment":"23 pages, 11 figures, 7 tables,\n  https://github.com/tub-rip/event_based_optical_flow"},{"id":"http://arxiv.org/abs/2203.11834v3","updated":"2022-07-21T17:23:42Z","published":"2022-03-22T16:01:04Z","title":"Improving Generalization in Federated Learning by Seeking Flat Minima","summary":"  Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2203.11834v3.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2111.05610v2","updated":"2022-07-21T17:19:19Z","published":"2021-11-10T10:05:11Z","title":"CLIP2TV: Align, Match and Distill for Video-Text Retrieval","summary":"  Modern video-text retrieval frameworks basically consist of three parts:\nvideo encoder, text encoder and the similarity head. With the success on both\nvisual and textual representation learning, transformer based encoders and\nfusion methods have also been adopted in the field of video-text retrieval. In\nthis report, we present CLIP2TV, aiming at exploring where the critical\nelements lie in transformer based methods. To achieve this, We first revisit\nsome recent works on multi-modal learning, then introduce some techniques into\nvideo-text retrieval, finally evaluate them through extensive experiments in\ndifferent configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,\noutperforming the previous SOTA result by 4.1%.\n","authors":["Zijian Gao","Jingyu Liu","Weiqi Sun","Sheng Chen","Dedan Chang","Lili Zhao"],"pdf_url":"https://arxiv.org/pdf/2111.05610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08914v2","updated":"2022-07-21T17:17:11Z","published":"2022-03-16T19:54:47Z","title":"Knee arthritis severity measurement using deep learning: a publicly\n  available algorithm with a multi-institutional validation showing\n  radiologist-level performance","summary":"  The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a\ncentral criteria for the use of total knee arthroplasty. However, this\nassessment suffers from imprecise standards and a remarkably high inter-reader\nvariability. An algorithmic, automated assessment of KOA severity could improve\noverall outcomes of knee replacement procedures by increasing the\nappropriateness of its use. We propose a novel deep learning-based five-step\nalgorithm to automatically grade KOA from posterior-anterior (PA) views of\nradiographs: (1) image preprocessing (2) localization of knees joints in the\nimage using the YOLO v3-Tiny model, (3) initial assessment of the severity of\nosteoarthritis using a convolutional neural network-based classifier, (4)\nsegmentation of the joints and calculation of the joint space narrowing (JSN),\nand (5), a combination of the JSN and the initial assessment to determine a\nfinal Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation\nmasks used to make the assessment, our algorithm demonstrates a higher degree\nof transparency compared to typical \"black box\" deep learning classifiers. We\nperform a comprehensive evaluation using two public datasets and one dataset\nfrom our institution, and show that our algorithm reaches state-of-the art\nperformance. Moreover, we also collected ratings from multiple radiologists at\nour institution and showed that our algorithm performs at the radiologist\nlevel.\n  The software has been made publicly available at\nhttps://github.com/MaciejMazurowski/osteoarthritis-classification.\n","authors":["Hanxue Gu","Keyu Li","Roy J. Colglazier","Jichen Yang","Michael Lebhar","Jonathan O'Donnell","William A. Jiranek","Richard C. Mather","Rob J. French","Nicholas Said","Jikai Zhang","Christine Park","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2203.08914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10614v1","updated":"2022-07-21T17:15:41Z","published":"2022-07-21T17:15:41Z","title":"A Dense Material Segmentation Dataset for Indoor and Outdoor Scene\n  Parsing","summary":"  A key algorithm for understanding the world is material segmentation, which\nassigns a label (metal, glass, etc.) to each pixel. We find that a model\ntrained on existing data underperforms in some settings and propose to address\nthis with a large-scale dataset of 3.2 million dense segments on 44,560 indoor\nand outdoor images, which is 23x more segments than existing data. Our data\ncovers a more diverse set of scenes, objects, viewpoints and materials, and\ncontains a more fair distribution of skin types. We show that a model trained\non our data outperforms a state-of-the-art model across datasets and\nviewpoints. We propose a large-scale scene parsing benchmark and baseline of\n0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across\n46 materials.\n","authors":["Paul Upchurch","Ransen Niu"],"pdf_url":"https://arxiv.org/pdf/2207.10614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.13715v2","updated":"2022-07-21T17:15:06Z","published":"2021-12-27T14:53:30Z","title":"SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos","summary":"  When analyzing human motion videos, the output jitters from existing pose\nestimators are highly-unbalanced with varied estimation errors across frames.\nMost frames in a video are relatively easy to estimate and only suffer from\nslight jitters. In contrast, for rarely seen or occluded actions, the estimated\npositions of multiple joints largely deviate from the ground truth values for a\nconsecutive sequence of frames, rendering significant jitters on them. To\ntackle this problem, we propose to attach a dedicated temporal-only refinement\nnetwork to existing pose estimators for jitter mitigation, named SmoothNet.\nUnlike existing learning-based solutions that employ spatio-temporal models to\nco-optimize per-frame precision and temporal smoothness at all the joints,\nSmoothNet models the natural smoothness characteristics in body movements by\nlearning the long-range temporal relations of every joint without considering\nthe noisy correlations among joints. With a simple yet effective motion-aware\nfully-connected network, SmoothNet improves the temporal smoothness of existing\npose estimators significantly and enhances the estimation accuracy of those\nchallenging frames as a side-effect. Moreover, as a temporal-only model, a\nunique advantage of SmoothNet is its strong transferability across various\ntypes of estimators and datasets. Comprehensive experiments on five datasets\nwith eleven popular backbone networks across 2D and 3D pose estimation and body\nrecovery tasks demonstrate the efficacy of the proposed solution. Code is\navailable at https://github.com/cure-lab/SmoothNet.\n","authors":["Ailing Zeng","Lei Yang","Xuan Ju","Jiefeng Li","Jianyi Wang","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2112.13715v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10607v1","updated":"2022-07-21T17:01:24Z","published":"2022-07-21T17:01:24Z","title":"Deep Statistic Shape Model for Myocardium Segmentation","summary":"  Accurate segmentation and motion estimation of myocardium have always been\nimportant in clinic field, which essentially contribute to the downstream\ndiagnosis. However, existing methods cannot always guarantee the shape\nintegrity for myocardium segmentation. In addition, motion estimation requires\npoint correspondence on the myocardium region across different frames. In this\npaper, we propose a novel end-to-end deep statistic shape model to focus on\nmyocardium segmentation with both shape integrity and boundary correspondence\npreserving. Specifically, myocardium shapes are represented by a fixed number\nof points, whose variations are extracted by Principal Component Analysis\n(PCA). Deep neural network is used to predict the transformation parameters\n(both affine and deformation), which are then used to warp the mean point cloud\nto the image domain. Furthermore, a differentiable rendering layer is\nintroduced to incorporate mask supervision into the framework to learn more\naccurate point clouds. In this way, the proposed method is able to consistently\nproduce anatomically reasonable segmentation mask without post processing.\nAdditionally, the predicted point cloud guarantees boundary correspondence for\nsequential images, which contributes to the downstream tasks, such as the\nmotion estimation of myocardium. We conduct several experiments to demonstrate\nthe effectiveness of the proposed method on several benchmark datasets.\n","authors":["Xiaoling Hu","Xiao Chen","Yikang Liu","Eric Z. Chen","Terrence Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2207.10607v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2207.10606v1","updated":"2022-07-21T16:59:54Z","published":"2022-07-21T16:59:54Z","title":"Approximate Differentiable Rendering with Algebraic Surfaces","summary":"  Differentiable renderers provide a direct mathematical link between an\nobject's 3D representation and images of that object. In this work, we develop\nan approximate differentiable renderer for a compact, interpretable\nrepresentation, which we call Fuzzy Metaballs. Our approximate renderer focuses\non rendering shapes via depth maps and silhouettes. It sacrifices fidelity for\nutility, producing fast runtimes and high-quality gradient information that can\nbe used to solve vision tasks. Compared to mesh-based differentiable renderers,\nour method has forward passes that are 5x faster and backwards passes that are\n30x faster. The depth maps and silhouette images generated by our method are\nsmooth and defined everywhere. In our evaluation of differentiable renderers\nfor pose estimation, we show that our method is the only one comparable to\nclassic techniques. In shape from silhouette, our method performs well using\nonly gradient descent and a per-pixel loss, without any surrogate losses or\nregularization. These reconstructions work well even on natural video sequences\nwith segmentation artifacts. Project page:\nhttps://leonidk.github.io/fuzzy-metaballs\n","authors":["Leonid Keselman","Martial Hebert"],"pdf_url":"https://arxiv.org/pdf/2207.10606v1.pdf","comment":"Accepted to the European Conference on Computer Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2205.15814v2","updated":"2022-07-21T16:58:01Z","published":"2022-05-31T14:14:36Z","title":"Contrasting quadratic assignments for set-based representation learning","summary":"  The standard approach to contrastive learning is to maximize the agreement\nbetween different views of the data. The views are ordered in pairs, such that\nthey are either positive, encoding different views of the same object, or\nnegative, corresponding to views of different objects. The supervisory signal\ncomes from maximizing the total similarity over positive pairs, while the\nnegative pairs are needed to avoid collapse. In this work, we note that the\napproach of considering individual pairs cannot account for both intra-set and\ninter-set similarities when the sets are formed from the views of the data. It\nthus limits the information content of the supervisory signal available to\ntrain representations. We propose to go beyond contrasting individual pairs of\nobjects by focusing on contrasting objects as sets. For this, we use\ncombinatorial quadratic assignment theory designed to evaluate set and graph\nsimilarities and derive set-contrastive objective as a regularizer for\ncontrastive learning methods. We conduct experiments and demonstrate that our\nmethod improves learned representations for the tasks of metric learning and\nself-supervised classification.\n","authors":["Artem Moskalev","Ivan Sosnovik","Volker Fischer","Arnold Smeulders"],"pdf_url":"https://arxiv.org/pdf/2205.15814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11726v2","updated":"2022-07-21T16:53:55Z","published":"2022-03-21T10:59:48Z","title":"AI-enabled Assessment of Cardiac Systolic and Diastolic Function from\n  Echocardiography","summary":"  Left ventricular (LV) function is an important factor in terms of patient\nmanagement, outcome, and long-term survival of patients with heart disease. The\nmost recently published clinical guidelines for heart failure recognise that\nover reliance on only one measure of cardiac function (LV ejection fraction) as\na diagnostic and treatment stratification biomarker is suboptimal. Recent\nadvances in AI-based echocardiography analysis have shown excellent results on\nautomated estimation of LV volumes and LV ejection fraction. However, from\ntime-varying 2-D echocardiography acquisition, a richer description of cardiac\nfunction can be obtained by estimating functional biomarkers from the complete\ncardiac cycle. In this work we propose for the first time an AI approach for\nderiving advanced biomarkers of systolic and diastolic LV function from 2-D\nechocardiography based on segmentations of the full cardiac cycle. These\nbiomarkers will allow clinicians to obtain a much richer picture of the heart\nin health and disease. The AI model is based on the 'nn-Unet' framework and was\ntrained and tested using four different databases. Results show excellent\nagreement between manual and automated analysis and showcase the potential of\nthe advanced systolic and diastolic biomarkers for patient stratification.\nFinally, for a subset of 50 cases, we perform a correlation analysis between\nclinical biomarkers derived from echocardiography and CMR and we show excellent\nagreement between the two modalities.\n","authors":["Esther Puyol-Antón","Bram Ruijsink","Baldeep S. Sidhu","Justin Gould","Bradley Porter","Mark K. Elliott","Vishal Mehta","Haotian Gu","Miguel Xochicale","Alberto Gomez","Christopher A. Rinaldi","Martin Cowie","Phil Chowienczyk","Reza Razavi","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2203.11726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.10412v3","updated":"2022-07-21T16:53:18Z","published":"2021-04-21T08:45:09Z","title":"Comprehensive Multi-Modal Interactions for Referring Image Segmentation","summary":"  We investigate Referring Image Segmentation (RIS), which outputs a\nsegmentation map corresponding to the natural language description. Addressing\nRIS efficiently requires considering the interactions happening \\emph{across}\nvisual and linguistic modalities and the interactions \\emph{within} each\nmodality. Existing methods are limited because they either compute different\nforms of interactions \\emph{sequentially} (leading to error propagation) or\n\\emph{ignore} intramodal interactions. We address this limitation by performing\nall three interactions \\emph{simultaneously} through a Synchronous Multi-Modal\nFusion Module (SFM). Moreover, to produce refined segmentation masks, we\npropose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where\nlinguistic features facilitate the exchange of contextual information across\nthe visual hierarchy. We present thorough ablation studies and validate our\napproach's performance on four benchmark datasets, showing considerable\nperformance gains over the existing state-of-the-art (SOTA) methods.\n","authors":["Kanishk Jain","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2104.10412v3.pdf","comment":"Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2207.10589v1","updated":"2022-07-21T16:32:05Z","published":"2022-07-21T16:32:05Z","title":"Boosting 3D Object Detection via Object-Focused Image Fusion","summary":"  3D object detection has achieved remarkable progress by taking point clouds\nas the only input. However, point clouds often suffer from incomplete geometric\nstructures and the lack of semantic information, which makes detectors hard to\naccurately classify detected objects. In this work, we focus on how to\neffectively utilize object-level information from images to boost the\nperformance of point-based 3D detector. We present DeMF, a simple yet effective\nmethod to fuse image information into point features. Given a set of point\nfeatures and image feature maps, DeMF adaptively aggregates image features by\ntaking the projected 2D location of the 3D point as reference. We evaluate our\nmethod on the challenging SUN RGB-D dataset, improving state-of-the-art results\nby a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at\nhttps://github.com/haoy945/DeMF.\n","authors":["Hao Yang","Chen Shi","Yihong Chen","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2207.10589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10582v1","updated":"2022-07-21T16:21:24Z","published":"2022-07-21T16:21:24Z","title":"Designing An Illumination-Aware Network for Deep Image Relighting","summary":"  Lighting is a determining factor in photography that affects the style,\nexpression of emotion, and even quality of images. Creating or finding\nsatisfying lighting conditions, in reality, is laborious and time-consuming, so\nit is of great value to develop a technology to manipulate illumination in an\nimage as post-processing. Although previous works have explored techniques\nbased on the physical viewpoint for relighting images, extensive supervisions\nand prior knowledge are necessary to generate reasonable images, restricting\nthe generalization ability of these works. In contrast, we take the viewpoint\nof image-to-image translation and implicitly merge ideas of the conventional\nphysical viewpoint. In this paper, we present an Illumination-Aware Network\n(IAN) which follows the guidance from hierarchical sampling to progressively\nrelight a scene from a single image with high efficiency. In addition, an\nIllumination-Aware Residual Block (IARB) is designed to approximate the\nphysical rendering process and to extract precise descriptors of light sources\nfor further manipulations. We also introduce a depth-guided geometry encoder\nfor acquiring valuable geometry- and structure-related representations once the\ndepth information is available. Experimental results show that our proposed\nmethod produces better quantitative and qualitative relighting results than\nprevious state-of-the-art methods. The code and models are publicly available\non https://github.com/NK-CS-ZZL/IAN.\n","authors":["Zuo-Liang Zhu","Zhen Li","Rui-Xun Zhang","Chun-Le Guo","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2207.10582v1.pdf","comment":"Accepted for publication as a Regular paper in the IEEE Transactions\n  on Image Processing (T-IP)"},{"id":"http://arxiv.org/abs/2207.10564v1","updated":"2022-07-21T16:10:24Z","published":"2022-07-21T16:10:24Z","title":"Unsupervised Night Image Enhancement: When Layer Decomposition Meets\n  Light-Effects Suppression","summary":"  Night images suffer not only from low light, but also from uneven\ndistributions of light. Most existing night visibility enhancement methods\nfocus mainly on enhancing low-light regions. This inevitably leads to over\nenhancement and saturation in bright regions, such as those regions affected by\nlight effects (glare, floodlight, etc). To address this problem, we need to\nsuppress the light effects in bright regions while, at the same time, boosting\nthe intensity of dark regions. With this idea in mind, we introduce an\nunsupervised method that integrates a layer decomposition network and a\nlight-effects suppression network. Given a single night image as input, our\ndecomposition network learns to decompose shading, reflectance and\nlight-effects layers, guided by unsupervised layer-specific prior losses. Our\nlight-effects suppression network further suppresses the light effects and, at\nthe same time, enhances the illumination in dark regions. This light-effects\nsuppression network exploits the estimated light-effects layer as the guidance\nto focus on the light-effects regions. To recover the background details and\nreduce hallucination/artefacts, we propose structure and high-frequency\nconsistency losses. Our quantitative and qualitative evaluations on real images\nshow that our method outperforms state-of-the-art methods in suppressing night\nlight effects and boosting the intensity of dark regions.\n","authors":["Yeying Jin","Wenhan Yang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2207.10564v1.pdf","comment":"Accepted to ECCV2022"},{"id":"http://arxiv.org/abs/2207.08920v2","updated":"2022-07-21T16:02:22Z","published":"2022-07-18T20:15:29Z","title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric\n  Video","summary":"  Introduction: Hand function is a central determinant of independence after\nstroke. Measuring hand use in the home environment is necessary to evaluate the\nimpact of new interventions, and calls for novel wearable technologies.\nEgocentric video can capture hand-object interactions in context, as well as\nshow how more-affected hands are used during bilateral tasks (for stabilization\nor manipulation). Automated methods are required to extract this information.\nObjective: To use artificial intelligence-based computer vision to classify\nhand use and hand role from egocentric videos recorded at home after stroke.\nMethods: Twenty-one stroke survivors participated in the study. A random forest\nclassifier, a SlowFast neural network, and the Hand Object Detector neural\nnetwork were applied to identify hand use and hand role at home.\nLeave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the\nperformance of the three models. Between-group differences of the models were\ncalculated based on the Mathews correlation coefficient (MCC). Results: For\nhand use detection, the Hand Object Detector had significantly higher\nperformance than the other models. The macro average MCCs using this model in\nthe LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for\nthe less-affected hands. Hand role classification had macro average MCCs in the\nLOSOCV that were close to zero for all models. Conclusion: Using egocentric\nvideo to capture the hand use of stroke survivors at home is feasible. Pose\nestimation to track finger movements may be beneficial to classifying hand\nroles in the future.\n","authors":["Meng-Fen Tsai","Rosalie H. Wang","Jośe Zariffa"],"pdf_url":"https://arxiv.org/pdf/2207.08920v2.pdf","comment":"Appendix is included"},{"id":"http://arxiv.org/abs/2105.14230v2","updated":"2022-07-21T15:59:06Z","published":"2021-05-29T06:42:23Z","title":"Transforming the Latent Space of StyleGAN for Real Face Editing","summary":"  Despite recent advances in semantic manipulation using StyleGAN, semantic\nediting of real faces remains challenging. The gap between the $W$ space and\nthe $W$+ space demands an undesirable trade-off between reconstruction quality\nand editing quality. To solve this problem, we propose to expand the latent\nspace by replacing fully-connected layers in the StyleGAN's mapping network\nwith attention-based transformers. This simple and effective technique\nintegrates the aforementioned two spaces and transforms them into one new\nlatent space called $W$++. Our modified StyleGAN maintains the state-of-the-art\ngeneration quality of the original StyleGAN with moderately better diversity.\nBut more importantly, the proposed $W$++ space achieves superior performance in\nboth reconstruction quality and editing quality. Despite these significant\nadvantages, our $W$++ space supports existing inversion algorithms and editing\nmethods with only negligible modifications thanks to its structural similarity\nwith the $W/W$+ space. Extensive experiments on the FFHQ dataset prove that our\nproposed $W$++ space is evidently more preferable than the previous $W/W$+\nspace for real face editing. The code is publicly available for research\npurposes at https://github.com/AnonSubm2021/TransStyleGAN.\n","authors":["Heyi Li","Jinlong Liu","Xinyu Zhang","Yunzhi Bai","Huayan Wang","Klaus Mueller"],"pdf_url":"https://arxiv.org/pdf/2105.14230v2.pdf","comment":"28 pages, 15 figures"},{"id":"http://arxiv.org/abs/2207.10553v1","updated":"2022-07-21T15:51:30Z","published":"2022-07-21T15:51:30Z","title":"The MABe22 Benchmarks for Representation Learning of Multi-Agent\n  Behavior","summary":"  Real-world behavior is often shaped by complex interactions between multiple\nagents. To scalably study multi-agent behavior, advances in unsupervised and\nself-supervised learning have enabled a variety of different behavioral\nrepresentations to be learned from trajectory data. To date, there does not\nexist a unified set of benchmarks that can enable comparing methods\nquantitatively and systematically across a broad set of behavior analysis\nsettings. We aim to address this by introducing a large-scale, multi-agent\ntrajectory dataset from real-world behavioral neuroscience experiments that\ncovers a range of behavior analysis tasks. Our dataset consists of trajectory\ndata from common model organisms, with 9.6 million frames of mouse data and 4.4\nmillion frames of fly data, in a variety of experimental settings, such as\ndifferent strains, lengths of interaction, and optogenetic stimulation. A\nsubset of the frames also consist of expert-annotated behavior labels.\nImprovements on our dataset corresponds to behavioral representations that work\nacross multiple organisms and is able to capture differences for common\nbehavior analysis tasks.\n","authors":["Jennifer J. Sun","Andrew Ulmer","Dipam Chakraborty","Brian Geuther","Edward Hayes","Heng Jia","Vivek Kumar","Zachary Partridge","Alice Robie","Catherine E. Schretter","Chao Sun","Keith Sheppard","Param Uttarwar","Pietro Perona","Yisong Yue","Kristin Branson","Ann Kennedy"],"pdf_url":"https://arxiv.org/pdf/2207.10553v1.pdf","comment":"Project website:\n  https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset"},{"id":"http://arxiv.org/abs/2207.10552v1","updated":"2022-07-21T15:51:01Z","published":"2022-07-21T15:51:01Z","title":"A Primer on Topological Data Analysis to Support Image Analysis Tasks in\n  Environmental Science","summary":"  Topological data analysis (TDA) is a tool from data science and mathematics\nthat is beginning to make waves in environmental science. In this work, we seek\nto provide an intuitive and understandable introduction to a tool from TDA that\nis particularly useful for the analysis of imagery, namely persistent homology.\nWe briefly discuss the theoretical background but focus primarily on\nunderstanding the output of this tool and discussing what information it can\nglean. To this end, we frame our discussion around a guiding example of\nclassifying satellite images from the Sugar, Fish, Flower, and Gravel Dataset\nproduced for the study of mesocale organization of clouds by Rasp et. al. in\n2020 (arXiv:1906:01906). We demonstrate how persistent homology and its\nvectorization, persistence landscapes, can be used in a workflow with a simple\nmachine learning algorithm to obtain good results, and explore in detail how we\ncan explain this behavior in terms of image-level features. One of the core\nstrengths of persistent homology is how interpretable it can be, so throughout\nthis paper we discuss not just the patterns we find, but why those results are\nto be expected given what we know about the theory of persistent homology. Our\ngoal is that a reader of this paper will leave with a better understanding of\nTDA and persistent homology, be able to identify problems and datasets of their\nown for which persistent homology could be helpful, and gain an understanding\nof results they obtain from applying the included GitHub example code.\n","authors":["Lander Ver Hoef","Henry Adams","Emily J. King","Imme Ebert-Uphoff"],"pdf_url":"https://arxiv.org/pdf/2207.10552v1.pdf","comment":"This work has been submitted to Artificial Intelligence for the Earth\n  Systems (AIES). Copyright in this work may be transferred without further\n  notice"},{"id":"http://arxiv.org/abs/2203.05683v2","updated":"2022-07-21T15:41:26Z","published":"2022-03-10T23:50:08Z","title":"Deep Multimodal Guidance for Medical Image Classification","summary":"  Medical imaging is a cornerstone of therapy and diagnosis in modern medicine.\nHowever, the choice of imaging modality for a particular theranostic task\ntypically involves trade-offs between the feasibility of using a particular\nmodality (e.g., short wait times, low cost, fast acquisition, reduced\nradiation/invasiveness) and the expected performance on a clinical task (e.g.,\ndiagnostic accuracy, efficacy of treatment planning and guidance). In this\nwork, we aim to apply the knowledge learned from the less feasible but\nbetter-performing (superior) modality to guide the utilization of the\nmore-feasible yet under-performing (inferior) modality and steer it towards\nimproved performance. We focus on the application of deep learning for\nimage-based diagnosis. We develop a light-weight guidance model that leverages\nthe latent representation learned from the superior modality, when training a\nmodel that consumes only the inferior modality. We examine the advantages of\nour method in the context of two clinical applications: multi-task skin lesion\nclassification from clinical and dermoscopic images and brain tumor\nclassification from multi-sequence magnetic resonance imaging (MRI) and\nhistopathology images. For both these scenarios we show a boost in diagnostic\nperformance of the inferior modality without requiring the superior modality.\nFurthermore, in the case of brain tumor classification, our method outperforms\nthe model trained on the superior modality while producing comparable results\nto the model that uses both modalities during inference.\n","authors":["Mayur Mallya","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2203.05683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14755v4","updated":"2022-07-21T15:33:21Z","published":"2021-10-27T20:30:57Z","title":"Algorithmic encoding of protected characteristics in image-based models\n  for disease detection","summary":"  It has been rightfully emphasized that the use of AI for clinical decision\nmaking could amplify health disparities. An algorithm may encode protected\ncharacteristics, and then use this information for making predictions due to\nundesirable correlations in the (historical) training data. It remains unclear\nhow we can establish whether such information is actually used. Besides the\nscarcity of data from underserved populations, very little is known about how\ndataset biases manifest in predictive models and how this may result in\ndisparate performance. This article aims to shed some light on these issues by\nexploring new methodology for subgroup analysis in image-based disease\ndetection models. We utilize two publicly available chest X-ray datasets,\nCheXpert and MIMIC-CXR, to study performance disparities across race and\nbiological sex in deep learning models. We explore test set resampling,\ntransfer learning, multitask learning, and model inspection to assess the\nrelationship between the encoding of protected characteristics and disease\ndetection performance across subgroups. We confirm subgroup disparities in\nterms of shifted true and false positive rates which are partially removed\nafter correcting for population and prevalence shifts in the test sets. We\nfurther find a previously used transfer learning method to be insufficient for\nestablishing whether specific patient information is used for making\npredictions. The proposed combination of test-set resampling, multitask\nlearning, and model inspection reveals valuable new insights about the way\nprotected characteristics are encoded in the feature representations of deep\nneural networks.\n","authors":["Ben Glocker","Charles Jones","Melanie Bernhardt","Stefan Winzeck"],"pdf_url":"https://arxiv.org/pdf/2110.14755v4.pdf","comment":"Code available on https://github.com/biomedia-mira/chexploration"},{"id":"http://arxiv.org/abs/2112.05140v2","updated":"2022-07-21T15:33:21Z","published":"2021-12-09T18:59:56Z","title":"NeRF for Outdoor Scene Relighting","summary":"  Photorealistic editing of outdoor scenes from photographs requires a profound\nunderstanding of the image formation process and an accurate estimation of the\nscene geometry, reflectance and illumination. A delicate manipulation of the\nlighting can then be performed while keeping the scene albedo and geometry\nunaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene\nrelighting based on neural radiance fields. In contrast to the prior art, our\ntechnique allows simultaneous editing of both scene illumination and camera\nviewpoint using only a collection of outdoor photos shot in uncontrolled\nsettings. Moreover, it enables direct control over the scene illumination, as\ndefined through a spherical harmonics model. For evaluation, we collect a new\nbenchmark dataset of several outdoor sites photographed from multiple\nviewpoints and at different times. For each time, a 360 degree environment map\nis captured together with a colour-calibration chequerboard to allow accurate\nnumerical evaluations on real data against ground truth. Comparisons against\nSoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at\nhigher quality and with realistic self-shadowing reproduction. Our method and\nthe dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.\n","authors":["Viktor Rudnev","Mohamed Elgharib","William Smith","Lingjie Liu","Vladislav Golyanik","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2112.05140v2.pdf","comment":"22 pages, 10 figures, 2 tables; ECCV 2022; project web page:\n  https://4dqv.mpi-inf.mpg.de/NeRF-OSR/"},{"id":"http://arxiv.org/abs/2207.10530v1","updated":"2022-07-21T15:11:51Z","published":"2022-07-21T15:11:51Z","title":"Neural Network Learning of Chemical Bond Representations in Spectral\n  Indices and Features","summary":"  In this paper we investigate neural networks for classification in\nhyperspectral imaging with a focus on connecting the architecture of the\nnetwork with the physics of the sensing and materials present. Spectroscopy is\nthe process of measuring light reflected or emitted by a material as a function\nwavelength. Molecular bonds present in the material have vibrational\nfrequencies which affect the amount of light measured at each wavelength. Thus\nthe measured spectrum contains information about the particular chemical\nconstituents and types of bonds. For example, chlorophyll reflects more light\nin the near-IR rage (800-900nm) than in the red (625-675nm) range, and this\ndifference can be measured using a normalized vegetation difference index\n(NDVI), which is commonly used to detect vegetation presence, health, and type\nin imagery collected at these wavelengths. In this paper we show that the\nweights in a Neural Network trained on different vegetation classes learn to\nmeasure this difference in reflectance. We then show that a Neural Network\ntrained on a more complex set of ten different polymer materials will learn\nspectral 'features' evident in the weights for the network, and these features\ncan be used to reliably distinguish between the different types of polymers.\nExamination of the weights provides a human-interpretable understanding of the\nnetwork.\n","authors":["Bill Basener"],"pdf_url":"https://arxiv.org/pdf/2207.10530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01759v3","updated":"2022-07-21T15:08:18Z","published":"2021-12-03T07:33:47Z","title":"NeRF-SR: High-Quality Neural Radiance Fields using Supersampling","summary":"  We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis\nwith mostly low-resolution (LR) inputs. Our method is built upon Neural\nRadiance Fields (NeRF) that predicts per-point density and color with a\nmulti-layer perceptron. While producing images at arbitrary scales, NeRF\nstruggles with resolutions that go beyond observed images. Our key insight is\nthat NeRF benefits from 3D consistency, which means an observed pixel absorbs\ninformation from nearby views. We first exploit it by a supersampling strategy\nthat shoots multiple rays at each image pixel, which further enforces\nmulti-view constraint at a sub-pixel level. Then, we show that NeRF-SR can\nfurther boost the performance of supersampling by a refinement network that\nleverages the estimated depth at hand to hallucinate details from related\npatches on only one HR reference image. Experiment results demonstrate that\nNeRF-SR generates high-quality results for novel view synthesis at HR on both\nsynthetic and real-world datasets without any external information.\n","authors":["Chen Wang","Xian Wu","Yuan-Chen Guo","Song-Hai Zhang","Yu-Wing Tai","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2112.01759v3.pdf","comment":"Accepted to MM 2022. Project Page:\n  https://cwchenwang.github.io/NeRF-SR"},{"id":"http://arxiv.org/abs/2107.02572v2","updated":"2022-07-21T15:05:18Z","published":"2021-07-06T12:19:16Z","title":"Unsupervised Knowledge-Transfer for Learned Image Reconstruction","summary":"  Deep learning-based image reconstruction approaches have demonstrated\nimpressive empirical performance in many imaging modalities. These approaches\nusually require a large amount of high-quality paired training data, which is\noften not available in medical imaging. To circumvent this issue we develop a\nnovel unsupervised knowledge-transfer paradigm for learned reconstruction\nwithin a Bayesian framework. The proposed approach learns a reconstruction\nnetwork in two phases. The first phase trains a reconstruction network with a\nset of ordered pairs comprising of ground truth images of ellipses and the\ncorresponding simulated measurement data. The second phase fine-tunes the\npretrained network to more realistic measurement data without supervision. By\nconstruction, the framework is capable of delivering predictive uncertainty\ninformation over the reconstructed image. We present extensive experimental\nresults on low-dose and sparse-view computed tomography showing that the\napproach is competitive with several state-of-the-art supervised and\nunsupervised reconstruction techniques. Moreover, for test data distributed\ndifferently from the training data, the proposed framework can significantly\nimprove reconstruction quality not only visually, but also quantitatively in\nterms of PSNR and SSIM, when compared with learned methods trained on the\nsynthetic dataset only.\n","authors":["Riccardo Barbano","Zeljko Kereta","Andreas Hauptmann","Simon R. Arridge","Bangti Jin"],"pdf_url":"https://arxiv.org/pdf/2107.02572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10519v1","updated":"2022-07-21T15:00:54Z","published":"2022-07-21T15:00:54Z","title":"Real-Time Elderly Monitoring for Senior Safety by Lightweight Human\n  Action Recognition","summary":"  With an increasing number of elders living alone, care-giving from a distance\nbecomes a compelling need, particularly for safety. Real-time monitoring and\naction recognition are essential to raise an alert timely when abnormal\nbehaviors or unusual activities occur. While wearable sensors are widely\nrecognized as a promising solution, highly depending on user's ability and\nwillingness makes them inefficient. In contrast, video streams collected\nthrough non-contact optical cameras provide richer information and release the\nburden on elders. In this paper, leveraging the Independently-Recurrent neural\nNetwork (IndRNN) we propose a novel Real-time Elderly Monitoring for senior\nSafety (REMS) based on lightweight human action recognition (HAR) technology.\nUsing captured skeleton images, the REMS scheme is able to recognize abnormal\nbehaviors or actions and preserve the user's privacy. To achieve high accuracy,\nthe HAR module is trained and fine-tuned using multiple databases. An extensive\nexperimental study verified that REMS system performs action recognition\naccurately and timely. REMS meets the design goals as a privacy-preserving\nelderly safety monitoring system and possesses the potential to be adopted in\nvarious smart monitoring systems.\n","authors":["Han Sun","Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2207.10519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.09817v4","updated":"2022-07-21T14:46:17Z","published":"2022-04-21T00:04:35Z","title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language\n  Processing","summary":"  Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n","authors":["Benedikt Boecking","Naoto Usuyama","Shruthi Bannur","Daniel C. Castro","Anton Schwaighofer","Stephanie Hyland","Maria Wetscherek","Tristan Naumann","Aditya Nori","Javier Alvarez-Valle","Hoifung Poon","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2204.09817v4.pdf","comment":"To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:\n  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook"},{"id":"http://arxiv.org/abs/2207.10506v1","updated":"2022-07-21T14:36:51Z","published":"2022-07-21T14:36:51Z","title":"Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel\n  Structure Aligning Network","summary":"  In ophthalmological imaging, multiple imaging systems, such as color fundus,\ninfrared, fluorescein angiography, optical coherence tomography (OCT) or OCT\nangiography, are often involved to make a diagnosis of retinal disease.\nMulti-modal retinal registration techniques can assist ophthalmologists by\nproviding a pixel-based comparison of aligned vessel structures in images from\ndifferent modalities or acquisition times. To this end, we propose an\nend-to-end trainable deep learning method for multi-modal retinal image\nregistration. Our method extracts convolutional features from the vessel\nstructure for keypoint detection and description and uses a graph neural\nnetwork for feature matching. The keypoint detection and description network\nand graph neural network are jointly trained in a self-supervised manner using\nsynthetic multi-modal image pairs and are guided by synthetically sampled\nground truth homographies. Our method demonstrates higher registration accuracy\nas competing methods for our synthetic retinal dataset and generalizes well for\nour real macula dataset and a public fundus dataset.\n","authors":["Aline Sindel","Bettina Hohberger","Andreas Maier","Vincent Christlein"],"pdf_url":"https://arxiv.org/pdf/2207.10506v1.pdf","comment":"11 pages, 3 figures, 3 tables, accepted to MICCAI 2022"},{"id":"http://arxiv.org/abs/2204.07049v2","updated":"2022-07-21T14:33:51Z","published":"2022-04-14T15:54:01Z","title":"Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for\n  Robotic Bin Picking","summary":"  In this paper, we propose an iterative self-training framework for\nsim-to-real 6D object pose estimation to facilitate cost-effective robotic\ngrasping. Given a bin-picking scenario, we establish a photo-realistic\nsimulator to synthesize abundant virtual data, and use this to train an initial\npose estimation network. This network then takes the role of a teacher model,\nwhich generates pose predictions for unlabeled real data. With these\npredictions, we further design a comprehensive adaptive selection scheme to\ndistinguish reliable results, and leverage them as pseudo labels to update a\nstudent model for pose estimation on real data. To continuously improve the\nquality of pseudo labels, we iterate the above steps by taking the trained\nstudent model as a new teacher and re-label real data using the refined teacher\nmodel. We evaluate our method on a public benchmark and our newly-released\ndataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.\nOur method is also able to improve robotic bin-picking success by 19.54%,\ndemonstrating the potential of iterative sim-to-real solutions for robotic\napplications.\n","authors":["Kai Chen","Rui Cao","Stephen James","Yichuan Li","Yun-Hui Liu","Pieter Abbeel","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2204.07049v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10498v1","updated":"2022-07-21T14:23:50Z","published":"2022-07-21T14:23:50Z","title":"Towards Efficient Adversarial Training on Vision Transformers","summary":"  Vision Transformer (ViT), as a powerful alternative to Convolutional Neural\nNetwork (CNN), has received much attention. Recent work showed that ViTs are\nalso vulnerable to adversarial examples like CNNs. To build robust ViTs, an\nintuitive way is to apply adversarial training since it has been shown as one\nof the most effective ways to accomplish robust CNNs. However, one major\nlimitation of adversarial training is its heavy computational cost. The\nself-attention mechanism adopted by ViTs is a computationally intense operation\nwhose expense increases quadratically with the number of input patches, making\nadversarial training on ViTs even more time-consuming. In this work, we first\ncomprehensively study fast adversarial training on a variety of vision\ntransformers and illustrate the relationship between the efficiency and\nrobustness. Then, to expediate adversarial training on ViTs, we propose an\nefficient Attention Guided Adversarial Training mechanism. Specifically,\nrelying on the specialty of self-attention, we actively remove certain patch\nembeddings of each layer with an attention-guided dropping strategy during\nadversarial training. The slimmed self-attention modules accelerate the\nadversarial training on ViTs significantly. With only 65\\% of the fast\nadversarial training time, we match the state-of-the-art results on the\nchallenging ImageNet benchmark.\n","authors":["Boxi Wu","Jindong Gu","Zhifeng Li","Deng Cai","Xiaofei He","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2207.10498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09508v2","updated":"2022-07-21T14:20:55Z","published":"2022-07-19T18:43:14Z","title":"HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition\n  and Learning from Synthetic Images","summary":"  In this paper, we present the results of the HSE-NN team in the 4th\ncompetition on Affective Behavior Analysis in-the-wild (ABAW). The novel\nmulti-task EfficientNet model is trained for simultaneous recognition of facial\nexpressions and prediction of valence and arousal on static photos. The\nresulting MT-EmotiEffNet extracts visual features that are fed into simple\nfeed-forward neural networks in the multi-task learning challenge. We obtain\nperformance measure 1.3 on the validation set, which is significantly greater\nwhen compared to either performance of baseline (0.3) or existing models that\nare trained only on the s-Aff-Wild2 database. In the learning from synthetic\ndata challenge, the quality of the original synthetic training set is increased\nby using the super-resolution techniques, such as Real-ESRGAN. Next, the\nMT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a\nsimple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our\naverage validation F1 score is 18% greater than the baseline convolutional\nneural network.\n","authors":["Andrey V. Savchenko"],"pdf_url":"https://arxiv.org/pdf/2207.09508v2.pdf","comment":"13 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2207.10494v1","updated":"2022-07-21T14:19:39Z","published":"2022-07-21T14:19:39Z","title":"Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused\n  Events Fusion","summary":"  Event cameras are bio-inspired sensors that offer advantages over traditional\ncameras. They work asynchronously, sampling the scene with microsecond\nresolution and producing a stream of brightness changes. This unconventional\noutput has sparked novel computer vision methods to unlock the camera's\npotential. We tackle the problem of event-based stereo 3D reconstruction for\nSLAM. Most event-based stereo methods try to exploit the camera's high temporal\nresolution and event simultaneity across cameras to establish matches and\nestimate depth. By contrast, we investigate how to estimate depth without\nexplicit data association by fusing Disparity Space Images (DSIs) originated in\nefficient monocular methods. We develop fusion theory and apply it to design\nmulti-camera 3D reconstruction algorithms that produce state-of-the-art\nresults, as we confirm by comparing against four baseline methods and testing\non a variety of available datasets.\n","authors":["Suman Ghosh","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2207.10494v1.pdf","comment":"19 pages, 18 figures, 9 tables"},{"id":"http://arxiv.org/abs/2206.07434v2","updated":"2022-07-21T14:19:11Z","published":"2022-06-15T10:13:34Z","title":"Self-Supervised Implicit Attention: Guided Attention by The Model Itself","summary":"  We propose Self-Supervised Implicit Attention (SSIA), a new approach that\nadaptively guides deep neural network models to gain attention by exploiting\nthe properties of the models themselves. SSIA is a novel attention mechanism\nthat does not require any extra parameters, computation, or memory access costs\nduring inference, which is in contrast to existing attention mechanism. In\nshort, by considering attention weights as higher-level semantic information,\nwe reconsidered the implementation of existing attention mechanisms and further\npropose generating supervisory signals from higher network layers to guide\nlower network layers for parameter updates. We achieved this by building a\nself-supervised learning task using the hierarchical features of the network\nitself, which only works at the training stage. To verify the effectiveness of\nSSIA, we performed a particular implementation (called an SSIA block) in\nconvolutional neural network models and validated it on several image\nclassification datasets. The experimental results show that an SSIA block can\nsignificantly improve the model performance, even outperforms many popular\nattention methods that require additional parameters and computation costs,\nsuch as Squeeze-and-Excitation and Convolutional Block Attention Module. Our\nimplementation will be available on GitHub.\n","authors":["Jinyi Wu","Xun Gong","Zhemin Zhang"],"pdf_url":"https://arxiv.org/pdf/2206.07434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16194v2","updated":"2022-07-21T14:12:06Z","published":"2022-03-30T10:33:09Z","title":"FlowFormer: A Transformer Architecture for Optical Flow","summary":"  We introduce optical Flow transFormer, dubbed as FlowFormer, a\ntransformer-based neural network architecture for learning optical flow.\nFlowFormer tokenizes the 4D cost volume built from an image pair, encodes the\ncost tokens into a cost memory with alternate-group transformer (AGT) layers in\na novel latent space, and decodes the cost memory via a recurrent transformer\ndecoder with dynamic positional cost queries. On the Sintel benchmark,\nFlowFormer achieves 1.144 and 2.183 average end-ponit-error (AEPE) on the clean\nand final pass, a 17.6% and 11.6% error reduction from the best published\nresult (1.388 and 2.47). Besides, FlowFormer also achieves strong\ngeneralization performance. Without being trained on Sintel, FlowFormer\nachieves 0.95 AEPE on the Sintel training set clean pass, outperforming the\nbest published result (1.29) by 26.9%.\n","authors":["Zhaoyang Huang","Xiaoyu Shi","Chao Zhang","Qiang Wang","Ka Chun Cheung","Hongwei Qin","Jifeng Dai","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2203.16194v2.pdf","comment":"Project Page: https://drinkingcoder.github.io/publication/flowformer/"},{"id":"http://arxiv.org/abs/2207.10489v1","updated":"2022-07-21T14:09:43Z","published":"2022-07-21T14:09:43Z","title":"Online Localisation and Colored Mesh Reconstruction Architecture for 3D\n  Visual Feedback in Robotic Exploration Missions","summary":"  This paper introduces an Online Localisation and Colored Mesh Reconstruction\n(OLCMR) ROS perception architecture for ground exploration robots aiming to\nperform robust Simultaneous Localisation And Mapping (SLAM) in challenging\nunknown environments and provide an associated colored 3D mesh representation\nin real time. It is intended to be used by a remote human operator to easily\nvisualise the mapped environment during or after the mission or as a\ndevelopment base for further researches in the field of exploration robotics.\nThe architecture is mainly composed of carefully-selected open-source ROS\nimplementations of a LiDAR-based SLAM algorithm alongside a colored surface\nreconstruction procedure using a point cloud and RGB camera images projected\ninto the 3D space. The overall performances are evaluated on the Newer College\nhandheld LiDAR-Vision reference dataset and on two experimental trajectories\ngathered on board of representative wheeled robots in respectively urban and\ncountryside outdoor environments. Index Terms: Field Robots, Mapping, SLAM,\nColored Surface Reconstruction\n","authors":["Quentin Serdel","Christophe Grand","Julien Marzat","Julien Moras"],"pdf_url":"https://arxiv.org/pdf/2207.10489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.01702v3","updated":"2022-07-21T14:08:46Z","published":"2022-04-01T18:18:12Z","title":"Personalized Prediction of Future Lesion Activity and Treatment Effect\n  in Multiple Sclerosis from Baseline MRI","summary":"  Precision medicine for chronic diseases such as multiple sclerosis (MS)\ninvolves choosing a treatment which best balances efficacy and side\neffects/preferences for individual patients. Making this choice as early as\npossible is important, as delays in finding an effective therapy can lead to\nirreversible disability accrual. To this end, we present the first deep neural\nnetwork model for individualized treatment decisions from baseline magnetic\nresonance imaging (MRI) (with clinical information if available) for MS\npatients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)\nlesion counts on follow-up MRI on multiple treatments and (b) estimates the\nconditional average treatment effect (CATE), as defined by the predicted future\nsuppression of NE-T2 lesions, between different treatment options relative to\nplacebo. Our model is validated on a proprietary federated dataset of 1817\nmulti-sequence MRIs acquired from MS patients during four multi-centre\nrandomized clinical trials. Our framework achieves high average precision in\nthe binarized regression of future NE-T2 lesions on five different treatments,\nidentifies heterogeneous treatment effects, and provides a personalized\ntreatment recommendation that accounts for treatment-associated risk (e.g. side\neffects, patient preference, administration difficulties).\n","authors":["Joshua Durso-Finley","Jean-Pierre R. Falet","Brennan Nichyporuk","Douglas L. Arnold","Tal Arbel"],"pdf_url":"https://arxiv.org/pdf/2204.01702v3.pdf","comment":"Accepted to MIDL 2022"},{"id":"http://arxiv.org/abs/2111.15264v3","updated":"2022-07-21T14:03:13Z","published":"2021-11-30T10:23:06Z","title":"EdiBERT, a generative model for image editing","summary":"  Advances in computer vision are pushing the limits of im-age manipulation,\nwith generative models sampling detailed images on various tasks. However, a\nspecialized model is often developed and trained for each specific task, even\nthough many image edition tasks share similarities. In denoising, inpainting,\nor image compositing, one always aims at generating a realistic image from a\nlow-quality one. In this paper, we aim at making a step towards a unified\napproach for image editing. To do so, we propose EdiBERT, a bi-directional\ntransformer trained in the discrete latent space built by a vector-quantized\nauto-encoder. We argue that such a bidirectional model is suited for image\nmanipulation since any patch can be re-sampled conditionally to the whole\nimage. Using this unique and straightforward training objective, we show that\nthe resulting model matches state-of-the-art performances on a wide variety of\ntasks: image denoising, image completion, and image composition.\n","authors":["Thibaut Issenhuth","Ugo Tanielian","Jérémie Mary","David Picard"],"pdf_url":"https://arxiv.org/pdf/2111.15264v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10485v1","updated":"2022-07-21T14:00:00Z","published":"2022-07-21T14:00:00Z","title":"Towards Confident Detection of Prostate Cancer using High Resolution\n  Micro-ultrasound","summary":"  MOTIVATION: Detection of prostate cancer during transrectal ultrasound-guided\nbiopsy is challenging. The highly heterogeneous appearance of cancer, presence\nof ultrasound artefacts, and noise all contribute to these difficulties. Recent\nadvancements in high-frequency ultrasound imaging - micro-ultrasound - have\ndrastically increased the capability of tissue imaging at high resolution. Our\naim is to investigate the development of a robust deep learning model\nspecifically for micro-ultrasound-guided prostate cancer biopsy. For the model\nto be clinically adopted, a key challenge is to design a solution that can\nconfidently identify the cancer, while learning from coarse histopathology\nmeasurements of biopsy samples that introduce weak labels. METHODS: We use a\ndataset of micro-ultrasound images acquired from 194 patients, who underwent\nprostate biopsy. We train a deep model using a co-teaching paradigm to handle\nnoise in labels, together with an evidential deep learning method for\nuncertainty estimation. We evaluate the performance of our model using the\nclinically relevant metric of accuracy vs. confidence. RESULTS: Our model\nachieves a well-calibrated estimation of predictive uncertainty with area under\nthe curve of 88$\\%$. The use of co-teaching and evidential deep learning in\ncombination yields significantly better uncertainty estimation than either\nalone. We also provide a detailed comparison against state-of-the-art in\nuncertainty estimation.\n","authors":["Mahdi Gilany","Paul Wilson","Amoon Jamzad","Fahimeh Fooladgar","Minh Nguyen Nhat To","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2207.10485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10482v1","updated":"2022-07-21T13:54:52Z","published":"2022-07-21T13:54:52Z","title":"LPYOLO: Low Precision YOLO for Face Detection on FPGA","summary":"  In recent years, number of edge computing devices and artificial intelligence\napplications on them have advanced excessively. In edge computing, decision\nmaking processes and computations are moved from servers to edge devices.\nHence, cheap and low power devices are required. FPGAs are very low power,\ninclined to do parallel operations and deeply suitable devices for running\nConvolutional Neural Networks (CNN) which are the fundamental unit of an\nartificial intelligence application. Face detection on surveillance systems is\nthe most expected application on the security market. In this work, TinyYolov3\narchitecture is redesigned and deployed for face detection. It is a CNN based\nobject detection method and developed for embedded systems. PYNQ-Z2 is selected\nas a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on\nit. Redesigned TinyYolov3 model is defined in numerous bit width precisions\nwith Brevitas library which brings fundamental CNN layers and activations in\ninteger quantized form. Then, the model is trained in a quantized structure\nwith WiderFace dataset. In order to decrease latency and power consumption,\nonchip memory of the FPGA is configured as a storage of whole network\nparameters and the last activation function is modified as rescaled HardTanh\ninstead of Sigmoid. Also, high degree of parallelism is applied to logical\nresources of the FPGA. The model is converted to an HLS based application with\nusing FINN framework and FINN-HLS library which includes the layer definitions\nin C++. Later, the model is synthesized and deployed. CPU of the SoC is\nemployed with multithreading mechanism and responsible for preprocessing,\npostprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total\nboard power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP\naccuracy rate on Easy category of the WiderFace are achieved with 4 bits\nprecision model.\n","authors":["Bestami Günay","Sefa Burak Okcu","Hasan Şakir Bilge"],"pdf_url":"https://arxiv.org/pdf/2207.10482v1.pdf","comment":"Accepted to MVML2022"},{"id":"http://arxiv.org/abs/2205.04992v2","updated":"2022-07-21T13:40:37Z","published":"2022-05-10T15:57:03Z","title":"KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative\n  Spatial Encoding of Keypoints","summary":"  Image-based volumetric humans using pixel-aligned features promise\ngeneralization to unseen poses and identities. Prior work leverages global\nspatial encodings and multi-view geometric consistency to reduce spatial\nambiguity. However, global encodings often suffer from overfitting to the\ndistribution of the training data, and it is difficult to learn multi-view\nconsistent reconstruction from sparse views. In this work, we investigate\ncommon issues with existing spatial encodings and propose a simple yet highly\neffective approach to modeling high-fidelity volumetric humans from sparse\nviews. One of the key ideas is to encode relative spatial 3D information via\nsparse 3D keypoints. This approach is robust to the sparsity of viewpoints and\ncross-dataset domain gap. Our approach outperforms state-of-the-art methods for\nhead reconstruction. On human body reconstruction for unseen subjects, we also\nachieve performance comparable to prior work that uses a parametric human body\nmodel and temporal feature aggregation. Our experiments show that a majority of\nerrors in prior work stem from an inappropriate choice of spatial encoding and\nthus we suggest a new direction for high-fidelity image-based human modeling.\nhttps://markomih.github.io/KeypointNeRF\n","authors":["Marko Mihajlovic","Aayush Bansal","Michael Zollhoefer","Siyu Tang","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2205.04992v2.pdf","comment":"To appear at ECCV 2022. The project page is available at\n  https://markomih.github.io/KeypointNeRF"},{"id":"http://arxiv.org/abs/2207.10469v1","updated":"2022-07-21T13:33:40Z","published":"2022-07-21T13:33:40Z","title":"Fast Data Driven Estimation of Cluster Number in Multiplex Images using\n  Embedded Density Outliers","summary":"  The usage of chemical imaging technologies is becoming a routine\naccompaniment to traditional methods in pathology. Significant technological\nadvances have developed these next generation techniques to provide rich,\nspatially resolved, multidimensional chemical images. The rise of digital\npathology has significantly enhanced the synergy of these imaging modalities\nwith optical microscopy and immunohistochemistry, enhancing our understanding\nof the biological mechanisms and progression of diseases. Techniques such as\nimaging mass cytometry provide labelled multidimensional (multiplex) images of\nspecific components used in conjunction with digital pathology techniques.\nThese powerful techniques generate a wealth of high dimensional data that\ncreate significant challenges in data analysis. Unsupervised methods such as\nclustering are an attractive way to analyse these data, however, they require\nthe selection of parameters such as the number of clusters. Here we propose a\nmethodology to estimate the number of clusters in an automatic data-driven\nmanner using a deep sparse autoencoder to embed the data into a lower\ndimensional space. We compute the density of regions in the embedded space, the\nmajority of which are empty, enabling the high density regions to be detected\nas outliers and provide an estimate for the number of clusters. This framework\nprovides a fully unsupervised and data-driven method to analyse\nmultidimensional data. In this work we demonstrate our method using 45\nmultiplex imaging mass cytometry datasets. Moreover, our model is trained using\nonly one of the datasets and the learned embedding is applied to the remaining\n44 images providing an efficient process for data analysis. Finally, we\ndemonstrate the high computational efficiency of our method which is two orders\nof magnitude faster than estimating via computing the sum squared distances as\na function of cluster number.\n","authors":["Spencer A. Thomas"],"pdf_url":"https://arxiv.org/pdf/2207.10469v1.pdf","comment":"8 pages, 6 figures, conference paper"},{"id":"http://arxiv.org/abs/2205.02301v2","updated":"2022-07-21T13:26:30Z","published":"2022-05-04T19:38:26Z","title":"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking","summary":"  Estimating human motion from video is an active research area due to its many\npotential applications. Most state-of-the-art methods predict human shape and\nposture estimates for individual images and do not leverage the temporal\ninformation available in video. Many \"in the wild\" sequences of human motion\nare captured by a moving camera, which adds the complication of conflated\ncamera and human motion to the estimation. We therefore present BodySLAM, a\nmonocular SLAM system that jointly estimates the position, shape, and posture\nof human bodies, as well as the camera trajectory. We also introduce a novel\nhuman motion model to constrain sequential body postures and observe the scale\nof the scene. Through a series of experiments on video sequences of human\nmotion captured by a moving monocular camera, we demonstrate that BodySLAM\nimproves estimates of all human body parameters and camera poses when compared\nto estimating these separately.\n","authors":["Dorian F. Henning","Tristan Laidlow","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2205.02301v2.pdf","comment":"ECCV 2022. Video: https://youtu.be/0-SL3VeWEvU"},{"id":"http://arxiv.org/abs/2105.09401v2","updated":"2022-07-21T12:58:21Z","published":"2021-05-19T21:01:41Z","title":"Contrastive Learning with Complex Heterogeneity","summary":"  With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and are characterized with multiple\nlabels, thus exhibiting the co-existence of multiple types of heterogeneity.\nAlthough state-of-the-art techniques are good at modeling complex heterogeneity\nwith sufficient label information, such label information can be quite\nexpensive to obtain in real applications. Recently, researchers pay great\nattention to contrastive learning due to its prominent performance by utilizing\nrich unlabeled data. However, existing work on contrastive learning is not able\nto address the problem of false negative pairs, i.e., some `negative' pairs may\nhave similar representations if they have the same label. To overcome the\nissues, in this paper, we propose a unified heterogeneous learning framework,\nwhich combines both the weighted unsupervised contrastive loss and the weighted\nsupervised contrastive loss to model multiple types of heterogeneity. We first\nprovide a theoretical analysis showing that the vanilla contrastive learning\nloss easily leads to the sub-optimal solution in the presence of false negative\npairs, whereas the proposed weighted loss could automatically adjust the weight\nbased on the similarity of the learned representations to mitigate this issue.\nExperimental results on real-world data sets demonstrate the effectiveness and\nthe efficiency of the proposed framework modeling multiple types of\nheterogeneity.\n","authors":["Lecheng Zheng","Jinjun Xiong","Yada Zhu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2105.09401v2.pdf","comment":"Accepted by KDD22"},{"id":"http://arxiv.org/abs/2207.10047v2","updated":"2022-07-21T12:52:17Z","published":"2022-07-20T17:24:22Z","title":"Densely Constrained Depth Estimator for Monocular 3D Object Detection","summary":"  Estimating accurate 3D locations of objects from monocular images is a\nchallenging problem because of lacking depth. Previous work shows that\nutilizing the object's keypoint projection constraints to estimate multiple\ndepth candidates boosts the detection performance. However, the existing\nmethods can only utilize vertical edges as projection constraints for depth\nestimation. So these methods only use a small number of projection constraints\nand produce insufficient depth candidates, leading to inaccurate depth\nestimation. In this paper, we propose a method that utilizes dense projection\nconstraints from edges of any direction. In this way, we employ much more\nprojection constraints and produce considerable depth candidates. Besides, we\npresent a graph matching weighting module to merge the depth candidates. The\nproposed method DCD (Densely Constrained Detector) achieves state-of-the-art\nperformance on the KITTI and WOD benchmarks. Code is released at\nhttps://github.com/BraveGroup/DCD.\n","authors":["Yingyan Li","Yuntao Chen","Jiawei He","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.10047v2.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.10456v1","updated":"2022-07-21T12:51:41Z","published":"2022-07-21T12:51:41Z","title":"Semantic-Aware Fine-Grained Correspondence","summary":"  Establishing visual correspondence across images is a challenging and\nessential task. Recently, an influx of self-supervised methods have been\nproposed to better learn representations for visual correspondence. However, we\nfind that these methods often fail to leverage semantic information and\nover-rely on the matching of low-level features. In contrast, human vision is\ncapable of distinguishing between distinct objects as a pretext to tracking.\nInspired by this paradigm, we propose to learn semantic-aware fine-grained\ncorrespondence. Firstly, we demonstrate that semantic correspondence is\nimplicitly available through a rich set of image-level self-supervised methods.\nWe further design a pixel-level self-supervised learning objective which\nspecifically targets fine-grained correspondence. For downstream tasks, we fuse\nthese two kinds of complementary correspondence representations together,\ndemonstrating that they boost performance synergistically. Our method surpasses\nprevious state-of-the-art self-supervised methods using convolutional networks\non a variety of visual correspondence tasks, including video object\nsegmentation, human pose tracking, and human part tracking.\n","authors":["Yingdong Hu","Renhao Wang","Kaifeng Zhang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2207.10456v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2207.10455v1","updated":"2022-07-21T12:50:54Z","published":"2022-07-21T12:50:54Z","title":"Magic ELF: Image Deraining Meets Association Learning and Transformer","summary":"  Convolutional neural network (CNN) and Transformer have achieved great\nsuccess in multimedia applications. However, little effort has been made to\neffectively and efficiently harmonize these two architectures to satisfy image\nderaining. This paper aims to unify these two architectures to take advantage\nof their learning merits for image deraining. In particular, the local\nconnectivity and translation equivariance of CNN and the global aggregation\nability of self-attention (SA) in Transformer are fully exploited for specific\nlocal context and global structure representations. Based on the observation\nthat rain distribution reveals the degradation location and degree, we\nintroduce degradation prior to help background recovery and accordingly present\nthe association refinement deraining scheme. A novel multi-input attention\nmodule (MAM) is proposed to associate rain perturbation removal and background\nrecovery. Moreover, we equip our model with effective depth-wise separable\nconvolutions to learn the specific feature representations and trade off\ncomputational complexity. Extensive experiments show that our proposed method\n(dubbed as ELF) outperforms the state-of-the-art approach (MPRNet) by 0.25 dB\non average, but only accounts for 11.7\\% and 42.1\\% of its computational cost\nand parameters. The source code is available at\nhttps://github.com/kuijiang94/Magic-ELF.\n","authors":["Kui Jiang","Zhongyuan Wang","Chen Chen","Zheng Wang","Laizhong Cui","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2207.10455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10448v1","updated":"2022-07-21T12:38:05Z","published":"2022-07-21T12:38:05Z","title":"An Efficient Spatio-Temporal Pyramid Transformer for Action Detection","summary":"  The task of action detection aims at deducing both the action category and\nlocalization of the start and end moment for each action instance in a long,\nuntrimmed video. While vision Transformers have driven the recent advances in\nvideo understanding, it is non-trivial to design an efficient architecture for\naction detection due to the prohibitively expensive self-attentions over a long\nsequence of video clips. To this end, we present an efficient hierarchical\nSpatio-Temporal Pyramid Transformer (STPT) for action detection, building upon\nthe fact that the early self-attention layers in Transformers still focus on\nlocal patterns. Specifically, we propose to use local window attention to\nencode rich local spatio-temporal representations in the early stages while\napplying global attention modules to capture long-term space-time dependencies\nin the later stages. In this way, our STPT can encode both locality and\ndependency with largely reduced redundancy, delivering a promising trade-off\nbetween accuracy and efficiency. For example, with only RGB input, the proposed\nSTPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%\nand performing favorably against state-of-the-art AFSD that uses additional\nflow features with 31% fewer GFLOPs, which serves as an effective and efficient\nend-to-end Transformer-based framework for action detection.\n","authors":["Yuetian Weng","Zizheng Pan","Mingfei Han","Xiaojun Chang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2207.10448v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10447v1","updated":"2022-07-21T12:37:15Z","published":"2022-07-21T12:37:15Z","title":"Weakly Supervised Object Localization via Transformer with Implicit\n  Spatial Calibration","summary":"  Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Recent studies leverage the advantage\nof self-attention in visual Transformer for long-range dependency to re-active\nsemantic regions, aiming to avoid partial activation in traditional class\nactivation mapping (CAM). However, the long-range modeling in Transformer\nneglects the inherent spatial coherence of the object, and it usually diffuses\nthe semantic-aware regions far from the object boundary, making localization\nresults significantly larger or far smaller. To address such an issue, we\nintroduce a simple yet effective Spatial Calibration Module (SCM) for accurate\nWSOL, incorporating semantic similarities of patch tokens and their spatial\nrelationships into a unified diffusion model. Specifically, we introduce a\nlearnable parameter to dynamically adjust the semantic correlations and spatial\ncontext intensities for effective information propagation. In practice, SCM is\ndesigned as an external module of Transformer, and can be removed during\ninference to reduce the computation cost. The object-sensitive localization\nability is implicitly embedded into the Transformer encoder through\noptimization in the training phase. It enables the generated attention maps to\ncapture the sharper object boundaries and filter the object-irrelevant\nbackground area. Extensive experimental results demonstrate the effectiveness\nof the proposed method, which significantly outperforms its counterpart TS-CAM\non both CUB-200 and ImageNet-1K benchmarks. The code is available at\nhttps://github.com/164140757/SCM.\n","authors":["Haotian Bai","Ruimao Zhang","Jiong Wang","Xiang Wan"],"pdf_url":"https://arxiv.org/pdf/2207.10447v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2207.10446v1","updated":"2022-07-21T12:34:04Z","published":"2022-07-21T12:34:04Z","title":"COBRA: Cpu-Only aBdominal oRgan segmentAtion","summary":"  Abdominal organ segmentation is a difficult and time-consuming task. To\nreduce the burden on clinical experts, fully-automated methods are highly\ndesirable. Current approaches are dominated by Convolutional Neural Networks\n(CNNs) however the computational requirements and the need for large data sets\nlimit their application in practice. By implementing a small and efficient\ncustom 3D CNN, compiling the trained model and optimizing the computational\ngraph: our approach produces high accuracy segmentations (Dice Similarity\nCoefficient (%): Liver: 97.3$\\pm$1.3, Kidneys: 94.8$\\pm$3.6, Spleen:\n96.4$\\pm$3.0, Pancreas: 80.9$\\pm$10.1) at a rate of 1.6 seconds per image.\nCrucially, we are able to perform segmentation inference solely on CPU (no GPU\nrequired), thereby facilitating easy and widespread deployment of the model\nwithout specialist hardware.\n","authors":["Edward G. A. Henderson","Dónal M. McSweeney","Andrew F. Green"],"pdf_url":"https://arxiv.org/pdf/2207.10446v1.pdf","comment":"MCR-RRR submission for the Fast and Low GPU memory Abdominal oRgan\n  sEgmentation Challenge (FLARE) at MICCAI 2021"},{"id":"http://arxiv.org/abs/2207.10436v1","updated":"2022-07-21T12:12:36Z","published":"2022-07-21T12:12:36Z","title":"Mining Relations among Cross-Frame Affinities for Video Semantic\n  Segmentation","summary":"  The essence of video semantic segmentation (VSS) is how to leverage temporal\ninformation for prediction. Previous efforts are mainly devoted to developing\nnew techniques to calculate the cross-frame affinities such as optical flow and\nattention. Instead, this paper contributes from a different angle by mining\nrelations among cross-frame affinities, upon which better temporal information\naggregation could be achieved. We explore relations among affinities in two\naspects: single-scale intrinsic correlations and multi-scale relations.\nInspired by traditional feature processing, we propose Single-scale Affinity\nRefinement (SAR) and Multi-scale Affinity Aggregation (MAA). To make it\nfeasible to execute MAA, we propose a Selective Token Masking (STM) strategy to\nselect a subset of consistent reference tokens for different scales when\ncalculating affinities, which also improves the efficiency of our method. At\nlast, the cross-frame affinities strengthened by SAR and MAA are adopted for\nadaptively aggregating temporal information. Our experiments demonstrate that\nthe proposed method performs favorably against state-of-the-art VSS methods.\nThe code is publicly available at https://github.com/GuoleiSun/VSS-MRCFA\n","authors":["Guolei Sun","Yun Liu","Hao Tang","Ajad Chhatkuli","Le Zhang","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2207.10436v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10435v1","updated":"2022-07-21T12:11:18Z","published":"2022-07-21T12:11:18Z","title":"Human Trajectory Prediction via Neural Social Physics","summary":"  Trajectory prediction has been widely pursued in many fields, and many\nmodel-based and model-free methods have been explored. The former include\nrule-based, geometric or optimization-based models, and the latter are mainly\ncomprised of deep learning approaches. In this paper, we propose a new method\ncombining both methodologies based on a new Neural Differential Equation model.\nOur new model (Neural Social Physics or NSP) is a deep neural network within\nwhich we use an explicit physics model with learnable parameters. The explicit\nphysics model serves as a strong inductive bias in modeling pedestrian\nbehaviors, while the rest of the network provides a strong data-fitting\ncapability in terms of system parameter estimation and dynamics stochasticity\nmodeling. We compare NSP with 15 recent deep learning methods on 6 datasets and\nimprove the state-of-the-art performance by 5.56%-70%. Besides, we show that\nNSP has better generalizability in predicting plausible trajectories in\ndrastically different scenarios where the density is 2-5 times as high as the\ntesting data. Finally, we show that the physics model in NSP can provide\nplausible explanations for pedestrian behaviors, as opposed to black-box deep\nlearning. Code is available:\nhttps://github.com/realcrane/Human-Trajectory-Prediction-via-Neural-Social-Physics.\n","authors":["Jiangbei Yue","Dinesh Manocha","He Wang"],"pdf_url":"https://arxiv.org/pdf/2207.10435v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2204.03039v3","updated":"2022-07-21T12:08:06Z","published":"2022-04-06T18:43:54Z","title":"DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors","summary":"  Camera-based 3D object detectors are welcome due to their wider deployment\nand lower price than LiDAR sensors. We first revisit the prior stereo detector\nDSGN for its stereo volume construction ways for representing both 3D geometry\nand semantics. We polish the stereo modeling and propose the advanced version,\nDSGN++, aiming to enhance effective information flow throughout the 2D-to-3D\npipeline in three main aspects. First, to effectively lift the 2D information\nto stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser\nconnections and extracts depth-guided features. Second, for grasping\ndifferently spaced features, we present a novel stereo volume -- Dual-view\nStereo Volume (DSV) that integrates front-view and top-view features and\nreconstructs sub-voxel depth in the camera frustum. Third, as the foreground\nregion becomes less dominant in 3D space, we propose a multi-modal data editing\nstrategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and\nimproves data efficiency. Without bells and whistles, extensive experiments in\nvarious modality setups on the popular KITTI benchmark show that our method\nconsistently outperforms other camera-based 3D detectors for all categories.\nCode is available at https://github.com/chenyilun95/DSGN2.\n","authors":["Yilun Chen","Shijia Huang","Shu Liu","Bei Yu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2204.03039v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2207.10434v1","updated":"2022-07-21T12:04:16Z","published":"2022-07-21T12:04:16Z","title":"DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using\n  Unsupervised Domain-Classifier Guided Network","summary":"  Shadow removal from a single image is generally still an open problem. Most\nexisting learning-based methods use supervised learning and require a large\nnumber of paired images (shadow and corresponding non-shadow images) for\ntraining. A recent unsupervised method, Mask-ShadowGAN, addresses this\nlimitation. However, it requires a binary mask to represent shadow regions,\nmaking it inapplicable to soft shadows. To address the problem, in this paper,\nwe propose an unsupervised domain-classifier guided shadow removal network,\nDC-ShadowNet. Specifically, we propose to integrate a shadow/shadow-free domain\nclassifier into a generator and its discriminator, enabling them to focus on\nshadow regions. To train our network, we introduce novel losses based on\nphysics-based shadow-free chromaticity, shadow-robust perceptual features, and\nboundary smoothness. Moreover, we show that our unsupervised network can be\nused for test-time training that further improves the results. Our experiments\nshow that all these novel components allow our method to handle soft shadows,\nand also to perform better on hard shadows both quantitatively and\nqualitatively than the existing state-of-the-art shadow removal methods.\n","authors":["Yeying Jin","Aashish Sharma","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2207.10434v1.pdf","comment":"Accepted to ICCV2021,\n  https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal"},{"id":"http://arxiv.org/abs/2207.10433v1","updated":"2022-07-21T12:03:02Z","published":"2022-07-21T12:03:02Z","title":"StreamYOLO: Real-time Object Detection for Streaming Perception","summary":"  The perceptive models of autonomous driving require fast inference within a\nlow latency for safety. While existing works ignore the inevitable\nenvironmental changes after processing, streaming perception jointly evaluates\nthe latency and accuracy into a single metric for video online perception,\nguiding the previous works to search trade-offs between accuracy and speed. In\nthis paper, we explore the performance of real time models on this metric and\nendow the models with the capacity of predicting the future, significantly\nimproving the results for streaming perception. Specifically, we build a simple\nframework with two effective modules. One is a Dual Flow Perception module\n(DFP). It consists of dynamic flow and static flow in parallel to capture\nmoving tendency and basic detection feature, respectively. Trend Aware Loss\n(TAL) is the other module which adaptively generates loss weight for each\nobject with its moving speed. Realistically, we consider multiple velocities\ndriving scene and further propose Velocity-awared streaming AP (VsAP) to\njointly evaluate the accuracy. In this realistic setting, we design a efficient\nmix-velocity training strategy to guide detector perceive any velocities. Our\nsimple method achieves the state-of-the-art performance on Argoverse-HD dataset\nand improves the sAP and VsAP by 4.7% and 8.2% respectively compared to the\nstrong baseline, validating its effectiveness.\n","authors":["Jinrong Yang","Songtao Liu","Zeming Li","Xiaoping Li","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2207.10433v1.pdf","comment":"Extended version of arXiv:2203.12338"},{"id":"http://arxiv.org/abs/2207.10425v1","updated":"2022-07-21T11:41:53Z","published":"2022-07-21T11:41:53Z","title":"KD-MVS: Knowledge Distillation Based Self-supervised Learning for MVS","summary":"  Supervised multi-view stereo (MVS) methods have achieved remarkable progress\nin terms of reconstruction quality, but suffer from the challenge of collecting\nlarge-scale ground-truth depth. In this paper, we propose a novel\nself-supervised training pipeline for MVS based on knowledge distillation,\ntermed \\textit{KD-MVS}, which mainly consists of self-supervised teacher\ntraining and distillation-based student training. Specifically, the teacher\nmodel is trained in a self-supervised fashion using both photometric and\nfeaturemetric consistency. Then we distill the knowledge of the teacher model\nto the student model through probabilistic knowledge transferring. With the\nsupervision of validated knowledge, the student model is able to outperform its\nteacher by a large margin. Extensive experiments performed on multiple datasets\nshow our method can even outperform supervised methods.\n","authors":["Yikang Ding","Qingtian Zhu","Xiangyue Liu","Wentao Yuan","Haotian Zhang","CHi Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.10425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10409v1","updated":"2022-07-21T11:00:44Z","published":"2022-07-21T11:00:44Z","title":"Sequence Models for Drone vs Bird Classification","summary":"  Drone detection has become an essential task in object detection as drone\ncosts have decreased and drone technology has improved. It is, however,\ndifficult to detect distant drones when there is weak contrast, long range, and\nlow visibility. In this work, we propose several sequence classification\narchitectures to reduce the detected false-positive ratio of drone tracks.\nMoreover, we propose a new drone vs. bird sequence classification dataset to\ntrain and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer\nbased sequence classification architectures have been trained on the proposed\ndataset to show the effectiveness of the proposed idea. As experiments show,\nusing sequence information, bird classification and overall F1 scores can be\nincreased by up to 73% and 35%, respectively. Among all sequence classification\nmodels, R(2+1)D-based fully convolutional model yields the best transfer\nlearning and fine-tuning results.\n","authors":["Fatih Cagatay Akyon","Erdem Akagunduz","Sinan Onur Altinuc","Alptekin Temizel"],"pdf_url":"https://arxiv.org/pdf/2207.10409v1.pdf","comment":"Submitted to AVSS 2022"},{"id":"http://arxiv.org/abs/2112.08775v2","updated":"2022-07-21T10:48:49Z","published":"2021-12-16T10:39:09Z","title":"DProST: Dynamic Projective Spatial Transformer Network for 6D Pose\n  Estimation","summary":"  Predicting the object's 6D pose from a single RGB image is a fundamental\ncomputer vision task. Generally, the distance between transformed object\nvertices is employed as an objective function for pose estimation methods.\nHowever, projective geometry in the camera space is not considered in those\nmethods and causes performance degradation. In this regard, we propose a new\npose estimation system based on a projective grid instead of object vertices.\nOur pose estimation method, dynamic projective spatial transformer network\n(DProST), localizes the region of interest grid on the rays in camera space and\ntransforms the grid to object space by estimated pose. The transformed grid is\nused as both a sampling grid and a new criterion of the estimated pose.\nAdditionally, because DProST does not require object vertices, our method can\nbe used in a mesh-less setting by replacing the mesh with a reconstructed\nfeature. Experimental results show that mesh-less DProST outperforms the\nstate-of-the-art mesh-based methods on the LINEMOD and LINEMOD-OCCLUSION\ndataset, and shows competitive performance on the YCBV dataset with mesh data.\nThe source code is available at https://github.com/parkjaewoo0611/DProST\n","authors":["Jaewoo Park","Nam Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2112.08775v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10404v1","updated":"2022-07-21T10:48:37Z","published":"2022-07-21T10:48:37Z","title":"Semantic-aware Modular Capsule Routing for Visual Question Answering","summary":"  Visual Question Answering (VQA) is fundamentally compositional in nature, and\nmany questions are simply answered by decomposing them into modular\nsub-problems. The recent proposed Neural Module Network (NMN) employ this\nstrategy to question answering, whereas heavily rest with off-the-shelf layout\nparser or additional expert policy regarding the network architecture design\ninstead of learning from the data. These strategies result in the\nunsatisfactory adaptability to the semantically-complicated variance of the\ninputs, thereby hindering the representational capacity and generalizability of\nthe model. To tackle this problem, we propose a Semantic-aware modUlar caPsulE\nRouting framework, termed as SUPER, to better capture the instance-specific\nvision-semantic characteristics and refine the discriminative representations\nfor prediction. Particularly, five powerful specialized modules as well as\ndynamic routers are tailored in each layer of the SUPER network, and the\ncompact routing spaces are constructed such that a variety of customizable\nroutes can be sufficiently exploited and the vision-semantic representations\ncan be explicitly calibrated. We comparatively justify the effectiveness and\ngeneralization ability of our proposed SUPER scheme over five benchmark\ndatasets, as well as the parametric-efficient advantage. It is worth\nemphasizing that this work is not to pursue the state-of-the-art results in\nVQA. Instead, we expect that our model is responsible to provide a novel\nperspective towards architecture learning and representation calibration for\nVQA.\n","authors":["Yudong Han","Jianhua Yin","Jianlong Wu","Yinwei Wei","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2207.10404v1.pdf","comment":"12 pages, 4 figures, submitted to IEEE Transactions on Image\n  Processing"},{"id":"http://arxiv.org/abs/2207.10402v1","updated":"2022-07-21T10:42:34Z","published":"2022-07-21T10:42:34Z","title":"Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption","summary":"  Despite encouraging progress in deepfake detection, generalization to unseen\nforgery types remains a significant challenge due to the limited forgery clues\nexplored during training. In contrast, we notice a common phenomenon in\ndeepfake: fake video creation inevitably disrupts the statistical regularity in\noriginal videos. Inspired by this observation, we propose to boost the\ngeneralization of deepfake detection by distinguishing the \"regularity\ndisruption\" that does not appear in real videos. Specifically, by carefully\nexamining the spatial and temporal properties, we propose to disrupt a real\nvideo through a Pseudo-fake Generator and create a wide range of pseudo-fake\nvideos for training. Such practice allows us to achieve deepfake detection\nwithout using fake videos and improves the generalization ability in a simple\nand efficient manner. To jointly capture the spatial and temporal disruptions,\nwe propose a Spatio-Temporal Enhancement block to learn the regularity\ndisruption across space and time on our self-created videos. Through\ncomprehensive experiments, our method exhibits excellent performance on several\ndatasets.\n","authors":["Jiazhi Guan","Hang Zhou","Mingming Gong","Youjian Zhao","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2207.10402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10400v1","updated":"2022-07-21T10:31:39Z","published":"2022-07-21T10:31:39Z","title":"Correspondence Matters for Video Referring Expression Comprehension","summary":"  We investigate the problem of video Referring Expression Comprehension (REC),\nwhich aims to localize the referent objects described in the sentence to visual\nregions in the video frames. Despite the recent progress, existing methods\nsuffer from two problems: 1) inconsistent localization results across video\nframes; 2) confusion between the referent and contextual objects. To this end,\nwe propose a novel Dual Correspondence Network (dubbed as DCNet) which\nexplicitly enhances the dense associations in both the inter-frame and\ncross-modal manners. Firstly, we aim to build the inter-frame correlations for\nall existing instances within the frames. Specifically, we compute the\ninter-frame patch-wise cosine similarity to estimate the dense alignment and\nthen perform the inter-frame contrastive learning to map them close in feature\nspace. Secondly, we propose to build the fine-grained patch-word alignment to\nassociate each patch with certain words. Due to the lack of this kind of\ndetailed annotations, we also predict the patch-word correspondence through the\ncosine similarity. Extensive experiments demonstrate that our DCNet achieves\nstate-of-the-art performance on both video and image REC benchmarks.\nFurthermore, we conduct comprehensive ablation studies and thorough analyses to\nexplore the optimal model designs. Notably, our inter-frame and cross-modal\ncontrastive losses are plug-and-play functions and are applicable to any video\nREC architectures. For example, by building on top of Co-grounding, we boost\nthe performance by 1.48% absolute improvement on Accu.@0.5 for VID-Sentence\ndataset.\n","authors":["Meng Cao","Ji Jiang","Long Chen","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2207.10400v1.pdf","comment":"Accepted by ACM MM"},{"id":"http://arxiv.org/abs/2203.07694v3","updated":"2022-07-21T10:24:23Z","published":"2022-03-15T07:22:52Z","title":"Implicit field supervision for robust non-rigid shape matching","summary":"  Establishing a correspondence between two non-rigidly deforming shapes is one\nof the most fundamental problems in visual computing. Existing methods often\nshow weak resilience when presented with challenges innate to real-world data\nsuch as noise, outliers, self-occlusion etc. On the other hand, auto-decoders\nhave demonstrated strong expressive power in learning geometrically meaningful\nlatent embeddings. However, their use in \\emph{shape analysis} has been\nlimited. In this paper, we introduce an approach based on an auto-decoder\nframework, that learns a continuous shape-wise deformation field over a fixed\ntemplate. By supervising the deformation field for points on-surface and\nregularising for points off-surface through a novel \\emph{Signed Distance\nRegularisation} (SDR), we learn an alignment between the template and shape\n\\emph{volumes}. Trained on clean water-tight meshes, \\emph{without} any\ndata-augmentation, we demonstrate compelling performance on compromised data\nand real-world scans.\n","authors":["Ramana Sundararaman","Gautam Pai","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2203.07694v3.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10398v1","updated":"2022-07-21T10:19:07Z","published":"2022-07-21T10:19:07Z","title":"D2-TPred: Discontinuous Dependency for Trajectory Prediction under\n  Traffic Lights","summary":"  A profound understanding of inter-agent relationships and motion behaviors is\nimportant to achieve high-quality planning when navigating in complex\nscenarios, especially at urban traffic intersections. We present a trajectory\nprediction approach with respect to traffic lights, D2-TPred, which uses a\nspatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG)\nto handle the problem of discontinuous dependency in the spatial-temporal\nspace. Specifically, the SDG is used to capture spatial interactions by\nreconstructing sub-graphs for different agents with dynamic and changeable\ncharacteristics during each frame. The BDG is used to infer motion tendency by\nmodeling the implicit dependency of the current state on priors behaviors,\nespecially the discontinuous motions corresponding to acceleration,\ndeceleration, or turning direction. Moreover, we present a new dataset for\nvehicle trajectory prediction under traffic lights called VTP-TL. Our\nexperimental results show that our model achieves more than {20.45% and 20.78%\n}improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to\nother trajectory prediction algorithms. The dataset and code are available at:\nhttps://github.com/VTP-TL/D2-TPred.\n","authors":["Yuzhen Zhang","Wentong Wang","Weizhi Guo","Pei Lv","Mingliang Xu","Wei Chen","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2207.10398v1.pdf","comment":"Accepted to ECCV2022, 17 pages, 6 figures. Project page:\n  https://github.com/VTP-TL/D2-TPred"},{"id":"http://arxiv.org/abs/2207.10395v1","updated":"2022-07-21T10:12:41Z","published":"2022-07-21T10:12:41Z","title":"Sobolev Training for Implicit Neural Representations with Approximated\n  Image Derivatives","summary":"  Recently, Implicit Neural Representations (INRs) parameterized by neural\nnetworks have emerged as a powerful and promising tool to represent different\nkinds of signals due to its continuous, differentiable properties, showing\nsuperiorities to classical discretized representations. However, the training\nof neural networks for INRs only utilizes input-output pairs, and the\nderivatives of the target output with respect to the input, which can be\naccessed in some cases, are usually ignored. In this paper, we propose a\ntraining paradigm for INRs whose target output is image pixels, to encode image\nderivatives in addition to image values in the neural network. Specifically, we\nuse finite differences to approximate image derivatives. We show how the\ntraining paradigm can be leveraged to solve typical INRs problems, i.e., image\nregression and inverse rendering, and demonstrate this training paradigm can\nimprove the data-efficiency and generalization capabilities of INRs. The code\nof our method is available at\n\\url{https://github.com/megvii-research/Sobolev_INRs}.\n","authors":["Wentao Yuan","Qingtian Zhu","Xiangyue Liu","Yikang Ding","Haotian Zhang","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.10395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.07982v2","updated":"2022-07-21T10:12:23Z","published":"2022-05-16T20:41:45Z","title":"TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion\n  Refinement","summary":"  We present TOCH, a method for refining incorrect 3D hand-object interaction\nsequences using a data prior. Existing hand trackers, especially those that\nrely on very few cameras, often produce visually unrealistic results with\nhand-object intersection or missing contacts. Although correcting such errors\nrequires reasoning about temporal aspects of interaction, most previous works\nfocus on static grasps and contacts. The core of our method are TOCH fields, a\nnovel spatio-temporal representation for modeling correspondences between hands\nand objects during interaction. TOCH fields are a point-wise, object-centric\nrepresentation, which encode the hand position relative to the object.\nLeveraging this novel representation, we learn a latent manifold of plausible\nTOCH fields with a temporal denoising auto-encoder. Experiments demonstrate\nthat TOCH outperforms state-of-the-art 3D hand-object interaction models, which\nare limited to static grasps and contacts. More importantly, our method\nproduces smooth interactions even before and after contact. Using a single\ntrained TOCH model, we quantitatively and qualitatively demonstrate its\nusefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D\nhand-object reconstruction methods and transferring grasps across objects.\n","authors":["Keyang Zhou","Bharat Lal Bhatnagar","Jan Eric Lenssen","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2205.07982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10392v1","updated":"2022-07-21T10:06:01Z","published":"2022-07-21T10:06:01Z","title":"FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic\n  Upsampling","summary":"  We consider the problem of task-agnostic feature upsampling in dense\nprediction where an upsampling operator is required to facilitate both\nregion-sensitive tasks like semantic segmentation and detail-sensitive tasks\nsuch as image matting. Existing upsampling operators often can work well in\neither type of the tasks, but not both. In this work, we present FADE, a novel,\nplug-and-play, and task-agnostic upsampling operator. FADE benefits from three\ndesign choices: i) considering encoder and decoder features jointly in\nupsampling kernel generation; ii) an efficient semi-shift convolutional\noperator that enables granular control over how each feature point contributes\nto upsampling kernels; iii) a decoder-dependent gating mechanism for enhanced\ndetail delineation. We first study the upsampling properties of FADE on toy\ndata and then evaluate it on large-scale semantic segmentation and image\nmatting. In particular, FADE reveals its effectiveness and task-agnostic\ncharacteristic by consistently outperforming recent dynamic upsampling\noperators in different tasks. It also generalizes well across convolutional and\ntransformer architectures with little computational overhead. Our work\nadditionally provides thoughtful insights on what makes for task-agnostic\nupsampling. Code is available at: http://lnkiy.in/fade_in\n","authors":["Hao Lu","Wenze Liu","Hongtao Fu","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2207.10392v1.pdf","comment":"Accepted to ECCV 2022. Code is available at http://lnkiy.in/fade_in"},{"id":"http://arxiv.org/abs/2207.10391v1","updated":"2022-07-21T10:02:57Z","published":"2022-07-21T10:02:57Z","title":"Error Compensation Framework for Flow-Guided Video Inpainting","summary":"  The key to video inpainting is to use correlation information from as many\nreference frames as possible. Existing flow-based propagation methods split the\nvideo synthesis process into multiple steps: flow completion -> pixel\npropagation -> synthesis. However, there is a significant drawback that the\nerrors in each step continue to accumulate and amplify in the next step. To\nthis end, we propose an Error Compensation Framework for Flow-guided Video\nInpainting (ECFVI), which takes advantage of the flow-based method and offsets\nits weaknesses. We address the weakness with the newly designed flow completion\nmodule and the error compensation network that exploits the error guidance map.\nOur approach greatly improves the temporal consistency and the visual quality\nof the completed videos. Experimental results show the superior performance of\nour proposed method with the speed up of x6, compared to the state-of-the-art\nmethods. In addition, we present a new benchmark dataset for evaluation by\nsupplementing the weaknesses of existing test datasets.\n","authors":["Jaeyeon Kang","Seoung Wug Oh","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2207.10391v1.pdf","comment":"ECCV2022 accepted"},{"id":"http://arxiv.org/abs/2207.09582v2","updated":"2022-07-21T10:02:03Z","published":"2022-07-19T23:17:54Z","title":"Segmentation of 3D Dental Images Using Deep Learning","summary":"  3D image segmentation is a recent and crucial step in many medical analysis\nand recognition schemes. In fact, it represents a relevant research subject and\na fundamental challenge due to its importance and influence. This paper\nprovides a multi-phase Deep Learning-based system that hybridizes various\nefficient methods in order to get the best 3D segmentation output. First, to\nreduce the amount of data and accelerate the processing time, the application\nof Decimate compression technique is suggested and justified. We then use a CNN\nmodel to segment dental images into fifteen separated classes. In the end, a\nspecial KNN-based transformation is applied for the purpose of removing\nisolated meshes and of correcting dental forms. Experimentations demonstrate\nthe precision and the robustness of the selected framework applied to 3D dental\nimages within a private clinical benchmark.\n","authors":["Omar Boudraa"],"pdf_url":"https://arxiv.org/pdf/2207.09582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1911.10686v4","updated":"2022-07-21T09:42:28Z","published":"2019-11-25T03:41:00Z","title":"Zero-Shot Imitating Collaborative Manipulation Plans from YouTube\n  Cooking Videos","summary":"  People often watch videos on the web to learn how to cook new recipes,\nassemble furniture or repair a computer. We wish to enable robots with the very\nsame capability. This is challenging; there is a large variation in\nmanipulation actions and some videos even involve multiple persons, who\ncollaborate by sharing and exchanging objects and tools. Furthermore, the\nlearned representations need to be general enough to be transferable to robotic\nsystems. On the other hand, previous work has shown that the space of human\nmanipulation actions has a linguistic, hierarchical structure that relates\nactions to manipulated objects and tools. Building upon this theory of language\nfor action, we propose a system for understanding and executing demonstrated\naction sequences from full-length, real-world cooking videos on the web. The\nsystem takes as input a new, previously unseen cooking video annotated with\nobject labels and bounding boxes, and outputs a collaborative manipulation\naction plan for one or more robotic arms. We demonstrate performance of the\nsystem in a standardized dataset of 100 YouTube cooking videos, as well as in\nsix full-length Youtube videos that include collaborative actions between two\nparticipants. We compare our system with a baseline system that consists of a\nstate-of-the-art action detection baseline and show our system achieves higher\naction detection accuracy. We additionally propose an open-source platform for\nexecuting the learned plans in a simulation environment as well as with an\nactual robotic arm.\n","authors":["Hejia Zhang","Jie Zhong","Stefanos Nikolaidis"],"pdf_url":"https://arxiv.org/pdf/1911.10686v4.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2207.10388v1","updated":"2022-07-21T09:41:22Z","published":"2022-07-21T09:41:22Z","title":"NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition","summary":"  It is challenging for artificial intelligence systems to achieve accurate\nvideo recognition under the scenario of low computation costs. Adaptive\ninference based efficient video recognition methods typically preview videos\nand focus on salient parts to reduce computation costs. Most existing works\nfocus on complex networks learning with video classification based objectives.\nTaking all frames as positive samples, few of them pay attention to the\ndiscrimination between positive samples (salient frames) and negative samples\n(non-salient frames) in supervisions. To fill this gap, in this paper, we\npropose a novel Non-saliency Suppression Network (NSNet), which effectively\nsuppresses the responses of non-salient frames. Specifically, on the frame\nlevel, effective pseudo labels that can distinguish between salient and\nnon-salient frames are generated to guide the frame saliency learning. On the\nvideo level, a temporal attention module is learned under dual video-level\nsupervisions on both the salient and the non-salient representations. Saliency\nmeasurements from both two levels are combined for exploitation of\nmulti-granularity complementary information. Extensive experiments conducted on\nfour well-known benchmarks verify our NSNet not only achieves the\nstate-of-the-art accuracy-efficiency trade-off but also present a significantly\nfaster (2.4~4.3x) practical inference speed than state-of-the-art methods. Our\nproject page is at https://lawrencexia2008.github.io/projects/nsnet .\n","authors":["Boyang Xia","Wenhao Wu","Haoran Wang","Rui Su","Dongliang He","Haosen Yang","Xiaoran Fan","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2207.10388v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10387v1","updated":"2022-07-21T09:40:54Z","published":"2022-07-21T09:40:54Z","title":"Pose for Everything: Towards Category-Agnostic Pose Estimation","summary":"  Existing works on 2D pose estimation mainly focus on a certain category, e.g.\nhuman, animal, and vehicle. However, there are lots of application scenarios\nthat require detecting the poses/keypoints of the unseen class of objects. In\nthis paper, we introduce the task of Category-Agnostic Pose Estimation (CAPE),\nwhich aims to create a pose estimation model capable of detecting the pose of\nany class of object given only a few samples with keypoint definition. To\nachieve this goal, we formulate the pose estimation problem as a keypoint\nmatching problem and design a novel CAPE framework, termed POse Matching\nNetwork (POMNet). A transformer-based Keypoint Interaction Module (KIM) is\nproposed to capture both the interactions among different keypoints and the\nrelationship between the support and query images. We also introduce\nMulti-category Pose (MP-100) dataset, which is a 2D pose dataset of 100 object\ncategories containing over 20K instances and is well-designed for developing\nCAPE algorithms. Experiments show that our method outperforms other baseline\napproaches by a large margin. Codes and data are available at\nhttps://github.com/luminxu/Pose-for-Everything.\n","authors":["Lumin Xu","Sheng Jin","Wang Zeng","Wentao Liu","Chen Qian","Wanli Ouyang","Ping Luo","Xiaogang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.10387v1.pdf","comment":"ECCV 2022 Oral"},{"id":"http://arxiv.org/abs/2207.10379v1","updated":"2022-07-21T09:23:34Z","published":"2022-07-21T09:23:34Z","title":"Temporal Saliency Query Network for Efficient Video Recognition","summary":"  Efficient video recognition is a hot-spot research topic with the explosive\ngrowth of multimedia data on the Internet and mobile devices. Most existing\nmethods select the salient frames without awareness of the class-specific\nsaliency scores, which neglect the implicit association between the saliency of\nframes and its belonging category. To alleviate this issue, we devise a novel\nTemporal Saliency Query (TSQ) mechanism, which introduces class-specific\ninformation to provide fine-grained cues for saliency measurement.\nSpecifically, we model the class-specific saliency measuring process as a\nquery-response task. For each category, the common pattern of it is employed as\na query and the most salient frames are responded to it. Then, the calculated\nsimilarities are adopted as the frame saliency scores. To achieve it, we\npropose a Temporal Saliency Query Network (TSQNet) that includes two\ninstantiations of the TSQ mechanism based on visual appearance similarities and\ntextual event-object relations. Afterward, cross-modality interactions are\nimposed to promote the information exchange between them. Finally, we use the\nclass-specific saliencies of the most confident categories generated by two\nmodalities to perform the selection of salient frames. Extensive experiments\ndemonstrate the effectiveness of our method by achieving state-of-the-art\nresults on ActivityNet, FCVID and Mini-Kinetics datasets. Our project page is\nat https://lawrencexia2008.github.io/projects/tsqnet .\n","authors":["Boyang Xia","Zhihao Wang","Wenhao Wu","Haoran Wang","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2207.10379v1.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2110.08708v4","updated":"2022-07-21T09:13:07Z","published":"2021-10-17T03:19:39Z","title":"Robust Pedestrian Attribute Recognition Using Group Sparsity for\n  Occlusion Videos","summary":"  Occlusion processing is a key issue in pedestrian attribute recognition\n(PAR). Nevertheless, several existing video-based PAR methods have not yet\nconsidered occlusion handling in depth. In this paper, we formulate finding\nnon-occluded frames as sparsity-based temporal attention of a crowded video. In\nthis manner, a model is guided not to pay attention to the occluded frame.\nHowever, temporal sparsity cannot include a correlation between attributes when\nocclusion occurs. For example, \"boots\" and \"shoe color\" cannot be recognized\nwhen the foot is invisible. To solve the uncorrelated attention issue, we also\npropose a novel group sparsity-based temporal attention module. Group sparsity\nis applied across attention weights in correlated attributes. Thus, attention\nweights in a group are forced to pay attention to the same frames. Experimental\nresults showed that the proposed method achieved a higher F1-score than the\nstate-of-the-art methods on two video-based PAR datasets.\n","authors":["Geonu Lee","Kimin Yun","Jungchan Cho"],"pdf_url":"https://arxiv.org/pdf/2110.08708v4.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.10368v1","updated":"2022-07-21T08:53:34Z","published":"2022-07-21T08:53:34Z","title":"Land Classification in Satellite Images by Injecting Traditional\n  Features to CNN Models","summary":"  Deep learning methods have been successfully applied to remote sensing\nproblems for several years. Among these methods, CNN based models have high\naccuracy in solving the land classification problem using satellite or aerial\nimages. Although these models have high accuracy, this generally comes with\nlarge memory size requirements. On the other hand, it is desirable to have\nsmall-sized models for applications, such as the ones implemented on unmanned\naerial vehicles, with low memory space. Unfortunately, small-sized CNN models\ndo not provide high accuracy as with their large-sized versions. In this study,\nwe propose a novel method to improve the accuracy of CNN models, especially the\nones with small size, by injecting traditional features to them. To test the\neffectiveness of the proposed method, we applied it to the CNN models\nSqueezeNet, MobileNetV2, ShuffleNetV2, VGG16, and ResNet50V2 having size 0.5 MB\nto 528 MB. We used the sample mean, gray level co-occurrence matrix features,\nHu moments, local binary patterns, histogram of oriented gradients, and color\ninvariants as traditional features for injection. We tested the proposed method\non the EuroSAT dataset to perform land classification. Our experimental results\nshow that the proposed method significantly improves the land classification\naccuracy especially when applied to small-sized CNN models.\n","authors":["Mehmet Cagri Aksoy","Beril Sirmacek","Cem Unsalan"],"pdf_url":"https://arxiv.org/pdf/2207.10368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10362v1","updated":"2022-07-21T08:43:51Z","published":"2022-07-21T08:43:51Z","title":"LocVTP: Video-Text Pre-training for Temporal Localization","summary":"  Video-Text Pre-training (VTP) aims to learn transferable representations for\nvarious downstream tasks from large-scale web videos. To date, almost all\nexisting VTP methods are limited to retrieval-based downstream tasks, e.g.,\nvideo retrieval, whereas their transfer potentials on localization-based tasks,\ne.g., temporal grounding, are under-explored. In this paper, we experimentally\nanalyze and demonstrate the incompatibility of current VTP methods with\nlocalization tasks, and propose a novel Localization-oriented Video-Text\nPre-training framework, dubbed as LocVTP. Specifically, we perform the\nfine-grained contrastive alignment as a complement to the coarse-grained one by\na clip-word correspondence discovery scheme. To further enhance the temporal\nreasoning ability of the learned feature, we propose a context projection head\nand a temporal aware contrastive loss to perceive the contextual relationships.\nExtensive experiments on four downstream tasks across six datasets demonstrate\nthat our LocVTP achieves state-of-the-art performance on both retrieval-based\nand localization-based tasks. Furthermore, we conduct comprehensive ablation\nstudies and thorough analyses to explore the optimum model designs and training\nstrategies.\n","authors":["Meng Cao","Tianyu Yang","Junwu Weng","Can Zhang","Jue Wang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2207.10362v1.pdf","comment":"Accepted by ECCV2022"},{"id":"http://arxiv.org/abs/2206.08194v2","updated":"2022-07-21T08:40:56Z","published":"2022-06-16T14:08:58Z","title":"Online Segmentation of LiDAR Sequences: Dataset and Algorithm","summary":"  Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.\nHowever, most semantic datasets and algorithms used for LiDAR sequence\nsegmentation operate on $360^\\circ$ frames, causing an acquisition latency\nincompatible with real-time applications. To address this issue, we first\nintroduce HelixNet, a $10$ billion point dataset with fine-grained labels,\ntimestamps, and sensor rotation information necessary to accurately assess the\nreal-time readiness of segmentation algorithms. Second, we propose Helix4D, a\ncompact and efficient spatio-temporal transformer architecture specifically\ndesigned for rotating LiDAR sequences. Helix4D operates on acquisition slices\ncorresponding to a fraction of a full sensor rotation, significantly reducing\nthe total latency. Helix4D reaches accuracy on par with the best segmentation\nalgorithms on HelixNet and SemanticKITTI with a reduction of over $5\\times$ in\nterms of latency and $50\\times$ in model size. The code and data are available\nat: https://romainloiseau.fr/helixnet\n","authors":["Romain Loiseau","Mathieu Aubry","Loïc Landrieu"],"pdf_url":"https://arxiv.org/pdf/2206.08194v2.pdf","comment":"Code and data are available at: https://romainloiseau.fr/helixnet"},{"id":"http://arxiv.org/abs/2207.09675v2","updated":"2022-07-21T08:37:24Z","published":"2022-07-20T06:09:26Z","title":"ERA: Expert Retrieval and Assembly for Early Action Prediction","summary":"  Early action prediction aims to successfully predict the class label of an\naction before it is completely performed. This is a challenging task because\nthe beginning stages of different actions can be very similar, with only minor\nsubtle differences for discrimination. In this paper, we propose a novel Expert\nRetrieval and Assembly (ERA) module that retrieves and assembles a set of\nexperts most specialized at using discriminative subtle differences, to\ndistinguish an input sample from other highly similar samples. To encourage our\nmodel to effectively use subtle differences for early action prediction, we\npush experts to discriminate exclusively between samples that are highly\nsimilar, forcing these experts to learn to use subtle differences that exist\nbetween those samples. Additionally, we design an effective Expert Learning\nRate Optimization method that balances the experts' optimization and leads to\nbetter performance. We evaluate our ERA module on four public action datasets\nand achieve state-of-the-art performance.\n","authors":["Lin Geng Foo","Tianjiao Li","Hossein Rahmani","Qiuhong Ke","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2207.09675v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2206.12923v2","updated":"2022-07-21T08:29:17Z","published":"2022-06-26T16:45:56Z","title":"Video Activity Localisation with Uncertainties in Temporal Boundary","summary":"  Current methods for video activity localisation over time assume implicitly\nthat activity temporal boundaries labelled for model training are determined\nand precise. However, in unscripted natural videos, different activities mostly\ntransit smoothly, so that it is intrinsically ambiguous to determine in\nlabelling precisely when an activity starts and ends over time. Such\nuncertainties in temporal labelling are currently ignored in model training,\nresulting in learning mis-matched video-text correlation with poor\ngeneralisation in test. In this work, we solve this problem by introducing\nElastic Moment Bounding (EMB) to accommodate flexible and adaptive activity\ntemporal boundaries towards modelling universally interpretable video-text\ncorrelation with tolerance to underlying temporal uncertainties in pre-fixed\nannotations. Specifically, we construct elastic boundaries adaptively by mining\nand discovering frame-wise temporal endpoints that can maximise the alignment\nbetween video segments and query sentences. To enable both more accurate\nmatching (segment content attention) and more robust localisation (segment\nelastic boundaries), we optimise the selection of frame-wise endpoints subject\nto segment-wise contents by a novel Guided Attention mechanism. Extensive\nexperiments on three video activity localisation benchmarks demonstrate\ncompellingly the EMB's advantages over existing methods without modelling\nuncertainty.\n","authors":["Jiabo Huang","Hailin Jin","Shaogang Gong","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2206.12923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07929v2","updated":"2022-07-21T08:26:04Z","published":"2022-07-16T12:46:10Z","title":"Towards Lightweight Super-Resolution with Dual Regression Learning","summary":"  Deep neural networks have exhibited remarkable performance in image\nsuper-resolution (SR) tasks by learning a mapping from low-resolution (LR)\nimages to high-resolution (HR) images. However, the SR problem is typically an\nill-posed problem and existing methods would come with several limitations.\nFirst, the possible mapping space of SR can be extremely large since there may\nexist many different HR images that can be downsampled to the same LR image. As\na result, it is hard to directly learn a promising SR mapping from such a large\nspace. Second, it is often inevitable to develop very large models with\nextremely high computational cost to yield promising SR performance. In\npractice, one can use model compression techniques to obtain compact models by\nreducing model redundancy. Nevertheless, it is hard for existing model\ncompression methods to accurately identify the redundant components due to the\nextremely large SR mapping space. To alleviate the first challenge, we propose\na dual regression learning scheme to reduce the space of possible SR mappings.\nSpecifically, in addition to the mapping from LR to HR images, we learn an\nadditional dual regression mapping to estimate the downsampling kernel and\nreconstruct LR images. In this way, the dual mapping acts as a constraint to\nreduce the space of possible mappings. To address the second challenge, we\npropose a lightweight dual regression compression method to reduce model\nredundancy in both layer-level and channel-level based on channel pruning.\nSpecifically, we first develop a channel number search method that minimizes\nthe dual regression loss to determine the redundancy of each layer. Given the\nsearched channel numbers, we further exploit the dual regression manner to\nevaluate the importance of channels and prune the redundant ones. Extensive\nexperiments show the effectiveness of our method in obtaining accurate and\nefficient SR models.\n","authors":["Yong Guo","Jingdong Wang","Qi Chen","Jiezhang Cao","Zeshuai Deng","Yanwu Xu","Jian Chen","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2207.07929v2.pdf","comment":"Journal extension of DRN. arXiv admin note: text overlap with\n  arXiv:2003.07018"},{"id":"http://arxiv.org/abs/2207.09445v2","updated":"2022-07-21T08:18:59Z","published":"2022-07-19T17:58:33Z","title":"PoserNet: Refining Relative Camera Poses Exploiting Object Detections","summary":"  The estimation of the camera poses associated with a set of images commonly\nrelies on feature matches between the images. In contrast, we are the first to\naddress this challenge by using objectness regions to guide the pose estimation\nproblem rather than explicit semantic object detections. We propose Pose\nRefiner Network (PoserNet) a light-weight Graph Neural Network to refine the\napproximate pair-wise relative camera poses. PoserNet exploits associations\nbetween the objectness regions - concisely expressed as bounding boxes - across\nmultiple views to globally refine sparsely connected view graphs. We evaluate\non the 7-Scenes dataset across varied sizes of graphs and show how this process\ncan be beneficial to optimisation-based Motion Averaging algorithms improving\nthe median error on the rotation by 62 degrees with respect to the initial\nestimates obtained based on bounding boxes. Code and data are available at\nhttps://github.com/IIT-PAVIS/PoserNet.\n","authors":["Matteo Taiana","Matteo Toso","Stuart James","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2207.09445v2.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10354v1","updated":"2022-07-21T08:16:31Z","published":"2022-07-21T08:16:31Z","title":"Learning from Data with Noisy Labels Using Temporal Self-Ensemble","summary":"  There are inevitably many mislabeled data in real-world datasets. Because\ndeep neural networks (DNNs) have an enormous capacity to memorize noisy labels,\na robust training scheme is required to prevent labeling errors from degrading\nthe generalization performance of DNNs. Current state-of-the-art methods\npresent a co-training scheme that trains dual networks using samples associated\nwith small losses. In practice, however, training two networks simultaneously\ncan burden computing resources. In this study, we propose a simple yet\neffective robust training scheme that operates by training only a single\nnetwork. During training, the proposed method generates temporal self-ensemble\nby sampling intermediate network parameters from the weight trajectory formed\nby stochastic gradient descent optimization. The loss sum evaluated with these\nself-ensembles is used to identify incorrectly labeled samples. In parallel,\nour method generates multi-view predictions by transforming an input data into\nvarious forms and considers their agreement to identify incorrectly labeled\nsamples. By combining the aforementioned metrics, we present the proposed {\\it\nself-ensemble-based robust training} (SRT) method, which can filter the samples\nwith noisy labels to reduce their influence on training. Experiments on\nwidely-used public datasets demonstrate that the proposed method achieves a\nstate-of-the-art performance in some categories without training the dual\nnetworks.\n","authors":["Jun Ho Lee","Jae Soon Baik","Tae Hwan Hwang","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2207.10354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10255v4","updated":"2022-07-21T08:15:48Z","published":"2022-06-21T11:01:49Z","title":"GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without\n  Bells and Whistles","summary":"  Multi-object tracking (MOT) is among crucial applications in modern advanced\ndriver assistance systems (ADAS) and autonomous driving (AD) systems. The\nglobal nearest neighbor (GNN) filter, as the earliest random vector-based\nBayesian tracking framework, has been adopted in most of state-of-the-arts\ntrackers and widely accepted in the automotive industry. With the development\nof random finite set (RFS) theory, which facilitates a mathematically rigorous\ntreatment of the MOT problem, different variants of RFS-based Bayesian filters\nhave been developed. However, their usefulness in the real traffic for ADAS and\nAD application is still open to doubt. In this paper, it is first demonstrated\nthat the latest RFS-based Bayesian tracking framework could be superior to\ntypical random vector-based Bayesian tracking framework like GNN, via a\nsystematic comparative study of both traditional random vector-based Bayesian\nfilters with rule-based heuristic track maintenance and RFS-based Bayesian\nfilters on the nuScenes validation dataset. Then, an RFS-based tracker, namely\nPoisson multi-Bernoulli filter using the global nearest neighbor (GNN-PMB), is\nproposed to LiDAR-based MOT tasks. This GNN-PMB tracker is simple to use but\ncan achieve competitive results on the nuScenes dataset. Specifically, the\nproposed GNN-PMB tracker outperforms most of the state-of-the-art LiDAR-only\ntrackers and LiDAR and camera fusion-based trackers, ranking the 3rd among all\nLiDAR-only trackers on nuScenes 3D tracking challenge leader board1 at the time\nof submission. Our code is available at here.\n","authors":["Jianan Liu","Liping Bai","Yuxuan Xia","Tao Huang","Bing Zhu","Qing-Long Han"],"pdf_url":"https://arxiv.org/pdf/2206.10255v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10351v1","updated":"2022-07-21T08:12:22Z","published":"2022-07-21T08:12:22Z","title":"Auto Machine Learning for Medical Image Analysis by Unifying the Search\n  on Data Augmentation and Neural Architecture","summary":"  Automated data augmentation, which aims at engineering augmentation policy\nautomatically, recently draw a growing research interest. Many previous\nauto-augmentation methods utilized a Density Matching strategy by evaluating\npolicies in terms of the test-time augmentation performance. In this paper, we\ntheoretically and empirically demonstrated the inconsistency between the train\nand validation set of small-scale medical image datasets, referred to as\nin-domain sampling bias. Next, we demonstrated that the in-domain sampling bias\nmight cause the inefficiency of Density Matching. To address the problem, an\nimproved augmentation search strategy, named Augmented Density Matching, was\nproposed by randomly sampling policies from a prior distribution for training.\nMoreover, an efficient automatical machine learning(AutoML) algorithm was\nproposed by unifying the search on data augmentation and neural architecture.\nExperimental results indicated that the proposed methods outperformed\nstate-of-the-art approaches on MedMNIST, a pioneering benchmark designed for\nAutoML in medical image analysis.\n","authors":["Jianwei Zhang","Dong Li","Lituan Wang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.10351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10025v2","updated":"2022-07-21T07:59:17Z","published":"2022-07-20T16:41:37Z","title":"Learning from Synthetic Data: Facial Expression Classification based on\n  Ensemble of Multi-task Networks","summary":"  Facial expression in-the-wild is essential for various interactive computing\ndomains. Especially, \"Learning from Synthetic Data\" (LSD) is an important topic\nin the facial expression recognition task. In this paper, we propose a\nmulti-task learning-based facial expression recognition approach which consists\nof emotion and appearance learning branches that can share all face\ninformation, and present preliminary results for the LSD challenge introduced\nin the 4th affective behavior analysis in-the-wild (ABAW) competition. Our\nmethod achieved the mean F1 score of 0.71.\n","authors":["Jae-Yeop Jeong","Yeong-Gi Hong","JiYeon Oh","Sumin Hong","Jin-Woo Jeong","Yuchul Jung"],"pdf_url":"https://arxiv.org/pdf/2207.10025v2.pdf","comment":"Page 3, Added reference [2], [33]"},{"id":"http://arxiv.org/abs/2109.06662v3","updated":"2022-07-21T07:53:58Z","published":"2021-09-14T13:11:34Z","title":"Identifying partial mouse brain microscopy images from Allen reference\n  atlas using a contrastively learned semantic space","summary":"  Precise identification of mouse brain microscopy images is a crucial first\nstep when anatomical structures in the mouse brain are to be registered to a\nreference atlas. Practitioners usually rely on manual comparison of images or\ntools that assume the presence of complete images. This work explores Siamese\nNetworks as the method for finding corresponding 2D reference atlas plates for\ngiven partial 2D mouse brain images. Siamese networks are a class of\nconvolutional neural networks (CNNs) that use weight-shared paths to obtain low\ndimensional embeddings of pairs of input images. The correspondence between the\npartial mouse brain image and reference atlas plate is determined based on the\ndistance between low dimensional embeddings of brain slices and atlas plates\nthat are obtained from Siamese networks using contrastive learning. Experiments\nshowed that Siamese CNNs can precisely identify brain slices using the Allen\nmouse brain atlas when training and testing images come from the same source.\nThey achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking\nonly 7.2 seconds to identify 29 images.\n","authors":["Justinas Antanavicius","Roberto Leiras","Raghavendra Selvan"],"pdf_url":"https://arxiv.org/pdf/2109.06662v3.pdf","comment":"Published in the Proceedings of International Workshop on Biomedical\n  Image Registration (WBIR-2022). Source code available at\n  https://github.com/Justinas256/2d-mouse-brain-identification. 12 pages, 6\n  figures"},{"id":"http://arxiv.org/abs/2204.02874v2","updated":"2022-07-21T07:52:00Z","published":"2022-04-06T14:43:42Z","title":"ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound","summary":"  We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.\n","authors":["Yan-Bo Lin","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2204.02874v2.pdf","comment":"ECCV 2022 Oral project page: https://yanbo.ml/project_page/eclipse/"},{"id":"http://arxiv.org/abs/2207.10345v1","updated":"2022-07-21T07:50:50Z","published":"2022-07-21T07:50:50Z","title":"CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution","summary":"  Despite breakthrough advances in image super-resolution (SR) with\nconvolutional neural networks (CNNs), SR has yet to enjoy ubiquitous\napplications due to the high computational complexity of SR networks.\nQuantization is one of the promising approaches to solve this problem. However,\nexisting methods fail to quantize SR models with a bit-width lower than 8 bits,\nsuffering from severe accuracy loss due to fixed bit-width quantization applied\neverywhere. In this work, to achieve high average bit-reduction with less\naccuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ)\nmethod for SR networks that allocates optimal bits to local regions and layers\nadaptively based on the local contents of an input image. To this end, a\ntrainable bit selector module is introduced to determine the proper bit-width\nand quantization level for each layer and a given local image patch. This\nmodule is governed by the quantization sensitivity that is estimated by using\nboth the average magnitude of image gradient of the patch and the standard\ndeviation of the input feature of the layer. The proposed quantization pipeline\nhas been tested on various SR networks and evaluated on several standard\nbenchmarks extensively. Significant reduction in computational complexity and\nthe elevated restoration accuracy clearly demonstrate the effectiveness of the\nproposed CADyQ framework for SR. Codes are available at\nhttps://github.com/Cheeun/CADyQ.\n","authors":["Cheeun Hong","Sungyong Baik","Heewon Kim","Seungjun Nah","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2207.10345v1.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2111.07640v2","updated":"2022-07-21T07:49:29Z","published":"2021-11-15T10:00:06Z","title":"AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head\n  Reenactment","summary":"  We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an\nanimation head reenactment. Different from previous animation head datasets, we\nutilize 3D animation models as the controllable image samplers, which can\nprovide a large amount of head images with their corresponding detailed pose\nannotations. To facilitate a data creation process, we build a semi-automatic\npipeline leveraging an open 3D computer graphics software with a developed\nannotation system. After training with the AnimeCeleb, recent head reenactment\nmodels produce high-quality animation head reenactment results, which are not\nachievable with existing datasets. Furthermore, motivated by metaverse\napplication, we propose a novel pose mapping method and architecture to tackle\na cross-domain head reenactment task. During inference, a user can easily\ntransfer one's motion to an arbitrary animation head. Experiments demonstrate\nthe usefulness of the AnimeCeleb to train animation head reenactment models,\nand the superiority of our cross-domain head reenactment model compared to\nstate-of-the-art methods. Our dataset and code are available at\nhttps://github.com/kangyeolk/AnimeCeleb.\n","authors":["Kangyeol Kim","Sunghyun Park","Jaeseong Lee","Sunghyo Chung","Junsoo Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2111.07640v2.pdf","comment":"40 pages; Accepted to ECCV 2022; code and dataset URL added"},{"id":"http://arxiv.org/abs/2206.04382v2","updated":"2022-07-21T07:43:04Z","published":"2022-06-09T09:50:39Z","title":"CLIP-Actor: Text-Driven Recommendation and Stylization for Animating\n  Human Meshes","summary":"  We propose CLIP-Actor, a text-driven motion recommendation and neural mesh\nstylization system for human mesh animation. CLIP-Actor animates a 3D human\nmesh to conform to a text prompt by recommending a motion sequence and\noptimizing mesh style attributes. We build a text-driven human motion\nrecommendation system by leveraging a large-scale human motion dataset with\nlanguage labels. Given a natural language prompt, CLIP-Actor suggests a\ntext-conforming human motion in a coarse-to-fine manner. Then, our novel\nzero-shot neural style optimization detailizes and texturizes the recommended\nmesh sequence to conform to the prompt in a temporally-consistent and\npose-agnostic manner. This is distinctive in that prior work fails to generate\nplausible results when the pose of an artist-designed mesh does not conform to\nthe text from the beginning. We further propose the spatio-temporal view\naugmentation and mask-weighted embedding attention, which stabilize the\noptimization process by leveraging multi-frame human motion and rejecting\npoorly rendered views. We demonstrate that CLIP-Actor produces plausible and\nhuman-recognizable style 3D human mesh in motion with detailed geometry and\ntexture solely from a natural language prompt.\n","authors":["Kim Youwang","Kim Ji-Yeon","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2206.04382v2.pdf","comment":"Accepted at ECCV 2022. [Project page] https://clip-actor.github.io\n  [Code] https://github.com/postech-ami/CLIP-Actor"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2204.07159v2","updated":"2022-07-21T17:59:32Z","published":"2022-04-14T17:59:39Z","title":"A Level Set Theory for Neural Implicit Evolution under Explicit Flows","summary":"  Coordinate-based neural networks parameterizing implicit surfaces have\nemerged as efficient representations of geometry. They effectively act as\nparametric level sets with the zero-level set defining the surface of interest.\nWe present a framework that allows applying deformation operations defined for\ntriangle meshes onto such implicit surfaces. Several of these operations can be\nviewed as energy-minimization problems that induce an instantaneous flow field\non the explicit surface. Our method uses the flow field to deform parametric\nimplicit surfaces by extending the classical theory of level sets. We also\nderive a consolidated view for existing methods on differentiable surface\nextraction and rendering, by formalizing connections to the level-set theory.\nWe show that these methods drift from the theory and that our approach exhibits\nimprovements for applications like surface smoothing, mean-curvature flow,\ninverse rendering and user-defined editing on implicit geometry.\n","authors":["Ishit Mehta","Manmohan Chandraker","Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2204.07159v2.pdf","comment":"ECCV 2022 (Oral); Project Page at https://ishit.github.io/nie"},{"id":"http://arxiv.org/abs/2207.10664v1","updated":"2022-07-21T17:59:06Z","published":"2022-07-21T17:59:06Z","title":"Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset","summary":"  We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing\nresearch on audiovisual fine-grained categorization. While our community has\nmade great strides in fine-grained visual categorization on images, the\ncounterparts in audio and video fine-grained categorization are relatively\nunexplored. To encourage advancements in this space, we have carefully\nconstructed the SSW60 dataset to enable researchers to experiment with\nclassifying the same set of categories in three different modalities: images,\naudio, and video. The dataset covers 60 species of birds and is comprised of\nimages from existing datasets, and brand new, expert-curated audio and video\ndatasets. We thoroughly benchmark audiovisual classification performance and\nmodality fusion experiments through the use of state-of-the-art transformer\nmethods. Our findings show that performance of audiovisual fusion methods is\nbetter than using exclusively image or audio based methods for the task of\nvideo classification. We also present interesting modality transfer\nexperiments, enabled by the unique construction of SSW60 to encompass three\ndifferent modalities. We hope the SSW60 dataset and accompanying baselines spur\nresearch in this fascinating area.\n","authors":["Grant Van Horn","Rui Qian","Kimberly Wilber","Hartwig Adam","Oisin Mac Aodha","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2207.10664v1.pdf","comment":"ECCV 2022 Camera Ready"},{"id":"http://arxiv.org/abs/2206.15475v2","updated":"2022-07-21T17:54:41Z","published":"2022-06-30T17:59:15Z","title":"Causal Machine Learning: A Survey and Open Problems","summary":"  Causal Machine Learning (CausalML) is an umbrella term for machine learning\nmethods that formalize the data-generation process as a structural causal model\n(SCM). This perspective enables us to reason about the effects of changes to\nthis process (interventions) and what would have happened in hindsight\n(counterfactuals). We categorize work in CausalML into five groups according to\nthe problems they address: (1) causal supervised learning, (2) causal\ngenerative modeling, (3) causal explanations, (4) causal fairness, and (5)\ncausal reinforcement learning. We systematically compare the methods in each\ncategory and point out open problems. Further, we review data-modality-specific\napplications in computer vision, natural language processing, and graph\nrepresentation learning. Finally, we provide an overview of causal benchmarks\nand a critical discussion of the state of this nascent field, including\nrecommendations for future work.\n","authors":["Jean Kaddour","Aengus Lynch","Qi Liu","Matt J. Kusner","Ricardo Silva"],"pdf_url":"https://arxiv.org/pdf/2206.15475v2.pdf","comment":"191 pages. v02. Work in progress. Feedback and comments are highly\n  appreciated!"},{"id":"http://arxiv.org/abs/2207.10659v1","updated":"2022-07-21T17:54:36Z","published":"2022-07-21T17:54:36Z","title":"Novel Class Discovery without Forgetting","summary":"  Humans possess an innate ability to identify and differentiate instances that\nthey are not familiar with, by leveraging and adapting the knowledge that they\nhave acquired so far. Importantly, they achieve this without deteriorating the\nperformance on their earlier learning. Inspired by this, we identify and\nformulate a new, pragmatic problem setting of NCDwF: Novel Class Discovery\nwithout Forgetting, which tasks a machine learning model to incrementally\ndiscover novel categories of instances from unlabeled data, while maintaining\nits performance on the previously seen categories. We propose 1) a method to\ngenerate pseudo-latent representations which act as a proxy for (no longer\navailable) labeled data, thereby alleviating forgetting, 2) a\nmutual-information based regularizer which enhances unsupervised discovery of\nnovel classes, and 3) a simple Known Class Identifier which aids generalized\ninference when the testing data contains instances form both seen and unseen\ncategories. We introduce experimental protocols based on CIFAR-10, CIFAR-100\nand ImageNet-1000 to measure the trade-off between knowledge retention and\nnovel class discovery. Our extensive evaluations reveal that existing models\ncatastrophically forget previously seen categories while identifying novel\ncategories, while our method is able to effectively balance between the\ncompeting objectives. We hope our work will attract further research into this\nnewly identified pragmatic problem setting.\n","authors":["K J Joseph","Sujoy Paul","Gaurav Aggarwal","Soma Biswas","Piyush Rai","Kai Han","Vineeth N Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2207.10659v1.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2202.03469v4","updated":"2022-07-21T17:51:06Z","published":"2022-02-07T19:20:00Z","title":"Locally Random P-adic Alloy Codes with Channel Coding Theorems for\n  Distributed Coded Tensors","summary":"  Tensors, i.e., multi-linear functions, are a fundamental building block of\nmachine learning algorithms. In order to train on large data-sets, it is common\npractice to distribute the computation amongst workers. However, stragglers and\nother faults can severely impact the performance and overall training time. A\nnovel strategy to mitigate these failures is the use of coded computation. We\nintroduce a new metric for analysis called the typical recovery threshold,\nwhich focuses on the most likely event and provide a novel construction of\ndistributed coded tensor operations which are optimal with this measure. We\nshow that our general framework encompasses many other computational schemes\nand metrics as a special case. In particular, we prove that the recovery\nthreshold and the tensor rank can be recovered as a special case of the typical\nrecovery threshold when the probability of noise, i.e., a fault, is equal to\nzero, thereby providing a noisy generalization of noiseless computation as a\nserendipitous result. Far from being a purely theoretical construction, these\ndefinitions lead us to practical random code constructions, i.e., locally\nrandom p-adic alloy codes, which are optimal with respect to the measures. We\nanalyze experiments conducted on Amazon EC2 and establish that they are faster\nand more numerically stable than many other benchmark computation schemes in\npractice, as is predicted by theory.\n","authors":["Pedro Soto","Haibin Guan","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2202.03469v4.pdf","comment":"9 pages, preprint"},{"id":"http://arxiv.org/abs/2207.10650v1","updated":"2022-07-21T17:50:57Z","published":"2022-07-21T17:50:57Z","title":"Deep Learning of Radiative Atmospheric Transfer with an Autoencoder","summary":"  As electro-optical energy from the sun propagates through the atmosphere it\nis affected by radiative transfer effects including absorption, emission, and\nscattering. Modeling these affects is essential for scientific remote sensing\nmeasurements of the earth and atmosphere. For example, hyperspectral imagery is\na form of digital imagery collected with many, often hundreds, of wavelengths\nof light in pixel. The amount of light measured at the sensor is the result of\nemitted sunlight, atmospheric radiative transfer, and the reflectance off the\nmaterials on the ground, all of which vary per wavelength resulting from\nmultiple physical phenomena. Therefore measurements of the ground spectra or\natmospheric constituents requires separating these different contributions per\nwavelength. In this paper, we create an autoencoder similar to denoising\nautoencoders treating the atmospheric affects as 'noise' and ground reflectance\nas truth per spectrum. We generate hundreds of thousands of training samples by\ntaking random samples of spectra from laboratory measurements and adding\natmospheric affects using physics-based modelling via MODTRAN\n(http://modtran.spectral.com/modtran\\_home) by varying atmospheric inputs. This\nprocess ideally could create an autoencoder that would separate atmospheric\neffects and ground reflectance in hyperspectral imagery, a process called\natmospheric compensation which is difficult and time-consuming requiring a\ncombination of heuristic approximations, estimates of physical quantities, and\nphysical modelling. While the accuracy of our method is not as good as other\nmethods in the field, this an important first step in applying the growing\nfield of deep learning of physical principles to atmospheric compensation in\nhyperspectral imagery and remote sensing.\n","authors":["Abigail Basener","Bill Basener"],"pdf_url":"https://arxiv.org/pdf/2207.10650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.02540v5","updated":"2022-07-21T17:47:52Z","published":"2020-09-05T14:26:48Z","title":"A Survey of Deep Learning Architectures for Intelligent Reflecting\n  Surfaces","summary":"  Intelligent reflecting surfaces (IRSs) have recently received significant\nattention for wireless communications because it reduces the hardware\ncomplexity, physical size, weight, and cost of conventional large arrays.\nHowever, deployment of IRS entails dealing with multiple channel links between\nthe base station (BS) and the users. Further, the BS and IRS beamformers\nrequire a joint design, wherein the IRS elements must be rapidly reconfigured.\nData-driven techniques, such as deep learning (DL), are critical in addressing\nthese challenges. The lower computation time and model-free nature of DL makes\nit robust against the data imperfections and environmental changes. At the\nphysical layer, DL has been shown to be effective for IRS signal detection,\nchannel estimation and active/passive beamforming using architectures such as\nsupervised, unsupervised and reinforcement learning. This article provides a\nsynopsis of these techniques for designing DL-based IRS-assisted wireless\nsystems.\n","authors":["Ahmet M. Elbir","Kumar Vijay Mishra"],"pdf_url":"https://arxiv.org/pdf/2009.02540v5.pdf","comment":"7 pages and 5 figures. This work has been submitted to the IEEE for\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2207.10625v1","updated":"2022-07-21T17:31:57Z","published":"2022-07-21T17:31:57Z","title":"A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery","summary":"  In this paper we present a new dynamical systems algorithm for clustering in\nhyperspectral images. The main idea of the algorithm is that data points are\n\\`pushed\\' in the direction of increasing density and groups of pixels that end\nup in the same dense regions belong to the same class. This is essentially a\nnumerical solution of the differential equation defined by the gradient of the\ndensity of data points on the data manifold. The number of classes is automated\nand the resulting clustering can be extremely accurate. In addition to\nproviding a accurate clustering, this algorithm presents a new tool for\nunderstanding hyperspectral data in high dimensions. We evaluate the algorithm\non the Urban (Available at www.tec.ary.mil/Hypercube/) scene comparing\nperformance against the k-means algorithm using pre-identified classes of\nmaterials as ground truth.\n","authors":["William F. Basener","Alexey Castrodad","David Messinger","Jennifer Mahle","Paul Prue"],"pdf_url":"https://arxiv.org/pdf/2207.10625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11834v3","updated":"2022-07-21T17:23:42Z","published":"2022-03-22T16:01:04Z","title":"Improving Generalization in Federated Learning by Seeking Flat Minima","summary":"  Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2203.11834v3.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2203.08914v2","updated":"2022-07-21T17:17:11Z","published":"2022-03-16T19:54:47Z","title":"Knee arthritis severity measurement using deep learning: a publicly\n  available algorithm with a multi-institutional validation showing\n  radiologist-level performance","summary":"  The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a\ncentral criteria for the use of total knee arthroplasty. However, this\nassessment suffers from imprecise standards and a remarkably high inter-reader\nvariability. An algorithmic, automated assessment of KOA severity could improve\noverall outcomes of knee replacement procedures by increasing the\nappropriateness of its use. We propose a novel deep learning-based five-step\nalgorithm to automatically grade KOA from posterior-anterior (PA) views of\nradiographs: (1) image preprocessing (2) localization of knees joints in the\nimage using the YOLO v3-Tiny model, (3) initial assessment of the severity of\nosteoarthritis using a convolutional neural network-based classifier, (4)\nsegmentation of the joints and calculation of the joint space narrowing (JSN),\nand (5), a combination of the JSN and the initial assessment to determine a\nfinal Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation\nmasks used to make the assessment, our algorithm demonstrates a higher degree\nof transparency compared to typical \"black box\" deep learning classifiers. We\nperform a comprehensive evaluation using two public datasets and one dataset\nfrom our institution, and show that our algorithm reaches state-of-the art\nperformance. Moreover, we also collected ratings from multiple radiologists at\nour institution and showed that our algorithm performs at the radiologist\nlevel.\n  The software has been made publicly available at\nhttps://github.com/MaciejMazurowski/osteoarthritis-classification.\n","authors":["Hanxue Gu","Keyu Li","Roy J. Colglazier","Jichen Yang","Michael Lebhar","Jonathan O'Donnell","William A. Jiranek","Richard C. Mather","Rob J. French","Nicholas Said","Jikai Zhang","Christine Park","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2203.08914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06422v2","updated":"2022-07-21T16:59:57Z","published":"2022-06-13T19:05:21Z","title":"Symbolic Regression in Materials Science: Discovering Interatomic\n  Potentials from Data","summary":"  Particle-based modeling of materials at atomic scale plays an important role\nin the development of new materials and understanding of their properties. The\naccuracy of particle simulations is determined by interatomic potentials, which\nallow to calculate the potential energy of an atomic system as a function of\natomic coordinates and potentially other properties. First-principles-based ab\ninitio potentials can reach arbitrary levels of accuracy, however their\naplicability is limited by their high computational cost.\n  Machine learning (ML) has recently emerged as an effective way to offset the\nhigh computational costs of ab initio atomic potentials by replacing expensive\nmodels with highly efficient surrogates trained on electronic structure data.\nAmong a plethora of current methods, symbolic regression (SR) is gaining\ntraction as a powerful \"white-box\" approach for discovering functional forms of\ninteratomic potentials.\n  This contribution discusses the role of symbolic regression in Materials\nScience (MS) and offers a comprehensive overview of current methodological\nchallenges and state-of-the-art results. A genetic programming-based approach\nfor modeling atomic potentials from raw data (consisting of snapshots of atomic\npositions and associated potential energy) is presented and empirically\nvalidated on ab initio electronic structure data.\n","authors":["Bogdan Burlacu","Michael Kommenda","Gabriel Kronberger","Stephan Winkler","Michael Affenzeller"],"pdf_url":"https://arxiv.org/pdf/2206.06422v2.pdf","comment":"Submitted to the GPTP XIX Workshop, June 2-4 2022, University of\n  Michigan, Ann Arbor, Michigan"},{"id":"http://arxiv.org/abs/2207.10603v1","updated":"2022-07-21T16:59:09Z","published":"2022-07-21T16:59:09Z","title":"Unsupervised pre-training of graph transformers on patient population\n  graphs","summary":"  Pre-training has shown success in different areas of machine learning, such\nas Computer Vision, Natural Language Processing (NLP), and medical imaging.\nHowever, it has not been fully explored for clinical data analysis. An immense\namount of clinical records are recorded, but still, data and labels can be\nscarce for data collected in small hospitals or dealing with rare diseases. In\nsuch scenarios, pre-training on a larger set of unlabelled clinical data could\nimprove performance. In this paper, we propose novel unsupervised pre-training\ntechniques designed for heterogeneous, multi-modal clinical data for patient\noutcome prediction inspired by masked language modeling (MLM), by leveraging\ngraph deep learning over population graphs. To this end, we further propose a\ngraph-transformer-based network, designed to handle heterogeneous clinical\ndata. By combining masking-based pre-training with a transformer-based network,\nwe translate the success of masking-based pre-training in other domains to\nheterogeneous clinical data. We show the benefit of our pre-training method in\na self-supervised and a transfer learning setting, utilizing three medical\ndatasets TADPOLE, MIMIC-III, and a Sepsis Prediction Dataset. We find that our\nproposed pre-training methods help in modeling the data at a patient and\npopulation level and improve performance in different fine-tuning tasks on all\ndatasets.\n","authors":["Chantal Pellegrini","Nassir Navab","Anees Kazi"],"pdf_url":"https://arxiv.org/pdf/2207.10603v1.pdf","comment":"20 pages, 3 figures, 20 tables"},{"id":"http://arxiv.org/abs/1911.12426v5","updated":"2022-07-21T16:30:54Z","published":"2019-11-27T21:22:04Z","title":"Conditional Hierarchical Bayesian Tucker Decomposition for Genetic Data\n  Analysis","summary":"  We develop methods for reducing the dimensionality of large data sets, common\nin biomedical applications. Learning about patients using genetic data often\nincludes more features than observations, which makes direct supervised\nlearning difficult. One method of reducing the feature space is to use latent\nDirichlet allocation to group genetic variants in an unsupervised manner.\nLatent Dirichlet allocation describes a patient as a mixture of topics\ncorresponding to genetic variants. This can be generalized as a Bayesian tensor\ndecomposition to account for multiple feature variables. Our most significant\ncontributions are with hierarchical topic modeling. We design distinct methods\nof incorporating hierarchical topic modeling, based on nested Chinese\nrestaurant processes and Pachinko Allocation Machine, into Bayesian tensor\ndecomposition. We apply these models to examine patients with one of four\ncommon types of cancer (breast, lung, prostate, and colorectal) and siblings\nwith and without autism spectrum disorder. We linked the genes with their\nbiological pathways and combine this information into a tensor of patients,\ncounts of their genetic variants, and the genes' membership in pathways. We\nfind that our trained models outperform baseline models, with respect to\ncoherence, by up to 40%.\n","authors":["Adam Sandler","Diego Klabjan","Yuan Luo"],"pdf_url":"https://arxiv.org/pdf/1911.12426v5.pdf","comment":"35 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2111.15664v2","updated":"2022-07-21T16:10:17Z","published":"2021-11-30T18:55:19Z","title":"OCR-free Document Understanding Transformer","summary":"  Understanding document images (e.g., invoices) is a core but challenging task\nsince it requires complex functions such as reading text and a holistic\nunderstanding of the document. Current Visual Document Understanding (VDU)\nmethods outsource the task of reading text to off-the-shelf Optical Character\nRecognition (OCR) engines and focus on the understanding task with the OCR\noutputs. Although such OCR-based approaches have shown promising performance,\nthey suffer from 1) high computational costs for using OCR; 2) inflexibility of\nOCR models on languages or types of document; 3) OCR error propagation to the\nsubsequent process. To address these issues, in this paper, we introduce a\nnovel OCR-free VDU model named Donut, which stands for Document understanding\ntransformer. As the first step in OCR-free VDU research, we propose a simple\narchitecture (i.e., Transformer) with a pre-training objective (i.e.,\ncross-entropy loss). Donut is conceptually simple yet effective. Through\nextensive experiments and analyses, we show a simple OCR-free VDU model, Donut,\nachieves state-of-the-art performances on various VDU tasks in terms of both\nspeed and accuracy. In addition, we offer a synthetic data generator that helps\nthe model pre-training to be flexible in various languages and domains. The\ncode, trained model and synthetic data are available at\nhttps://github.com/clovaai/donut.\n","authors":["Geewook Kim","Teakgyu Hong","Moonbin Yim","Jeongyeon Nam","Jinyoung Park","Jinyeong Yim","Wonseok Hwang","Sangdoo Yun","Dongyoon Han","Seunghyun Park"],"pdf_url":"https://arxiv.org/pdf/2111.15664v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2110.08323v2","updated":"2022-07-21T16:07:06Z","published":"2021-10-15T19:20:25Z","title":"On Learning the Transformer Kernel","summary":"  In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data\ndriven framework for learning the kernel function in Transformers. Our\nframework approximates the Transformer kernel as a dot product between spectral\nfeature maps and learns the kernel by learning the spectral distribution. This\nnot only helps in learning a generic kernel end-to-end, but also reduces the\ntime and space complexity of Transformers from quadratic to linear. We show\nthat KERNELIZED TRANSFORMERS achieve performance comparable to existing\nefficient Transformer architectures, both in terms of accuracy as well as\ncomputational efficiency. Our study also demonstrates that the choice of the\nkernel has a substantial impact on performance, and kernel learning variants\nare competitive alternatives to fixed kernel Transformers, both in long as well\nas short sequence tasks.\n","authors":["Sankalan Pal Chowdhury","Adamos Solomou","Avinava Dubey","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2110.08323v2.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2207.10561v1","updated":"2022-07-21T16:04:37Z","published":"2022-07-21T16:04:37Z","title":"Careful What You Wish For: on the Extraction of Adversarially Trained\n  Models","summary":"  Recent attacks on Machine Learning (ML) models such as evasion attacks with\nadversarial examples and models stealing through extraction attacks pose\nseveral security and privacy threats. Prior work proposes to use adversarial\ntraining to secure models from adversarial examples that can evade the\nclassification of a model and deteriorate its performance. However, this\nprotection technique affects the model's decision boundary and its prediction\nprobabilities, hence it might raise model privacy risks. In fact, a malicious\nuser using only a query access to the prediction output of a model can extract\nit and obtain a high-accuracy and high-fidelity surrogate model. To have a\ngreater extraction, these attacks leverage the prediction probabilities of the\nvictim model. Indeed, all previous work on extraction attacks do not take into\nconsideration the changes in the training process for security purposes. In\nthis paper, we propose a framework to assess extraction attacks on\nadversarially trained models with vision datasets. To the best of our\nknowledge, our work is the first to perform such evaluation. Through an\nextensive empirical study, we demonstrate that adversarially trained models are\nmore vulnerable to extraction attacks than models obtained under natural\ntraining circumstances. They can achieve up to $\\times1.2$ higher accuracy and\nagreement with a fraction lower than $\\times0.75$ of the queries. We\nadditionally find that the adversarial robustness capability is transferable\nthrough extraction attacks, i.e., extracted Deep Neural Networks (DNNs) from\nrobust models show an enhanced accuracy to adversarial examples compared to\nextracted DNNs from naturally trained (i.e. standard) models.\n","authors":["Kacem Khaled","Gabriela Nicolescu","Felipe Gohring de Magalhães"],"pdf_url":"https://arxiv.org/pdf/2207.10561v1.pdf","comment":"To be published in the proceedings of the 19th Annual International\n  Conference on Privacy, Security & Trust (PST 2022). The conference\n  proceedings will be included in IEEE Xplore as in previous editions of the\n  conference"},{"id":"http://arxiv.org/abs/2202.03390v3","updated":"2022-07-21T15:51:54Z","published":"2022-02-07T18:26:32Z","title":"Geometric Multimodal Contrastive Representation Learning","summary":"  Learning representations of multimodal data that are both informative and\nrobust to missing modalities at test time remains a challenging problem due to\nthe inherent heterogeneity of data obtained from different channels. To address\nit, we present a novel Geometric Multimodal Contrastive (GMC) representation\nlearning method consisting of two main components: i) a two-level architecture\nconsisting of modality-specific base encoders, allowing to process an arbitrary\nnumber of modalities to an intermediate representation of fixed dimensionality,\nand a shared projection head, mapping the intermediate representations to a\nlatent representation space; ii) a multimodal contrastive loss function that\nencourages the geometric alignment of the learned representations. We\nexperimentally demonstrate that GMC representations are semantically rich and\nachieve state-of-the-art performance with missing modality information on three\ndifferent learning problems including prediction and reinforcement learning\ntasks.\n","authors":["Petra Poklukar","Miguel Vasco","Hang Yin","Francisco S. Melo","Ana Paiva","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2202.03390v3.pdf","comment":"ICML 2022 Camera ready version"},{"id":"http://arxiv.org/abs/2207.10553v1","updated":"2022-07-21T15:51:30Z","published":"2022-07-21T15:51:30Z","title":"The MABe22 Benchmarks for Representation Learning of Multi-Agent\n  Behavior","summary":"  Real-world behavior is often shaped by complex interactions between multiple\nagents. To scalably study multi-agent behavior, advances in unsupervised and\nself-supervised learning have enabled a variety of different behavioral\nrepresentations to be learned from trajectory data. To date, there does not\nexist a unified set of benchmarks that can enable comparing methods\nquantitatively and systematically across a broad set of behavior analysis\nsettings. We aim to address this by introducing a large-scale, multi-agent\ntrajectory dataset from real-world behavioral neuroscience experiments that\ncovers a range of behavior analysis tasks. Our dataset consists of trajectory\ndata from common model organisms, with 9.6 million frames of mouse data and 4.4\nmillion frames of fly data, in a variety of experimental settings, such as\ndifferent strains, lengths of interaction, and optogenetic stimulation. A\nsubset of the frames also consist of expert-annotated behavior labels.\nImprovements on our dataset corresponds to behavioral representations that work\nacross multiple organisms and is able to capture differences for common\nbehavior analysis tasks.\n","authors":["Jennifer J. Sun","Andrew Ulmer","Dipam Chakraborty","Brian Geuther","Edward Hayes","Heng Jia","Vivek Kumar","Zachary Partridge","Alice Robie","Catherine E. Schretter","Chao Sun","Keith Sheppard","Param Uttarwar","Pietro Perona","Yisong Yue","Kristin Branson","Ann Kennedy"],"pdf_url":"https://arxiv.org/pdf/2207.10553v1.pdf","comment":"Project website:\n  https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset"},{"id":"http://arxiv.org/abs/2207.10552v1","updated":"2022-07-21T15:51:01Z","published":"2022-07-21T15:51:01Z","title":"A Primer on Topological Data Analysis to Support Image Analysis Tasks in\n  Environmental Science","summary":"  Topological data analysis (TDA) is a tool from data science and mathematics\nthat is beginning to make waves in environmental science. In this work, we seek\nto provide an intuitive and understandable introduction to a tool from TDA that\nis particularly useful for the analysis of imagery, namely persistent homology.\nWe briefly discuss the theoretical background but focus primarily on\nunderstanding the output of this tool and discussing what information it can\nglean. To this end, we frame our discussion around a guiding example of\nclassifying satellite images from the Sugar, Fish, Flower, and Gravel Dataset\nproduced for the study of mesocale organization of clouds by Rasp et. al. in\n2020 (arXiv:1906:01906). We demonstrate how persistent homology and its\nvectorization, persistence landscapes, can be used in a workflow with a simple\nmachine learning algorithm to obtain good results, and explore in detail how we\ncan explain this behavior in terms of image-level features. One of the core\nstrengths of persistent homology is how interpretable it can be, so throughout\nthis paper we discuss not just the patterns we find, but why those results are\nto be expected given what we know about the theory of persistent homology. Our\ngoal is that a reader of this paper will leave with a better understanding of\nTDA and persistent homology, be able to identify problems and datasets of their\nown for which persistent homology could be helpful, and gain an understanding\nof results they obtain from applying the included GitHub example code.\n","authors":["Lander Ver Hoef","Henry Adams","Emily J. King","Imme Ebert-Uphoff"],"pdf_url":"https://arxiv.org/pdf/2207.10552v1.pdf","comment":"This work has been submitted to Artificial Intelligence for the Earth\n  Systems (AIES). Copyright in this work may be transferred without further\n  notice"},{"id":"http://arxiv.org/abs/2207.10551v1","updated":"2022-07-21T15:50:22Z","published":"2022-07-21T15:50:22Z","title":"Scaling Laws vs Model Architectures: How does Inductive Bias Influence\n  Scaling?","summary":"  There have been a lot of interest in the scaling properties of Transformer\nmodels. However, not much has been done on the front of investigating the\neffect of scaling properties of different inductive biases and model\narchitectures. Do model architectures scale differently? If so, how does\ninductive bias affect scaling behaviour? How does this influence upstream\n(pretraining) and downstream (transfer)? This paper conducts a systematic study\nof scaling behaviour of ten diverse model architectures such as Transformers,\nSwitch Transformers, Universal Transformers, Dynamic convolutions, Performers,\nand recently proposed MLP-Mixers. Via extensive experiments, we show that (1)\narchitecture is an indeed an important consideration when performing scaling\nand (2) the best performing model can fluctuate at different scales. We believe\nthat the findings outlined in this work has significant implications to how\nmodel architectures are currently evaluated in the community.\n","authors":["Yi Tay","Mostafa Dehghani","Samira Abnar","Hyung Won Chung","William Fedus","Jinfeng Rao","Sharan Narang","Vinh Q. Tran","Dani Yogatama","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2207.10551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.06126v4","updated":"2022-07-21T15:48:43Z","published":"2021-09-13T17:05:43Z","title":"Neural Network Guided Evolutionary Fuzzing for Finding Traffic\n  Violations of Autonomous Vehicles","summary":"  Self-driving cars and trucks, autonomous vehicles (AVs), should not be\naccepted by regulatory bodies and the public until they have much higher\nconfidence in their safety and reliability -- which can most practically and\nconvincingly be achieved by testing. But existing testing methods are\ninadequate for checking the end-to-end behaviors of AV controllers against\ncomplex, real-world corner cases involving interactions with multiple\nindependent agents such as pedestrians and human-driven vehicles. While\ntest-driving AVs on streets and highways fails to capture many rare events,\nexisting simulation-based testing methods mainly focus on simple scenarios and\ndo not scale well for complex driving situations that require sophisticated\nawareness of the surroundings. To address these limitations, we propose a new\nfuzz testing technique, called AutoFuzz, which can leverage widely-used AV\nsimulators' API grammars to generate semantically and temporally valid complex\ndriving scenarios (sequences of scenes). To efficiently search for traffic\nviolations-inducing scenarios in a large search space, we propose a constrained\nneural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation\nof our prototype on one state-of-the-art learning-based controller, two\nrule-based controllers, and one industrial-grade controller in five scenarios\nshows that AutoFuzz efficiently finds hundreds of traffic violations in\nhigh-fidelity simulation environments. For each scenario, AutoFuzz can find on\naverage 10-39% more unique traffic violations than the best-performing baseline\nmethod. Further, fine-tuning the learning-based controller with the traffic\nviolations found by AutoFuzz successfully reduced the traffic violations found\nin the new version of the AV controller software.\n","authors":["Ziyuan Zhong","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2109.06126v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14755v4","updated":"2022-07-21T15:33:21Z","published":"2021-10-27T20:30:57Z","title":"Algorithmic encoding of protected characteristics in image-based models\n  for disease detection","summary":"  It has been rightfully emphasized that the use of AI for clinical decision\nmaking could amplify health disparities. An algorithm may encode protected\ncharacteristics, and then use this information for making predictions due to\nundesirable correlations in the (historical) training data. It remains unclear\nhow we can establish whether such information is actually used. Besides the\nscarcity of data from underserved populations, very little is known about how\ndataset biases manifest in predictive models and how this may result in\ndisparate performance. This article aims to shed some light on these issues by\nexploring new methodology for subgroup analysis in image-based disease\ndetection models. We utilize two publicly available chest X-ray datasets,\nCheXpert and MIMIC-CXR, to study performance disparities across race and\nbiological sex in deep learning models. We explore test set resampling,\ntransfer learning, multitask learning, and model inspection to assess the\nrelationship between the encoding of protected characteristics and disease\ndetection performance across subgroups. We confirm subgroup disparities in\nterms of shifted true and false positive rates which are partially removed\nafter correcting for population and prevalence shifts in the test sets. We\nfurther find a previously used transfer learning method to be insufficient for\nestablishing whether specific patient information is used for making\npredictions. The proposed combination of test-set resampling, multitask\nlearning, and model inspection reveals valuable new insights about the way\nprotected characteristics are encoded in the feature representations of deep\nneural networks.\n","authors":["Ben Glocker","Charles Jones","Melanie Bernhardt","Stefan Winzeck"],"pdf_url":"https://arxiv.org/pdf/2110.14755v4.pdf","comment":"Code available on https://github.com/biomedia-mira/chexploration"},{"id":"http://arxiv.org/abs/2207.10541v1","updated":"2022-07-21T15:29:35Z","published":"2022-07-21T15:29:35Z","title":"Optimal precision for GANs","summary":"  When learning disconnected distributions, Generative adversarial networks\n(GANs) are known to face model misspecification. Indeed, a continuous mapping\nfrom a unimodal latent distribution to a disconnected one is impossible, so\nGANs necessarily generate samples outside of the support of the target\ndistribution. This raises a fundamental question: what is the latent space\npartition that minimizes the measure of these areas? Building on a recent\nresult of geometric measure theory, we prove that an optimal GANs must\nstructure its latent space as a 'simplicial cluster' - a Voronoi partition\nwhere cells are convex cones - when the dimension of the latent space is larger\nthan the number of modes. In this configuration, each Voronoi cell maps to a\ndistinct mode of the data. We derive both an upper and a lower bound on the\noptimal precision of GANs learning disconnected manifolds. Interestingly, these\ntwo bounds have the same order of decrease: $\\sqrt{\\log m}$, $m$ being the\nnumber of modes. Finally, we perform several experiments to exhibit the\ngeometry of the latent space and experimentally show that GANs have a geometry\nwith similar properties to the theoretical one.\n","authors":["Thibaut Issenhuth","Ugo Tanielian","Jérémie Mary","David Picard"],"pdf_url":"https://arxiv.org/pdf/2207.10541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10530v1","updated":"2022-07-21T15:11:51Z","published":"2022-07-21T15:11:51Z","title":"Neural Network Learning of Chemical Bond Representations in Spectral\n  Indices and Features","summary":"  In this paper we investigate neural networks for classification in\nhyperspectral imaging with a focus on connecting the architecture of the\nnetwork with the physics of the sensing and materials present. Spectroscopy is\nthe process of measuring light reflected or emitted by a material as a function\nwavelength. Molecular bonds present in the material have vibrational\nfrequencies which affect the amount of light measured at each wavelength. Thus\nthe measured spectrum contains information about the particular chemical\nconstituents and types of bonds. For example, chlorophyll reflects more light\nin the near-IR rage (800-900nm) than in the red (625-675nm) range, and this\ndifference can be measured using a normalized vegetation difference index\n(NDVI), which is commonly used to detect vegetation presence, health, and type\nin imagery collected at these wavelengths. In this paper we show that the\nweights in a Neural Network trained on different vegetation classes learn to\nmeasure this difference in reflectance. We then show that a Neural Network\ntrained on a more complex set of ten different polymer materials will learn\nspectral 'features' evident in the weights for the network, and these features\ncan be used to reliably distinguish between the different types of polymers.\nExamination of the weights provides a human-interpretable understanding of the\nnetwork.\n","authors":["Bill Basener"],"pdf_url":"https://arxiv.org/pdf/2207.10530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13979v2","updated":"2022-07-21T15:11:23Z","published":"2022-06-27T12:30:44Z","title":"Attack Agnostic Dataset: Towards Generalization and Stabilization of\n  Audio DeepFake Detection","summary":"  Audio DeepFakes allow the creation of high-quality, convincing utterances and\ntherefore pose a threat due to its potential applications such as impersonation\nor fake news. Methods for detecting these manipulations should be characterized\nby good generalization and stability leading to robustness against attacks\nconducted with techniques that are not explicitly included in the training. In\nthis work, we introduce Attack Agnostic Dataset - a combination of two audio\nDeepFakes and one anti-spoofing datasets that, thanks to the disjoint use of\nattacks, can lead to better generalization of detection methods. We present a\nthorough analysis of current DeepFake detection methods and consider different\naudio features (front-ends). In addition, we propose a model based on LCNN with\nLFCC and mel-spectrogram front-end, which not only is characterized by a good\ngeneralization and stability results but also shows improvement over LFCC-based\nmode - we decrease standard deviation on all folds and EER in two folds by up\nto 5%.\n","authors":["Piotr Kawa","Marcin Plata","Piotr Syga"],"pdf_url":"https://arxiv.org/pdf/2206.13979v2.pdf","comment":"Proceedings of INTERSPEECH 2022 (Updated version: corrected ASVspoof\n  dataset description)"},{"id":"http://arxiv.org/abs/2112.03478v5","updated":"2022-07-21T14:53:49Z","published":"2021-12-07T03:55:03Z","title":"Generative Adversarial Networks for Labeled Acceleration Data\n  Augmentation for Structural Damage Detection","summary":"  There has been a major advance in the field of Data Science in the last few\ndecades, and these have been utilized for different engineering disciplines and\napplications. Artificial Intelligence (AI), Machine Learning (ML) and Deep\nLearning (DL) algorithms have been utilized for civil Structural Health\nMonitoring (SHM) especially for damage detection applications using sensor\ndata. Although ML and DL methods show superior learning skills for complex data\nstructures, they require plenty of data for training. However, in SHM, data\ncollection from civil structures can be expensive and time taking; particularly\ngetting useful data (damage associated data) can be challenging. The objective\nof this study is to address the data scarcity problem for damage detection\napplications. This paper employs 1-D Wasserstein Deep Convolutional Generative\nAdversarial Networks using Gradient Penalty (1-D WDCGAN-GP) for synthetic\nlabelled acceleration data generation. Then, the generated data is augmented\nwith varying ratios for the training dataset of a 1-D Deep Convolutional Neural\nNetwork (1-D DCNN) for damage detection application. The damage detection\nresults show that the 1-D WDCGAN-GP can be successfully utilized to tackle data\nscarcity in vibration-based damage detection applications of civil structures.\n  Keywords: Structural Health Monitoring (SHM), Structural Damage Detection,\n1-D Deep Convolutional Neural Networks (1-D DCNN), 1-D Generative Adversarial\nNetworks (1-D GAN), Wasserstein Generative Adversarial Networks with Gradient\nPenalty (WGAN-GP)\n","authors":["Furkan Luleci","F. Necati Catbas","Onur Avci"],"pdf_url":"https://arxiv.org/pdf/2112.03478v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10517v1","updated":"2022-07-21T14:51:58Z","published":"2022-07-21T14:51:58Z","title":"MQRetNN: Multi-Horizon Time Series Forecasting with Retrieval\n  Augmentation","summary":"  Multi-horizon probabilistic time series forecasting has wide applicability to\nreal-world tasks such as demand forecasting. Recent work in neural time-series\nforecasting mainly focus on the use of Seq2Seq architectures. For example,\nMQTransformer - an improvement of MQCNN - has shown the state-of-the-art\nperformance in probabilistic demand forecasting. In this paper, we consider\nincorporating cross-entity information to enhance model performance by adding a\ncross-entity attention mechanism along with a retrieval mechanism to select\nwhich entities to attend over. We demonstrate how our new neural architecture,\nMQRetNN, leverages the encoded contexts from a pretrained baseline model on the\nentire population to improve forecasting accuracy. Using MQCNN as the baseline\nmodel (due to computational constraints, we do not use MQTransformer), we first\nshow on a small demand forecasting dataset that it is possible to achieve ~3%\nimprovement in test loss by adding a cross-entity attention mechanism where\neach entity attends to all others in the population. We then evaluate the model\nwith our proposed retrieval methods - as a means of approximating an attention\nover a large population - on a large-scale demand forecasting application with\nover 2 million products and observe ~1% performance gain over the MQCNN\nbaseline.\n","authors":["Sitan Yang","Carson Eisenach","Dhruv Madeka"],"pdf_url":"https://arxiv.org/pdf/2207.10517v1.pdf","comment":"Accepted at 8th SIGKDD International Workshop on Mining and Learning\n  from Time Series"},{"id":"http://arxiv.org/abs/2204.07049v2","updated":"2022-07-21T14:33:51Z","published":"2022-04-14T15:54:01Z","title":"Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for\n  Robotic Bin Picking","summary":"  In this paper, we propose an iterative self-training framework for\nsim-to-real 6D object pose estimation to facilitate cost-effective robotic\ngrasping. Given a bin-picking scenario, we establish a photo-realistic\nsimulator to synthesize abundant virtual data, and use this to train an initial\npose estimation network. This network then takes the role of a teacher model,\nwhich generates pose predictions for unlabeled real data. With these\npredictions, we further design a comprehensive adaptive selection scheme to\ndistinguish reliable results, and leverage them as pseudo labels to update a\nstudent model for pose estimation on real data. To continuously improve the\nquality of pseudo labels, we iterate the above steps by taking the trained\nstudent model as a new teacher and re-label real data using the refined teacher\nmodel. We evaluate our method on a public benchmark and our newly-released\ndataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.\nOur method is also able to improve robotic bin-picking success by 19.54%,\ndemonstrating the potential of iterative sim-to-real solutions for robotic\napplications.\n","authors":["Kai Chen","Rui Cao","Stephen James","Yichuan Li","Yun-Hui Liu","Pieter Abbeel","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2204.07049v2.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10495v1","updated":"2022-07-21T14:21:34Z","published":"2022-07-21T14:21:34Z","title":"A Forgotten Danger in DNN Supervision Testing: Generating and Detecting\n  True Ambiguity","summary":"  Deep Neural Networks (DNNs) are becoming a crucial component of modern\nsoftware systems, but they are prone to fail under conditions that are\ndifferent from the ones observed during training (out-of-distribution inputs)\nor on inputs that are truly ambiguous, i.e., inputs that admit multiple classes\nwith nonzero probability in their ground truth labels. Recent work proposed DNN\nsupervisors to detect high-uncertainty inputs before their possible\nmisclassification leads to any harm. To test and compare the capabilities of\nDNN supervisors, researchers proposed test generation techniques, to focus the\ntesting effort on high-uncertainty inputs that should be recognized as\nanomalous by supervisors. However, existing test generators can only produce\nout-of-distribution inputs. No existing model- and supervisor-independent\ntechnique supports the generation of truly ambiguous test inputs.\n  In this paper, we propose a novel way to generate ambiguous inputs to test\nDNN supervisors and used it to empirically compare several existing supervisor\ntechniques. In particular, we propose AmbiGuess to generate ambiguous samples\nfor image classification problems. AmbiGuess is based on gradient-guided\nsampling in the latent space of a regularized adversarial autoencoder.\nMoreover, we conducted what is - to the best of our knowledge - the most\nextensive comparative study of DNN supervisors, considering their capabilities\nto detect 4 distinct types of high-uncertainty inputs, including truly\nambiguous ones.\n","authors":["Michael Weiss","André García Gómez","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2207.10495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.01702v3","updated":"2022-07-21T14:08:46Z","published":"2022-04-01T18:18:12Z","title":"Personalized Prediction of Future Lesion Activity and Treatment Effect\n  in Multiple Sclerosis from Baseline MRI","summary":"  Precision medicine for chronic diseases such as multiple sclerosis (MS)\ninvolves choosing a treatment which best balances efficacy and side\neffects/preferences for individual patients. Making this choice as early as\npossible is important, as delays in finding an effective therapy can lead to\nirreversible disability accrual. To this end, we present the first deep neural\nnetwork model for individualized treatment decisions from baseline magnetic\nresonance imaging (MRI) (with clinical information if available) for MS\npatients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)\nlesion counts on follow-up MRI on multiple treatments and (b) estimates the\nconditional average treatment effect (CATE), as defined by the predicted future\nsuppression of NE-T2 lesions, between different treatment options relative to\nplacebo. Our model is validated on a proprietary federated dataset of 1817\nmulti-sequence MRIs acquired from MS patients during four multi-centre\nrandomized clinical trials. Our framework achieves high average precision in\nthe binarized regression of future NE-T2 lesions on five different treatments,\nidentifies heterogeneous treatment effects, and provides a personalized\ntreatment recommendation that accounts for treatment-associated risk (e.g. side\neffects, patient preference, administration difficulties).\n","authors":["Joshua Durso-Finley","Jean-Pierre R. Falet","Brennan Nichyporuk","Douglas L. Arnold","Tal Arbel"],"pdf_url":"https://arxiv.org/pdf/2204.01702v3.pdf","comment":"Accepted to MIDL 2022"},{"id":"http://arxiv.org/abs/2207.10488v1","updated":"2022-07-21T14:06:04Z","published":"2022-07-21T14:06:04Z","title":"Metropolis Monte Carlo sampling: convergence, localization transition\n  and optimality","summary":"  Among random sampling methods, Markov Chain Monte Carlo algorithms are\nforemost. Using a combination of analytical and numerical approaches, we study\ntheir convergence properties towards the steady state, within a random walk\nMetropolis scheme. We show that the deviations from the target steady-state\ndistribution feature a localization transition as a function of the\ncharacteristic length of the attempted jumps defining the random walk. This\ntransition changes drastically the error which is introduced by incomplete\nconvergence, and discriminates two regimes where the relaxation mechanism is\nlimited respectively by diffusion and by rejection.\n","authors":["Alexei D. Chepelianskii","Satya N. Majumdar","Hendrik Schawe","Emmanuel Trizac"],"pdf_url":"https://arxiv.org/pdf/2207.10488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.15264v3","updated":"2022-07-21T14:03:13Z","published":"2021-11-30T10:23:06Z","title":"EdiBERT, a generative model for image editing","summary":"  Advances in computer vision are pushing the limits of im-age manipulation,\nwith generative models sampling detailed images on various tasks. However, a\nspecialized model is often developed and trained for each specific task, even\nthough many image edition tasks share similarities. In denoising, inpainting,\nor image compositing, one always aims at generating a realistic image from a\nlow-quality one. In this paper, we aim at making a step towards a unified\napproach for image editing. To do so, we propose EdiBERT, a bi-directional\ntransformer trained in the discrete latent space built by a vector-quantized\nauto-encoder. We argue that such a bidirectional model is suited for image\nmanipulation since any patch can be re-sampled conditionally to the whole\nimage. Using this unique and straightforward training objective, we show that\nthe resulting model matches state-of-the-art performances on a wide variety of\ntasks: image denoising, image completion, and image composition.\n","authors":["Thibaut Issenhuth","Ugo Tanielian","Jérémie Mary","David Picard"],"pdf_url":"https://arxiv.org/pdf/2111.15264v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10486v1","updated":"2022-07-21T14:00:52Z","published":"2022-07-21T14:00:52Z","title":"Bayesian Recurrent Units and the Forward-Backward Algorithm","summary":"  Using Bayes's theorem, we derive a unit-wise recurrence as well as a backward\nrecursion similar to the forward-backward algorithm. The resulting Bayesian\nrecurrent units can be integrated as recurrent neural networks within deep\nlearning frameworks, while retaining a probabilistic interpretation from the\ndirect correspondence with hidden Markov models. Whilst the contribution is\nmainly theoretical, experiments on speech recognition indicate that adding the\nderived units at the end of state-of-the-art recurrent architectures can\nimprove the performance at a very low cost in terms of trainable parameters.\n","authors":["Alexandre Bittar","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2207.10486v1.pdf","comment":"Submitted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2207.10482v1","updated":"2022-07-21T13:54:52Z","published":"2022-07-21T13:54:52Z","title":"LPYOLO: Low Precision YOLO for Face Detection on FPGA","summary":"  In recent years, number of edge computing devices and artificial intelligence\napplications on them have advanced excessively. In edge computing, decision\nmaking processes and computations are moved from servers to edge devices.\nHence, cheap and low power devices are required. FPGAs are very low power,\ninclined to do parallel operations and deeply suitable devices for running\nConvolutional Neural Networks (CNN) which are the fundamental unit of an\nartificial intelligence application. Face detection on surveillance systems is\nthe most expected application on the security market. In this work, TinyYolov3\narchitecture is redesigned and deployed for face detection. It is a CNN based\nobject detection method and developed for embedded systems. PYNQ-Z2 is selected\nas a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on\nit. Redesigned TinyYolov3 model is defined in numerous bit width precisions\nwith Brevitas library which brings fundamental CNN layers and activations in\ninteger quantized form. Then, the model is trained in a quantized structure\nwith WiderFace dataset. In order to decrease latency and power consumption,\nonchip memory of the FPGA is configured as a storage of whole network\nparameters and the last activation function is modified as rescaled HardTanh\ninstead of Sigmoid. Also, high degree of parallelism is applied to logical\nresources of the FPGA. The model is converted to an HLS based application with\nusing FINN framework and FINN-HLS library which includes the layer definitions\nin C++. Later, the model is synthesized and deployed. CPU of the SoC is\nemployed with multithreading mechanism and responsible for preprocessing,\npostprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total\nboard power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP\naccuracy rate on Easy category of the WiderFace are achieved with 4 bits\nprecision model.\n","authors":["Bestami Günay","Sefa Burak Okcu","Hasan Şakir Bilge"],"pdf_url":"https://arxiv.org/pdf/2207.10482v1.pdf","comment":"Accepted to MVML2022"},{"id":"http://arxiv.org/abs/2207.09657v2","updated":"2022-07-21T13:43:46Z","published":"2022-07-20T05:22:26Z","title":"Multigraph Topology Design for Cross-Silo Federated Learning","summary":"  Cross-silo federated learning utilizes a few hundred reliable data silos with\nhigh-speed access links to jointly train a model. While this approach becomes a\npopular setting in federated learning, designing a robust topology to reduce\nthe training time is still an open problem. In this paper, we present a new\nmultigraph topology for cross-silo federated learning. We first construct the\nmultigraph using the overlay graph. We then parse this multigraph into\ndifferent simple graphs with isolated nodes. The existence of isolated nodes\nallows us to perform model aggregation without waiting for other nodes, hence\nreducing the training time. We further propose a new distributed learning\nalgorithm to use with our multigraph topology. The intensive experiments on\npublic datasets show that our proposed method significantly reduces the\ntraining time compared with recent state-of-the-art topologies while ensuring\nconvergence and maintaining the model's accuracy.\n","authors":["Binh X. Nguyen","Tuong Do","Hien Nguyen","Vuong Pham","Toan Tran","Erman Tjiputra","Quang Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2207.09657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10469v1","updated":"2022-07-21T13:33:40Z","published":"2022-07-21T13:33:40Z","title":"Fast Data Driven Estimation of Cluster Number in Multiplex Images using\n  Embedded Density Outliers","summary":"  The usage of chemical imaging technologies is becoming a routine\naccompaniment to traditional methods in pathology. Significant technological\nadvances have developed these next generation techniques to provide rich,\nspatially resolved, multidimensional chemical images. The rise of digital\npathology has significantly enhanced the synergy of these imaging modalities\nwith optical microscopy and immunohistochemistry, enhancing our understanding\nof the biological mechanisms and progression of diseases. Techniques such as\nimaging mass cytometry provide labelled multidimensional (multiplex) images of\nspecific components used in conjunction with digital pathology techniques.\nThese powerful techniques generate a wealth of high dimensional data that\ncreate significant challenges in data analysis. Unsupervised methods such as\nclustering are an attractive way to analyse these data, however, they require\nthe selection of parameters such as the number of clusters. Here we propose a\nmethodology to estimate the number of clusters in an automatic data-driven\nmanner using a deep sparse autoencoder to embed the data into a lower\ndimensional space. We compute the density of regions in the embedded space, the\nmajority of which are empty, enabling the high density regions to be detected\nas outliers and provide an estimate for the number of clusters. This framework\nprovides a fully unsupervised and data-driven method to analyse\nmultidimensional data. In this work we demonstrate our method using 45\nmultiplex imaging mass cytometry datasets. Moreover, our model is trained using\nonly one of the datasets and the learned embedding is applied to the remaining\n44 images providing an efficient process for data analysis. Finally, we\ndemonstrate the high computational efficiency of our method which is two orders\nof magnitude faster than estimating via computing the sum squared distances as\na function of cluster number.\n","authors":["Spencer A. Thomas"],"pdf_url":"https://arxiv.org/pdf/2207.10469v1.pdf","comment":"8 pages, 6 figures, conference paper"},{"id":"http://arxiv.org/abs/2207.09980v2","updated":"2022-07-21T13:33:26Z","published":"2022-07-20T15:39:30Z","title":"ReFactorGNNs: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective","summary":"  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactorGNNs. Across a\nmultitude of well-established KGC benchmarks, our ReFactorGNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n","authors":["Yihong Chen","Pushkar Mishra","Luca Franceschi","Pasquale Minervini","Pontus Stenetorp","Sebastian Riedel"],"pdf_url":"https://arxiv.org/pdf/2207.09980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.12019v2","updated":"2022-07-21T13:26:08Z","published":"2021-05-25T15:38:28Z","title":"On learning parametric distributions from quantized samples","summary":"  We consider the problem of learning parametric distributions from their\nquantized samples in a network. Specifically, $n$ agents or sensors observe\nindependent samples of an unknown parametric distribution; and each of them\nuses $k$ bits to describe its observed sample to a central processor whose goal\nis to estimate the unknown distribution. First, we establish a generalization\nof the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in\nterms of Generalized Fisher information. Then, we develop minimax lower bounds\non the estimation error for two losses: general $L_p$-norms and the related\nWasserstein loss from optimal transport.\n","authors":["Septimia Sarbu","Abdellatif Zaidi"],"pdf_url":"https://arxiv.org/pdf/2105.12019v2.pdf","comment":"Short version accepted for publication at the IEEE Information Theory\n  Symposium (ISIT) 2021; this version contains the detailed proofs with some\n  minor corrections"},{"id":"http://arxiv.org/abs/1602.08927v3","updated":"2022-07-21T13:15:18Z","published":"2016-02-29T12:05:53Z","title":"High-Dimensional $L_2$Boosting: Rate of Convergence","summary":"  Boosting is one of the most significant developments in machine learning.\nThis paper studies the rate of convergence of $L_2$Boosting, which is tailored\nfor regression, in a high-dimensional setting. Moreover, we introduce so-called\n\\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection\nestimator which applies ordinary least squares to the variables selected in the\nfirst stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal\nBoosting\\textquotedblright\\ where after each step an orthogonal projection is\nconducted. We show that both post-$L_2$Boosting and the orthogonal boosting\nachieve the same rate of convergence as LASSO in a sparse, high-dimensional\nsetting. We show that the rate of convergence of the classical $L_2$Boosting\ndepends on the design matrix described by a sparse eigenvalue constant. To show\nthe latter results, we derive new approximation results for the pure greedy\nalgorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also\nintroduce feasible rules for early stopping, which can be easily implemented\nand used in applied work. Our results also allow a direct comparison between\nLASSO and boosting which has been missing from the literature. Finally, we\npresent simulation studies and applications to illustrate the relevance of our\ntheoretical results and to provide insights into the practical aspects of\nboosting. In these simulation studies, post-$L_2$Boosting clearly outperforms\nLASSO.\n","authors":["Ye Luo","Martin Spindler","Jannis Kück"],"pdf_url":"https://arxiv.org/pdf/1602.08927v3.pdf","comment":"19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,\n  62J07, 41A25; secondary 49M15, 68Q32"},{"id":"http://arxiv.org/abs/2207.09860v2","updated":"2022-07-21T13:11:05Z","published":"2022-07-20T12:51:06Z","title":"Learning to Solve Soft-Constrained Vehicle Routing Problems with\n  Lagrangian Relaxation","summary":"  Vehicle Routing Problems (VRPs) in real-world applications often come with\nvarious constraints, therefore bring additional computational challenges to\nexact solution methods or heuristic search approaches. The recent idea to learn\nheuristic move patterns from sample data has become increasingly promising to\nreduce solution developing costs. However, using learning-based approaches to\naddress more types of constrained VRP remains a challenge. The difficulty lies\nin controlling for constraint violations while searching for optimal solutions.\nTo overcome this challenge, we propose a Reinforcement Learning based method to\nsolve soft-constrained VRPs by incorporating the Lagrangian relaxation\ntechnique and using constrained policy optimization. We apply the method on\nthree common types of VRPs, the Travelling Salesman Problem with Time Windows\n(TSPTW), the Capacitated VRP (CVRP) and the Capacitated VRP with Time Windows\n(CVRPTW), to show the generalizability of the proposed method. After comparing\nto existing RL-based methods and open-source heuristic solvers, we demonstrate\nits competitive performance in finding solutions with a good balance in travel\ndistance, constraint violations and inference speed.\n","authors":["Qiaoyue Tang","Yangzhe Kong","Lemeng Pan","Choonmeng Lee"],"pdf_url":"https://arxiv.org/pdf/2207.09860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.09818v2","updated":"2022-07-21T13:06:28Z","published":"2021-01-24T22:46:08Z","title":"Encrypted Internet traffic classification using a supervised Spiking\n  Neural Network","summary":"  Internet traffic recognition is an essential tool for access providers since\nrecognizing traffic categories related to different data packets transmitted on\na network help them define adapted priorities. That means, for instance, high\npriority requirements for an audio conference and low ones for a file transfer,\nto enhance user experience. As internet traffic becomes increasingly encrypted,\nthe mainstream classic traffic recognition technique, payload inspection, is\nrendered ineffective. This paper uses machine learning techniques for encrypted\ntraffic classification, looking only at packet size and time of arrival.\nSpiking neural networks (SNN), largely inspired by how biological neurons\noperate, were used for two reasons. Firstly, they are able to recognize\ntime-related data packet features. Secondly, they can be implemented\nefficiently on neuromorphic hardware with a low energy footprint. Here we used\na very simple feedforward SNN, with only one fully-connected hidden layer, and\ntrained in a supervised manner using the newly introduced method known as\nSurrogate Gradient Learning. Surprisingly, such a simple SNN reached an\naccuracy of 95.9% on ISCX datasets, outperforming previous approaches. Besides\nbetter accuracy, there is also a very significant improvement on simplicity:\ninput size, number of neurons, trainable parameters are all reduced by one to\nfour orders of magnitude. Next, we analyzed the reasons for this good accuracy.\nIt turns out that, beyond spatial (i.e. packet size) features, the SNN also\nexploits temporal ones, mostly the nearly synchronous (within a 200ms range)\narrival times of packets with certain sizes. Taken together, these results show\nthat SNNs are an excellent fit for encrypted internet traffic classification:\nthey can be more accurate than conventional artificial neural networks (ANN),\nand they could be implemented efficiently on low power embedded systems.\n","authors":["Ali Rasteh","Florian Delpech","Carlos Aguilar-Melchor","Romain Zimmer","Saeed Bagheri Shouraki","Timothée Masquelier"],"pdf_url":"https://arxiv.org/pdf/2101.09818v2.pdf","comment":"22 pages, 8 figures. Neurocomputing (2022)"},{"id":"http://arxiv.org/abs/2110.12379v4","updated":"2022-07-21T13:00:15Z","published":"2021-10-24T07:48:51Z","title":"Variational quantum algorithm for Gaussian discrete solitons and their\n  boson sampling","summary":"  In the context of quantum information, highly nonlinear regimes, such as\nthose supporting solitons, are marginally investigated. We miss general methods\nfor quantum solitons, although they can act as entanglement generators or as\nself-organized quantum processors. We develop a computational approach that\nuses a neural network as a variational ansatz for quantum solitons in an array\nof waveguides. By training the resulting phase-space quantum machine learning\nmodel, we find different soliton solutions varying the number of particles and\ninteraction strength. We consider Gaussian states that enable measuring the\ndegree of entanglement and sampling the probability distribution of\nmany-particle events. We also determine the probability of generating particle\npairs and unveil that soliton bound states emit correlated pairs. These results\nmay have a role in boson sampling with nonlinear systems and in quantum\nprocessors for entangled nonlinear waves.\n","authors":["Claudio Conti"],"pdf_url":"https://arxiv.org/pdf/2110.12379v4.pdf","comment":"Minor changes. 21 figures and 20 pages"},{"id":"http://arxiv.org/abs/2105.09401v2","updated":"2022-07-21T12:58:21Z","published":"2021-05-19T21:01:41Z","title":"Contrastive Learning with Complex Heterogeneity","summary":"  With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and are characterized with multiple\nlabels, thus exhibiting the co-existence of multiple types of heterogeneity.\nAlthough state-of-the-art techniques are good at modeling complex heterogeneity\nwith sufficient label information, such label information can be quite\nexpensive to obtain in real applications. Recently, researchers pay great\nattention to contrastive learning due to its prominent performance by utilizing\nrich unlabeled data. However, existing work on contrastive learning is not able\nto address the problem of false negative pairs, i.e., some `negative' pairs may\nhave similar representations if they have the same label. To overcome the\nissues, in this paper, we propose a unified heterogeneous learning framework,\nwhich combines both the weighted unsupervised contrastive loss and the weighted\nsupervised contrastive loss to model multiple types of heterogeneity. We first\nprovide a theoretical analysis showing that the vanilla contrastive learning\nloss easily leads to the sub-optimal solution in the presence of false negative\npairs, whereas the proposed weighted loss could automatically adjust the weight\nbased on the similarity of the learned representations to mitigate this issue.\nExperimental results on real-world data sets demonstrate the effectiveness and\nthe efficiency of the proposed framework modeling multiple types of\nheterogeneity.\n","authors":["Lecheng Zheng","Jinjun Xiong","Yada Zhu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2105.09401v2.pdf","comment":"Accepted by KDD22"},{"id":"http://arxiv.org/abs/1806.00582v2","updated":"2022-07-21T12:33:15Z","published":"2018-06-02T04:45:58Z","title":"Federated Learning with Non-IID Data","summary":"  Federated learning enables resource-constrained edge compute devices, such as\nmobile phones and IoT devices, to learn a shared model for prediction, while\nkeeping the training data local. This decentralized approach to train models\nprovides privacy, security, regulatory and economic benefits. In this work, we\nfocus on the statistical challenge of federated learning when local data is\nnon-IID. We first show that the accuracy of federated learning reduces\nsignificantly, by up to 55% for neural networks trained for highly skewed\nnon-IID data, where each client device trains only on a single class of data.\nWe further show that this accuracy reduction can be explained by the weight\ndivergence, which can be quantified by the earth mover's distance (EMD) between\nthe distribution over classes on each device and the population distribution.\nAs a solution, we propose a strategy to improve training on non-IID data by\ncreating a small subset of data which is globally shared between all the edge\ndevices. Experiments show that accuracy can be increased by 30% for the\nCIFAR-10 dataset with only 5% globally shared data.\n","authors":["Yue Zhao","Meng Li","Liangzhen Lai","Naveen Suda","Damon Civin","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/1806.00582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10442v1","updated":"2022-07-21T12:26:45Z","published":"2022-07-21T12:26:45Z","title":"Estimation of Non-Crossing Quantile Regression Process with Deep ReQU\n  Neural Networks","summary":"  We propose a penalized nonparametric approach to estimating the quantile\nregression process (QRP) in a nonseparable model using rectifier quadratic unit\n(ReQU) activated deep neural networks and introduce a novel penalty function to\nenforce non-crossing of quantile regression curves. We establish the\nnon-asymptotic excess risk bounds for the estimated QRP and derive the mean\nintegrated squared error for the estimated QRP under mild smoothness and\nregularity conditions. To establish these non-asymptotic risk and estimation\nerror bounds, we also develop a new error bound for approximating $C^s$ smooth\nfunctions with $s >0$ and their derivatives using ReQU activated neural\nnetworks. This is a new approximation result for ReQU networks and is of\nindependent interest and may be useful in other problems. Our numerical\nexperiments demonstrate that the proposed method is competitive with or\noutperforms two existing methods, including methods using reproducing kernels\nand random forests, for nonparametric quantile regression.\n","authors":["Guohao Shen","Yuling Jiao","Yuanyuan Lin","Joel L. Horowitz","Jian Huang"],"pdf_url":"https://arxiv.org/pdf/2207.10442v1.pdf","comment":"44 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2207.10441v1","updated":"2022-07-21T12:25:03Z","published":"2022-07-21T12:25:03Z","title":"Deep Audio Waveform Prior","summary":"  Convolutional neural networks contain strong priors for generating natural\nlooking images [1]. These priors enable image denoising, super resolution, and\ninpainting in an unsupervised manner. Previous attempts to demonstrate similar\nideas in audio, namely deep audio priors, (i) use hand picked architectures\nsuch as harmonic convolutions, (ii) only work with spectrogram input, and (iii)\nhave been used mostly for eliminating Gaussian noise [2]. In this work we show\nthat existing SOTA architectures for audio source separation contain deep\npriors even when working with the raw waveform. Deep priors can be discovered\nby training a neural network to generate a single corrupted signal when given\nwhite noise as input. A network with relevant deep priors is likely to generate\na cleaner version of the signal before converging on the corrupted signal. We\ndemonstrate this restoration effect with several corruptions: background noise,\nreverberations, and a gap in the signal (audio inpainting).\n","authors":["Arnon Turetzky","Tzvi Michelson","Yossi Adi","Shmuel Peleg"],"pdf_url":"https://arxiv.org/pdf/2207.10441v1.pdf","comment":"Interspeech 2022"},{"id":"http://arxiv.org/abs/2207.10430v1","updated":"2022-07-21T12:01:03Z","published":"2022-07-21T12:01:03Z","title":"The Neural Race Reduction: Dynamics of Abstraction in Gated Networks","summary":"  Our theoretical understanding of deep learning has not kept pace with its\nempirical success. While network architecture is known to be critical, we do\nnot yet understand its effect on learned representations and network behavior,\nor how this architecture should reflect task structure.In this work, we begin\nto address this gap by introducing the Gated Deep Linear Network framework that\nschematizes how pathways of information flow impact learning dynamics within an\narchitecture. Crucially, because of the gating, these networks can compute\nnonlinear functions of their input. We derive an exact reduction and, for\ncertain cases, exact solutions to the dynamics of learning. Our analysis\ndemonstrates that the learning dynamics in structured networks can be\nconceptualized as a neural race with an implicit bias towards shared\nrepresentations, which then govern the model's ability to systematically\ngeneralize, multi-task, and transfer. We validate our key insights on\nnaturalistic datasets and with relaxed assumptions. Taken together, our work\ngives rise to general hypotheses relating neural architecture to learning and\nprovides a mathematical approach towards understanding the design of more\ncomplex architectures and the role of modularity and compositionality in\nsolving real-world problems. The code and results are available at\nhttps://www.saxelab.org/gated-dln .\n","authors":["Andrew M. Saxe","Shagun Sodhani","Sam Lewallen"],"pdf_url":"https://arxiv.org/pdf/2207.10430v1.pdf","comment":"ICML 2022; 23 pages; 10 figures"},{"id":"http://arxiv.org/abs/2207.10415v1","updated":"2022-07-21T11:14:47Z","published":"2022-07-21T11:14:47Z","title":"Log Barriers for Safe Black-box Optimization with Application to Safe\n  Reinforcement Learning","summary":"  Optimizing noisy functions online, when evaluating the objective requires\nexperiments on a deployed system, is a crucial task arising in manufacturing,\nrobotics and many others. Often, constraints on safe inputs are unknown ahead\nof time, and we only obtain noisy information, indicating how close we are to\nviolating the constraints. Yet, safety must be guaranteed at all times, not\nonly for the final output of the algorithm.\n  We introduce a general approach for seeking a stationary point in high\ndimensional non-linear stochastic optimization problems in which maintaining\nsafety during learning is crucial. Our approach called LB-SGD is based on\napplying stochastic gradient descent (SGD) with a carefully chosen adaptive\nstep size to a logarithmic barrier approximation of the original problem. We\nprovide a complete convergence analysis of non-convex, convex, and\nstrongly-convex smooth constrained problems, with first-order and zeroth-order\nfeedback. Our approach yields efficient updates and scales better with\ndimensionality compared to existing approaches.\n  We empirically compare the sample complexity and the computational cost of\nour method with existing safe learning approaches. Beyond synthetic benchmarks,\nwe demonstrate the effectiveness of our approach on minimizing constraint\nviolation in policy search tasks in safe reinforcement learning (RL).\n","authors":["Ilnura Usmanova","Yarden As","Maryam Kamgarpour","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2207.10415v1.pdf","comment":"34 pages, 8 pages of appendix"},{"id":"http://arxiv.org/abs/2207.10409v1","updated":"2022-07-21T11:00:44Z","published":"2022-07-21T11:00:44Z","title":"Sequence Models for Drone vs Bird Classification","summary":"  Drone detection has become an essential task in object detection as drone\ncosts have decreased and drone technology has improved. It is, however,\ndifficult to detect distant drones when there is weak contrast, long range, and\nlow visibility. In this work, we propose several sequence classification\narchitectures to reduce the detected false-positive ratio of drone tracks.\nMoreover, we propose a new drone vs. bird sequence classification dataset to\ntrain and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer\nbased sequence classification architectures have been trained on the proposed\ndataset to show the effectiveness of the proposed idea. As experiments show,\nusing sequence information, bird classification and overall F1 scores can be\nincreased by up to 73% and 35%, respectively. Among all sequence classification\nmodels, R(2+1)D-based fully convolutional model yields the best transfer\nlearning and fine-tuning results.\n","authors":["Fatih Cagatay Akyon","Erdem Akagunduz","Sinan Onur Altinuc","Alptekin Temizel"],"pdf_url":"https://arxiv.org/pdf/2207.10409v1.pdf","comment":"Submitted to AVSS 2022"},{"id":"http://arxiv.org/abs/2202.03874v4","updated":"2022-07-21T10:48:22Z","published":"2022-02-01T04:28:48Z","title":"Combining Intra-Risk and Contagion Risk for Enterprise Bankruptcy\n  Prediction Using Graph Neural Networks","summary":"  Predicting the bankruptcy risk of small and medium-sized enterprises (SMEs)\nis an important step for financial institutions when making decisions about\nloans. Existing studies in both finance and AI research fields, however, tend\nto only consider either the intra-risk or contagion risk of enterprises,\nignoring their interactions and combinatorial effects. This study for the first\ntime considers both types of risk and their joint effects in bankruptcy\nprediction. Specifically, we first propose an enterprise intra-risk encoder\nbased on statistically significant enterprise risk indicators for its\nintra-risk learning. Then, we propose an enterprise contagion risk encoder\nbased on enterprise relation information from an enterprise knowledge graph for\nits contagion risk embedding. In particular, the contagion risk encoder\nincludes both the newly proposed Hyper-Graph Neural Networks and Heterogeneous\nGraph Neural Networks, which can model contagion risk in two different aspects,\ni.e. common risk factors based on hyperedges and direct diffusion risk from\nneighbors, respectively. To evaluate the model, we collect real-world\nmulti-sources data on SMEs and build a novel benchmark dataset called SMEsD. We\nprovide open access to the dataset, which is expected to further promote\nresearch on financial risk analysis. Experiments on SMEsD against twelve\nstate-of-the-art baselines demonstrate the effectiveness of the proposed model\nfor bankruptcy prediction.\n","authors":["Yu Zhao","Shaopeng Wei","Yu Guo","Qing Yang","Xingyan Chen","Qing Li","Fuzhen Zhuang","Ji Liu","Gang Kou"],"pdf_url":"https://arxiv.org/pdf/2202.03874v4.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2207.03851v2","updated":"2022-07-21T10:23:51Z","published":"2022-07-08T12:04:25Z","title":"Storehouse: a Reinforcement Learning Environment for Optimizing\n  Warehouse Management","summary":"  Warehouse Management Systems have been evolving and improving thanks to new\nData Intelligence techniques. However, many current optimizations have been\napplied to specific cases or are in great need of manual interaction. Here is\nwhere Reinforcement Learning techniques come into play, providing\nautomatization and adaptability to current optimization policies. In this\npaper, we present Storehouse, a customizable environment that generalizes the\ndefinition of warehouse simulations for Reinforcement Learning. We also\nvalidate this environment against state-of-the-art reinforcement learning\nalgorithms and compare these results to human and random policies.\n","authors":["Julen Cestero","Marco Quartulli","Alberto Maria Metelli","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2207.03851v2.pdf","comment":"9 pages, 6 figures, accepted in WCCI 2022"},{"id":"http://arxiv.org/abs/2204.00492v2","updated":"2022-07-21T10:05:49Z","published":"2022-04-01T14:51:38Z","title":"Provable concept learning for interpretable predictions using\n  variational autoencoders","summary":"  In safety-critical applications, practitioners are reluctant to trust neural\nnetworks when no interpretable explanations are available. Many attempts to\nprovide such explanations revolve around pixel-based attributions or use\npreviously known concepts. In this paper we aim to provide explanations by\nprovably identifying \\emph{high-level, previously unknown ground-truth\nconcepts}. To this end, we propose a probabilistic modeling framework to derive\n(C)oncept (L)earning and (P)rediction (CLAP) -- a VAE-based classifier that\nuses visually interpretable concepts as predictors for a simple classifier.\nAssuming a generative model for the ground-truth concepts, we prove that CLAP\nis able to identify them while attaining optimal classification accuracy. Our\nexperiments on synthetic datasets verify that CLAP identifies distinct\nground-truth concepts on synthetic datasets and yields promising results on the\nmedical Chest X-Ray dataset.\n","authors":["Armeen Taeb","Nicolo Ruggeri","Carina Schnuck","Fanny Yang"],"pdf_url":"https://arxiv.org/pdf/2204.00492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10391v1","updated":"2022-07-21T10:02:57Z","published":"2022-07-21T10:02:57Z","title":"Error Compensation Framework for Flow-Guided Video Inpainting","summary":"  The key to video inpainting is to use correlation information from as many\nreference frames as possible. Existing flow-based propagation methods split the\nvideo synthesis process into multiple steps: flow completion -> pixel\npropagation -> synthesis. However, there is a significant drawback that the\nerrors in each step continue to accumulate and amplify in the next step. To\nthis end, we propose an Error Compensation Framework for Flow-guided Video\nInpainting (ECFVI), which takes advantage of the flow-based method and offsets\nits weaknesses. We address the weakness with the newly designed flow completion\nmodule and the error compensation network that exploits the error guidance map.\nOur approach greatly improves the temporal consistency and the visual quality\nof the completed videos. Experimental results show the superior performance of\nour proposed method with the speed up of x6, compared to the state-of-the-art\nmethods. In addition, we present a new benchmark dataset for evaluation by\nsupplementing the weaknesses of existing test datasets.\n","authors":["Jaeyeon Kang","Seoung Wug Oh","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2207.10391v1.pdf","comment":"ECCV2022 accepted"},{"id":"http://arxiv.org/abs/2207.10390v1","updated":"2022-07-21T09:58:38Z","published":"2022-07-21T09:58:38Z","title":"On the Implementation of a Reinforcement Learning-based Capacity Sharing\n  Algorithm in O-RAN","summary":"  The capacity sharing problem in Radio Access Network (RAN) slicing deals with\nthe distribution of the capacity available in each RAN node among various RAN\nslices to satisfy their traffic demands and efficiently use the radio\nresources. While several capacity sharing algorithmic solutions have been\nproposed in the literature, their practical implementation still remains as a\ngap. In this paper, the implementation of a Reinforcement Learning-based\ncapacity sharing algorithm over the O-RAN architecture is discussed, providing\ninsights into the operation of the involved interfaces and the containerization\nof the solution. Moreover, the description of the testbed implemented to\nvalidate the solution is included and some performance and validation results\nare presented.\n","authors":["Irene Vilà","Oriol Sallent","Jordi Pérez-Romero"],"pdf_url":"https://arxiv.org/pdf/2207.10390v1.pdf","comment":"Submitted to IEEE GLOBECOM 2022 workshop on NextGenRAN"},{"id":"http://arxiv.org/abs/2206.06957v2","updated":"2022-07-21T09:58:12Z","published":"2022-06-14T16:22:54Z","title":"Continual-Learning-as-a-Service (CLaaS): On-Demand Efficient Adaptation\n  of Predictive Models","summary":"  Predictive machine learning models nowadays are often updated in a stateless\nand expensive way. The two main future trends for companies that want to build\nmachine learning-based applications and systems are real-time inference and\ncontinual updating. Unfortunately, both trends require a mature infrastructure\nthat is hard and costly to realize on-premise. This paper defines a novel\nsoftware service and model delivery infrastructure termed Continual\nLearning-as-a-Service (CLaaS) to address these issues. Specifically, it\nembraces continual machine learning and continuous integration techniques. It\nprovides support for model updating and validation tools for data scientists\nwithout an on-premise solution and in an efficient, stateful and easy-to-use\nmanner. Finally, this CL model service is easy to encapsulate in any machine\nlearning infrastructure or cloud system. This paper presents the design and\nimplementation of a CLaaS instantiation, called LiquidBrain, evaluated in two\nreal-world scenarios. The former is a robotic object recognition setting using\nthe CORe50 dataset while the latter is a named category and attribute\nprediction using the DeepFashion-C dataset in the fashion domain. Our\npreliminary results suggest the usability and efficiency of the Continual\nLearning model services and the effectiveness of the solution in addressing\nreal-world use-cases regardless of where the computation happens in the\ncontinuum Edge-Cloud.\n","authors":["Rudy Semola","Vincenzo Lomonaco","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2206.06957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1907.03532v2","updated":"2022-07-21T09:57:31Z","published":"2019-07-01T03:49:01Z","title":"Classification of Macromolecule Type Based on Sequences of Amino Acids\n  Using Deep Learning","summary":"  The classification of amino acids and their sequence analysis plays a vital\nrole in life sciences and is a challenging task. This article uses and compares\nstate-of-the-art deep learning models like convolution neural networks (CNN),\nlong short-term memory (LSTM), and gated recurrent units (GRU) to solve\nmacromolecule classification problems using amino acids. These models have\nefficient frameworks for solving a broad spectrum of complex learning problems\ncompared to traditional machine learning techniques. We use word embedding to\nrepresent the amino acid sequences as vectors. The CNN extracts features from\namino acid sequences, which are treated as vectors, then fed to the models\nmentioned above to train a robust classifier. Our results show that word2vec as\nembedding combined with VGG-16 performs better than LSTM and GRU. The proposed\napproach gets an error rate of 1.5%.\n","authors":["Sarwar Khan","Faisal Ghaffar","Imad ali","qazi mazhar"],"pdf_url":"https://arxiv.org/pdf/1907.03532v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2207.10384v1","updated":"2022-07-21T09:35:38Z","published":"2022-07-21T09:35:38Z","title":"Detecting and Preventing Shortcut Learning for Fair Medical AI using\n  Shortcut Testing (ShorT)","summary":"  Machine learning (ML) holds great promise for improving healthcare, but it is\ncritical to ensure that its use will not propagate or amplify health\ndisparities. An important step is to characterize the (un)fairness of ML models\n- their tendency to perform differently across subgroups of the population -\nand to understand its underlying mechanisms. One potential driver of\nalgorithmic unfairness, shortcut learning, arises when ML models base\npredictions on improper correlations in the training data. However, diagnosing\nthis phenomenon is difficult, especially when sensitive attributes are causally\nlinked with disease. Using multi-task learning, we propose the first method to\nassess and mitigate shortcut learning as a part of the fairness assessment of\nclinical ML systems, and demonstrate its application to clinical tasks in\nradiology and dermatology. Finally, our approach reveals instances when\nshortcutting is not responsible for unfairness, highlighting the need for a\nholistic approach to fairness mitigation in medical AI.\n","authors":["Alexander Brown","Nenad Tomasev","Jan Freyberg","Yuan Liu","Alan Karthikesalingam","Jessica Schrouff"],"pdf_url":"https://arxiv.org/pdf/2207.10384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.00553v2","updated":"2022-07-21T09:14:07Z","published":"2022-02-01T16:52:16Z","title":"Neural Tangent Kernel Beyond the Infinite-Width Limit: Effects of Depth\n  and Initialization","summary":"  Neural Tangent Kernel (NTK) is widely used to analyze overparametrized neural\nnetworks due to the famous result by Jacot et al. (2018): in the infinite-width\nlimit, the NTK is deterministic and constant during training. However, this\nresult cannot explain the behavior of deep networks, since it generally does\nnot hold if depth and width tend to infinity simultaneously. In this paper, we\nstudy the NTK of fully-connected ReLU networks with depth comparable to width.\nWe prove that the NTK properties depend significantly on the depth-to-width\nratio and the distribution of parameters at initialization. In fact, our\nresults indicate the importance of the three phases in the hyperparameter space\nidentified in Poole et al. (2016): ordered, chaotic and the edge of chaos\n(EOC). We derive exact expressions for the NTK dispersion in the\ninfinite-depth-and-width limit in all three phases and conclude that the NTK\nvariability grows exponentially with depth at the EOC and in the chaotic phase\nbut not in the ordered phase. We also show that the NTK of deep networks may\nstay constant during training only in the ordered phase and discuss how the\nstructure of the NTK matrix changes during training.\n","authors":["Mariia Seleznova","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2202.00553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10376v1","updated":"2022-07-21T09:12:03Z","published":"2022-07-21T09:12:03Z","title":"Multi-Asset Closed-Loop Reservoir Management Using Deep Reinforcement\n  Learning","summary":"  Closed-loop reservoir management (CLRM), in which history matching and\nproduction optimization are performed multiple times over the life of an asset,\ncan provide significant improvement in the specified objective. These\nprocedures are computationally expensive due to the large number of flow\nsimulations required for data assimilation and optimization. Existing CLRM\nprocedures are applied asset by asset, without utilizing information that could\nbe useful over a range assets. Here, we develop a CLRM framework for multiple\nassets with varying numbers of wells. We use deep reinforcement learning to\ntrain a single global control policy that is applicable for all assets\nconsidered. The new framework is an extension of a recently introduced control\npolicy methodology for individual assets. Embedding layers are incorporated\ninto the representation to handle the different numbers of decision variables\nthat arise for the different assets. Because the global control policy learns a\nunified representation of useful features from multiple assets, it is less\nexpensive to construct than asset-by-asset training (we observe about 3x\nspeedup in our examples). The production optimization problem includes a\nrelative-change constraint on the well settings, which renders the results\nsuitable for practical use. We apply the multi-asset CLRM framework to 2D and\n3D water-flooding examples. In both cases, four assets with different well\ncounts, well configurations, and geostatistical descriptions are considered.\nNumerical experiments demonstrate that the global control policy provides\nobjective function values, for both the 2D and 3D cases, that are nearly\nidentical to those from control policies trained individually for each asset.\nThis promising finding suggests that multi-asset CLRM may indeed represent a\nviable practical strategy.\n","authors":["Yusuf Nasir","Louis J. Durlofsky"],"pdf_url":"https://arxiv.org/pdf/2207.10376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11008v4","updated":"2022-07-21T08:58:22Z","published":"2022-04-23T06:51:37Z","title":"Long-term Spatio-temporal Forecasting via Dynamic Multiple-Graph\n  Attention","summary":"  Many real-world ubiquitous applications, such as parking recommendations and\nair pollution monitoring, benefit significantly from accurate long-term\nspatio-temporal forecasting (LSTF). LSTF makes use of long-term dependency\nbetween spatial and temporal domains, contextual information, and inherent\npattern in the data. Recent studies have revealed the potential of multi-graph\nneural networks (MGNNs) to improve prediction performance. However, existing\nMGNN methods cannot be directly applied to LSTF due to several issues: the low\nlevel of generality, insufficient use of contextual information, and the\nimbalanced graph fusion approach. To address these issues, we construct new\ngraph models to represent the contextual information of each node and the\nlong-term spatio-temporal data dependency structure. To fuse the information\nacross multiple graphs, we propose a new dynamic multi-graph fusion module to\ncharacterize the correlations of nodes within a graph and the nodes across\ngraphs via the spatial attention and graph attention mechanisms. Furthermore,\nwe introduce a trainable weight tensor to indicate the importance of each node\nin different graphs. Extensive experiments on two large-scale datasets\ndemonstrate that our proposed approaches significantly improve the performance\nof existing graph neural network models in LSTF prediction tasks.\n","authors":["Wei Shao","Zhiling Jin","Shuo Wang","Yufan Kang","Xiao Xiao","Hamid Menouar","Zhaofeng Zhang","Junshan Zhang","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2204.11008v4.pdf","comment":"Accepted by the 31st International Joint Conference on Artificial\n  Intelligence and the 25th European Conference on Artificial Intelligence\n  (IJCAI-ECAI 2022)"},{"id":"http://arxiv.org/abs/2207.09971v2","updated":"2022-07-21T08:47:54Z","published":"2022-07-20T15:29:45Z","title":"NeuralNEB -- Neural Networks can find Reaction Paths Fast","summary":"  Quantum mechanical methods like Density Functional Theory (DFT) are used with\ngreat success alongside efficient search algorithms for studying kinetics of\nreactive systems. However, DFT is prohibitively expensive for large scale\nexploration. Machine Learning (ML) models have turned out to be excellent\nemulators of small molecule DFT calculations and could possibly replace DFT in\nsuch tasks. For kinetics, success relies primarily on the models capability to\naccurately predict the Potential Energy Surface (PES) around transition-states\nand Minimal Energy Paths (MEPs). Previously this has not been possible due to\nscarcity of relevant data in the literature. In this paper we train state of\nthe art equivariant Graph Neural Network (GNN)-based models on around 10.000\nelementary reactions from the Transition1x dataset. We apply the models as\npotentials for the Nudged Elastic Band (NEB) algorithm and achieve a Mean\nAverage Error (MAE) of 0.13+/-0.03 eV on barrier energies on unseen reactions.\nWe compare the results against equivalent models trained on QM9 and ANI1x. We\nalso compare with and outperform Density Functional based Tight Binding (DFTB)\non both accuracy and computational resource. The implication is that ML models,\ngiven relevant data, are now at a level where they can be applied for\ndownstream tasks in quantum chemistry transcending prediction of simple\nmolecular features.\n","authors":["Mathias Schreiner","Arghya Bhowmik","Tejs Vegge","Ole Winther"],"pdf_url":"https://arxiv.org/pdf/2207.09971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10355v1","updated":"2022-07-21T08:20:24Z","published":"2022-07-21T08:20:24Z","title":"Unimodal vs. Multimodal Siamese Networks for Outfit Completion","summary":"  The popularity of online fashion shopping continues to grow. The ability to\noffer an effective recommendation to customers is becoming increasingly\nimportant. In this work, we focus on Fashion Outfits Challenge, part of SIGIR\n2022 Workshop on eCommerce. The challenge is centered around Fill in the Blank\n(FITB) task that implies predicting the missing outfit, given an incomplete\noutfit and a list of candidates. In this paper, we focus on applying siamese\nnetworks on the task. More specifically, we explore how combining information\nfrom multiple modalities (textual and visual modality) impacts the performance\nof the model on the task. We evaluate our model on the test split provided by\nthe challenge organizers and the test split with gold assignments that we\ncreated during the development phase. We discover that using both visual, and\nvisual and textual data demonstrates promising results on the task. We conclude\nby suggesting directions for further improvement of our method.\n","authors":["Mariya Hendriksen","Viggo Overes"],"pdf_url":"https://arxiv.org/pdf/2207.10355v1.pdf","comment":"3 pages, 2 figures"},{"id":"http://arxiv.org/abs/2110.14284v2","updated":"2022-07-21T07:58:21Z","published":"2021-10-27T09:05:23Z","title":"APPTeK: Agent-Based Predicate Prediction in Temporal Knowledge Graphs","summary":"  In temporal Knowledge Graphs (tKGs), the temporal dimension is attached to\nfacts in a knowledge base resulting in quadruples between entities such as\n(Nintendo, released, Super Mario, Sep-13-1985), where the predicate holds\nwithin a time interval or at a timestamp. We propose a reinforcement learning\nagent gathering temporal relevant information about the query entities'\nneighborhoods, simultaneously. We refer to the encodings of the explored graph\nstructures as fingerprints which are used as input to a Q-network. Our agent\ndecides sequentially which relation type needs to be explored next to expand\nthe local subgraphs of the query entities. Our evaluation shows that the\nproposed method yields competitive results compared to state-of-the-art\nembedding algorithms for tKGs, and we additionally gain information about the\nrelevant structures between subjects and objects.\n","authors":["Christian M. M. Frey","Yunpu Ma","Matthias Schubert"],"pdf_url":"https://arxiv.org/pdf/2110.14284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.06662v3","updated":"2022-07-21T07:53:58Z","published":"2021-09-14T13:11:34Z","title":"Identifying partial mouse brain microscopy images from Allen reference\n  atlas using a contrastively learned semantic space","summary":"  Precise identification of mouse brain microscopy images is a crucial first\nstep when anatomical structures in the mouse brain are to be registered to a\nreference atlas. Practitioners usually rely on manual comparison of images or\ntools that assume the presence of complete images. This work explores Siamese\nNetworks as the method for finding corresponding 2D reference atlas plates for\ngiven partial 2D mouse brain images. Siamese networks are a class of\nconvolutional neural networks (CNNs) that use weight-shared paths to obtain low\ndimensional embeddings of pairs of input images. The correspondence between the\npartial mouse brain image and reference atlas plate is determined based on the\ndistance between low dimensional embeddings of brain slices and atlas plates\nthat are obtained from Siamese networks using contrastive learning. Experiments\nshowed that Siamese CNNs can precisely identify brain slices using the Allen\nmouse brain atlas when training and testing images come from the same source.\nThey achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking\nonly 7.2 seconds to identify 29 images.\n","authors":["Justinas Antanavicius","Roberto Leiras","Raghavendra Selvan"],"pdf_url":"https://arxiv.org/pdf/2109.06662v3.pdf","comment":"Published in the Proceedings of International Workshop on Biomedical\n  Image Registration (WBIR-2022). Source code available at\n  https://github.com/Justinas256/2d-mouse-brain-identification. 12 pages, 6\n  figures"},{"id":"http://arxiv.org/abs/2111.02080v6","updated":"2022-07-21T07:44:13Z","published":"2021-11-03T09:12:33Z","title":"An Explanation of In-context Learning as Implicit Bayesian Inference","summary":"  Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.\n","authors":["Sang Michael Xie","Aditi Raghunathan","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2111.02080v6.pdf","comment":"ICLR 2022"},{"id":"http://arxiv.org/abs/2207.10334v1","updated":"2022-07-21T07:06:03Z","published":"2022-07-21T07:06:03Z","title":"Efficient Search of Multiple Neural Architectures with Different\n  Complexities via Importance Sampling","summary":"  Neural architecture search (NAS) aims to automate architecture design\nprocesses and improve the performance of deep neural networks. Platform-aware\nNAS methods consider both performance and complexity and can find\nwell-performing architectures with low computational resources. Although\nordinary NAS methods result in tremendous computational costs owing to the\nrepetition of model training, one-shot NAS, which trains the weights of a\nsupernetwork containing all candidate architectures only once during the search\nprocess, has been reported to result in a lower search cost. This study focuses\non the architecture complexity-aware one-shot NAS that optimizes the objective\nfunction composed of the weighted sum of two metrics, such as the predictive\nperformance and number of parameters. In existing methods, the architecture\nsearch process must be run multiple times with different coefficients of the\nweighted sum to obtain multiple architectures with different complexities. This\nstudy aims at reducing the search cost associated with finding multiple\narchitectures. The proposed method uses multiple distributions to generate\narchitectures with different complexities and updates each distribution using\nthe samples obtained from multiple distributions based on importance sampling.\nThe proposed method allows us to obtain multiple architectures with different\ncomplexities in a single architecture search, resulting in reducing the search\ncost. The proposed method is applied to the architecture search of\nconvolutional neural networks on the CIAFR-10 and ImageNet datasets.\nConsequently, compared with baseline methods, the proposed method finds\nmultiple architectures with varying complexities while requiring less\ncomputational effort.\n","authors":["Yuhei Noda","Shota Saito","Shinichi Shirakawa"],"pdf_url":"https://arxiv.org/pdf/2207.10334v1.pdf","comment":"Accepted as a conference paper at the 31st International Conference\n  on Artificial Neural Networks (ICANN 2022). The final authenticated\n  publication will be available in the Springer Lecture Notes in Computer\n  Science (LNCS)"},{"id":"http://arxiv.org/abs/2103.01955v3","updated":"2022-07-21T06:57:33Z","published":"2021-03-02T18:59:56Z","title":"The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games","summary":"  Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement\nlearning algorithm but is significantly less utilized than off-policy learning\nalgorithms in multi-agent settings. This is often due to the belief that PPO is\nsignificantly less sample efficient than off-policy methods in multi-agent\nsystems. In this work, we carefully study the performance of PPO in cooperative\nmulti-agent settings. We show that PPO-based multi-agent algorithms achieve\nsurprisingly strong performance in four popular multi-agent testbeds: the\nparticle-world environments, the StarCraft multi-agent challenge, the Hanabi\nchallenge, and Google Research Football, with minimal hyperparameter tuning and\nwithout any domain-specific algorithmic modifications or architectures.\nImportantly, compared to strong off-policy methods, PPO often achieves\ncompetitive or superior results in both final rewards and sample efficiency.\nFinally, through ablation studies, we analyze implementation and hyperparameter\nfactors that are critical to PPO's empirical performance, and give concrete\npractical suggestions regarding these factors. Our results show that when using\nthese practices, simple PPO-based methods are a strong baseline in cooperative\nmulti-agent reinforcement learning. Source code is released at\nhttps://github.com/marlbenchmark/on-policy.\n","authors":["Chao Yu","Akash Velu","Eugene Vinitsky","Yu Wang","Alexandre Bayen","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2103.01955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00798v3","updated":"2022-07-21T06:52:17Z","published":"2022-03-01T23:55:09Z","title":"TANDEM: Learning Joint Exploration and Decision Making with Tactile\n  Sensors","summary":"  Inspired by the human ability to perform complex manipulation in the complete\nabsence of vision (like retrieving an object from a pocket), the robotic\nmanipulation field is motivated to develop new methods for tactile-based object\ninteraction. However, tactile sensing presents the challenge of being an active\nsensing modality: a touch sensor provides sparse, local data, and must be used\nin conjunction with effective exploration strategies in order to collect\ninformation. In this work, we focus on the process of guiding tactile\nexploration, and its interplay with task-related decision making. We propose\nTANDEM (TActile exploration aNd DEcision Making), an architecture to learn\nefficient exploration strategies in conjunction with decision making. Our\napproach is based on separate but co-trained modules for exploration and\ndiscrimination. We demonstrate this method on a tactile object recognition\ntask, where a robot equipped with a touch sensor must explore and identify an\nobject from a known set based on binary contact signals alone. TANDEM achieves\nhigher accuracy with fewer actions than alternative methods and is also shown\nto be more robust to sensor noise.\n","authors":["Jingxi Xu","Shuran Song","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2203.00798v3.pdf","comment":"Accepted to Robotics and Automation Letters (RA-L) and International\n  Conference on Intelligent Robots and Systems (IROS) 2022"},{"id":"http://arxiv.org/abs/2207.10324v1","updated":"2022-07-21T06:42:12Z","published":"2022-07-21T06:42:12Z","title":"Improved Generative Model for Weakly Supervised Chest Anomaly\n  Localization via Pseudo-paired Registration with Bilaterally Symmetrical Data\n  Augmentation","summary":"  Image translation based on a generative adversarial network (GAN-IT) is a\npromising method for precise localization of abnormal regions in chest X-ray\nimages (AL-CXR). However, heterogeneous unpaired datasets undermine existing\nmethods to extract key features and distinguish normal from abnormal cases,\nresulting in inaccurate and unstable AL-CXR. To address this problem, we\npropose an improved two-stage GAN-IT involving registration and data\naugmentation. For the first stage, we introduce an invertible\ndeep-learning-based registration technique that virtually and reasonably\nconverts unpaired data into paired data for learning registration maps. This\nnovel approach achieves high registration performance. For the second stage, we\napply data augmentation to diversify anomaly locations by swapping the left and\nright lung regions on the uniform registered frames, further improving the\nperformance by alleviating imbalance in data distribution showing left and\nright lung lesions. Our method is intended for application to existing GAN-IT\nmodels, allowing existing architecture to benefit from key features for\ntranslation. By showing that the AL-CXR performance is uniformly improved when\napplying the proposed method, we believe that GAN-IT for AL-CXR can be deployed\nin clinical environments, even if learning data are scarce.\n","authors":["Kyung-Su Kim","Seong Je Oh","Tae Uk Kim","Myung Jin Chung"],"pdf_url":"https://arxiv.org/pdf/2207.10324v1.pdf","comment":"Kyung-Su Kim and Seong Je Oh have contributed equally to this work as\n  the co-first author. Kyung-Su Kim (kskim.doc@gmail.com) and Myung Jin Chung\n  (mj1.chung@samsung.com) have contributed equally to this work as the\n  co-corresponding author"},{"id":"http://arxiv.org/abs/2203.10157v2","updated":"2022-07-21T06:03:51Z","published":"2022-03-18T21:08:23Z","title":"ViewFormer: NeRF-free Neural Rendering from Few Images Using\n  Transformers","summary":"  Novel view synthesis is a long-standing problem. In this work, we consider a\nvariant of the problem where we are given only a few context views sparsely\ncovering a scene or an object. The goal is to predict novel viewpoints in the\nscene, which requires learning priors. The current state of the art is based on\nNeural Radiance Field (NeRF), and while achieving impressive results, the\nmethods suffer from long training times as they require evaluating millions of\n3D point samples via a neural network for each image. We propose a 2D-only\nmethod that maps multiple context views and a query pose to a new image in a\nsingle pass of a neural network. Our model uses a two-stage architecture\nconsisting of a codebook and a transformer model. The codebook is used to embed\nindividual images into a smaller latent space, and the transformer solves the\nview synthesis task in this more compact space. To train our model efficiently,\nwe introduce a novel branching attention mechanism that allows us to use the\nsame model not only for neural rendering but also for camera pose estimation.\nExperimental results on real-world scenes show that our approach is competitive\ncompared to NeRF-based methods while not reasoning explicitly in 3D, and it is\nfaster to train.\n","authors":["Jonáš Kulhánek","Erik Derner","Torsten Sattler","Robert Babuška"],"pdf_url":"https://arxiv.org/pdf/2203.10157v2.pdf","comment":"ECCV 2022 poster"},{"id":"http://arxiv.org/abs/2108.13650v2","updated":"2022-07-21T05:33:56Z","published":"2021-08-31T07:18:48Z","title":"Heterogeneous Graph Neural Network with Multi-view Representation\n  Learning","summary":"  Graph neural networks for heterogeneous graph embedding is to project nodes\ninto a low-dimensional space by exploring the heterogeneity and semantics of\nthe heterogeneous graph. However, on the one hand, most of existing\nheterogeneous graph embedding methods either insufficiently model the local\nstructure under specific semantic, or neglect the heterogeneity when\naggregating information from it. On the other hand, representations from\nmultiple semantics are not comprehensively integrated to obtain versatile node\nembeddings. To address the problem, we propose a Heterogeneous Graph Neural\nNetwork with Multi-View Representation Learning (named MV-HetGNN) for\nheterogeneous graph embedding by introducing the idea of multi-view\nrepresentation learning. The proposed model consists of node feature\ntransformation, view-specific ego graph encoding and auto multi-view fusion to\nthoroughly learn complex structural and semantic information for generating\ncomprehensive node representations. Extensive experiments on three real-world\nheterogeneous graph datasets show that the proposed MV-HetGNN model\nconsistently outperforms all the state-of-the-art GNN baselines in various\ndownstream tasks, e.g., node classification, node clustering, and link\nprediction.\n","authors":["Zezhi Shao","Yongjun Xu","Wei Wei","Fei Wang","Zhao Zhang","Feida Zhu"],"pdf_url":"https://arxiv.org/pdf/2108.13650v2.pdf","comment":"Submitted to TKDE"},{"id":"http://arxiv.org/abs/2206.15477v4","updated":"2022-07-21T05:15:45Z","published":"2022-06-30T17:59:49Z","title":"Denoised MDPs: Learning World Models Better Than the World Itself","summary":"  The ability to separate signal from noise, and reason with clean\nabstractions, is critical to intelligence. With this ability, humans can\nefficiently perform real world tasks without considering all possible nuisance\nfactors.How can artificial agents do the same? What kind of information can\nagents safely discard as noises?\n  In this work, we categorize information out in the wild into four types based\non controllability and relation with reward, and formulate useful information\nas that which is both controllable and reward-relevant. This framework\nclarifies the kinds information removed by various prior work on representation\nlearning in reinforcement learning (RL), and leads to our proposed approach of\nlearning a Denoised MDP that explicitly factors out certain noise distractors.\nExtensive experiments on variants of DeepMind Control Suite and RoboDesk\ndemonstrate superior performance of our denoised world model over using raw\nobservations alone, and over prior works, across policy optimization control\ntasks as well as the non-control task of joint position regression.\n","authors":["Tongzhou Wang","Simon S. Du","Antonio Torralba","Phillip Isola","Amy Zhang","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2206.15477v4.pdf","comment":"Project page: https://ssnl.github.io/denoised_mdp/ Code:\n  https://github.com/facebookresearch/denoised_mdp"},{"id":"http://arxiv.org/abs/2207.06652v2","updated":"2022-07-21T05:05:16Z","published":"2022-07-14T04:29:54Z","title":"Every Preference Changes Differently: Neural Multi-Interest Preference\n  Model with Temporal Dynamics for Recommendation","summary":"  User embeddings (vectorized representations of a user) are essential in\nrecommendation systems. Numerous approaches have been proposed to construct a\nrepresentation for the user in order to find similar items for retrieval tasks,\nand they have been proven effective in industrial recommendation systems as\nwell. Recently people have discovered the power of using multiple embeddings to\nrepresent a user, with the hope that each embedding represents the user's\ninterest in a certain topic. With multi-interest representation, it's important\nto model the user's preference over the different topics and how the preference\nchange with time. However, existing approaches either fail to estimate the\nuser's affinity to each interest or unreasonably assume every interest of every\nuser fades with an equal rate with time, thus hurting the recall of candidate\nretrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,\nan approach that not only produces multi-interest for users by using the user's\nsequential engagement more effectively but also automatically learns a set of\nweights to represent the preference over each embedding so that the candidates\ncan be retrieved from each interest proportionally. Extensive experiments have\nbeen done on various industrial-scale datasets to demonstrate the effectiveness\nof our approach.\n","authors":["Hui Shi","Yupeng Gu","Yitong Zhou","Bo Zhao","Sicun Gao","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.06652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10308v1","updated":"2022-07-21T05:03:04Z","published":"2022-07-21T05:03:04Z","title":"UniFed: A Benchmark for Federated Learning Frameworks","summary":"  Federated Learning (FL) has become a practical and popular paradigm in\nmachine learning. However, currently, there is no systematic solution that\ncovers diverse use cases. Practitioners often face the challenge of how to\nselect a matching FL framework for their use case. In this work, we present\nUniFed, the first unified benchmark for standardized evaluation of the existing\nopen-source FL frameworks. With 15 evaluation scenarios, we present both\nqualitative and quantitative evaluation results of nine existing popular\nopen-sourced FL frameworks, from the perspectives of functionality, usability,\nand system performance. We also provide suggestions on framework selection\nbased on the benchmark conclusions and point out future improvement directions.\n","authors":["Xiaoyuan Liu","Tianneng Shi","Chulin Xie","Qinbin Li","Kangping Hu","Haoyu Kim","Xiaojun Xu","Bo Li","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2207.10308v1.pdf","comment":"Code: https://github.com/AI-secure/FLBenchmark-toolkit Website:\n  https://unifedbenchmark.github.io/"},{"id":"http://arxiv.org/abs/2207.10307v1","updated":"2022-07-21T04:59:31Z","published":"2022-07-21T04:59:31Z","title":"Knowledge-enhanced Black-box Attacks for Recommendations","summary":"  Recent studies have shown that deep neural networks-based recommender systems\nare vulnerable to adversarial attacks, where attackers can inject carefully\ncrafted fake user profiles (i.e., a set of items that fake users have\ninteracted with) into a target recommender system to achieve malicious\npurposes, such as promote or demote a set of target items. Due to the security\nand privacy concerns, it is more practical to perform adversarial attacks under\nthe black-box setting, where the architecture/parameters and training data of\ntarget systems cannot be easily accessed by attackers. However, generating\nhigh-quality fake user profiles under black-box setting is rather challenging\nwith limited resources to target systems. To address this challenge, in this\nwork, we introduce a novel strategy by leveraging items' attribute information\n(i.e., items' knowledge graph), which can be publicly accessible and provide\nrich auxiliary knowledge to enhance the generation of fake user profiles. More\nspecifically, we propose a knowledge graph-enhanced black-box attacking\nframework (KGAttack) to effectively learn attacking policies through deep\nreinforcement learning techniques, in which knowledge graph is seamlessly\nintegrated into hierarchical policy networks to generate fake user profiles for\nperforming adversarial black-box attacks. Comprehensive experiments on various\nreal-world datasets demonstrate the effectiveness of the proposed attacking\nframework under the black-box setting.\n","authors":["Jingfan Chen","Wenqi Fan","Guanghui Zhu","Xiangyu Zhao","Chunfeng Yuan","Qing Li","Yihua Huang"],"pdf_url":"https://arxiv.org/pdf/2207.10307v1.pdf","comment":"Accepted in the KDD'22"},{"id":"http://arxiv.org/abs/2207.05223v2","updated":"2022-07-21T04:57:18Z","published":"2022-07-11T23:32:54Z","title":"Bootstrapping a User-Centered Task-Oriented Dialogue System","summary":"  We present TacoBot, a task-oriented dialogue system built for the inaugural\nAlexa Prize TaskBot Challenge, which assists users in completing multi-step\ncooking and home improvement tasks. TacoBot is designed with a user-centered\nprinciple and aspires to deliver a collaborative and accessible dialogue\nexperience. Towards that end, it is equipped with accurate language\nunderstanding, flexible dialogue management, and engaging response generation.\nFurthermore, TacoBot is backed by a strong search engine and an automated\nend-to-end test suite. In bootstrapping the development of TacoBot, we explore\na series of data augmentation strategies to train advanced neural language\nprocessing models and continuously improve the dialogue experience with\ncollected real conversations. At the end of the semifinals, TacoBot achieved an\naverage rating of 3.55/5.0.\n","authors":["Shijie Chen","Ziru Chen","Xiang Deng","Ashley Lewis","Lingbo Mo","Samuel Stevens","Zhen Wang","Xiang Yue","Tianshu Zhang","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2207.05223v2.pdf","comment":"Published in 1st Proceedings of Alexa Prize TaskBot (Alexa Prize\n  2021). TacoBot won 3rd place in the challenge. See project website\n  https://sunlab-osu.github.io/tacobot/ for details"},{"id":"http://arxiv.org/abs/2207.10305v1","updated":"2022-07-21T04:47:21Z","published":"2022-07-21T04:47:21Z","title":"Subgraph Matching via Query-Conditioned Subgraph Matching Neural\n  Networks and Bi-Level Tree Search","summary":"  Recent advances have shown the success of using reinforcement learning and\nsearch to solve NP-hard graph-related tasks, such as Traveling Salesman\nOptimization, Graph Edit Distance computation, etc. However, it remains unclear\nhow one can efficiently and accurately detect the occurrences of a small query\ngraph in a large target graph, which is a core operation in graph database\nsearch, biomedical analysis, social group finding, etc. This task is called\nSubgraph Matching which essentially performs subgraph isomorphism check between\na query graph and a large target graph. One promising approach to this\nclassical problem is the \"learning-to-search\" paradigm, where a reinforcement\nlearning (RL) agent is designed with a learned policy to guide a search\nalgorithm to quickly find the solution without any solved instances for\nsupervision. However, for the specific task of Subgraph Matching, though the\nquery graph is usually small given by the user as input, the target graph is\noften orders-of-magnitude larger. It poses challenges to the neural network\ndesign and can lead to solution and reward sparsity. In this paper, we propose\nN-BLS with two innovations to tackle the challenges: (1) A novel\nencoder-decoder neural network architecture to dynamically compute the matching\ninformation between the query and the target graphs at each search state; (2) A\nMonte Carlo Tree Search enhanced bi-level search framework for training the\npolicy and value networks. Experiments on five large real-world target graphs\nshow that N-BLS can significantly improve the subgraph matching performance.\n","authors":["Yunsheng Bai","Derek Xu","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2207.10305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03463v2","updated":"2022-07-21T04:38:16Z","published":"2021-11-01T19:58:29Z","title":"RADAMS: Resilient and Adaptive Alert and Attention Management Strategy\n  against Informational Denial-of-Service (IDoS) Attacks","summary":"  Attacks exploiting human attentional vulnerability have posed severe threats\nto cybersecurity. In this work, we identify and formally define a new type of\nproactive attentional attacks called Informational Denial-of-Service (IDoS)\nattacks that generate a large volume of feint attacks to overload human\noperators and hide real attacks among feints. We incorporate human factors\n(e.g., levels of expertise, stress, and efficiency) and empirical psychological\nresults (e.g., the Yerkes-Dodson law and the sunk cost fallacy) to model the\noperators' attention dynamics and their decision-making processes along with\nthe real-time alert monitoring and inspection. To assist human operators in\ndismissing the feints and escalating the real attacks timely and accurately, we\ndevelop a Resilient and Adaptive Data-driven alert and Attention Management\nStrategy (RADAMS) that de-emphasizes alerts selectively based on the abstracted\ncategory labels of the alerts. RADAMS uses reinforcement learning to achieve a\ncustomized and transferable design for various human operators and evolving\nIDoS attacks. The integrated modeling and theoretical analysis lead to the\nProduct Principle of Attention (PPoA), fundamental limits, and the tradeoff\namong crucial human and economic factors. Experimental results corroborate that\nthe proposed strategy outperforms the default strategy and can reduce the IDoS\nrisk by as much as 20%. Besides, the strategy is resilient to large variations\nof costs, attack frequencies, and human attention capacities. We have\nrecognized interesting phenomena such as attentional risk equivalency,\nattacker's dilemma, and the half-truth optimal attack strategy.\n","authors":["Linan Huang","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2111.03463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10297v1","updated":"2022-07-21T04:23:14Z","published":"2022-07-21T04:23:14Z","title":"Action2Score: An Embedding Approach To Score Player Action","summary":"  Multiplayer Online Battle Arena (MOBA) is one of the most successful game\ngenres. MOBA games such as League of Legends have competitive environments\nwhere players race for their rank. In most MOBA games, a player's rank is\ndetermined by the match result (win or lose). It seems natural because of the\nnature of team play, but in some sense, it is unfair because the players who\nput a lot of effort lose their rank just in case of loss and some players even\nget free-ride on teammates' efforts in case of a win. To reduce the\nside-effects of the team-based ranking system and evaluate a player's\nperformance impartially, we propose a novel embedding model that converts a\nplayer's actions into quantitative scores based on the actions' respective\ncontribution to the team's victory. Our model is built using a sequence-based\ndeep learning model with a novel loss function working on the team match. The\nsequence-based deep learning model process the action sequence from the game\nstart to the end of a player in a team play using a GRU unit that takes a\nhidden state from the previous step and the current input selectively. The loss\nfunction is designed to help the action score to reflect the final score and\nthe success of the team. We showed that our model can evaluate a player's\nindividual performance fairly and analyze the contributions of the player's\nrespective actions.\n","authors":["Junho Jang","Ji Young Woo","Huy Kang Kim"],"pdf_url":"https://arxiv.org/pdf/2207.10297v1.pdf","comment":"20 pages, 8 figures, 4 tables; accepted to ACM CHIPLAY 2022, and PACM\n  on Human-Computer Interaction"},{"id":"http://arxiv.org/abs/2207.10295v1","updated":"2022-07-21T04:12:48Z","published":"2022-07-21T04:12:48Z","title":"Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning","summary":"  Impressive results in natural language processing (NLP) based on the\nTransformer neural network architecture have inspired researchers to explore\nviewing offline reinforcement learning (RL) as a generic sequence modeling\nproblem. Recent works based on this paradigm have achieved state-of-the-art\nresults in several of the mostly deterministic offline Atari and D4RL\nbenchmarks. However, because these methods jointly model the states and actions\nas a single sequencing problem, they struggle to disentangle the effects of the\npolicy and world dynamics on the return. Thus, in adversarial or stochastic\nenvironments, these methods lead to overly optimistic behavior that can be\ndangerous in safety-critical systems like autonomous driving. In this work, we\npropose a method that addresses this optimism bias by explicitly disentangling\nthe policy and world models, which allows us at test time to search for\npolicies that are robust to multiple possible futures in the environment. We\ndemonstrate our method's superior performance on a variety of autonomous\ndriving tasks in simulation.\n","authors":["Adam Villaflor","Zhe Huang","Swapnil Pande","John Dolan","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2207.10295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10289v1","updated":"2022-07-21T03:57:27Z","published":"2022-07-21T03:57:27Z","title":"A comprehensive study of non-adaptive and residual-based adaptive\n  sampling for physics-informed neural networks","summary":"  Physics-informed neural networks (PINNs) have shown to be an effective tool\nfor solving forward and inverse problems of partial differential equations\n(PDEs). PINNs embed the PDEs into the loss of the neural network, and this PDE\nloss is evaluated at a set of scattered residual points. The distribution of\nthese points are highly important to the performance of PINNs. However, in the\nexisting studies on PINNs, only a few simple residual point sampling methods\nhave mainly been used. Here, we present a comprehensive study of two categories\nof sampling: non-adaptive uniform sampling and adaptive nonuniform sampling. We\nconsider six uniform sampling, including (1) equispaced uniform grid, (2)\nuniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence,\n(5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling\nstrategy for uniform sampling. To improve the sampling efficiency and the\naccuracy of PINNs, we propose two new residual-based adaptive sampling methods:\nresidual-based adaptive distribution (RAD) and residual-based adaptive\nrefinement with distribution (RAR-D), which dynamically improve the\ndistribution of residual points based on the PDE residuals during training.\nHence, we have considered a total of 10 different sampling methods, including\nsix non-adaptive uniform sampling, uniform sampling with resampling, two\nproposed adaptive sampling, and an existing adaptive sampling. We extensively\ntested the performance of these sampling methods for four forward problems and\ntwo inverse problems in many setups. Our numerical results presented in this\nstudy are summarized from more than 6000 simulations of PINNs. We show that the\nproposed adaptive sampling methods of RAD and RAR-D significantly improve the\naccuracy of PINNs with fewer residual points. The results obtained in this\nstudy can also be used as a practical guideline in choosing sampling methods.\n","authors":["Chenxi Wu","Min Zhu","Qinyang Tan","Yadhu Kartha","Lu Lu"],"pdf_url":"https://arxiv.org/pdf/2207.10289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10286v1","updated":"2022-07-21T03:50:21Z","published":"2022-07-21T03:50:21Z","title":"Comparative Study on Supervised versus Semi-supervised Machine Learning\n  for Anomaly Detection of In-vehicle CAN Network","summary":"  As the central nerve of the intelligent vehicle control system, the\nin-vehicle network bus is crucial to the security of vehicle driving. One of\nthe best standards for the in-vehicle network is the Controller Area Network\n(CAN bus) protocol. However, the CAN bus is designed to be vulnerable to\nvarious attacks due to its lack of security mechanisms. To enhance the security\nof in-vehicle networks and promote the research in this area, based upon a\nlarge scale of CAN network traffic data with the extracted valuable features,\nthis study comprehensively compared fully-supervised machine learning with\nsemi-supervised machine learning methods for CAN message anomaly detection.\nBoth traditional machine learning models (including single classifier and\nensemble models) and neural network based deep learning models are evaluated.\nFurthermore, this study proposed a deep autoencoder based semi-supervised\nlearning method applied for CAN message anomaly detection and verified its\nsuperiority over other semi-supervised methods. Extensive experiments show that\nthe fully-supervised methods generally outperform semi-supervised ones as they\nare using more information as inputs. Typically the developed XGBoost based\nmodel obtained state-of-the-art performance with the best accuracy (98.65%),\nprecision (0.9853), and ROC AUC (0.9585) beating other methods reported in the\nliterature.\n","authors":["Yongqi Dong","Kejia Chen","Yinxuan Peng","Zhiyuan Ma"],"pdf_url":"https://arxiv.org/pdf/2207.10286v1.pdf","comment":"6 pages, 5 figures, accepted by the 25th IEEE International\n  Conference on Intelligent Transportation Systems (IEEE ITSC 2022)"},{"id":"http://arxiv.org/abs/2207.10285v1","updated":"2022-07-21T03:43:38Z","published":"2022-07-21T03:43:38Z","title":"Grounding Visual Representations with Texts for Domain Generalization","summary":"  Reducing the representational discrepancy between source and target domains\nis a key component to maximize the model generalization. In this work, we\nadvocate for leveraging natural language supervision for the domain\ngeneralization task. We introduce two modules to ground visual representations\nwith texts containing typical reasoning of humans: (1) Visual and Textual Joint\nEmbedder and (2) Textual Explanation Generator. The former learns the\nimage-text joint embedding space where we can ground high-level\nclass-discriminative information into the model. The latter leverages an\nexplainable model and generates explanations justifying the rationale behind\nits decision. To the best of our knowledge, this is the first work to leverage\nthe vision-and-language cross-modality approach for the domain generalization\ntask. Our experiments with a newly created CUB-DG benchmark dataset demonstrate\nthat cross-modality supervision can be successfully used to ground\ndomain-invariant visual representations and improve the model generalization.\nFurthermore, in the large-scale DomainBed benchmark, our proposed method\nachieves state-of-the-art results and ranks 1st in average performance for five\nmulti-domain datasets. The dataset and codes are available at\nhttps://github.com/mswzeus/GVRT.\n","authors":["Seonwoo Min","Nokyung Park","Siwon Kim","Seunghyun Park","Jinkyu Kim"],"pdf_url":"https://arxiv.org/pdf/2207.10285v1.pdf","comment":"25 pages (including Supplementary Materials), ECCV 2022 camera ready\n  version"},{"id":"http://arxiv.org/abs/2207.10284v1","updated":"2022-07-21T03:36:30Z","published":"2022-07-21T03:36:30Z","title":"Multi Resolution Analysis (MRA) for Approximate Self-Attention","summary":"  Transformers have emerged as a preferred model for many tasks in natural\nlangugage processing and vision. Recent efforts on training and deploying\nTransformers more efficiently have identified many strategies to approximate\nthe self-attention matrix, a key module in a Transformer architecture.\nEffective ideas include various prespecified sparsity patterns, low-rank basis\nexpansions and combinations thereof. In this paper, we revisit classical\nMultiresolution Analysis (MRA) concepts such as Wavelets, whose potential value\nin this setting remains underexplored thus far. We show that simple\napproximations based on empirical feedback and design choices informed by\nmodern hardware and implementation challenges, eventually yield a MRA-based\napproach for self-attention with an excellent performance profile across most\ncriteria of interest. We undertake an extensive set of experiments and\ndemonstrate that this multi-resolution scheme outperforms most efficient\nself-attention proposals and is favorable for both short and long sequences.\nCode is available at \\url{https://github.com/mlpen/mra-attention}.\n","authors":["Zhanpeng Zeng","Sourav Pal","Jeffery Kline","Glenn M Fung","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2207.10284v1.pdf","comment":"ICML2022"},{"id":"http://arxiv.org/abs/2207.07656v2","updated":"2022-07-21T03:29:36Z","published":"2022-07-15T16:32:23Z","title":"FLOWGEN: Fast and slow graph generation","summary":"  We present FLOWGEN, a graph-generation model inspired by the dual-process\ntheory of mind that generates large graphs incrementally. Depending on the\ndifficulty of completing the graph at the current step, graph generation is\nrouted to either a fast~(weaker) or a slow~(stronger) model. fast and slow\nmodels have identical architectures, but vary in the number of parameters and\nconsequently the strength. Experiments on real-world graphs show that ours can\nsuccessfully generate graphs similar to those generated by a single large model\nin a fraction of time.\n","authors":["Aman Madaan","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2207.07656v2.pdf","comment":"This version to be presented at Dynn workshop @ ICML 2022"},{"id":"http://arxiv.org/abs/2207.10283v1","updated":"2022-07-21T03:28:25Z","published":"2022-07-21T03:28:25Z","title":"Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for\n  Adversarial Robustness","summary":"  Defending deep neural networks against adversarial examples is a key\nchallenge for AI safety. To improve the robustness effectively, recent methods\nfocus on important data points near the decision boundary in adversarial\ntraining. However, these methods are vulnerable to Auto-Attack, which is an\nensemble of parameter-free attacks for reliable evaluation. In this paper, we\nexperimentally investigate the causes of their vulnerability and find that\nexisting methods reduce margins between logits for the true label and the other\nlabels while keeping their gradient norms non-small values. Reduced margins and\nnon-small gradient norms cause their vulnerability since the largest logit can\nbe easily flipped by the perturbation. Our experiments also show that the\nhistogram of the logit margins has two peaks, i.e., small and large logit\nmargins. From the observations, we propose switching one-versus-the-rest loss\n(SOVR), which uses one-versus-the-rest loss when data have small logit margins\nso that it increases the margins. We find that SOVR increases logit margins\nmore than existing methods while keeping gradient norms small and outperforms\nthem in terms of the robustness against Auto-Attack.\n","authors":["Sekitoshi Kanai","Shin'ya Yamaguchi","Masanori Yamada","Hiroshi Takahashi","Yasutoshi Ida"],"pdf_url":"https://arxiv.org/pdf/2207.10283v1.pdf","comment":"20 pages, 16 figures"},{"id":"http://arxiv.org/abs/2109.03159v5","updated":"2022-07-21T03:13:11Z","published":"2021-09-07T15:51:12Z","title":"Analysis of Regularized Learning for Generalized Data in Banach Spaces","summary":"  In this article, we study the whole theory of regularized learning for\ngeneralized data in Banach spaces including representer theorems, approximation\ntheorems, and convergence theorems. The generalized input data are composed of\nlinear functionals in the predual spaces of the Banach spaces to represent the\ndiscrete local information of different engineering and physics models. The\ngeneralized data and the multi-loss functions are used to compute the empirical\nrisks, and the regularized learning is to minimize the regularized empirical\nrisks over the Banach spaces. Even if the original problems are unknown or\nunformulated, then the exact solutions of the original problems are\napproximated globally by the regularized learning. In the proof of the\nconvergence theorems, the strong convergence condition is replaced to the weak\nconvergence condition with the additional checkable condition which is\nindependent of the original problems. The theorems of the regularized learning\ncan be used to solve many problems of machine learning such as support vector\nmachines and neural networks.\n","authors":["Qi Ye"],"pdf_url":"https://arxiv.org/pdf/2109.03159v5.pdf","comment":"36 pages, 1 figure"},{"id":"http://arxiv.org/abs/2207.10276v1","updated":"2022-07-21T03:01:04Z","published":"2022-07-21T03:01:04Z","title":"ProMix: Combating Label Noise via Maximizing Clean Sample Utility","summary":"  The ability to train deep neural networks under label noise is appealing, as\nimperfectly annotated data are relatively cheaper to obtain. State-of-the-art\napproaches are based on semi-supervised learning(SSL), which selects small loss\nexamples as clean and then applies SSL techniques for boosted performance.\nHowever, the selection step mostly provides a medium-sized and decent-enough\nclean subset, which overlooks a rich set of clean samples. In this work, we\npropose a novel noisy label learning framework ProMix that attempts to maximize\nthe utility of clean samples for boosted performance. Key to our method, we\npropose a matched high-confidence selection technique that selects those\nexamples having high confidence and matched prediction with its given labels.\nCombining with the small-loss selection, our method is able to achieve a\nprecision of 99.27 and a recall of 98.22 in detecting clean samples on the\nCIFAR-10N dataset. Based on such a large set of clean data, ProMix improves the\nbest baseline method by +2.67% on CIFAR-10N and +1.61% on CIFAR-100N datasets.\nThe code and data are available at https://github.com/Justherozen/ProMix\n","authors":["Haobo Wang","Ruixuan Xiao","Yiwen Dong","Lei Feng","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.10276v1.pdf","comment":"Winner of the 1st Learning and Mining with Noisy Labels Challenge in\n  IJCAI-ECAI 2022 (an informal technical report)"},{"id":"http://arxiv.org/abs/2204.01599v2","updated":"2022-07-21T02:45:14Z","published":"2022-04-04T15:52:55Z","title":"DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic\n  Segmentation","summary":"  Deep learning approaches achieve prominent success in 3D semantic\nsegmentation. However, collecting densely annotated real-world 3D datasets is\nextremely time-consuming and expensive. Training models on synthetic data and\ngeneralizing on real-world scenarios becomes an appealing alternative, but\nunfortunately suffers from notorious domain shifts. In this work, we propose a\nData-Oriented Domain Adaptation (DODA) framework to mitigate pattern and\ncontext gaps caused by different sensing mechanisms and layout placements\nacross domains. Our DODA encompasses virtual scan simulation to imitate\nreal-world point cloud patterns and tail-aware cuboid mixing to alleviate the\ninterior context gap with a cuboid-based intermediate domain. The first\nunsupervised sim-to-real adaptation benchmark on 3D indoor semantic\nsegmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular\nUnsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA\napproaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code\nis available at https://github.com/CVMI-Lab/DODA.\n","authors":["Runyu Ding","Jihan Yang","Li Jiang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2204.01599v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2207.10265v1","updated":"2022-07-21T02:21:03Z","published":"2022-07-21T02:21:03Z","title":"FOCUS: Fairness via Agent-Awareness for Federated Learning on\n  Heterogeneous Data","summary":"  Federated learning (FL) provides an effective paradigm to train machine\nlearning models over distributed data with privacy protection. However, recent\nstudies show that FL is subject to various security, privacy, and fairness\nthreats due to the potentially malicious and heterogeneous local agents. For\ninstance, it is vulnerable to local adversarial agents who only contribute\nlow-quality data, with the goal of harming the performance of those with\nhigh-quality data. This kind of attack hence breaks existing definitions of\nfairness in FL that mainly focus on a certain notion of performance parity. In\nthis work, we aim to address this limitation and propose a formal definition of\nfairness via agent-awareness for FL (FAA), which takes the heterogeneous data\ncontributions of local agents into account. In addition, we propose a fair FL\ntraining algorithm based on agent clustering (FOCUS) to achieve FAA.\nTheoretically, we prove the convergence and optimality of FOCUS under mild\nconditions for linear models and general convex loss functions with bounded\nsmoothness. We also prove that FOCUS always achieves higher fairness measured\nby FAA compared with standard FedAvg protocol under both linear models and\ngeneral convex loss functions. Empirically, we evaluate FOCUS on four datasets,\nincluding synthetic data, images, and texts under different settings, and we\nshow that FOCUS achieves significantly higher fairness based on FAA while\nmaintaining similar or even higher prediction accuracy compared with FedAvg.\n","authors":["Wenda Chu","Chulin Xie","Boxin Wang","Linyi Li","Lang Yin","Han Zhao","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2207.10265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09933v2","updated":"2022-07-21T02:09:55Z","published":"2022-07-20T14:20:03Z","title":"Robust Landmark-based Stent Tracking in X-ray Fluoroscopy","summary":"  In clinical procedures of angioplasty (i.e., open clogged coronary arteries),\ndevices such as balloons and stents need to be placed and expanded in arteries\nunder the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,\nthe resulting images are often noisy. To check the correct placement of these\ndevices, typically multiple motion-compensated frames are averaged to enhance\nthe view. Therefore, device tracking is a necessary procedure for this purpose.\nEven though angioplasty devices are designed to have radiopaque markers for the\nease of tracking, current methods struggle to deliver satisfactory results due\nto the small marker size and complex scenes in angioplasty. In this paper, we\npropose an end-to-end deep learning framework for single stent tracking, which\nconsists of three hierarchical modules: U-Net based landmark detection, ResNet\nbased stent proposal and feature extraction, and graph convolutional neural\nnetwork (GCN) based stent tracking that temporally aggregates both spatial\ninformation and appearance features. The experiments show that our method\nperforms significantly better in detection compared with the state-of-the-art\npoint-based tracking models. In addition, its fast inference speed satisfies\nclinical requirements.\n","authors":["Luojie Huang","Yikang Liu","Li Chen","Eric Z. Chen","Xiao Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2207.09933v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2201.04343v2","updated":"2022-07-21T02:02:45Z","published":"2022-01-12T07:26:19Z","title":"An Efficient and Adaptive Granular-ball Generation Method in\n  Classification Problem","summary":"  Granular-ball computing is an efficient, robust, and scalable learning method\nfor granular computing. The basis of granular-ball computing is the\ngranular-ball generation method. This paper proposes a method for accelerating\nthe granular-ball generation using the division to replace $k$-means. It can\ngreatly improve the efficiency of granular-ball generation while ensuring the\naccuracy similar to the existing method. Besides, a new adaptive method for the\ngranular-ball generation is proposed by considering granular-ball's overlap\neliminating and some other factors. This makes the granular-ball generation\nprocess of parameter-free and completely adaptive in the true sense. In\naddition, this paper first provides the mathematical models for the\ngranular-ball covering. The experimental results on some real data sets\ndemonstrate that the proposed two granular-ball generation methods have similar\naccuracies with the existing method while adaptiveness or acceleration is\nrealized.\n","authors":["Shuyin Xia","Xiaochuan Dai","Guoyin Wang","Xinbo Gao","Elisabeth Giem"],"pdf_url":"https://arxiv.org/pdf/2201.04343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.12081v3","updated":"2022-07-21T02:02:22Z","published":"2021-04-25T06:56:48Z","title":"How Well Does Self-Supervised Pre-Training Perform with Streaming Data?","summary":"  Prior works on self-supervised pre-training focus on the joint training\nscenario, where massive unlabeled data are assumed to be given as input all at\nonce, and only then is a learner trained. Unfortunately, such a problem setting\nis often impractical if not infeasible since many real-world tasks rely on\nsequential learning, e.g., data are decentralized or collected in a streaming\nfashion. In this paper, we conduct the first thorough and dedicated\ninvestigation on self-supervised pre-training with streaming data, aiming to\nshed light on the model behavior under this overlooked setup. Specifically, we\npre-train over 500 models on four categories of pre-training streaming data\nfrom ImageNet and DomainNet and evaluate them on three types of downstream\ntasks and 12 different downstream datasets. Our studies show that, somehow\nbeyond our expectation, with simple data replay or parameter regularization,\nsequential self-supervised pre-training turns out to be an efficient\nalternative for joint pre-training, as the performances of the former are\nmostly on par with those of the latter. Moreover, catastrophic forgetting, a\ncommon issue in sequential supervised learning, is much alleviated in\nsequential self-supervised learning (SSL), which is well justified through our\ncomprehensive empirical analysis on representations and the sharpness of minima\nin the loss landscape. Our findings, therefore, suggest that, in practice, for\nSSL, the cumbersome joint training can be replaced mainly by sequential\nlearning, which in turn enables a much broader spectrum of potential\napplication scenarios.\n","authors":["Dapeng Hu","Shipeng Yan","Qizhengqiu Lu","Lanqing Hong","Hailin Hu","Yifan Zhang","Zhenguo Li","Xinchao Wang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2104.12081v3.pdf","comment":"Accepted to ICLR 2022"},{"id":"http://arxiv.org/abs/2207.10255v1","updated":"2022-07-21T01:37:07Z","published":"2022-07-21T01:37:07Z","title":"SplitMixer: Fat Trimmed From MLP-like Models","summary":"  We present SplitMixer, a simple and lightweight isotropic MLP-like\narchitecture, for visual recognition. It contains two types of interleaving\nconvolutional operations to mix information across spatial locations (spatial\nmixing) and channels (channel mixing). The first one includes sequentially\napplying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial\ninformation. The second one is splitting the channels into overlapping or\nnon-overlapping segments, with or without shared parameters, and applying our\nproposed channel mixing approaches or 3D convolution to mix channel\ninformation. Depending on design choices, a number of SplitMixer variants can\nbe constructed to balance accuracy, the number of parameters, and speed. We\nshow, both theoretically and experimentally, that SplitMixer performs on par\nwith the state-of-the-art MLP-like models while having a significantly lower\nnumber of parameters and FLOPS. For example, without strong data augmentation\nand optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only\n0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M\nparameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On\nCIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with\nConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our\nresults spark further research towards finding more efficient vision\narchitectures and facilitate the development of MLP-like models. Code is\navailable at https://github.com/aliborji/splitmixer.\n","authors":["Ali Borji","Sikun Lin"],"pdf_url":"https://arxiv.org/pdf/2207.10255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.03938v3","updated":"2022-07-21T01:36:31Z","published":"2020-02-10T16:47:57Z","title":"Distribution Approximation and Statistical Estimation Guarantees of\n  Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have achieved a great success in\nunsupervised learning. Despite its remarkable empirical performance, there are\nlimited theoretical studies on the statistical properties of GANs. This paper\nprovides approximation and statistical guarantees of GANs for the estimation of\ndata distributions that have densities in a H\\\"{o}lder space. Our main result\nshows that, if the generator and discriminator network architectures are\nproperly chosen, GANs are consistent estimators of data distributions under\nstrong discrepancy metrics, such as the Wasserstein-1 distance. Furthermore,\nwhen the data distribution exhibits low-dimensional structures, we show that\nGANs are capable of capturing the unknown low-dimensional structures in data\nand enjoy a fast statistical convergence, which is free of curse of the ambient\ndimensionality. Our analysis for low-dimensional data builds upon a universal\napproximation theory of neural networks with Lipschitz continuity guarantees,\nwhich may be of independent interest.\n","authors":["Minshuo Chen","Wenjing Liao","Hongyuan Zha","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2002.03938v3.pdf","comment":"Version two extends to low-dimensional linear and mixture\n  distributions"},{"id":"http://arxiv.org/abs/2207.10246v1","updated":"2022-07-21T01:00:40Z","published":"2022-07-21T01:00:40Z","title":"GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection","summary":"  Facial forgery by deepfakes has raised severe societal concerns. Several\nsolutions have been proposed by the vision community to effectively combat the\nmisinformation on the internet via automated deepfake detection systems. Recent\nstudies have demonstrated that facial analysis-based deep learning models can\ndiscriminate based on protected attributes. For the commercial adoption and\nmassive roll-out of the deepfake detection technology, it is vital to evaluate\nand understand the fairness (the absence of any prejudice or favoritism) of\ndeepfake detectors across demographic variations such as gender and race. As\nthe performance differential of deepfake detectors between demographic\nsubgroups would impact millions of people of the deprived sub-group. This paper\naims to evaluate the fairness of the deepfake detectors across males and\nfemales. However, existing deepfake datasets are not annotated with demographic\nlabels to facilitate fairness analysis. To this aim, we manually annotated\nexisting popular deepfake datasets with gender labels and evaluated the\nperformance differential of current deepfake detectors across gender. Our\nanalysis on the gender-labeled version of the datasets suggests (a) current\ndeepfake datasets have skewed distribution across gender, and (b) commonly\nadopted deepfake detectors obtain unequal performance across gender with mostly\nmales outperforming females. Finally, we contributed a gender-balanced and\nannotated deepfake dataset, GBDF, to mitigate the performance differential and\nto promote research and development towards fairness-aware deep fake detectors.\nThe GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF\n","authors":["Aakash Varma Nadimpalli","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2207.10246v1.pdf","comment":"26th International Conference on Pattern Recognition (ICPR) 2022|\n  Montreal, Canada"},{"id":"http://arxiv.org/abs/2206.00065v3","updated":"2022-07-21T00:55:25Z","published":"2022-05-31T19:19:40Z","title":"FELARE: Fair Scheduling of Machine Learning Tasks on Heterogeneous Edge\n  Systems","summary":"  Edge computing enables smart IoT-based systems via concurrent and continuous\nexecution of latency-sensitive machine learning (ML) applications. These\nedge-based machine learning systems are often battery-powered (i.e.,\nenergy-limited). They use heterogeneous resources with diverse computing\nperformance (e.g., CPU, GPU, and/or FPGAs) to fulfill the latency constraints\nof ML applications. The challenge is to allocate user requests for different ML\napplications on the Heterogeneous Edge Computing Systems (HEC) with respect to\nboth the energy and latency constraints of these systems. To this end, we study\nand analyze resource allocation solutions that can increase the on-time task\ncompletion rate while considering the energy constraint. Importantly, we\ninvestigate edge-friendly (lightweight) multi-objective mapping heuristics that\ndo not become biased toward a particular application type to achieve the\nobjectives; instead, the heuristics consider \"fairness\" across the concurrent\nML applications in their mapping decisions. Performance evaluations demonstrate\nthat the proposed heuristic outperforms widely-used heuristics in heterogeneous\nsystems in terms of the latency and energy objectives, particularly, at low to\nmoderate request arrival rates. We observed 8.9% improvement in on-time task\ncompletion rate and 12.6% in energy-saving without imposing any significant\noverhead on the edge system.\n","authors":["Ali Mokhtari","Md Abir Hossen","Pooyan Jamshidi","Mohsen Amini Salehi"],"pdf_url":"https://arxiv.org/pdf/2206.00065v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10241v1","updated":"2022-07-21T00:47:47Z","published":"2022-07-21T00:47:47Z","title":"Unsupervised Legendre-Galerkin Neural Network for Stiff Partial\n  Differential Equations","summary":"  Machine learning methods have been lately used to solve differential\nequations and dynamical systems. These approaches have been developed into a\nnovel research field known as scientific machine learning in which techniques\nsuch as deep neural networks and statistical learning are applied to classical\nproblems of applied mathematics. Because neural networks provide an\napproximation capability, computational parameterization through machine\nlearning and optimization methods achieve noticeable performance when solving\nvarious partial differential equations (PDEs). In this paper, we develop a\nnovel numerical algorithm that incorporates machine learning and artificial\nintelligence to solve PDEs. In particular, we propose an unsupervised machine\nlearning algorithm based on the Legendre-Galerkin neural network to find an\naccurate approximation to the solution of different types of PDEs. The proposed\nneural network is applied to the general 1D and 2D PDEs as well as singularly\nperturbed PDEs that possess boundary layer behavior.\n","authors":["Junho Choi","Namjung Kim","Youngjoon Hong"],"pdf_url":"https://arxiv.org/pdf/2207.10241v1.pdf","comment":"29 pages, 8 figures"},{"id":"http://arxiv.org/abs/2207.06819v3","updated":"2022-07-21T00:23:48Z","published":"2022-07-14T10:59:39Z","title":"Anomal-E: A Self-Supervised Network Intrusion Detection System based on\n  Graph Neural Networks","summary":"  This paper investigates Graph Neural Networks (GNNs) application for\nself-supervised network intrusion and anomaly detection. GNNs are a deep\nlearning approach for graph-based data that incorporate graph structures into\nlearning to generalise graph representations and output embeddings. As network\nflows are naturally graph-based, GNNs are a suitable fit for analysing and\nlearning network behaviour. The majority of current implementations of\nGNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled\nnetwork traffic which can not only restrict the amount and structure of input\ntraffic, but also the NIDSs potential to adapt to unseen attacks. To overcome\nthese restrictions, we present Anomal-E, a GNN approach to intrusion and\nanomaly detection that leverages edge features and graph topological structure\nin a self-supervised process. This approach is, to the best our knowledge, the\nfirst successful and practical approach to network intrusion detection that\nutilises network flows in a self-supervised, edge leveraging GNN. Experimental\nresults on two modern benchmark NIDS datasets not only clearly display the\nimprovement of using Anomal-E embeddings rather than raw features, but also the\npotential Anomal-E has for detection on wild network traffic.\n","authors":["Evan Caville","Wai Weng Lo","Siamak Layeghy","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2207.06819v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2112.02274v4","updated":"2022-07-21T08:24:27Z","published":"2021-12-04T08:10:06Z","title":"Self-supervised Graph Learning for Occasional Group Recommendation","summary":"  As an important branch in Recommender System, occasional group recommendation\nhas received more and more attention. In this scenario, each occasional group\n(cold-start group) has no or few historical interacted items. As each\noccasional group has extremely sparse interactions with items, traditional\ngroup recommendation methods can not learn high-quality group representations.\nThe recent proposed Graph Neural Networks (GNNs), which incorporate the\nhigh-order neighbors of the target occasional group, can alleviate the above\nproblem in some extent. However, these GNNs still can not explicitly strengthen\nthe embedding quality of the high-order neighbors with few interactions.\nMotivated by the Self-supervised Learning technique, which is able to find the\ncorrelations within the data itself, we propose a self-supervised graph\nlearning framework, which takes the user/item/group embedding reconstruction as\nthe pretext task to enhance the embeddings of the cold-start\nusers/items/groups. In order to explicitly enhance the high-order cold-start\nneighbors' embedding quality, we further introduce an embedding enhancer, which\nleverages the self-attention mechanism to improve the embedding quality for\nthem. Comprehensive experiments show the advantages of our proposed framework\nthan the state-of-the-art methods.\n","authors":["Bowen Hao","Hongzhi Yin","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2112.02274v4.pdf","comment":"This paper uses self-supervised learning technique to enhance the\n  embeddings of users/groups/items, the idea is novel in group recommendation\n  scenario. However, some presentations need to be revised, so as to let the\n  readers understand"},{"id":"http://arxiv.org/abs/2207.10355v1","updated":"2022-07-21T08:20:24Z","published":"2022-07-21T08:20:24Z","title":"Unimodal vs. Multimodal Siamese Networks for Outfit Completion","summary":"  The popularity of online fashion shopping continues to grow. The ability to\noffer an effective recommendation to customers is becoming increasingly\nimportant. In this work, we focus on Fashion Outfits Challenge, part of SIGIR\n2022 Workshop on eCommerce. The challenge is centered around Fill in the Blank\n(FITB) task that implies predicting the missing outfit, given an incomplete\noutfit and a list of candidates. In this paper, we focus on applying siamese\nnetworks on the task. More specifically, we explore how combining information\nfrom multiple modalities (textual and visual modality) impacts the performance\nof the model on the task. We evaluate our model on the test split provided by\nthe challenge organizers and the test split with gold assignments that we\ncreated during the development phase. We discover that using both visual, and\nvisual and textual data demonstrates promising results on the task. We conclude\nby suggesting directions for further improvement of our method.\n","authors":["Mariya Hendriksen","Viggo Overes"],"pdf_url":"https://arxiv.org/pdf/2207.10355v1.pdf","comment":"3 pages, 2 figures"},{"id":"http://arxiv.org/abs/2201.00689v2","updated":"2022-07-21T06:41:13Z","published":"2021-12-21T01:59:16Z","title":"CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch\n  Attribution","summary":"  Multi-touch attribution (MTA), aiming to estimate the contribution of each\nadvertisement touchpoint in conversion journeys, is essential for budget\nallocation and automatically advertising. Existing methods first train a model\nto predict the conversion probability of the advertisement journeys with\nhistorical data and calculate the attribution of each touchpoint using\ncounterfactual predictions. An assumption of these works is the conversion\nprediction model is unbiased, i.e., it can give accurate predictions on any\nrandomly assigned journey, including both the factual and counterfactual ones.\nNevertheless, this assumption does not always hold as the exposed\nadvertisements are recommended according to user preferences. This confounding\nbias of users would lead to an out-of-distribution (OOD) problem in the\ncounterfactual prediction and cause concept drift in attribution. In this\npaper, we define the causal MTA task and propose CausalMTA to eliminate the\ninfluence of user preferences. It systemically eliminates the confounding bias\nfrom both static and dynamic preferences to learn the conversion prediction\nmodel using historical data. We also provide a theoretical analysis to prove\nCausalMTA can learn an unbiased prediction model with sufficient data.\nExtensive experiments on both public datasets and the impression data in an\ne-commerce company show that CausalMTA not only achieves better prediction\nperformance than the state-of-the-art method but also generates meaningful\nattribution credits across different advertising channels.\n","authors":["Di Yao","Chang Gong","Lei Zhang","Sheng Chen","Jingping Bi"],"pdf_url":"https://arxiv.org/pdf/2201.00689v2.pdf","comment":"11 pages, 5 figures This paper has been accepted in KDD 2022"},{"id":"http://arxiv.org/abs/2207.06652v2","updated":"2022-07-21T05:05:16Z","published":"2022-07-14T04:29:54Z","title":"Every Preference Changes Differently: Neural Multi-Interest Preference\n  Model with Temporal Dynamics for Recommendation","summary":"  User embeddings (vectorized representations of a user) are essential in\nrecommendation systems. Numerous approaches have been proposed to construct a\nrepresentation for the user in order to find similar items for retrieval tasks,\nand they have been proven effective in industrial recommendation systems as\nwell. Recently people have discovered the power of using multiple embeddings to\nrepresent a user, with the hope that each embedding represents the user's\ninterest in a certain topic. With multi-interest representation, it's important\nto model the user's preference over the different topics and how the preference\nchange with time. However, existing approaches either fail to estimate the\nuser's affinity to each interest or unreasonably assume every interest of every\nuser fades with an equal rate with time, thus hurting the recall of candidate\nretrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,\nan approach that not only produces multi-interest for users by using the user's\nsequential engagement more effectively but also automatically learns a set of\nweights to represent the preference over each embedding so that the candidates\ncan be retrieved from each interest proportionally. Extensive experiments have\nbeen done on various industrial-scale datasets to demonstrate the effectiveness\nof our approach.\n","authors":["Hui Shi","Yupeng Gu","Yitong Zhou","Bo Zhao","Sicun Gao","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2207.06652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10307v1","updated":"2022-07-21T04:59:31Z","published":"2022-07-21T04:59:31Z","title":"Knowledge-enhanced Black-box Attacks for Recommendations","summary":"  Recent studies have shown that deep neural networks-based recommender systems\nare vulnerable to adversarial attacks, where attackers can inject carefully\ncrafted fake user profiles (i.e., a set of items that fake users have\ninteracted with) into a target recommender system to achieve malicious\npurposes, such as promote or demote a set of target items. Due to the security\nand privacy concerns, it is more practical to perform adversarial attacks under\nthe black-box setting, where the architecture/parameters and training data of\ntarget systems cannot be easily accessed by attackers. However, generating\nhigh-quality fake user profiles under black-box setting is rather challenging\nwith limited resources to target systems. To address this challenge, in this\nwork, we introduce a novel strategy by leveraging items' attribute information\n(i.e., items' knowledge graph), which can be publicly accessible and provide\nrich auxiliary knowledge to enhance the generation of fake user profiles. More\nspecifically, we propose a knowledge graph-enhanced black-box attacking\nframework (KGAttack) to effectively learn attacking policies through deep\nreinforcement learning techniques, in which knowledge graph is seamlessly\nintegrated into hierarchical policy networks to generate fake user profiles for\nperforming adversarial black-box attacks. Comprehensive experiments on various\nreal-world datasets demonstrate the effectiveness of the proposed attacking\nframework under the black-box setting.\n","authors":["Jingfan Chen","Wenqi Fan","Guanghui Zhu","Xiangyu Zhao","Chunfeng Yuan","Qing Li","Yihua Huang"],"pdf_url":"https://arxiv.org/pdf/2207.10307v1.pdf","comment":"Accepted in the KDD'22"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.10355v1","updated":"2022-07-21T08:20:24Z","published":"2022-07-21T08:20:24Z","title":"Unimodal vs. Multimodal Siamese Networks for Outfit Completion","summary":"  The popularity of online fashion shopping continues to grow. The ability to\noffer an effective recommendation to customers is becoming increasingly\nimportant. In this work, we focus on Fashion Outfits Challenge, part of SIGIR\n2022 Workshop on eCommerce. The challenge is centered around Fill in the Blank\n(FITB) task that implies predicting the missing outfit, given an incomplete\noutfit and a list of candidates. In this paper, we focus on applying siamese\nnetworks on the task. More specifically, we explore how combining information\nfrom multiple modalities (textual and visual modality) impacts the performance\nof the model on the task. We evaluate our model on the test split provided by\nthe challenge organizers and the test split with gold assignments that we\ncreated during the development phase. We discover that using both visual, and\nvisual and textual data demonstrates promising results on the task. We conclude\nby suggesting directions for further improvement of our method.\n","authors":["Mariya Hendriksen","Viggo Overes"],"pdf_url":"https://arxiv.org/pdf/2207.10355v1.pdf","comment":"3 pages, 2 figures"},{"id":"http://arxiv.org/abs/2207.07394v3","updated":"2022-07-21T07:11:36Z","published":"2022-07-15T10:48:41Z","title":"FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud\n  Video Streaming","summary":"  Point cloud video transmission is challenging due to high encoding/decoding\ncomplexity, high video bitrate, and low latency requirement. Consequently,\nconventional adaptive streaming methodologies often find themselves\nunsatisfactory to meet the requirements in threefold: 1) current algorithms\nreuse existing quality of experience (QoE) definitions while overlooking the\nunique features of point cloud video thus failing to provide optimal user\nexperience, 2) most deep learning approaches require long-span data collections\nto learn sufficiently varied network conditions and result in long training\nperiod and capacity occupation, 3) cloud training approaches pose privacy risks\ncaused by leakage of user reported service usage and networking conditions.\n  To overcome the limitations, we present FRAS, the first federated\nreinforcement learning framework, to the best of our knowledge, for adaptive\npoint cloud video streaming. We define a new QoE model which takes the unique\nfeatures of point cloud video into account. Each client uses reinforcement\nlearning (RL) to train encoding rate selection with the objective of optimizing\nthe user's QoE under multiple constraints. Then, a federated learning framework\nis integrated with the RL algorithm to enhance training performance with\nprivacy preservation. Extensive simulations using real point cloud videos and\nnetwork traces reveal the superiority of the proposed scheme over baseline\nschemes. We also implement a prototype that demonstrates the performance of\nFRAS via real-world tests.\n","authors":["Yu Gao","Zhi Liu","Bo Han","Pan Hui","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2207.07394v3.pdf","comment":null}]}}