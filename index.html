<!DOCTYPE html>
<html lang="en">

<head>
    <title>ArxivDaily</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <a href="https://github.com/LooperXX/ArxivDaily" style="text-decoration: none;">
                <div class="header-title">
                    <span class="header-title-preffix">LooperXX</span>//ArxivDaily
                </div>
            </a>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-21T00:00:00Z">2022-07-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Natural Supervision for Language Representation Learning and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in Natural Language Processing (NLP) have been driven by
language models trained on a massive amount of plain text. While powerful,
deriving supervision from textual resources is still an open question. For
example, language model pretraining often neglects the rich, freely-available
structures in textual data. In this thesis, we describe three lines of work
that seek to improve the training and evaluation of neural models using
naturally-occurring supervision.
  We first investigate self-supervised training losses to help enhance the
performance of pretrained language models for various NLP tasks. Specifically,
we alter the sentence prediction loss to make it better suited to other
pretraining losses and more challenging to solve. We design an intermediate
finetuning step that uses self-supervised training to promote models' ability
in cross-task generalization.
  Then we describe methods to leverage the structures in Wikipedia and
paraphrases. In particular, we propose training losses to exploit hyperlinks,
article structures, and article category graphs for entity-, discourse-,
entailment-related knowledge. We propose a framework that uses paraphrase pairs
to disentangle semantics and syntax in sentence representations. We extend the
framework for a novel generation task that controls the syntax of output text
with a sentential exemplar.
  Lastly, we discuss our work on tailoring textual resources for establishing
challenging evaluation tasks. We introduce three datasets by defining novel
tasks using various fan-contributed websites, including a long-form
data-to-text generation dataset, a screenplay summarization dataset, and a
long-form story generation dataset. These datasets have unique characteristics
offering challenges to future work in their respective task settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Scaling Laws vs Model Architectures: How does Inductive Bias Influence
  Scaling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, <span class="highlight-author">William Fedus</span>, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been a lot of interest in the scaling properties of Transformer
models. However, not much has been done on the front of investigating the
effect of scaling properties of different inductive biases and model
architectures. Do model architectures scale differently? If so, how does
inductive bias affect scaling behaviour? How does this influence upstream
(pretraining) and downstream (transfer)? This paper conducts a systematic study
of scaling behaviour of ten diverse model architectures such as Transformers,
Switch Transformers, Universal Transformers, Dynamic convolutions, Performers,
and recently proposed MLP-Mixers. Via extensive experiments, we show that (1)
architecture is an indeed an important consideration when performing scaling
and (2) the best performing model can fluctuate at different scales. We believe
that the findings outlined in this work has significant implications to how
model architectures are currently evaluated in the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Alham Fikri Aji, Holy Lovenia, Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Fajri Koto, David Moeljadi, Karissa Vincentio, Ade Romadhony, Ayu Purwarianti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At the center of the underlying issues that halt Indonesian natural language
processing (NLP) research advancement, we find data scarcity. Resources in
Indonesian languages, especially the local ones, are extremely scarce and
underrepresented. Many Indonesian researchers do not publish their dataset.
Furthermore, the few public datasets that we have are scattered across
different platforms, thus makes performing reproducible and data-centric
research in Indonesian NLP even more arduous. Rising to this challenge, we
initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd
strives to provide the largest datasheets aggregation with standardized data
loading for NLP tasks in all Indonesian languages. By enabling open and
centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle
the data scarcity problem hindering NLP progress in Indonesia and bring NLP
practitioners to move towards collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeT: Code Generation with Generated Tests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a programming problem, pre-trained language models such as Codex have
demonstrated the ability to generate multiple different code solutions via
sampling. However, selecting a correct or best solution from those samples
still remains a challenge. While an easy way to verify the correctness of a
code solution is through executing test cases, producing high-quality test
cases is prohibitively expensive. In this paper, we explore the use of
pre-trained language models to automatically generate test cases, calling our
method CodeT: Code generation with generated Tests. CodeT executes the code
solutions using the generated test cases, and then chooses the best solution
based on a dual execution agreement with both the generated test cases and
other generated solutions. We evaluate CodeT on five different pre-trained
models with both HumanEval and MBPP benchmarks. Extensive experimental results
demonstrate CodeT can achieve significant, consistent, and surprising
improvements over previous methods. For example, CodeT improves the pass@1 on
HumanEval to 65.8%, an increase of absolute 18.8% on the code-davinci-002
model, and an absolute 20+% improvement over previous state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Cascades <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompted models have demonstrated impressive few-shot learning abilities.
Repeated interactions at test-time with a single model, or the composition of
multiple models together, further expands capabilities. These compositions are
probabilistic models, and may be expressed in the language of graphical models
with random variables whose values are complex data types such as strings.
Cases with control flow and dynamic structure require techniques from
probabilistic programming, which allow implementing disparate model structures
and inference strategies in a unified language. We formalize several existing
techniques from this perspective, including scratchpads / chain of thought,
verifiers, STaR, selection-inference, and tool use. We refer to the resulting
programs as language model cascades.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as spotlight at the Beyond Bases workshop at ICML 2022
  (https://beyond-bayes.github.io)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Visual Representations with Texts for Domain Generalization <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonwoo Min, Nokyung Park, Siwon Kim, Seunghyun Park, Jinkyu Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the representational discrepancy between source and target domains
is a key component to maximize the model generalization. In this work, we
advocate for leveraging natural language supervision for the domain
generalization task. We introduce two modules to ground visual representations
with texts containing typical reasoning of humans: (1) Visual and Textual Joint
Embedder and (2) Textual Explanation Generator. The former learns the
image-text joint embedding space where we can ground high-level
class-discriminative information into the model. The latter leverages an
explainable model and generates explanations justifying the rationale behind
its decision. To the best of our knowledge, this is the first work to leverage
the vision-and-language cross-modality approach for the domain generalization
task. Our experiments with a newly created CUB-DG benchmark dataset demonstrate
that cross-modality supervision can be successfully used to ground
domain-invariant visual representations and improve the model generalization.
Furthermore, in the large-scale DomainBed benchmark, our proposed method
achieves state-of-the-art results and ranks 1st in average performance for five
multi-domain datasets. The dataset and codes are available at
https://github.com/mswzeus/GVRT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages (including Supplementary Materials), ECCV 2022 camera ready
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Resolution Analysis (MRA) for Approximate Self-Attention <span class="chip">ICML2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have emerged as a preferred model for many tasks in natural
langugage processing and vision. Recent efforts on training and deploying
Transformers more efficiently have identified many strategies to approximate
the self-attention matrix, a key module in a Transformer architecture.
Effective ideas include various prespecified sparsity patterns, low-rank basis
expansions and combinations thereof. In this paper, we revisit classical
Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value
in this setting remains underexplored thus far. We show that simple
approximations based on empirical feedback and design choices informed by
modern hardware and implementation challenges, eventually yield a MRA-based
approach for self-attention with an excellent performance profile across most
criteria of interest. We undertake an extensive set of experiments and
demonstrate that this multi-resolution scheme outperforms most efficient
self-attention proposals and is favorable for both short and long sequences.
Code is available at \url{https://github.com/mlpen/mra-attention}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Birth of Bias: A case study on the evolution of gender bias in an
  English language model <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar van der Wal, Jaap Jumelet, Katrin Schulz, Willem Zuidema
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and mitigating harmful biases in modern language models are widely
recognized as crucial, open problems. In this paper, we take a step back and
investigate how language models come to be biased in the first place. We use a
relatively small language model, using the LSTM architecture trained on an
English Wikipedia corpus. With full access to the data and to the model
parameters as they change during every step while training, we can map in
detail how the representation of gender develops, what patterns in the dataset
drive this, and how the model's internal state relates to the bias in a
downstream task (semantic textual similarity). We find that the representation
of gender is dynamic and identify different phases during training.
Furthermore, we show that gender information is represented increasingly
locally in the input embeddings of the model and that, as a consequence,
debiasing these can be effective in reducing the downstream bias. Monitoring
the training dynamics, allows us to detect an asymmetry in how the female and
male gender are represented in the input embeddings. This is important, as it
may cause naive mitigation strategies to introduce new undesirable biases. We
discuss the relevance of the findings for mitigation strategies more generally
and the prospects of generalizing our methods to larger language models, the
Transformer architecture, other languages and other undesirable biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 4th Workshop on Gender Bias in Natural Language
  Processing (NAACL, 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP2TV: Align, Match and Distill for Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.05610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.05610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Gao, Jingyu Liu, Weiqi Sun, Sheng Chen, Dedan Chang, Lili Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern video-text retrieval frameworks basically consist of three parts:
video encoder, text encoder and the similarity head. With the success on both
visual and textual representation learning, transformer based encoders and
fusion methods have also been adopted in the field of video-text retrieval. In
this report, we present CLIP2TV, aiming at exploring where the critical
elements lie in transformer based methods. To achieve this, We first revisit
some recent works on multi-modal learning, then introduce some techniques into
video-text retrieval, finally evaluate them through extensive experiments in
different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,
outperforming the previous SOTA result by 4.1%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Learning the <span class="highlight-title">Transformer</span> Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankalan Pal Chowdhury, Adamos Solomou, Avinava Dubey, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data
driven framework for learning the kernel function in Transformers. Our
framework approximates the Transformer kernel as a dot product between spectral
feature maps and learns the kernel by learning the spectral distribution. This
not only helps in learning a generic kernel end-to-end, but also reduces the
time and space complexity of Transformers from quadratic to linear. We show
that KERNELIZED TRANSFORMERS achieve performance comparable to existing
efficient Transformer architectures, both in terms of accuracy as well as
computational efficiency. Our study also demonstrates that the choice of the
kernel has a substantial impact on performance, and kernel learning variants
are competitive alternatives to fixed kernel Transformers, both in long as well
as short sequence tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making the Most of Text Semantics to Improve Biomedical Vision--Language
  Processing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09817v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09817v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, Ozan Oktay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:
  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFactorGNNs: Revisiting Factorisation-based Models from a
  Message-Passing Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and to generalise to unseen nodes in inductive settings. Our work
bridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new
architecture draws upon both modelling paradigms, which previously were largely
thought of as disjoint. Concretely, using a message-passing formalism, we show
how FMs can be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactorGNNs. Across a
multitude of well-established KGC benchmarks, our ReFactorGNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, <span class="highlight-author">Jie Lei</span>, <span class="highlight-author">Mohit Bansal</span>, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an audiovisual method for long-range text-to-video retrieval.
Unlike previous approaches designed for short video retrieval (e.g., 5-15
seconds in duration), our approach aims to retrieve minute-long videos that
capture complex human actions. One challenge of standard video-only approaches
is the large computational cost associated with processing hundreds of densely
extracted frames from such long videos. To address this issue, we propose to
replace parts of the video with compact audio cues that succinctly summarize
dynamic audio events and are cheap to process. Our method, named ECLIPSE
(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an
audiovisual video setting, by adding a unified audiovisual transformer block
that captures complementary cues from the video and audio streams. In addition
to being 2.92x faster and 2.34x memory-efficient than long-range video-only
approaches, our method also achieves better text-to-video retrieval accuracy on
several diverse long-range video datasets such as ActivityNet, QVHighlights,
YouCook2, DiDeMo and Charades.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral project page: https://yanbo.ml/project_page/eclipse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> An Explanation of In-context Learning as Implicit Bayesian Inference <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02080v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02080v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang Michael Xie, Aditi Raghunathan, <span class="highlight-author">Percy Liang</span>, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is
unclear what enables in-context learning. In this paper, we study how
in-context learning can emerge when pretraining documents have long-range
coherence. Here, the LM must infer a latent document-level concept to generate
coherent next tokens during pretraining. At test time, in-context learning
occurs when the LM also infers a shared latent concept between examples in a
prompt. We prove when this occurs despite a distribution mismatch between
prompts and pretraining data in a setting where the pretraining distribution is
a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs
capable of in-context learning, we generate a small-scale synthetic dataset
(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond
the theory, experiments on GINC exhibit large-scale real-world phenomena
including improved in-context performance with model scaling (despite the same
pretraining loss), sensitivity to example order, and instances where zero-shot
is better than few-shot in-context learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bottom Up Top Down Detection <span class="highlight-title">Transformer</span>s for Language Grounding in
  Images and Point Clouds <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08879v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08879v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most models tasked to ground referential utterances in 2D and 3D scenes learn
to select the referred object from a pool of object proposals provided by a
pre-trained detector. This is limiting because an utterance may refer to visual
entities at various levels of granularity, such as the chair, the leg of the
chair, or the tip of the front leg of the chair, which may be missed by the
detector. We propose a language grounding model that attends on the referential
utterance and on the object proposal pool computed from a pre-trained detector
to decode referenced objects with a detection head, without selecting them from
the pool. In this way, it is helped by powerful pre-trained object detectors
without being restricted by their misses. We call our model Bottom Up Top Down
DEtection TRansformers (BUTD-DETR) because it uses both language guidance (top
down) and objectness guidance (bottom-up) to ground referential utterances in
images and point clouds. Moreover, BUTD-DETR casts object detection as
referential grounding and uses object labels as language prompts to be grounded
in the visual scene, augmenting supervision for the referential grounding task
in this way. The proposed model sets a new state-of-the-art across popular 3D
language grounding benchmarks with significant performance gains over previous
3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When
applied in 2D images, it performs on par with the previous state of the art. We
ablate the design choices of our model and quantify their contribution to
performance. Our code and checkpoints can be found at the project website
https://butd-detr.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally | ECCV 2022 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping a User-Centered <span class="highlight-title">Task-Oriented</span> <span class="highlight-title">Dialogue</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Lingbo Mo, Samuel Stevens, Zhen Wang, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TacoBot, a task-oriented dialogue system built for the inaugural
Alexa Prize TaskBot Challenge, which assists users in completing multi-step
cooking and home improvement tasks. TacoBot is designed with a user-centered
principle and aspires to deliver a collaborative and accessible dialogue
experience. Towards that end, it is equipped with accurate language
understanding, flexible dialogue management, and engaging response generation.
Furthermore, TacoBot is backed by a strong search engine and an automated
end-to-end test suite. In bootstrapping the development of TacoBot, we explore
a series of data augmentation strategies to train advanced neural language
processing models and continuously improve the dialogue experience with
collected real conversations. At the end of the semifinals, TacoBot achieved an
average rating of 3.55/5.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 1st Proceedings of Alexa Prize TaskBot (Alexa Prize
  2021). TacoBot won 3rd place in the challenge. See project website
  https://sunlab-osu.github.io/tacobot/ for details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive</span> Triple Extraction with Generative <span class="highlight-title">Transformer</span> <span class="chip">AAAI 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.06207v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.06207v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Triple extraction is an essential task in information extraction for natural
language processing and knowledge graph construction. In this paper, we revisit
the end-to-end triple extraction task for sequence generation. Since generative
triple extraction may struggle to capture long-term dependencies and generate
unfaithful triples, we introduce a novel model, contrastive triple extraction
with a generative transformer. Specifically, we introduce a single shared
transformer module for encoder-decoder-based generation. To generate faithful
results, we propose a novel triplet contrastive training object. Moreover, we
introduce two mechanisms to further improve model performance (i.e., batch-wise
dynamic attention-masking and triple-wise calibration). Experimental results on
three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves
better performance than that of baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion analysis and detection during COVID-19 <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.11020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.11020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiberiu Sosea, Chau Pham, Alexander Tekle, Cornelia Caragea, Junyi Jessy Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crises such as natural disasters, global pandemics, and social unrest
continuously threaten our world and emotionally affect millions of people
worldwide in distinct ways. Understanding emotions that people express during
large-scale crises helps inform policy makers and first responders about the
emotional states of the population as well as provide emotional support to
those who need such support. We present CovidEmo, ~3K English tweets labeled
with emotions and temporally distributed across 18 months. Our analyses reveal
the emotional toll caused by COVID-19, and changes of the social narrative and
associated emotions over time. Motivated by the time-sensitive nature of crises
and the cost of large-scale annotation efforts, we examine how well large
pre-trained language models generalize across domains and timeline in the task
of perceived emotion prediction in the context of COVID-19. Our analyses
suggest that cross-domain information transfers occur, yet there are still
significant gaps. We propose semi-supervised learning as a way to bridge this
gap, obtaining significantly better performance using unlabeled data from the
target domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Webly Supervised Concept Expansion for General Purpose Vision Models <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, Aniruddha Kembhavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General Purpose Vision (GPV) systems are models that are designed to solve a
wide array of visual tasks without requiring architectural changes. Today, GPVs
primarily learn both skills and concepts from large fully supervised datasets.
Scaling GPVs to tens of thousands of concepts by acquiring data to learn each
concept for every skill quickly becomes prohibitive. This work presents an
effective and inexpensive alternative: learn skills from supervised datasets,
learn concepts from web image search, and leverage a key characteristic of
GPVs: the ability to transfer visual knowledge across skills. We use a dataset
of 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised
concept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5
COCO-based datasets (80 primary concepts), a newly curated series of 5 datasets
based on the OpenImages and VisualGenome repositories (~500 concepts), and the
Web-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2
that supports a variety of tasks -- from vision tasks like classification and
localization to vision+language tasks like QA and captioning, to more niche
ones like human-object interaction detection. GPV-2 benefits hugely from web
data and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,
and web demo are available at https://prior.allenai.org/projects/gpv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Domain Adaptation for Semantic Segmentation in Ever-Changing
  Conditions <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Panagiotakopoulos, Pier Luigi Dovesi, Linus Härenstam-Nielsen, Matteo Poggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between
training and testing data and is, in most cases, carried out in offline manner.
However, domain changes may occur continuously and unpredictably during
deployment (e.g. sudden weather changes). In such conditions, deep neural
networks witness dramatic drops in accuracy and offline adaptation may not be
enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA)
for semantic segmentation. We design a pipeline that is robust to continuous
domain shifts, either gradual or sudden, and we evaluate it in the case of
rainy and foggy scenarios. Our experiments show that our framework can
effectively adapt to new domains during deployment, while not being affected by
catastrophic forgetting of the previous domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project page: https://theo2021.github.io/onda-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> TinyViT: Fast <span class="highlight-title">Pretrain</span>ing <span class="highlight-title">Distillation</span> for Small Vision <span class="highlight-title">Transformer</span>s <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, <span class="highlight-author">Jianlong Fu</span>, <span class="highlight-author">Lu Yuan</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformer (ViT) recently has drawn great attention in computer
vision due to its remarkable model capability. However, most prevailing ViT
models suffer from huge number of parameters, restricting their applicability
on devices with limited resources. To alleviate this issue, we propose TinyViT,
a new family of tiny and efficient small vision transformers pretrained on
large-scale datasets with our proposed fast distillation framework. The central
idea is to transfer knowledge from large pretrained models to small ones, while
enabling small models to get the dividends of massive pretraining data. More
specifically, we apply distillation during pretraining for knowledge transfer.
The logits of large teacher models are sparsified and stored in disk in advance
to save the memory cost and computation overheads. The tiny student
transformers are automatically scaled down from a large pretrained model with
computation and parameter constraints. Comprehensive experiments demonstrate
the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k
with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k
while using 4.2 times fewer parameters. Moreover, increasing image resolutions,
TinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using
only 11% parameters. Last but not the least, we demonstrate a good transfer
ability of TinyViT on various downstream tasks. Code and models are available
at https://github.com/microsoft/Cream/tree/main/TinyViT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Fine-Grained Audiovisual Categorization with the SSW60 <span class="highlight-title">Dataset</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grant Van Horn, Rui Qian, Kimberly Wilber, Hartwig Adam, Oisin Mac Aodha, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing
research on audiovisual fine-grained categorization. While our community has
made great strides in fine-grained visual categorization on images, the
counterparts in audio and video fine-grained categorization are relatively
unexplored. To encourage advancements in this space, we have carefully
constructed the SSW60 dataset to enable researchers to experiment with
classifying the same set of categories in three different modalities: images,
audio, and video. The dataset covers 60 species of birds and is comprised of
images from existing datasets, and brand new, expert-curated audio and video
datasets. We thoroughly benchmark audiovisual classification performance and
modality fusion experiments through the use of state-of-the-art transformer
methods. Our findings show that performance of audiovisual fusion methods is
better than using exclusively image or audio based methods for the task of
video classification. We also present interesting modality transfer
experiments, enabled by the unique construction of SSW60 to encompass three
different modalities. We hope the SSW60 dataset and accompanying baselines spur
research in this fascinating area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aayush Bansal, Michael Zollhoefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Neural Pixel Composition (NPC), a novel approach for continuous
3D-4D view synthesis given only a discrete set of multi-view observations as
input. Existing state-of-the-art approaches require dense multi-view
supervision and an extensive computational budget. The proposed formulation
reliably operates on sparse and wide-baseline multi-view imagery and can be
trained efficiently within a few seconds to 10 minutes for hi-res (12MP)
content, i.e., 200-400X faster convergence than existing methods. Crucial to
our approach are two core novelties: 1) a representation of a pixel that
contains color and depth information accumulated from multi-views for a
particular location and time along a line of sight, and 2) a multi-layer
perceptron (MLP) that enables the composition of this rich information provided
for a pixel location to obtain the final color output. We experiment with a
large variety of multi-view sequences, compare to existing approaches, and
achieve better results in diverse and challenging settings. Finally, our
approach enables dense 3D reconstruction from sparse multi-views, where COLMAP,
a state-of-the-art 3D reconstruction approach, struggles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical report on 3D-4D view synthesis (40 pages, 22 figures and
  18 tables). High-resolution version of paper:
  http://www.aayushbansal.xyz/npc/npc_hi-res.pdf. Project page (containing
  video results): http://www.aayushbansal.xyz/npc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable Patch-Based Neural Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural rendering has received tremendous attention since the advent of Neural
Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view
synthesis considerably. The recent focus has been on models that overfit to a
single scene, and the few attempts to learn models that can synthesize novel
views of unseen scenes mostly consist of combining deep convolutional features
with a NeRF-like model. We propose a different paradigm, where no deep features
and no NeRF-like volume rendering are needed. Our method is capable of
predicting the color of a target ray in a novel scene directly, just from a
collection of patches sampled from the scene. We first leverage epipolar
geometry to extract patches along the epipolar lines of each reference view.
Each patch is linearly projected into a 1D feature vector and a sequence of
transformers process the collection. For positional encoding, we parameterize
rays as in a light field representation, with the crucial difference that the
coordinates are canonicalized with respect to the target ray, which makes our
method independent of the reference frame and improves generalization. We show
that our approach outperforms the state-of-the-art on novel view synthesis of
unseen scenes even when being trained with considerably less data than prior
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page with code and results at
  https://mohammedsuhail.net/gen_patch_neural_rendering/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In Defense of Online Models for Video Instance Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Wu, Qihao Liu, Yi Jiang, Song Bai, Alan Yuille, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, video instance segmentation (VIS) has been largely advanced
by offline models, while online models gradually attracted less attention
possibly due to their inferior performance. However, online methods have their
inherent advantage in handling long video sequences and ongoing videos while
offline models fail due to the limit of computational resources. Therefore, it
would be highly desirable if online models can achieve comparable or even
better performance than offline models. By dissecting current online models and
offline models, we demonstrate that the main cause of the performance gap is
the error-prone association between frames caused by the similar appearance
among different instances in the feature space. Observing this, we propose an
online framework based on contrastive learning that is able to learn more
discriminative instance embeddings for association and fully exploit history
information for stability. Despite its simplicity, our method outperforms all
online and offline methods on three benchmarks. Specifically, we achieve 49.5
AP on YouTube-VIS 2019, a significant improvement of 13.2 AP and 2.1 AP over
the prior online and offline art, respectively. Moreover, we achieve 30.2 AP on
OVIS, a more challenging dataset with significant crowding and occlusions,
surpassing the prior art by 14.8 AP. The proposed method won first place in the
video instance segmentation track of the 4th Large-scale Video Object
Segmentation Challenge (CVPR2022). We hope the simplicity and effectiveness of
our method, as well as our insight into current methods, could shed light on
the exploration of VIS models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Garrick Brazil, Julian Straub, Nikhila Ravi, <span class="highlight-author">Justin Johnson</span>, Georgia Gkioxari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing scenes and objects in 3D from a single image is a longstanding
goal of computer vision with applications in robotics and AR/VR. For 2D
recognition, large datasets and scalable solutions have led to unprecedented
advances. In 3D, existing benchmarks are small in size and approaches
specialize in few object categories and specific domains, e.g. urban driving
scenes. Motivated by the success of 2D recognition, we revisit the task of 3D
object detection by introducing a large benchmark, called Omni3D. Omni3D
re-purposes and combines existing datasets resulting in 234k images annotated
with more than 3 million instances and 97 categories.3D detection at such scale
is challenging due to variations in camera intrinsics and the rich diversity of
scene and object types. We propose a model, called Cube R-CNN, designed to
generalize across camera and scene types with a unified approach. We show that
Cube R-CNN outperforms prior works on the larger Omni3D and existing
benchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D object
recognition, show that it improves single-dataset performance and can
accelerate learning on new smaller datasets via pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://garrickbrazil.com/omni3d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel Class Discovery without Forgetting <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K J Joseph, Sujoy Paul, Gaurav Aggarwal, Soma Biswas, Piyush Rai, Kai Han, Vineeth N Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess an innate ability to identify and differentiate instances that
they are not familiar with, by leveraging and adapting the knowledge that they
have acquired so far. Importantly, they achieve this without deteriorating the
performance on their earlier learning. Inspired by this, we identify and
formulate a new, pragmatic problem setting of NCDwF: Novel Class Discovery
without Forgetting, which tasks a machine learning model to incrementally
discover novel categories of instances from unlabeled data, while maintaining
its performance on the previously seen categories. We propose 1) a method to
generate pseudo-latent representations which act as a proxy for (no longer
available) labeled data, thereby alleviating forgetting, 2) a
mutual-information based regularizer which enhances unsupervised discovery of
novel classes, and 3) a simple Known Class Identifier which aids generalized
inference when the testing data contains instances form both seen and unseen
categories. We introduce experimental protocols based on CIFAR-10, CIFAR-100
and ImageNet-1000 to measure the trade-off between knowledge retention and
novel class discovery. Our extensive evaluations reveal that existing models
catastrophically forget previously seen categories while identifying novel
categories, while our method is able to effectively balance between the
competing objectives. We hope our work will attract further research into this
newly identified pragmatic problem setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multiplane Images: Making a 2D GAN 3D-Aware <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoming Zhao, Fangchang Ma, David Güera, Zhile Ren, Alexander G. Schwing, Alex Colburn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What is really needed to make an existing 2D GAN 3D-aware? To answer this
question, we modify a classical GAN, i.e., StyleGANv2, as little as possible.
We find that only two modifications are absolutely necessary: 1) a multiplane
image style generator branch which produces a set of alpha maps conditioned on
their depth; 2) a pose-conditioned discriminator. We refer to the generated
output as a 'generative multiplane image' (GMPI) and emphasize that its
renderings are not only high-quality but also guaranteed to be view-consistent,
which makes GMPIs different from many prior works. Importantly, the number of
alpha maps can be dynamically adjusted and can differ between training and
inference, alleviating memory concerns and enabling fast training of GMPIs in
less than half a day at a resolution of $1024^2$. Our findings are consistent
across three challenging and common high-resolution datasets, including FFHQ,
AFHQv2, and MetFaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022; Project Page:
  https://xiaoming-zhao.github.io/projects/gmpi/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Basener, Alexey Castrodad, David Messinger, Jennifer Mahle, Paul Prue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a new dynamical systems algorithm for clustering in
hyperspectral images. The main idea of the algorithm is that data points are
\`pushed\' in the direction of increasing density and groups of pixels that end
up in the same dense regions belong to the same class. This is essentially a
numerical solution of the differential equation defined by the gradient of the
density of data points on the data manifold. The number of classes is automated
and the resulting clustering can be extremely accurate. In addition to
providing a accurate clustering, this algorithm presents a new tool for
understanding hyperspectral data in high dimensions. We evaluate the algorithm
on the Urban (Available at www.tec.ary.mil/Hypercube/) scene comparing
performance against the k-means algorithm using pre-identified classes of
materials as ground truth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaComp: Learning to Adapt for Online Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Shanshan Zhao, Wei Ji, Mingming Gong, Liping Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relying on deep supervised or self-supervised learning, previous methods for
depth completion from paired single image and sparse depth data have achieved
impressive performance in recent years. However, facing a new environment where
the test data occurs online and differs from the training data in the RGB image
content and depth sparsity, the trained model might suffer severe performance
drop. To encourage the trained model to work well in such conditions, we expect
it to be capable of adapting to the new environment continuously and
effectively. To achieve this, we propose MetaComp. It utilizes the
meta-learning technique to simulate adaptation policies during the training
phase, and then adapts the model to new environments in a self-supervised
manner in testing. Considering that the input is multi-modal data, it would be
challenging to adapt a model to variations in two modalities simultaneously,
due to significant differences in structure and form of the two modal data.
Therefore, we further propose to disentangle the adaptation procedure in the
basic meta-learning training into two steps, the first one focusing on the
depth sparsity while the second attending to the image content. During testing,
we take the same strategy to adapt the model online to new multi-modal data.
Experimental results and comprehensive ablations show that our MetaComp is
capable of adapting to the depth completion in a new environment effectively
and robust to changes in different modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dense Material Segmentation <span class="highlight-title">Dataset</span> for Indoor and Outdoor Scene
  Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Upchurch, Ransen Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key algorithm for understanding the world is material segmentation, which
assigns a label (metal, glass, etc.) to each pixel. We find that a model
trained on existing data underperforms in some settings and propose to address
this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor
and outdoor images, which is 23x more segments than existing data. Our data
covers a more diverse set of scenes, objects, viewpoints and materials, and
contains a more fair distribution of skin types. We show that a model trained
on our data outperforms a state-of-the-art model across datasets and
viewpoints. We propose a large-scale scene parsing benchmark and baseline of
0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across
46 materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Deep Statistic Shape Model for Myocardium Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoling Hu, Xiao Chen, Yi<span class="highlight-author">kang Liu</span>, Eric Z. Chen, Terrence Chen, Shanhui Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation and motion estimation of myocardium have always been
important in clinic field, which essentially contribute to the downstream
diagnosis. However, existing methods cannot always guarantee the shape
integrity for myocardium segmentation. In addition, motion estimation requires
point correspondence on the myocardium region across different frames. In this
paper, we propose a novel end-to-end deep statistic shape model to focus on
myocardium segmentation with both shape integrity and boundary correspondence
preserving. Specifically, myocardium shapes are represented by a fixed number
of points, whose variations are extracted by Principal Component Analysis
(PCA). Deep neural network is used to predict the transformation parameters
(both affine and deformation), which are then used to warp the mean point cloud
to the image domain. Furthermore, a differentiable rendering layer is
introduced to incorporate mask supervision into the framework to learn more
accurate point clouds. In this way, the proposed method is able to consistently
produce anatomically reasonable segmentation mask without post processing.
Additionally, the predicted point cloud guarantees boundary correspondence for
sequential images, which contributes to the downstream tasks, such as the
motion estimation of myocardium. We conduct several experiments to demonstrate
the effectiveness of the proposed method on several benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Differentiable Rendering with Algebraic Surfaces <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Keselman, Martial Hebert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable renderers provide a direct mathematical link between an
object's 3D representation and images of that object. In this work, we develop
an approximate differentiable renderer for a compact, interpretable
representation, which we call Fuzzy Metaballs. Our approximate renderer focuses
on rendering shapes via depth maps and silhouettes. It sacrifices fidelity for
utility, producing fast runtimes and high-quality gradient information that can
be used to solve vision tasks. Compared to mesh-based differentiable renderers,
our method has forward passes that are 5x faster and backwards passes that are
30x faster. The depth maps and silhouette images generated by our method are
smooth and defined everywhere. In our evaluation of differentiable renderers
for pose estimation, we show that our method is the only one comparable to
classic techniques. In shape from silhouette, our method performs well using
only gradient descent and a per-pixel loss, without any surrogate losses or
regularization. These reconstructions work well even on natural video sequences
with segmentation artifacts. Project page:
https://leonidk.github.io/fuzzy-metaballs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the European Conference on Computer Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting 3D Object Detection via Object-Focused Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Chen Shi, Yihong Chen, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection has achieved remarkable progress by taking point clouds
as the only input. However, point clouds often suffer from incomplete geometric
structures and the lack of semantic information, which makes detectors hard to
accurately classify detected objects. In this work, we focus on how to
effectively utilize object-level information from images to boost the
performance of point-based 3D detector. We present DeMF, a simple yet effective
method to fuse image information into point features. Given a set of point
features and image feature maps, DeMF adaptively aggregates image features by
taking the projected 2D location of the 3D point as reference. We evaluate our
method on the challenging SUN RGB-D dataset, improving state-of-the-art results
by a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at
https://github.com/haoy945/DeMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing An Illumination-Aware Network for Deep Image Relighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuo-Liang Zhu, Zhen Li, Rui-Xun Zhang, Chun-Le Guo, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lighting is a determining factor in photography that affects the style,
expression of emotion, and even quality of images. Creating or finding
satisfying lighting conditions, in reality, is laborious and time-consuming, so
it is of great value to develop a technology to manipulate illumination in an
image as post-processing. Although previous works have explored techniques
based on the physical viewpoint for relighting images, extensive supervisions
and prior knowledge are necessary to generate reasonable images, restricting
the generalization ability of these works. In contrast, we take the viewpoint
of image-to-image translation and implicitly merge ideas of the conventional
physical viewpoint. In this paper, we present an Illumination-Aware Network
(IAN) which follows the guidance from hierarchical sampling to progressively
relight a scene from a single image with high efficiency. In addition, an
Illumination-Aware Residual Block (IARB) is designed to approximate the
physical rendering process and to extract precise descriptors of light sources
for further manipulations. We also introduce a depth-guided geometry encoder
for acquiring valuable geometry- and structure-related representations once the
depth information is available. Experimental results show that our proposed
method produces better quantitative and qualitative relighting results than
previous state-of-the-art methods. The code and models are publicly available
on https://github.com/NK-CS-ZZL/IAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication as a Regular paper in the IEEE Transactions
  on Image Processing (T-IP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Night Image Enhancement: When Layer Decomposition Meets
  Light-Effects Suppression <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeying Jin, Wenhan Yang, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Night images suffer not only from low light, but also from uneven
distributions of light. Most existing night visibility enhancement methods
focus mainly on enhancing low-light regions. This inevitably leads to over
enhancement and saturation in bright regions, such as those regions affected by
light effects (glare, floodlight, etc). To address this problem, we need to
suppress the light effects in bright regions while, at the same time, boosting
the intensity of dark regions. With this idea in mind, we introduce an
unsupervised method that integrates a layer decomposition network and a
light-effects suppression network. Given a single night image as input, our
decomposition network learns to decompose shading, reflectance and
light-effects layers, guided by unsupervised layer-specific prior losses. Our
light-effects suppression network further suppresses the light effects and, at
the same time, enhances the illumination in dark regions. This light-effects
suppression network exploits the estimated light-effects layer as the guidance
to focus on the light-effects regions. To recover the background details and
reduce hallucination/artefacts, we propose structure and high-frequency
consistency losses. Our quantitative and qualitative evaluations on real images
show that our method outperforms state-of-the-art methods in suppressing night
light effects and boosting the intensity of dark regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The MABe22 Benchmarks for Representation Learning of Multi-Agent
  Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer J. Sun, Andrew Ulmer, Dipam Chakraborty, Brian Geuther, Edward Hayes, Heng Jia, Vivek Kumar, Zachary Partridge, Alice Robie, Catherine E. Schretter, Chao Sun, Keith Sheppard, Param Uttarwar, Pietro Perona, Yisong Yue, Kristin Branson, Ann Kennedy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world behavior is often shaped by complex interactions between multiple
agents. To scalably study multi-agent behavior, advances in unsupervised and
self-supervised learning have enabled a variety of different behavioral
representations to be learned from trajectory data. To date, there does not
exist a unified set of benchmarks that can enable comparing methods
quantitatively and systematically across a broad set of behavior analysis
settings. We aim to address this by introducing a large-scale, multi-agent
trajectory dataset from real-world behavioral neuroscience experiments that
covers a range of behavior analysis tasks. Our dataset consists of trajectory
data from common model organisms, with 9.6 million frames of mouse data and 4.4
million frames of fly data, in a variety of experimental settings, such as
different strains, lengths of interaction, and optogenetic stimulation. A
subset of the frames also consist of expert-annotated behavior labels.
Improvements on our dataset corresponds to behavioral representations that work
across multiple organisms and is able to capture differences for common
behavior analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website:
  https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Primer on Topological Data Analysis to Support Image Analysis Tasks in
  Environmental Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lander Ver Hoef, Henry Adams, Emily J. King, Imme Ebert-Uphoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological data analysis (TDA) is a tool from data science and mathematics
that is beginning to make waves in environmental science. In this work, we seek
to provide an intuitive and understandable introduction to a tool from TDA that
is particularly useful for the analysis of imagery, namely persistent homology.
We briefly discuss the theoretical background but focus primarily on
understanding the output of this tool and discussing what information it can
glean. To this end, we frame our discussion around a guiding example of
classifying satellite images from the Sugar, Fish, Flower, and Gravel Dataset
produced for the study of mesocale organization of clouds by Rasp et. al. in
2020 (arXiv:1906:01906). We demonstrate how persistent homology and its
vectorization, persistence landscapes, can be used in a workflow with a simple
machine learning algorithm to obtain good results, and explore in detail how we
can explain this behavior in terms of image-level features. One of the core
strengths of persistent homology is how interpretable it can be, so throughout
this paper we discuss not just the patterns we find, but why those results are
to be expected given what we know about the theory of persistent homology. Our
goal is that a reader of this paper will leave with a better understanding of
TDA and persistent homology, be able to identify problems and datasets of their
own for which persistent homology could be helpful, and gain an understanding
of results they obtain from applying the included GitHub example code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to Artificial Intelligence for the Earth
  Systems (AIES). Copyright in this work may be transferred without further
  notice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Learning of Chemical Bond Representations in Spectral
  Indices and Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bill Basener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate neural networks for classification in
hyperspectral imaging with a focus on connecting the architecture of the
network with the physics of the sensing and materials present. Spectroscopy is
the process of measuring light reflected or emitted by a material as a function
wavelength. Molecular bonds present in the material have vibrational
frequencies which affect the amount of light measured at each wavelength. Thus
the measured spectrum contains information about the particular chemical
constituents and types of bonds. For example, chlorophyll reflects more light
in the near-IR rage (800-900nm) than in the red (625-675nm) range, and this
difference can be measured using a normalized vegetation difference index
(NDVI), which is commonly used to detect vegetation presence, health, and type
in imagery collected at these wavelengths. In this paper we show that the
weights in a Neural Network trained on different vegetation classes learn to
measure this difference in reflectance. We then show that a Neural Network
trained on a more complex set of ten different polymer materials will learn
spectral 'features' evident in the weights for the network, and these features
can be used to reliably distinguish between the different types of polymers.
Examination of the weights provides a human-interpretable understanding of the
network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Elderly Monitoring for Senior Safety by Lightweight Human
  Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Sun, Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing number of elders living alone, care-giving from a distance
becomes a compelling need, particularly for safety. Real-time monitoring and
action recognition are essential to raise an alert timely when abnormal
behaviors or unusual activities occur. While wearable sensors are widely
recognized as a promising solution, highly depending on user's ability and
willingness makes them inefficient. In contrast, video streams collected
through non-contact optical cameras provide richer information and release the
burden on elders. In this paper, leveraging the Independently-Recurrent neural
Network (IndRNN) we propose a novel Real-time Elderly Monitoring for senior
Safety (REMS) based on lightweight human action recognition (HAR) technology.
Using captured skeleton images, the REMS scheme is able to recognize abnormal
behaviors or actions and preserve the user's privacy. To achieve high accuracy,
the HAR module is trained and fine-tuned using multiple databases. An extensive
experimental study verified that REMS system performs action recognition
accurately and timely. REMS meets the design goals as a privacy-preserving
elderly safety monitoring system and possesses the potential to be adopted in
various smart monitoring systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-modal</span> Retinal Image Registration Using a Keypoint-Based Vessel
  Structure Aligning Network <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aline Sindel, Bettina Hohberger, Andreas Maier, Vincent Christlein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In ophthalmological imaging, multiple imaging systems, such as color fundus,
infrared, fluorescein angiography, optical coherence tomography (OCT) or OCT
angiography, are often involved to make a diagnosis of retinal disease.
Multi-modal retinal registration techniques can assist ophthalmologists by
providing a pixel-based comparison of aligned vessel structures in images from
different modalities or acquisition times. To this end, we propose an
end-to-end trainable deep learning method for multi-modal retinal image
registration. Our method extracts convolutional features from the vessel
structure for keypoint detection and description and uses a graph neural
network for feature matching. The keypoint detection and description network
and graph neural network are jointly trained in a self-supervised manner using
synthetic multi-modal image pairs and are guided by synthetically sampled
ground truth homographies. Our method demonstrates higher registration accuracy
as competing methods for our synthetic retinal dataset and generalizes well for
our real macula dataset and a public fundus dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 3 tables, accepted to MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Towards Efficient Adversarial Training on Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Wu, Jindong Gu, Zhifeng Li, <span class="highlight-author">Deng Cai</span>, Xiaofei He, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT), as a powerful alternative to Convolutional Neural
Network (CNN), has received much attention. Recent work showed that ViTs are
also vulnerable to adversarial examples like CNNs. To build robust ViTs, an
intuitive way is to apply adversarial training since it has been shown as one
of the most effective ways to accomplish robust CNNs. However, one major
limitation of adversarial training is its heavy computational cost. The
self-attention mechanism adopted by ViTs is a computationally intense operation
whose expense increases quadratically with the number of input patches, making
adversarial training on ViTs even more time-consuming. In this work, we first
comprehensively study fast adversarial training on a variety of vision
transformers and illustrate the relationship between the efficiency and
robustness. Then, to expediate adversarial training on ViTs, we propose an
efficient Attention Guided Adversarial Training mechanism. Specifically,
relying on the specialty of self-attention, we actively remove certain patch
embeddings of each layer with an attention-guided dropping strategy during
adversarial training. The slimmed self-attention modules accelerate the
adversarial training on ViTs significantly. With only 65\% of the fast
adversarial training time, we match the state-of-the-art results on the
challenging ImageNet benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused
  Events Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Ghosh, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are bio-inspired sensors that offer advantages over traditional
cameras. They work asynchronously, sampling the scene with microsecond
resolution and producing a stream of brightness changes. This unconventional
output has sparked novel computer vision methods to unlock the camera's
potential. We tackle the problem of event-based stereo 3D reconstruction for
SLAM. Most event-based stereo methods try to exploit the camera's high temporal
resolution and event simultaneity across cameras to establish matches and
estimate depth. By contrast, we investigate how to estimate depth without
explicit data association by fusing Disparity Space Images (DSIs) originated in
efficient monocular methods. We develop fusion theory and apply it to design
multi-camera 3D reconstruction algorithms that produce state-of-the-art
results, as we confirm by comparing against four baseline methods and testing
on a variety of available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Localisation and Colored Mesh Reconstruction Architecture for 3D
  Visual Feedback in Robotic Exploration Missions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Serdel, Christophe Grand, Julien Marzat, Julien Moras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an Online Localisation and Colored Mesh Reconstruction
(OLCMR) ROS perception architecture for ground exploration robots aiming to
perform robust Simultaneous Localisation And Mapping (SLAM) in challenging
unknown environments and provide an associated colored 3D mesh representation
in real time. It is intended to be used by a remote human operator to easily
visualise the mapped environment during or after the mission or as a
development base for further researches in the field of exploration robotics.
The architecture is mainly composed of carefully-selected open-source ROS
implementations of a LiDAR-based SLAM algorithm alongside a colored surface
reconstruction procedure using a point cloud and RGB camera images projected
into the 3D space. The overall performances are evaluated on the Newer College
handheld LiDAR-Vision reference dataset and on two experimental trajectories
gathered on board of representative wheeled robots in respectively urban and
countryside outdoor environments. Index Terms: Field Robots, Mapping, SLAM,
Colored Surface Reconstruction
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Confident Detection of Prostate Cancer using High Resolution
  Micro-ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Gilany, Paul Wilson, Amoon Jamzad, Fahimeh Fooladgar, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MOTIVATION: Detection of prostate cancer during transrectal ultrasound-guided
biopsy is challenging. The highly heterogeneous appearance of cancer, presence
of ultrasound artefacts, and noise all contribute to these difficulties. Recent
advancements in high-frequency ultrasound imaging - micro-ultrasound - have
drastically increased the capability of tissue imaging at high resolution. Our
aim is to investigate the development of a robust deep learning model
specifically for micro-ultrasound-guided prostate cancer biopsy. For the model
to be clinically adopted, a key challenge is to design a solution that can
confidently identify the cancer, while learning from coarse histopathology
measurements of biopsy samples that introduce weak labels. METHODS: We use a
dataset of micro-ultrasound images acquired from 194 patients, who underwent
prostate biopsy. We train a deep model using a co-teaching paradigm to handle
noise in labels, together with an evidential deep learning method for
uncertainty estimation. We evaluate the performance of our model using the
clinically relevant metric of accuracy vs. confidence. RESULTS: Our model
achieves a well-calibrated estimation of predictive uncertainty with area under
the curve of 88$\%$. The use of co-teaching and evidential deep learning in
combination yields significantly better uncertainty estimation than either
alone. We also provide a detailed comparison against state-of-the-art in
uncertainty estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LPYOLO: Low Precision YOLO for Face Detection on FPGA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bestami Günay, Sefa Burak Okcu, Hasan Şakir Bilge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, number of edge computing devices and artificial intelligence
applications on them have advanced excessively. In edge computing, decision
making processes and computations are moved from servers to edge devices.
Hence, cheap and low power devices are required. FPGAs are very low power,
inclined to do parallel operations and deeply suitable devices for running
Convolutional Neural Networks (CNN) which are the fundamental unit of an
artificial intelligence application. Face detection on surveillance systems is
the most expected application on the security market. In this work, TinyYolov3
architecture is redesigned and deployed for face detection. It is a CNN based
object detection method and developed for embedded systems. PYNQ-Z2 is selected
as a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on
it. Redesigned TinyYolov3 model is defined in numerous bit width precisions
with Brevitas library which brings fundamental CNN layers and activations in
integer quantized form. Then, the model is trained in a quantized structure
with WiderFace dataset. In order to decrease latency and power consumption,
onchip memory of the FPGA is configured as a storage of whole network
parameters and the last activation function is modified as rescaled HardTanh
instead of Sigmoid. Also, high degree of parallelism is applied to logical
resources of the FPGA. The model is converted to an HLS based application with
using FINN framework and FINN-HLS library which includes the layer definitions
in C++. Later, the model is synthesized and deployed. CPU of the SoC is
employed with multithreading mechanism and responsible for preprocessing,
postprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total
board power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP
accuracy rate on Easy category of the WiderFace are achieved with 4 bits
precision model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MVML2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Data Driven Estimation of Cluster Number in Multiplex Images using
  Embedded Density Outliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer A. Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The usage of chemical imaging technologies is becoming a routine
accompaniment to traditional methods in pathology. Significant technological
advances have developed these next generation techniques to provide rich,
spatially resolved, multidimensional chemical images. The rise of digital
pathology has significantly enhanced the synergy of these imaging modalities
with optical microscopy and immunohistochemistry, enhancing our understanding
of the biological mechanisms and progression of diseases. Techniques such as
imaging mass cytometry provide labelled multidimensional (multiplex) images of
specific components used in conjunction with digital pathology techniques.
These powerful techniques generate a wealth of high dimensional data that
create significant challenges in data analysis. Unsupervised methods such as
clustering are an attractive way to analyse these data, however, they require
the selection of parameters such as the number of clusters. Here we propose a
methodology to estimate the number of clusters in an automatic data-driven
manner using a deep sparse autoencoder to embed the data into a lower
dimensional space. We compute the density of regions in the embedded space, the
majority of which are empty, enabling the high density regions to be detected
as outliers and provide an estimate for the number of clusters. This framework
provides a fully unsupervised and data-driven method to analyse
multidimensional data. In this work we demonstrate our method using 45
multiplex imaging mass cytometry datasets. Moreover, our model is trained using
only one of the datasets and the learned embedding is applied to the remaining
44 images providing an efficient process for data analysis. Finally, we
demonstrate the high computational efficiency of our method which is two orders
of magnitude faster than estimating via computing the sum squared distances as
a function of cluster number.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Fine-Grained Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingdong Hu, Renhao Wang, Kaifeng Zhang, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Establishing visual correspondence across images is a challenging and
essential task. Recently, an influx of self-supervised methods have been
proposed to better learn representations for visual correspondence. However, we
find that these methods often fail to leverage semantic information and
over-rely on the matching of low-level features. In contrast, human vision is
capable of distinguishing between distinct objects as a pretext to tracking.
Inspired by this paradigm, we propose to learn semantic-aware fine-grained
correspondence. Firstly, we demonstrate that semantic correspondence is
implicitly available through a rich set of image-level self-supervised methods.
We further design a pixel-level self-supervised learning objective which
specifically targets fine-grained correspondence. For downstream tasks, we fuse
these two kinds of complementary correspondence representations together,
demonstrating that they boost performance synergistically. Our method surpasses
previous state-of-the-art self-supervised methods using convolutional networks
on a variety of visual correspondence tasks, including video object
segmentation, human pose tracking, and human part tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magic ELF: Image Deraining Meets Association Learning and <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kui Jiang, Zhongyuan Wang, Chen Chen, Zheng Wang, Laizhong Cui, Chia-Wen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network (CNN) and Transformer have achieved great
success in multimedia applications. However, little effort has been made to
effectively and efficiently harmonize these two architectures to satisfy image
deraining. This paper aims to unify these two architectures to take advantage
of their learning merits for image deraining. In particular, the local
connectivity and translation equivariance of CNN and the global aggregation
ability of self-attention (SA) in Transformer are fully exploited for specific
local context and global structure representations. Based on the observation
that rain distribution reveals the degradation location and degree, we
introduce degradation prior to help background recovery and accordingly present
the association refinement deraining scheme. A novel multi-input attention
module (MAM) is proposed to associate rain perturbation removal and background
recovery. Moreover, we equip our model with effective depth-wise separable
convolutions to learn the specific feature representations and trade off
computational complexity. Extensive experiments show that our proposed method
(dubbed as ELF) outperforms the state-of-the-art approach (MPRNet) by 0.25 dB
on average, but only accounts for 11.7\% and 42.1\% of its computational cost
and parameters. The source code is available at
https://github.com/kuijiang94/Magic-ELF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Spatio-Temporal Pyramid <span class="highlight-title">Transformer</span> for Action Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuetian Weng, Zizheng Pan, Mingfei Han, Xiaojun Chang, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of action detection aims at deducing both the action category and
localization of the start and end moment for each action instance in a long,
untrimmed video. While vision Transformers have driven the recent advances in
video understanding, it is non-trivial to design an efficient architecture for
action detection due to the prohibitively expensive self-attentions over a long
sequence of video clips. To this end, we present an efficient hierarchical
Spatio-Temporal Pyramid Transformer (STPT) for action detection, building upon
the fact that the early self-attention layers in Transformers still focus on
local patterns. Specifically, we propose to use local window attention to
encode rich local spatio-temporal representations in the early stages while
applying global attention modules to capture long-term space-time dependencies
in the later stages. In this way, our STPT can encode both locality and
dependency with largely reduced redundancy, delivering a promising trade-off
between accuracy and efficiency. For example, with only RGB input, the proposed
STPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%
and performing favorably against state-of-the-art AFSD that uses additional
flow features with 31% fewer GFLOPs, which serves as an effective and efficient
end-to-end Transformer-based framework for action detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Supervised Object Localization via <span class="highlight-title">Transformer</span> with Implicit
  Spatial Calibration <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Bai, Ruimao Zhang, Jiong Wang, Xiang Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Object Localization (WSOL), which aims to localize objects
by only using image-level labels, has attracted much attention because of its
low annotation cost in real applications. Recent studies leverage the advantage
of self-attention in visual Transformer for long-range dependency to re-active
semantic regions, aiming to avoid partial activation in traditional class
activation mapping (CAM). However, the long-range modeling in Transformer
neglects the inherent spatial coherence of the object, and it usually diffuses
the semantic-aware regions far from the object boundary, making localization
results significantly larger or far smaller. To address such an issue, we
introduce a simple yet effective Spatial Calibration Module (SCM) for accurate
WSOL, incorporating semantic similarities of patch tokens and their spatial
relationships into a unified diffusion model. Specifically, we introduce a
learnable parameter to dynamically adjust the semantic correlations and spatial
context intensities for effective information propagation. In practice, SCM is
designed as an external module of Transformer, and can be removed during
inference to reduce the computation cost. The object-sensitive localization
ability is implicitly embedded into the Transformer encoder through
optimization in the training phase. It enables the generated attention maps to
capture the sharper object boundaries and filter the object-irrelevant
background area. Extensive experimental results demonstrate the effectiveness
of the proposed method, which significantly outperforms its counterpart TS-CAM
on both CUB-200 and ImageNet-1K benchmarks. The code is available at
https://github.com/164140757/SCM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COBRA: Cpu-Only aBdominal oRgan segmentAtion <span class="chip">MICCAI 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward G. A. Henderson, Dónal M. McSweeney, Andrew F. Green
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abdominal organ segmentation is a difficult and time-consuming task. To
reduce the burden on clinical experts, fully-automated methods are highly
desirable. Current approaches are dominated by Convolutional Neural Networks
(CNNs) however the computational requirements and the need for large data sets
limit their application in practice. By implementing a small and efficient
custom 3D CNN, compiling the trained model and optimizing the computational
graph: our approach produces high accuracy segmentations (Dice Similarity
Coefficient (%): Liver: 97.3$\pm$1.3, Kidneys: 94.8$\pm$3.6, Spleen:
96.4$\pm$3.0, Pancreas: 80.9$\pm$10.1) at a rate of 1.6 seconds per image.
Crucially, we are able to perform segmentation inference solely on CPU (no GPU
required), thereby facilitating easy and widespread deployment of the model
without specialist hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MCR-RRR submission for the Fast and Low GPU memory Abdominal oRgan
  sEgmentation Challenge (FLARE) at MICCAI 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Mining Relations among Cross-Frame Affinities for Video Semantic
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guolei Sun, Yun Liu, <span class="highlight-author">Hao Tan</span>g, Ajad Chhatkuli, Le Zhang, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The essence of video semantic segmentation (VSS) is how to leverage temporal
information for prediction. Previous efforts are mainly devoted to developing
new techniques to calculate the cross-frame affinities such as optical flow and
attention. Instead, this paper contributes from a different angle by mining
relations among cross-frame affinities, upon which better temporal information
aggregation could be achieved. We explore relations among affinities in two
aspects: single-scale intrinsic correlations and multi-scale relations.
Inspired by traditional feature processing, we propose Single-scale Affinity
Refinement (SAR) and Multi-scale Affinity Aggregation (MAA). To make it
feasible to execute MAA, we propose a Selective Token Masking (STM) strategy to
select a subset of consistent reference tokens for different scales when
calculating affinities, which also improves the efficiency of our method. At
last, the cross-frame affinities strengthened by SAR and MAA are adopted for
adaptively aggregating temporal information. Our experiments demonstrate that
the proposed method performs favorably against state-of-the-art VSS methods.
The code is publicly available at https://github.com/GuoleiSun/VSS-MRCFA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Trajectory Prediction via Neural Social Physics <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangbei Yue, Dinesh Manocha, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction has been widely pursued in many fields, and many
model-based and model-free methods have been explored. The former include
rule-based, geometric or optimization-based models, and the latter are mainly
comprised of deep learning approaches. In this paper, we propose a new method
combining both methodologies based on a new Neural Differential Equation model.
Our new model (Neural Social Physics or NSP) is a deep neural network within
which we use an explicit physics model with learnable parameters. The explicit
physics model serves as a strong inductive bias in modeling pedestrian
behaviors, while the rest of the network provides a strong data-fitting
capability in terms of system parameter estimation and dynamics stochasticity
modeling. We compare NSP with 15 recent deep learning methods on 6 datasets and
improve the state-of-the-art performance by 5.56%-70%. Besides, we show that
NSP has better generalizability in predicting plausible trajectories in
drastically different scenarios where the density is 2-5 times as high as the
testing data. Finally, we show that the physics model in NSP can provide
plausible explanations for pedestrian behaviors, as opposed to black-box deep
learning. Code is available:
https://github.com/realcrane/Human-Trajectory-Prediction-via-Neural-Social-Physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using
  Unsupervised Domain-Classifier Guided Network <span class="chip">ICCV2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeying Jin, Aashish Sharma, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shadow removal from a single image is generally still an open problem. Most
existing learning-based methods use supervised learning and require a large
number of paired images (shadow and corresponding non-shadow images) for
training. A recent unsupervised method, Mask-ShadowGAN, addresses this
limitation. However, it requires a binary mask to represent shadow regions,
making it inapplicable to soft shadows. To address the problem, in this paper,
we propose an unsupervised domain-classifier guided shadow removal network,
DC-ShadowNet. Specifically, we propose to integrate a shadow/shadow-free domain
classifier into a generator and its discriminator, enabling them to focus on
shadow regions. To train our network, we introduce novel losses based on
physics-based shadow-free chromaticity, shadow-robust perceptual features, and
boundary smoothness. Moreover, we show that our unsupervised network can be
used for test-time training that further improves the results. Our experiments
show that all these novel components allow our method to handle soft shadows,
and also to perform better on hard shadows both quantitatively and
qualitatively than the existing state-of-the-art shadow removal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV2021,
  https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamYOLO: Real-time Object Detection for Streaming Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinrong Yang, Songtao Liu, Zeming Li, Xiaoping Li, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The perceptive models of autonomous driving require fast inference within a
low latency for safety. While existing works ignore the inevitable
environmental changes after processing, streaming perception jointly evaluates
the latency and accuracy into a single metric for video online perception,
guiding the previous works to search trade-offs between accuracy and speed. In
this paper, we explore the performance of real time models on this metric and
endow the models with the capacity of predicting the future, significantly
improving the results for streaming perception. Specifically, we build a simple
framework with two effective modules. One is a Dual Flow Perception module
(DFP). It consists of dynamic flow and static flow in parallel to capture
moving tendency and basic detection feature, respectively. Trend Aware Loss
(TAL) is the other module which adaptively generates loss weight for each
object with its moving speed. Realistically, we consider multiple velocities
driving scene and further propose Velocity-awared streaming AP (VsAP) to
jointly evaluate the accuracy. In this realistic setting, we design a efficient
mix-velocity training strategy to guide detector perceive any velocities. Our
simple method achieves the state-of-the-art performance on Argoverse-HD dataset
and improves the sAP and VsAP by 4.7% and 8.2% respectively compared to the
strong baseline, validating its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of arXiv:2203.12338</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KD-MVS: Knowledge <span class="highlight-title">Distillation</span> Based <span class="highlight-title">Self-supervised</span> Learning for MVS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Ding, Qingtian Zhu, Xiangyue Liu, Wentao Yuan, Haotian Zhang, CHi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised multi-view stereo (MVS) methods have achieved remarkable progress
in terms of reconstruction quality, but suffer from the challenge of collecting
large-scale ground-truth depth. In this paper, we propose a novel
self-supervised training pipeline for MVS based on knowledge distillation,
termed \textit{KD-MVS}, which mainly consists of self-supervised teacher
training and distillation-based student training. Specifically, the teacher
model is trained in a self-supervised fashion using both photometric and
featuremetric consistency. Then we distill the knowledge of the teacher model
to the student model through probabilistic knowledge transferring. With the
supervision of validated knowledge, the student model is able to outperform its
teacher by a large margin. Extensive experiments performed on multiple datasets
show our method can even outperform supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequence Models for Drone vs Bird Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Cagatay Akyon, Erdem Akagunduz, Sinan Onur Altinuc, Alptekin Temizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone detection has become an essential task in object detection as drone
costs have decreased and drone technology has improved. It is, however,
difficult to detect distant drones when there is weak contrast, long range, and
low visibility. In this work, we propose several sequence classification
architectures to reduce the detected false-positive ratio of drone tracks.
Moreover, we propose a new drone vs. bird sequence classification dataset to
train and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer
based sequence classification architectures have been trained on the proposed
dataset to show the effectiveness of the proposed idea. As experiments show,
using sequence information, bird classification and overall F1 scores can be
increased by up to 73% and 35%, respectively. Among all sequence classification
models, R(2+1)D-based fully convolutional model yields the best transfer
learning and fine-tuning results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AVSS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-aware Modular Capsule Routing for Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Han, Jianhua Yin, Jianlong Wu, Yinwei Wei, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is fundamentally compositional in nature, and
many questions are simply answered by decomposing them into modular
sub-problems. The recent proposed Neural Module Network (NMN) employ this
strategy to question answering, whereas heavily rest with off-the-shelf layout
parser or additional expert policy regarding the network architecture design
instead of learning from the data. These strategies result in the
unsatisfactory adaptability to the semantically-complicated variance of the
inputs, thereby hindering the representational capacity and generalizability of
the model. To tackle this problem, we propose a Semantic-aware modUlar caPsulE
Routing framework, termed as SUPER, to better capture the instance-specific
vision-semantic characteristics and refine the discriminative representations
for prediction. Particularly, five powerful specialized modules as well as
dynamic routers are tailored in each layer of the SUPER network, and the
compact routing spaces are constructed such that a variety of customizable
routes can be sufficiently exploited and the vision-semantic representations
can be explicitly calibrated. We comparatively justify the effectiveness and
generalization ability of our proposed SUPER scheme over five benchmark
datasets, as well as the parametric-efficient advantage. It is worth
emphasizing that this work is not to pursue the state-of-the-art results in
VQA. Instead, we expect that our model is responsible to provide a novel
perspective towards architecture learning and representation calibration for
VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, submitted to IEEE Transactions on Image
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhi Guan, Hang Zhou, Mingming Gong, Youjian Zhao, Errui Ding, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite encouraging progress in deepfake detection, generalization to unseen
forgery types remains a significant challenge due to the limited forgery clues
explored during training. In contrast, we notice a common phenomenon in
deepfake: fake video creation inevitably disrupts the statistical regularity in
original videos. Inspired by this observation, we propose to boost the
generalization of deepfake detection by distinguishing the "regularity
disruption" that does not appear in real videos. Specifically, by carefully
examining the spatial and temporal properties, we propose to disrupt a real
video through a Pseudo-fake Generator and create a wide range of pseudo-fake
videos for training. Such practice allows us to achieve deepfake detection
without using fake videos and improves the generalization ability in a simple
and efficient manner. To jointly capture the spatial and temporal disruptions,
we propose a Spatio-Temporal Enhancement block to learn the regularity
disruption across space and time on our self-created videos. Through
comprehensive experiments, our method exhibits excellent performance on several
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correspondence Matters for Video Referring Expression Comprehension <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Cao, Ji Jiang, Long Chen, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of video Referring Expression Comprehension (REC),
which aims to localize the referent objects described in the sentence to visual
regions in the video frames. Despite the recent progress, existing methods
suffer from two problems: 1) inconsistent localization results across video
frames; 2) confusion between the referent and contextual objects. To this end,
we propose a novel Dual Correspondence Network (dubbed as DCNet) which
explicitly enhances the dense associations in both the inter-frame and
cross-modal manners. Firstly, we aim to build the inter-frame correlations for
all existing instances within the frames. Specifically, we compute the
inter-frame patch-wise cosine similarity to estimate the dense alignment and
then perform the inter-frame contrastive learning to map them close in feature
space. Secondly, we propose to build the fine-grained patch-word alignment to
associate each patch with certain words. Due to the lack of this kind of
detailed annotations, we also predict the patch-word correspondence through the
cosine similarity. Extensive experiments demonstrate that our DCNet achieves
state-of-the-art performance on both video and image REC benchmarks.
Furthermore, we conduct comprehensive ablation studies and thorough analyses to
explore the optimal model designs. Notably, our inter-frame and cross-modal
contrastive losses are plug-and-play functions and are applicable to any video
REC architectures. For example, by building on top of Co-grounding, we boost
the performance by 1.48% absolute improvement on Accu.@0.5 for VID-Sentence
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2-TPred: Discontinuous Dependency for Trajectory Prediction under
  Traffic Lights <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhen Zhang, Wentong Wang, Weizhi Guo, Pei Lv, Mingliang Xu, Wei Chen, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A profound understanding of inter-agent relationships and motion behaviors is
important to achieve high-quality planning when navigating in complex
scenarios, especially at urban traffic intersections. We present a trajectory
prediction approach with respect to traffic lights, D2-TPred, which uses a
spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG)
to handle the problem of discontinuous dependency in the spatial-temporal
space. Specifically, the SDG is used to capture spatial interactions by
reconstructing sub-graphs for different agents with dynamic and changeable
characteristics during each frame. The BDG is used to infer motion tendency by
modeling the implicit dependency of the current state on priors behaviors,
especially the discontinuous motions corresponding to acceleration,
deceleration, or turning direction. Moreover, we present a new dataset for
vehicle trajectory prediction under traffic lights called VTP-TL. Our
experimental results show that our model achieves more than {20.45% and 20.78%
}improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to
other trajectory prediction algorithms. The dataset and code are available at:
https://github.com/VTP-TL/D2-TPred.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV2022, 17 pages, 6 figures. Project page:
  https://github.com/VTP-TL/D2-TPred</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sobolev Training for Implicit Neural Representations with Approximated
  Image Derivatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Yuan, Qingtian Zhu, Xiangyue Liu, Yikang Ding, Haotian Zhang, Chi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Implicit Neural Representations (INRs) parameterized by neural
networks have emerged as a powerful and promising tool to represent different
kinds of signals due to its continuous, differentiable properties, showing
superiorities to classical discretized representations. However, the training
of neural networks for INRs only utilizes input-output pairs, and the
derivatives of the target output with respect to the input, which can be
accessed in some cases, are usually ignored. In this paper, we propose a
training paradigm for INRs whose target output is image pixels, to encode image
derivatives in addition to image values in the neural network. Specifically, we
use finite differences to approximate image derivatives. We show how the
training paradigm can be leveraged to solve typical INRs problems, i.e., image
regression and inverse rendering, and demonstrate this training paradigm can
improve the data-efficiency and generalization capabilities of INRs. The code
of our method is available at
\url{https://github.com/megvii-research/Sobolev_INRs}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic
  Upsampling <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Lu, Wenze Liu, Hongtao Fu, Zhiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of task-agnostic feature upsampling in dense
prediction where an upsampling operator is required to facilitate both
region-sensitive tasks like semantic segmentation and detail-sensitive tasks
such as image matting. Existing upsampling operators often can work well in
either type of the tasks, but not both. In this work, we present FADE, a novel,
plug-and-play, and task-agnostic upsampling operator. FADE benefits from three
design choices: i) considering encoder and decoder features jointly in
upsampling kernel generation; ii) an efficient semi-shift convolutional
operator that enables granular control over how each feature point contributes
to upsampling kernels; iii) a decoder-dependent gating mechanism for enhanced
detail delineation. We first study the upsampling properties of FADE on toy
data and then evaluate it on large-scale semantic segmentation and image
matting. In particular, FADE reveals its effectiveness and task-agnostic
characteristic by consistently outperforming recent dynamic upsampling
operators in different tasks. It also generalizes well across convolutional and
transformer architectures with little computational overhead. Our work
additionally provides thoughtful insights on what makes for task-agnostic
upsampling. Code is available at: http://lnkiy.in/fade_in
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at http://lnkiy.in/fade_in</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error Compensation Framework for Flow-Guided Video Inpainting <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyeon Kang, Seoung Wug Oh, Seon Joo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -> pixel
propagation -> synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xia, Wenhao Wu, Haoran Wang, Rui Su, Dongliang He, Haosen Yang, Xiaoran Fan, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is challenging for artificial intelligence systems to achieve accurate
video recognition under the scenario of low computation costs. Adaptive
inference based efficient video recognition methods typically preview videos
and focus on salient parts to reduce computation costs. Most existing works
focus on complex networks learning with video classification based objectives.
Taking all frames as positive samples, few of them pay attention to the
discrimination between positive samples (salient frames) and negative samples
(non-salient frames) in supervisions. To fill this gap, in this paper, we
propose a novel Non-saliency Suppression Network (NSNet), which effectively
suppresses the responses of non-salient frames. Specifically, on the frame
level, effective pseudo labels that can distinguish between salient and
non-salient frames are generated to guide the frame saliency learning. On the
video level, a temporal attention module is learned under dual video-level
supervisions on both the salient and the non-salient representations. Saliency
measurements from both two levels are combined for exploitation of
multi-granularity complementary information. Extensive experiments conducted on
four well-known benchmarks verify our NSNet not only achieves the
state-of-the-art accuracy-efficiency trade-off but also present a significantly
faster (2.4~4.3x) practical inference speed than state-of-the-art methods. Our
project page is at https://lawrencexia2008.github.io/projects/nsnet .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose for Everything: Towards Category-Agnostic Pose Estimation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lumin Xu, Sheng Jin, Wang Zeng, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, Xiaogang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works on 2D pose estimation mainly focus on a certain category, e.g.
human, animal, and vehicle. However, there are lots of application scenarios
that require detecting the poses/keypoints of the unseen class of objects. In
this paper, we introduce the task of Category-Agnostic Pose Estimation (CAPE),
which aims to create a pose estimation model capable of detecting the pose of
any class of object given only a few samples with keypoint definition. To
achieve this goal, we formulate the pose estimation problem as a keypoint
matching problem and design a novel CAPE framework, termed POse Matching
Network (POMNet). A transformer-based Keypoint Interaction Module (KIM) is
proposed to capture both the interactions among different keypoints and the
relationship between the support and query images. We also introduce
Multi-category Pose (MP-100) dataset, which is a 2D pose dataset of 100 object
categories containing over 20K instances and is well-designed for developing
CAPE algorithms. Experiments show that our method outperforms other baseline
approaches by a large margin. Codes and data are available at
https://github.com/luminxu/Pose-for-Everything.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Saliency Query Network for Efficient Video Recognition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xia, Zhihao Wang, Wenhao Wu, Haoran Wang, Jungong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient video recognition is a hot-spot research topic with the explosive
growth of multimedia data on the Internet and mobile devices. Most existing
methods select the salient frames without awareness of the class-specific
saliency scores, which neglect the implicit association between the saliency of
frames and its belonging category. To alleviate this issue, we devise a novel
Temporal Saliency Query (TSQ) mechanism, which introduces class-specific
information to provide fine-grained cues for saliency measurement.
Specifically, we model the class-specific saliency measuring process as a
query-response task. For each category, the common pattern of it is employed as
a query and the most salient frames are responded to it. Then, the calculated
similarities are adopted as the frame saliency scores. To achieve it, we
propose a Temporal Saliency Query Network (TSQNet) that includes two
instantiations of the TSQ mechanism based on visual appearance similarities and
textual event-object relations. Afterward, cross-modality interactions are
imposed to promote the information exchange between them. Finally, we use the
class-specific saliencies of the most confident categories generated by two
modalities to perform the selection of salient frames. Extensive experiments
demonstrate the effectiveness of our method by achieving state-of-the-art
results on ActivityNet, FCVID and Mini-Kinetics datasets. Our project page is
at https://lawrencexia2008.github.io/projects/tsqnet .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Land Classification in Satellite Images by Injecting Traditional
  Features to CNN Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Cagri Aksoy, Beril Sirmacek, Cem Unsalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning methods have been successfully applied to remote sensing
problems for several years. Among these methods, CNN based models have high
accuracy in solving the land classification problem using satellite or aerial
images. Although these models have high accuracy, this generally comes with
large memory size requirements. On the other hand, it is desirable to have
small-sized models for applications, such as the ones implemented on unmanned
aerial vehicles, with low memory space. Unfortunately, small-sized CNN models
do not provide high accuracy as with their large-sized versions. In this study,
we propose a novel method to improve the accuracy of CNN models, especially the
ones with small size, by injecting traditional features to them. To test the
effectiveness of the proposed method, we applied it to the CNN models
SqueezeNet, MobileNetV2, ShuffleNetV2, VGG16, and ResNet50V2 having size 0.5 MB
to 528 MB. We used the sample mean, gray level co-occurrence matrix features,
Hu moments, local binary patterns, histogram of oriented gradients, and color
invariants as traditional features for injection. We tested the proposed method
on the EuroSAT dataset to perform land classification. Our experimental results
show that the proposed method significantly improves the land classification
accuracy especially when applied to small-sized CNN models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LocVTP: Video-Text <span class="highlight-title">Pre-train</span>ing for Temporal Localization <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-Text Pre-training (VTP) aims to learn transferable representations for
various downstream tasks from large-scale web videos. To date, almost all
existing VTP methods are limited to retrieval-based downstream tasks, e.g.,
video retrieval, whereas their transfer potentials on localization-based tasks,
e.g., temporal grounding, are under-explored. In this paper, we experimentally
analyze and demonstrate the incompatibility of current VTP methods with
localization tasks, and propose a novel Localization-oriented Video-Text
Pre-training framework, dubbed as LocVTP. Specifically, we perform the
fine-grained contrastive alignment as a complement to the coarse-grained one by
a clip-word correspondence discovery scheme. To further enhance the temporal
reasoning ability of the learned feature, we propose a context projection head
and a temporal aware contrastive loss to perceive the contextual relationships.
Extensive experiments on four downstream tasks across six datasets demonstrate
that our LocVTP achieves state-of-the-art performance on both retrieval-based
and localization-based tasks. Furthermore, we conduct comprehensive ablation
studies and thorough analyses to explore the optimum model designs and training
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Data with Noisy Labels Using Temporal Self-Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Ho Lee, Jae Soon Baik, Tae Hwan Hwang, Jun Won Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are inevitably many mislabeled data in real-world datasets. Because
deep neural networks (DNNs) have an enormous capacity to memorize noisy labels,
a robust training scheme is required to prevent labeling errors from degrading
the generalization performance of DNNs. Current state-of-the-art methods
present a co-training scheme that trains dual networks using samples associated
with small losses. In practice, however, training two networks simultaneously
can burden computing resources. In this study, we propose a simple yet
effective robust training scheme that operates by training only a single
network. During training, the proposed method generates temporal self-ensemble
by sampling intermediate network parameters from the weight trajectory formed
by stochastic gradient descent optimization. The loss sum evaluated with these
self-ensembles is used to identify incorrectly labeled samples. In parallel,
our method generates multi-view predictions by transforming an input data into
various forms and considers their agreement to identify incorrectly labeled
samples. By combining the aforementioned metrics, we present the proposed {\it
self-ensemble-based robust training} (SRT) method, which can filter the samples
with noisy labels to reduce their influence on training. Experiments on
widely-used public datasets demonstrate that the proposed method achieves a
state-of-the-art performance in some categories without training the dual
networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto Machine Learning for Medical Image Analysis by Unifying the Search
  on Data Augmentation and Neural Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Zhang, Dong Li, Lituan Wang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated data augmentation, which aims at engineering augmentation policy
automatically, recently draw a growing research interest. Many previous
auto-augmentation methods utilized a Density Matching strategy by evaluating
policies in terms of the test-time augmentation performance. In this paper, we
theoretically and empirically demonstrated the inconsistency between the train
and validation set of small-scale medical image datasets, referred to as
in-domain sampling bias. Next, we demonstrated that the in-domain sampling bias
might cause the inefficiency of Density Matching. To address the problem, an
improved augmentation search strategy, named Augmented Density Matching, was
proposed by randomly sampling policies from a prior distribution for training.
Moreover, an efficient automatical machine learning(AutoML) algorithm was
proposed by unifying the search on data augmentation and neural architecture.
Experimental results indicated that the proposed methods outperformed
state-of-the-art approaches on MedMNIST, a pioneering benchmark designed for
AutoML in medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheeun Hong, Sungyong Baik, Heewon Kim, Seungjun Nah, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite breakthrough advances in image super-resolution (SR) with
convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous
applications due to the high computational complexity of SR networks.
Quantization is one of the promising approaches to solve this problem. However,
existing methods fail to quantize SR models with a bit-width lower than 8 bits,
suffering from severe accuracy loss due to fixed bit-width quantization applied
everywhere. In this work, to achieve high average bit-reduction with less
accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ)
method for SR networks that allocates optimal bits to local regions and layers
adaptively based on the local contents of an input image. To this end, a
trainable bit selector module is introduced to determine the proper bit-width
and quantization level for each layer and a given local image patch. This
module is governed by the quantization sensitivity that is estimated by using
both the average magnitude of image gradient of the patch and the standard
deviation of the input feature of the layer. The proposed quantization pipeline
has been tested on various SR networks and evaluated on several standard
benchmarks extensively. Significant reduction in computational complexity and
the elevated restoration accuracy clearly demonstrate the effectiveness of the
proposed CADyQ framework for SR. Codes are available at
https://github.com/Cheeun/CADyQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Level Set Theory for Neural Implicit Evolution under Explicit Flows <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishit Mehta, Manmohan Chandraker, Ravi Ramamoorthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordinate-based neural networks parameterizing implicit surfaces have
emerged as efficient representations of geometry. They effectively act as
parametric level sets with the zero-level set defining the surface of interest.
We present a framework that allows applying deformation operations defined for
triangle meshes onto such implicit surfaces. Several of these operations can be
viewed as energy-minimization problems that induce an instantaneous flow field
on the explicit surface. Our method uses the flow field to deform parametric
implicit surfaces by extending the classical theory of level sets. We also
derive a consolidated view for existing methods on differentiable surface
extraction and rendering, by formalizing connections to the level-set theory.
We show that these methods drift from the theory and that our approach exhibits
improvements for applications like surface smoothing, mean-curvature flow,
inverse rendering and user-defined editing on implicit geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 (Oral); Project Page at https://ishit.github.io/nie</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeqFormer: Sequential <span class="highlight-title">Transformer</span> for Video Instance Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present SeqFormer for video instance segmentation. SeqFormer
follows the principle of vision transformer that models instance relationships
among video frames. Nevertheless, we observe that a stand-alone instance query
suffices for capturing a time sequence of instances in a video, but attention
mechanisms shall be done with each frame independently. To achieve this,
SeqFormer locates an instance in each frame and aggregates temporal information
to learn a powerful representation of a video-level instance, which is used to
predict the mask sequences on each frame dynamically. Instance tracking is
achieved naturally without tracking branches or post-processing. On
YouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP
with a ResNet-101 backbone without bells and whistles. Such achievement
significantly exceeds the previous state-of-the-art performance by 4.6 and 4.4,
respectively. In addition, integrated with the recently-proposed Swin
transformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer
could be a strong baseline that fosters future research in video instance
segmentation, and in the meantime, advances this field with a more robust,
accurate, neat model. The code is available at
https://github.com/wjf5203/SeqFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Secrets of Event-Based Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shintaro Shiba, Yoshimitsu Aoki, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras respond to scene dynamics and offer advantages to estimate
motion. Following recent image-based deep-learning achievements, optical flow
estimation methods for event cameras have rushed to combine those image-based
methods with event data. However, it requires several adaptations (data
conversion, loss function, etc.) as they have very different properties. We
develop a principled method to extend the Contrast Maximization framework to
estimate optical flow from events alone. We investigate key elements: how to
design the objective function to prevent overfitting, how to warp events to
deal better with occlusions, and how to improve convergence with multi-scale
raw events. With these key elements, our method ranks first among unsupervised
methods on the MVSEC benchmark, and is competitive on the DSEC benchmark.
Moreover, our method allows us to expose the issues of the ground truth flow in
those benchmarks, and produces remarkable results when it is transferred to
unsupervised learning settings. Our code is available at
https://github.com/tub-rip/event_based_optical_flow
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 7 tables,
  https://github.com/tub-rip/event_based_optical_flow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Generalization in Federated Learning by Seeking Flat Minima <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11834v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11834v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Caldarola, Barbara Caputo, Marco Ciccone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP2TV: Align, Match and Distill for Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.05610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.05610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Gao, Jingyu Liu, Weiqi Sun, Sheng Chen, Dedan Chang, Lili Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern video-text retrieval frameworks basically consist of three parts:
video encoder, text encoder and the similarity head. With the success on both
visual and textual representation learning, transformer based encoders and
fusion methods have also been adopted in the field of video-text retrieval. In
this report, we present CLIP2TV, aiming at exploring where the critical
elements lie in transformer based methods. To achieve this, We first revisit
some recent works on multi-modal learning, then introduce some techniques into
video-text retrieval, finally evaluate them through extensive experiments in
different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,
outperforming the previous SOTA result by 4.1%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knee arthritis severity measurement using deep learning: a publicly
  available algorithm with a multi-institutional validation showing
  radiologist-level performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxue Gu, Keyu Li, Roy J. Colglazier, Jichen Yang, Michael Lebhar, Jonathan O'Donnell, William A. Jiranek, Richard C. Mather, Rob J. French, Nicholas Said, Jikai Zhang, Christine Park, Maciej A. Mazurowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical "black box" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.13715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.13715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ailing Zeng, Lei Yang, Xuan Ju, Jiefeng Li, Jianyi Wang, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When analyzing human motion videos, the output jitters from existing pose
estimators are highly-unbalanced with varied estimation errors across frames.
Most frames in a video are relatively easy to estimate and only suffer from
slight jitters. In contrast, for rarely seen or occluded actions, the estimated
positions of multiple joints largely deviate from the ground truth values for a
consecutive sequence of frames, rendering significant jitters on them. To
tackle this problem, we propose to attach a dedicated temporal-only refinement
network to existing pose estimators for jitter mitigation, named SmoothNet.
Unlike existing learning-based solutions that employ spatio-temporal models to
co-optimize per-frame precision and temporal smoothness at all the joints,
SmoothNet models the natural smoothness characteristics in body movements by
learning the long-range temporal relations of every joint without considering
the noisy correlations among joints. With a simple yet effective motion-aware
fully-connected network, SmoothNet improves the temporal smoothness of existing
pose estimators significantly and enhances the estimation accuracy of those
challenging frames as a side-effect. Moreover, as a temporal-only model, a
unique advantage of SmoothNet is its strong transferability across various
types of estimators and datasets. Comprehensive experiments on five datasets
with eleven popular backbone networks across 2D and 3D pose estimation and body
recovery tasks demonstrate the efficacy of the proposed solution. Code is
available at https://github.com/cure-lab/SmoothNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrasting quadratic assignments for set-based representation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Moskalev, Ivan Sosnovik, Volker Fischer, Arnold Smeulders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard approach to contrastive learning is to maximize the agreement
between different views of the data. The views are ordered in pairs, such that
they are either positive, encoding different views of the same object, or
negative, corresponding to views of different objects. The supervisory signal
comes from maximizing the total similarity over positive pairs, while the
negative pairs are needed to avoid collapse. In this work, we note that the
approach of considering individual pairs cannot account for both intra-set and
inter-set similarities when the sets are formed from the views of the data. It
thus limits the information content of the supervisory signal available to
train representations. We propose to go beyond contrasting individual pairs of
objects by focusing on contrasting objects as sets. For this, we use
combinatorial quadratic assignment theory designed to evaluate set and graph
similarities and derive set-contrastive objective as a regularizer for
contrastive learning methods. We conduct experiments and demonstrate that our
method improves learned representations for the tasks of metric learning and
self-supervised classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-enabled Assessment of Cardiac Systolic and Diastolic Function from
  Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esther Puyol-Antón, Bram Ruijsink, Baldeep S. Sidhu, Justin Gould, Bradley Porter, Mark K. Elliott, Vishal Mehta, Haotian Gu, Miguel Xochicale, Alberto Gomez, Christopher A. Rinaldi, Martin Cowie, Phil Chowienczyk, Reza Razavi, Andrew P. King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Left ventricular (LV) function is an important factor in terms of patient
management, outcome, and long-term survival of patients with heart disease. The
most recently published clinical guidelines for heart failure recognise that
over reliance on only one measure of cardiac function (LV ejection fraction) as
a diagnostic and treatment stratification biomarker is suboptimal. Recent
advances in AI-based echocardiography analysis have shown excellent results on
automated estimation of LV volumes and LV ejection fraction. However, from
time-varying 2-D echocardiography acquisition, a richer description of cardiac
function can be obtained by estimating functional biomarkers from the complete
cardiac cycle. In this work we propose for the first time an AI approach for
deriving advanced biomarkers of systolic and diastolic LV function from 2-D
echocardiography based on segmentations of the full cardiac cycle. These
biomarkers will allow clinicians to obtain a much richer picture of the heart
in health and disease. The AI model is based on the 'nn-Unet' framework and was
trained and tested using four different databases. Results show excellent
agreement between manual and automated analysis and showcase the potential of
the advanced systolic and diastolic biomarkers for patient stratification.
Finally, for a subset of 50 cases, we perform a correlation analysis between
clinical biomarkers derived from echocardiography and CMR and we show excellent
agreement between the two modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive <span class="highlight-title">Multi-Modal</span> Interactions for Referring Image Segmentation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.10412v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.10412v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanishk Jain, Vineet Gandhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate Referring Image Segmentation (RIS), which outputs a
segmentation map corresponding to the natural language description. Addressing
RIS efficiently requires considering the interactions happening \emph{across}
visual and linguistic modalities and the interactions \emph{within} each
modality. Existing methods are limited because they either compute different
forms of interactions \emph{sequentially} (leading to error propagation) or
\emph{ignore} intramodal interactions. We address this limitation by performing
all three interactions \emph{simultaneously} through a Synchronous Multi-Modal
Fusion Module (SFM). Moreover, to produce refined segmentation masks, we
propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where
linguistic features facilitate the exchange of contextual information across
the visual hierarchy. We present thorough ablation studies and validate our
approach's performance on four benchmark datasets, showing considerable
performance gains over the existing state-of-the-art (SOTA) methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric
  Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng-Fen Tsai, Rosalie H. Wang, Jośe Zariffa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introduction: Hand function is a central determinant of independence after
stroke. Measuring hand use in the home environment is necessary to evaluate the
impact of new interventions, and calls for novel wearable technologies.
Egocentric video can capture hand-object interactions in context, as well as
show how more-affected hands are used during bilateral tasks (for stabilization
or manipulation). Automated methods are required to extract this information.
Objective: To use artificial intelligence-based computer vision to classify
hand use and hand role from egocentric videos recorded at home after stroke.
Methods: Twenty-one stroke survivors participated in the study. A random forest
classifier, a SlowFast neural network, and the Hand Object Detector neural
network were applied to identify hand use and hand role at home.
Leave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the
performance of the three models. Between-group differences of the models were
calculated based on the Mathews correlation coefficient (MCC). Results: For
hand use detection, the Hand Object Detector had significantly higher
performance than the other models. The macro average MCCs using this model in
the LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for
the less-affected hands. Hand role classification had macro average MCCs in the
LOSOCV that were close to zero for all models. Conclusion: Using egocentric
video to capture the hand use of stroke survivors at home is feasible. Pose
estimation to track finger movements may be beneficial to classifying hand
roles in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appendix is included</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming the Latent Space of StyleGAN for Real Face Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.14230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.14230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyi Li, Jinlong Liu, Xinyu Zhang, Yunzhi Bai, Huayan Wang, Klaus Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in semantic manipulation using StyleGAN, semantic
editing of real faces remains challenging. The gap between the $W$ space and
the $W$+ space demands an undesirable trade-off between reconstruction quality
and editing quality. To solve this problem, we propose to expand the latent
space by replacing fully-connected layers in the StyleGAN's mapping network
with attention-based transformers. This simple and effective technique
integrates the aforementioned two spaces and transforms them into one new
latent space called $W$++. Our modified StyleGAN maintains the state-of-the-art
generation quality of the original StyleGAN with moderately better diversity.
But more importantly, the proposed $W$++ space achieves superior performance in
both reconstruction quality and editing quality. Despite these significant
advantages, our $W$++ space supports existing inversion algorithms and editing
methods with only negligible modifications thanks to its structural similarity
with the $W/W$+ space. Extensive experiments on the FFHQ dataset prove that our
proposed $W$++ space is evidently more preferable than the previous $W/W$+
space for real face editing. The code is publicly available for research
purposes at https://github.com/AnonSubm2021/TransStyleGAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep <span class="highlight-title">Multimodal</span> Guidance for Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayur Mallya, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging is a cornerstone of therapy and diagnosis in modern medicine.
However, the choice of imaging modality for a particular theranostic task
typically involves trade-offs between the feasibility of using a particular
modality (e.g., short wait times, low cost, fast acquisition, reduced
radiation/invasiveness) and the expected performance on a clinical task (e.g.,
diagnostic accuracy, efficacy of treatment planning and guidance). In this
work, we aim to apply the knowledge learned from the less feasible but
better-performing (superior) modality to guide the utilization of the
more-feasible yet under-performing (inferior) modality and steer it towards
improved performance. We focus on the application of deep learning for
image-based diagnosis. We develop a light-weight guidance model that leverages
the latent representation learned from the superior modality, when training a
model that consumes only the inferior modality. We examine the advantages of
our method in the context of two clinical applications: multi-task skin lesion
classification from clinical and dermoscopic images and brain tumor
classification from multi-sequence magnetic resonance imaging (MRI) and
histopathology images. For both these scenarios we show a boost in diagnostic
performance of the inferior modality without requiring the superior modality.
Furthermore, in the case of brain tumor classification, our method outperforms
the model trained on the superior modality while producing comparable results
to the model that uses both modalities during inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic encoding of protected characteristics in image-based models
  for disease detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14755v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14755v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Glocker, Charles Jones, Melanie Bernhardt, Stefan Winzeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been rightfully emphasized that the use of AI for clinical decision
making could amplify health disparities. An algorithm may encode protected
characteristics, and then use this information for making predictions due to
undesirable correlations in the (historical) training data. It remains unclear
how we can establish whether such information is actually used. Besides the
scarcity of data from underserved populations, very little is known about how
dataset biases manifest in predictive models and how this may result in
disparate performance. This article aims to shed some light on these issues by
exploring new methodology for subgroup analysis in image-based disease
detection models. We utilize two publicly available chest X-ray datasets,
CheXpert and MIMIC-CXR, to study performance disparities across race and
biological sex in deep learning models. We explore test set resampling,
transfer learning, multitask learning, and model inspection to assess the
relationship between the encoding of protected characteristics and disease
detection performance across subgroups. We confirm subgroup disparities in
terms of shifted true and false positive rates which are partially removed
after correcting for population and prevalence shifts in the test sets. We
further find a previously used transfer learning method to be insufficient for
establishing whether specific patient information is used for making
predictions. The proposed combination of test-set resampling, multitask
learning, and model inspection reveals valuable new insights about the way
protected characteristics are encoded in the feature representations of deep
neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available on https://github.com/biomedia-mira/chexploration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeRF for Outdoor Scene Relighting <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.05140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.05140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic editing of outdoor scenes from photographs requires a profound
understanding of the image formation process and an accurate estimation of the
scene geometry, reflectance and illumination. A delicate manipulation of the
lighting can then be performed while keeping the scene albedo and geometry
unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene
relighting based on neural radiance fields. In contrast to the prior art, our
technique allows simultaneous editing of both scene illumination and camera
viewpoint using only a collection of outdoor photos shot in uncontrolled
settings. Moreover, it enables direct control over the scene illumination, as
defined through a spherical harmonics model. For evaluation, we collect a new
benchmark dataset of several outdoor sites photographed from multiple
viewpoints and at different times. For each time, a 360 degree environment map
is captured together with a colour-calibration chequerboard to allow accurate
numerical evaluations on real data against ground truth. Comparisons against
SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at
higher quality and with realistic self-shadowing reproduction. Our method and
the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures, 2 tables; ECCV 2022; project web page:
  https://4dqv.mpi-inf.mpg.de/NeRF-OSR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeRF-SR: High-Quality Neural Radiance Fields using Supersampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang, Yu-Wing Tai, Shi-Min Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis
with mostly low-resolution (LR) inputs. Our method is built upon Neural
Radiance Fields (NeRF) that predicts per-point density and color with a
multi-layer perceptron. While producing images at arbitrary scales, NeRF
struggles with resolutions that go beyond observed images. Our key insight is
that NeRF benefits from 3D consistency, which means an observed pixel absorbs
information from nearby views. We first exploit it by a supersampling strategy
that shoots multiple rays at each image pixel, which further enforces
multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can
further boost the performance of supersampling by a refinement network that
leverages the estimated depth at hand to hallucinate details from related
patches on only one HR reference image. Experiment results demonstrate that
NeRF-SR generates high-quality results for novel view synthesis at HR on both
synthetic and real-world datasets without any external information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MM 2022. Project Page:
  https://cwchenwang.github.io/NeRF-SR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Knowledge-Transfer for Learned Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Barbano, Zeljko Kereta, Andreas Hauptmann, Simon R. Arridge, Bangti Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based image reconstruction approaches have demonstrated
impressive empirical performance in many imaging modalities. These approaches
usually require a large amount of high-quality paired training data, which is
often not available in medical imaging. To circumvent this issue we develop a
novel unsupervised knowledge-transfer paradigm for learned reconstruction
within a Bayesian framework. The proposed approach learns a reconstruction
network in two phases. The first phase trains a reconstruction network with a
set of ordered pairs comprising of ground truth images of ellipses and the
corresponding simulated measurement data. The second phase fine-tunes the
pretrained network to more realistic measurement data without supervision. By
construction, the framework is capable of delivering predictive uncertainty
information over the reconstructed image. We present extensive experimental
results on low-dose and sparse-view computed tomography showing that the
approach is competitive with several state-of-the-art supervised and
unsupervised reconstruction techniques. Moreover, for test data distributed
differently from the training data, the proposed framework can significantly
improve reconstruction quality not only visually, but also quantitatively in
terms of PSNR and SSIM, when compared with learned methods trained on the
synthetic dataset only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making the Most of Text Semantics to Improve Biomedical Vision--Language
  Processing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09817v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09817v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, Ozan Oktay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:
  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for
  Robotic Bin Picking <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Rui Cao, Stephen James, Yichuan Li, Yun-Hui Liu, Pieter Abbeel, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an iterative self-training framework for
sim-to-real 6D object pose estimation to facilitate cost-effective robotic
grasping. Given a bin-picking scenario, we establish a photo-realistic
simulator to synthesize abundant virtual data, and use this to train an initial
pose estimation network. This network then takes the role of a teacher model,
which generates pose predictions for unlabeled real data. With these
predictions, we further design a comprehensive adaptive selection scheme to
distinguish reliable results, and leverage them as pseudo labels to update a
student model for pose estimation on real data. To continuously improve the
quality of pseudo labels, we iterate the above steps by taking the trained
student model as a new teacher and re-label real data using the refined teacher
model. We evaluate our method on a public benchmark and our newly-released
dataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.
Our method is also able to improve robotic bin-picking success by 19.54%,
demonstrating the potential of iterative sim-to-real solutions for robotic
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition
  and Learning from Synthetic Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey V. Savchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the results of the HSE-NN team in the 4th
competition on Affective Behavior Analysis in-the-wild (ABAW). The novel
multi-task EfficientNet model is trained for simultaneous recognition of facial
expressions and prediction of valence and arousal on static photos. The
resulting MT-EmotiEffNet extracts visual features that are fed into simple
feed-forward neural networks in the multi-task learning challenge. We obtain
performance measure 1.3 on the validation set, which is significantly greater
when compared to either performance of baseline (0.3) or existing models that
are trained only on the s-Aff-Wild2 database. In the learning from synthetic
data challenge, the quality of the original synthetic training set is increased
by using the super-resolution techniques, such as Real-ESRGAN. Next, the
MT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a
simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our
average validation F1 score is 18% greater than the baseline convolutional
neural network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Implicit Attention: Guided Attention by The Model Itself 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyi Wu, Xun Gong, Zhemin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Self-Supervised Implicit Attention (SSIA), a new approach that
adaptively guides deep neural network models to gain attention by exploiting
the properties of the models themselves. SSIA is a novel attention mechanism
that does not require any extra parameters, computation, or memory access costs
during inference, which is in contrast to existing attention mechanism. In
short, by considering attention weights as higher-level semantic information,
we reconsidered the implementation of existing attention mechanisms and further
propose generating supervisory signals from higher network layers to guide
lower network layers for parameter updates. We achieved this by building a
self-supervised learning task using the hierarchical features of the network
itself, which only works at the training stage. To verify the effectiveness of
SSIA, we performed a particular implementation (called an SSIA block) in
convolutional neural network models and validated it on several image
classification datasets. The experimental results show that an SSIA block can
significantly improve the model performance, even outperforms many popular
attention methods that require additional parameters and computation costs,
such as Squeeze-and-Excitation and Convolutional Block Attention Module. Our
implementation will be available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowFormer: A <span class="highlight-title">Transformer</span> Architecture for Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce optical Flow transFormer, dubbed as FlowFormer, a
transformer-based neural network architecture for learning optical flow.
FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the
cost tokens into a cost memory with alternate-group transformer (AGT) layers in
a novel latent space, and decodes the cost memory via a recurrent transformer
decoder with dynamic positional cost queries. On the Sintel benchmark,
FlowFormer achieves 1.144 and 2.183 average end-ponit-error (AEPE) on the clean
and final pass, a 17.6% and 11.6% error reduction from the best published
result (1.388 and 2.47). Besides, FlowFormer also achieves strong
generalization performance. Without being trained on Sintel, FlowFormer
achieves 0.95 AEPE on the Sintel training set clean pass, outperforming the
best published result (1.29) by 26.9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://drinkingcoder.github.io/publication/flowformer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Prediction of Future Lesion Activity and Treatment Effect
  in Multiple Sclerosis from Baseline MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Durso-Finley, Jean-Pierre R. Falet, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision medicine for chronic diseases such as multiple sclerosis (MS)
involves choosing a treatment which best balances efficacy and side
effects/preferences for individual patients. Making this choice as early as
possible is important, as delays in finding an effective therapy can lead to
irreversible disability accrual. To this end, we present the first deep neural
network model for individualized treatment decisions from baseline magnetic
resonance imaging (MRI) (with clinical information if available) for MS
patients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)
lesion counts on follow-up MRI on multiple treatments and (b) estimates the
conditional average treatment effect (CATE), as defined by the predicted future
suppression of NE-T2 lesions, between different treatment options relative to
placebo. Our model is validated on a proprietary federated dataset of 1817
multi-sequence MRIs acquired from MS patients during four multi-centre
randomized clinical trials. Our framework achieves high average precision in
the binarized regression of future NE-T2 lesions on five different treatments,
identifies heterogeneous treatment effects, and provides a personalized
treatment recommendation that accounts for treatment-associated risk (e.g. side
effects, patient preference, administration difficulties).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MIDL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edi<span class="highlight-title">BERT</span>, a generative model for image editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.15264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.15264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in computer vision are pushing the limits of im-age manipulation,
with generative models sampling detailed images on various tasks. However, a
specialized model is often developed and trained for each specific task, even
though many image edition tasks share similarities. In denoising, inpainting,
or image compositing, one always aims at generating a realistic image from a
low-quality one. In this paper, we aim at making a step towards a unified
approach for image editing. To do so, we propose EdiBERT, a bi-directional
transformer trained in the discrete latent space built by a vector-quantized
auto-encoder. We argue that such a bidirectional model is suited for image
manipulation since any patch can be re-sampled conditionally to the whole
image. Using this unique and straightforward training objective, we show that
the resulting model matches state-of-the-art performances on a wide variety of
tasks: image denoising, image completion, and image composition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative
  Spatial Encoding of Keypoints <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.04992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.04992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, Shunsuke Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based volumetric humans using pixel-aligned features promise
generalization to unseen poses and identities. Prior work leverages global
spatial encodings and multi-view geometric consistency to reduce spatial
ambiguity. However, global encodings often suffer from overfitting to the
distribution of the training data, and it is difficult to learn multi-view
consistent reconstruction from sparse views. In this work, we investigate
common issues with existing spatial encodings and propose a simple yet highly
effective approach to modeling high-fidelity volumetric humans from sparse
views. One of the key ideas is to encode relative spatial 3D information via
sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and
cross-dataset domain gap. Our approach outperforms state-of-the-art methods for
head reconstruction. On human body reconstruction for unseen subjects, we also
achieve performance comparable to prior work that uses a parametric human body
model and temporal feature aggregation. Our experiments show that a majority of
errors in prior work stem from an inappropriate choice of spatial encoding and
thus we suggest a new direction for high-fidelity image-based human modeling.
https://markomih.github.io/KeypointNeRF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ECCV 2022. The project page is available at
  https://markomih.github.io/KeypointNeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorian F. Henning, Tristan Laidlow, Stefan Leutenegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many "in the wild" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Video: https://youtu.be/0-SL3VeWEvU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive Learning</span> with Complex Heterogeneity <span class="chip">KDD22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.09401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.09401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lecheng Zheng, Jinjun Xiong, Yada Zhu, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and are characterized with multiple
labels, thus exhibiting the co-existence of multiple types of heterogeneity.
Although state-of-the-art techniques are good at modeling complex heterogeneity
with sufficient label information, such label information can be quite
expensive to obtain in real applications. Recently, researchers pay great
attention to contrastive learning due to its prominent performance by utilizing
rich unlabeled data. However, existing work on contrastive learning is not able
to address the problem of false negative pairs, i.e., some `negative' pairs may
have similar representations if they have the same label. To overcome the
issues, in this paper, we propose a unified heterogeneous learning framework,
which combines both the weighted unsupervised contrastive loss and the weighted
supervised contrastive loss to model multiple types of heterogeneity. We first
provide a theoretical analysis showing that the vanilla contrastive learning
loss easily leads to the sub-optimal solution in the presence of false negative
pairs, whereas the proposed weighted loss could automatically adjust the weight
based on the similarity of the learned representations to mitigate this issue.
Experimental results on real-world data sets demonstrate the effectiveness and
the efficiency of the proposed framework modeling multiple types of
heterogeneity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Densely Constrained Depth Estimator for Monocular 3D Object Detection <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyan Li, Yuntao Chen, Jiawei He, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Chen, Shijia Huang, Shu Liu, Bei Yu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-based 3D object detectors are welcome due to their wider deployment
and lower price than LiDAR sensors. We first revisit the prior stereo detector
DSGN for its stereo volume construction ways for representing both 3D geometry
and semantics. We polish the stereo modeling and propose the advanced version,
DSGN++, aiming to enhance effective information flow throughout the 2D-to-3D
pipeline in three main aspects. First, to effectively lift the 2D information
to stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser
connections and extracts depth-guided features. Second, for grasping
differently spaced features, we present a novel stereo volume -- Dual-view
Stereo Volume (DSV) that integrates front-view and top-view features and
reconstructs sub-voxel depth in the camera frustum. Third, as the foreground
region becomes less dominant in 3D space, we propose a multi-modal data editing
strategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and
improves data efficiency. Without bells and whistles, extensive experiments in
various modality setups on the popular KITTI benchmark show that our method
consistently outperforms other camera-based 3D detectors for all categories.
Code is available at https://github.com/chenyilun95/DSGN2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DProST: Dynamic Projective Spatial <span class="highlight-title">Transformer</span> Network for 6D Pose
  Estimation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Park, Nam Ik Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the object's 6D pose from a single RGB image is a fundamental
computer vision task. Generally, the distance between transformed object
vertices is employed as an objective function for pose estimation methods.
However, projective geometry in the camera space is not considered in those
methods and causes performance degradation. In this regard, we propose a new
pose estimation system based on a projective grid instead of object vertices.
Our pose estimation method, dynamic projective spatial transformer network
(DProST), localizes the region of interest grid on the rays in camera space and
transforms the grid to object space by estimated pose. The transformed grid is
used as both a sampling grid and a new criterion of the estimated pose.
Additionally, because DProST does not require object vertices, our method can
be used in a mesh-less setting by replacing the mesh with a reconstructed
feature. Experimental results show that mesh-less DProST outperforms the
state-of-the-art mesh-based methods on the LINEMOD and LINEMOD-OCCLUSION
dataset, and shows competitive performance on the YCBV dataset with mesh data.
The source code is available at https://github.com/parkjaewoo0611/DProST
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit field supervision for robust non-rigid shape matching <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07694v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07694v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramana Sundararaman, Gautam Pai, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Establishing a correspondence between two non-rigidly deforming shapes is one
of the most fundamental problems in visual computing. Existing methods often
show weak resilience when presented with challenges innate to real-world data
such as noise, outliers, self-occlusion etc. On the other hand, auto-decoders
have demonstrated strong expressive power in learning geometrically meaningful
latent embeddings. However, their use in \emph{shape analysis} has been
limited. In this paper, we introduce an approach based on an auto-decoder
framework, that learns a continuous shape-wise deformation field over a fixed
template. By supervising the deformation field for points on-surface and
regularising for points off-surface through a novel \emph{Signed Distance
Regularisation} (SDR), we learn an alignment between the template and shape
\emph{volumes}. Trained on clean water-tight meshes, \emph{without} any
data-augmentation, we demonstrate compelling performance on compromised data
and real-world scans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, Gerard Pons-Moll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TOCH, a method for refining incorrect 3D hand-object interaction
sequences using a data prior. Existing hand trackers, especially those that
rely on very few cameras, often produce visually unrealistic results with
hand-object intersection or missing contacts. Although correcting such errors
requires reasoning about temporal aspects of interaction, most previous works
focus on static grasps and contacts. The core of our method are TOCH fields, a
novel spatio-temporal representation for modeling correspondences between hands
and objects during interaction. TOCH fields are a point-wise, object-centric
representation, which encode the hand position relative to the object.
Leveraging this novel representation, we learn a latent manifold of plausible
TOCH fields with a temporal denoising auto-encoder. Experiments demonstrate
that TOCH outperforms state-of-the-art 3D hand-object interaction models, which
are limited to static grasps and contacts. More importantly, our method
produces smooth interactions even before and after contact. Using a single
trained TOCH model, we quantitatively and qualitatively demonstrate its
usefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D
hand-object reconstruction methods and transferring grasps across objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmentation of 3D Dental Images Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Boudraa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D image segmentation is a recent and crucial step in many medical analysis
and recognition schemes. In fact, it represents a relevant research subject and
a fundamental challenge due to its importance and influence. This paper
provides a multi-phase Deep Learning-based system that hybridizes various
efficient methods in order to get the best 3D segmentation output. First, to
reduce the amount of data and accelerate the processing time, the application
of Decimate compression technique is suggested and justified. We then use a CNN
model to segment dental images into fifteen separated classes. In the end, a
special KNN-based transformation is applied for the purpose of removing
isolated meshes and of correcting dental forms. Experimentations demonstrate
the precision and the robustness of the selected framework applied to 3D dental
images within a private clinical benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Imitating Collaborative Manipulation Plans from YouTube
  Cooking Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.10686v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.10686v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hejia Zhang, Jie Zhong, Stefanos Nikolaidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People often watch videos on the web to learn how to cook new recipes,
assemble furniture or repair a computer. We wish to enable robots with the very
same capability. This is challenging; there is a large variation in
manipulation actions and some videos even involve multiple persons, who
collaborate by sharing and exchanging objects and tools. Furthermore, the
learned representations need to be general enough to be transferable to robotic
systems. On the other hand, previous work has shown that the space of human
manipulation actions has a linguistic, hierarchical structure that relates
actions to manipulated objects and tools. Building upon this theory of language
for action, we propose a system for understanding and executing demonstrated
action sequences from full-length, real-world cooking videos on the web. The
system takes as input a new, previously unseen cooking video annotated with
object labels and bounding boxes, and outputs a collaborative manipulation
action plan for one or more robotic arms. We demonstrate performance of the
system in a standardized dataset of 100 YouTube cooking videos, as well as in
six full-length Youtube videos that include collaborative actions between two
participants. We compare our system with a baseline system that consists of a
state-of-the-art action detection baseline and show our system achieves higher
action detection accuracy. We additionally propose an open-source platform for
executing the learned plans in a simulation environment as well as with an
actual robotic arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Pedestrian Attribute Recognition Using Group Sparsity for
  Occlusion Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08708v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08708v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonu Lee, Kimin Yun, Jungchan Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occlusion processing is a key issue in pedestrian attribute recognition
(PAR). Nevertheless, several existing video-based PAR methods have not yet
considered occlusion handling in depth. In this paper, we formulate finding
non-occluded frames as sparsity-based temporal attention of a crowded video. In
this manner, a model is guided not to pay attention to the occluded frame.
However, temporal sparsity cannot include a correlation between attributes when
occlusion occurs. For example, "boots" and "shoe color" cannot be recognized
when the foot is invisible. To solve the uncorrelated attention issue, we also
propose a novel group sparsity-based temporal attention module. Group sparsity
is applied across attention weights in correlated attributes. Thus, attention
weights in a group are forced to pay attention to the same frames. Experimental
results showed that the proposed method achieved a higher F1-score than the
state-of-the-art methods on two video-based PAR datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Segmentation of LiDAR Sequences: <span class="highlight-title">Dataset</span> and Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Loiseau, Mathieu Aubry, Loïc Landrieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.
However, most semantic datasets and algorithms used for LiDAR sequence
segmentation operate on $360^\circ$ frames, causing an acquisition latency
incompatible with real-time applications. To address this issue, we first
introduce HelixNet, a $10$ billion point dataset with fine-grained labels,
timestamps, and sensor rotation information necessary to accurately assess the
real-time readiness of segmentation algorithms. Second, we propose Helix4D, a
compact and efficient spatio-temporal transformer architecture specifically
designed for rotating LiDAR sequences. Helix4D operates on acquisition slices
corresponding to a fraction of a full sensor rotation, significantly reducing
the total latency. Helix4D reaches accuracy on par with the best segmentation
algorithms on HelixNet and SemanticKITTI with a reduction of over $5\times$ in
terms of latency and $50\times$ in model size. The code and data are available
at: https://romainloiseau.fr/helixnet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data are available at: https://romainloiseau.fr/helixnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERA: Expert Retrieval and Assembly for Early Action Prediction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Qiuhong Ke, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early action prediction aims to successfully predict the class label of an
action before it is completely performed. This is a challenging task because
the beginning stages of different actions can be very similar, with only minor
subtle differences for discrimination. In this paper, we propose a novel Expert
Retrieval and Assembly (ERA) module that retrieves and assembles a set of
experts most specialized at using discriminative subtle differences, to
distinguish an input sample from other highly similar samples. To encourage our
model to effectively use subtle differences for early action prediction, we
push experts to discriminate exclusively between samples that are highly
similar, forcing these experts to learn to use subtle differences that exist
between those samples. Additionally, we design an effective Expert Learning
Rate Optimization method that balances the experts' optimization and leads to
better performance. We evaluate our ERA module on four public action datasets
and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Video Activity Localisation with Uncertainties in Temporal Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabo Huang, Hailin Jin, Shaogang Gong, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods for video activity localisation over time assume implicitly
that activity temporal boundaries labelled for model training are determined
and precise. However, in unscripted natural videos, different activities mostly
transit smoothly, so that it is intrinsically ambiguous to determine in
labelling precisely when an activity starts and ends over time. Such
uncertainties in temporal labelling are currently ignored in model training,
resulting in learning mis-matched video-text correlation with poor
generalisation in test. In this work, we solve this problem by introducing
Elastic Moment Bounding (EMB) to accommodate flexible and adaptive activity
temporal boundaries towards modelling universally interpretable video-text
correlation with tolerance to underlying temporal uncertainties in pre-fixed
annotations. Specifically, we construct elastic boundaries adaptively by mining
and discovering frame-wise temporal endpoints that can maximise the alignment
between video segments and query sentences. To enable both more accurate
matching (segment content attention) and more robust localisation (segment
elastic boundaries), we optimise the selection of frame-wise endpoints subject
to segment-wise contents by a novel Guided Attention mechanism. Extensive
experiments on three video activity localisation benchmarks demonstrate
compellingly the EMB's advantages over existing methods without modelling
uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Lightweight Super-Resolution with Dual Regression Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Guo, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, Yanwu Xu, Jian Chen, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have exhibited remarkable performance in image
super-resolution (SR) tasks by learning a mapping from low-resolution (LR)
images to high-resolution (HR) images. However, the SR problem is typically an
ill-posed problem and existing methods would come with several limitations.
First, the possible mapping space of SR can be extremely large since there may
exist many different HR images that can be downsampled to the same LR image. As
a result, it is hard to directly learn a promising SR mapping from such a large
space. Second, it is often inevitable to develop very large models with
extremely high computational cost to yield promising SR performance. In
practice, one can use model compression techniques to obtain compact models by
reducing model redundancy. Nevertheless, it is hard for existing model
compression methods to accurately identify the redundant components due to the
extremely large SR mapping space. To alleviate the first challenge, we propose
a dual regression learning scheme to reduce the space of possible SR mappings.
Specifically, in addition to the mapping from LR to HR images, we learn an
additional dual regression mapping to estimate the downsampling kernel and
reconstruct LR images. In this way, the dual mapping acts as a constraint to
reduce the space of possible mappings. To address the second challenge, we
propose a lightweight dual regression compression method to reduce model
redundancy in both layer-level and channel-level based on channel pruning.
Specifically, we first develop a channel number search method that minimizes
the dual regression loss to determine the redundancy of each layer. Given the
searched channel numbers, we further exploit the dual regression manner to
evaluate the importance of channels and prune the redundant ones. Extensive
experiments show the effectiveness of our method in obtaining accurate and
efficient SR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal extension of DRN. arXiv admin note: text overlap with
  arXiv:2003.07018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoserNet: Refining Relative Camera Poses Exploiting Object Detections <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Taiana, Matteo Toso, Stuart James, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of the camera poses associated with a set of images commonly
relies on feature matches between the images. In contrast, we are the first to
address this challenge by using objectness regions to guide the pose estimation
problem rather than explicit semantic object detections. We propose Pose
Refiner Network (PoserNet) a light-weight Graph Neural Network to refine the
approximate pair-wise relative camera poses. PoserNet exploits associations
between the objectness regions - concisely expressed as bounding boxes - across
multiple views to globally refine sparsely connected view graphs. We evaluate
on the 7-Scenes dataset across varied sizes of graphs and show how this process
can be beneficial to optimisation-based Motion Averaging algorithms improving
the median error on the rotation by 62 degrees with respect to the initial
estimates obtained based on bounding boxes. Code and data are available at
https://github.com/IIT-PAVIS/PoserNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without
  Bells and Whistles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10255v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10255v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Liu, Liping Bai, Yuxuan Xia, Tao Huang, Bing Zhu, Qing-Long Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object tracking (MOT) is among crucial applications in modern advanced
driver assistance systems (ADAS) and autonomous driving (AD) systems. The
global nearest neighbor (GNN) filter, as the earliest random vector-based
Bayesian tracking framework, has been adopted in most of state-of-the-arts
trackers and widely accepted in the automotive industry. With the development
of random finite set (RFS) theory, which facilitates a mathematically rigorous
treatment of the MOT problem, different variants of RFS-based Bayesian filters
have been developed. However, their usefulness in the real traffic for ADAS and
AD application is still open to doubt. In this paper, it is first demonstrated
that the latest RFS-based Bayesian tracking framework could be superior to
typical random vector-based Bayesian tracking framework like GNN, via a
systematic comparative study of both traditional random vector-based Bayesian
filters with rule-based heuristic track maintenance and RFS-based Bayesian
filters on the nuScenes validation dataset. Then, an RFS-based tracker, namely
Poisson multi-Bernoulli filter using the global nearest neighbor (GNN-PMB), is
proposed to LiDAR-based MOT tasks. This GNN-PMB tracker is simple to use but
can achieve competitive results on the nuScenes dataset. Specifically, the
proposed GNN-PMB tracker outperforms most of the state-of-the-art LiDAR-only
trackers and LiDAR and camera fusion-based trackers, ranking the 3rd among all
LiDAR-only trackers on nuScenes 3D tracking challenge leader board1 at the time
of submission. Our code is available at here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Synthetic Data: Facial Expression Classification based on
  Ensemble of Multi-task Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-Yeop Jeong, Yeong-Gi Hong, JiYeon Oh, Sumin Hong, Jin-Woo Jeong, Yuchul Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression in-the-wild is essential for various interactive computing
domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic
in the facial expression recognition task. In this paper, we propose a
multi-task learning-based facial expression recognition approach which consists
of emotion and appearance learning branches that can share all face
information, and present preliminary results for the LSD challenge introduced
in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our
method achieved the mean F1 score of 0.71.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Page 3, Added reference [2], [33]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying partial mouse brain microscopy images from Allen reference
  atlas using a <span class="highlight-title">contrastive</span>ly learned semantic space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justinas Antanavicius, Roberto Leiras, Raghavendra Selvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise identification of mouse brain microscopy images is a crucial first
step when anatomical structures in the mouse brain are to be registered to a
reference atlas. Practitioners usually rely on manual comparison of images or
tools that assume the presence of complete images. This work explores Siamese
Networks as the method for finding corresponding 2D reference atlas plates for
given partial 2D mouse brain images. Siamese networks are a class of
convolutional neural networks (CNNs) that use weight-shared paths to obtain low
dimensional embeddings of pairs of input images. The correspondence between the
partial mouse brain image and reference atlas plate is determined based on the
distance between low dimensional embeddings of brain slices and atlas plates
that are obtained from Siamese networks using contrastive learning. Experiments
showed that Siamese CNNs can precisely identify brain slices using the Allen
mouse brain atlas when training and testing images come from the same source.
They achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking
only 7.2 seconds to identify 29 images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Proceedings of International Workshop on Biomedical
  Image Registration (WBIR-2022). Source code available at
  https://github.com/Justinas256/2d-mouse-brain-identification. 12 pages, 6
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, <span class="highlight-author">Jie Lei</span>, <span class="highlight-author">Mohit Bansal</span>, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an audiovisual method for long-range text-to-video retrieval.
Unlike previous approaches designed for short video retrieval (e.g., 5-15
seconds in duration), our approach aims to retrieve minute-long videos that
capture complex human actions. One challenge of standard video-only approaches
is the large computational cost associated with processing hundreds of densely
extracted frames from such long videos. To address this issue, we propose to
replace parts of the video with compact audio cues that succinctly summarize
dynamic audio events and are cheap to process. Our method, named ECLIPSE
(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an
audiovisual video setting, by adding a unified audiovisual transformer block
that captures complementary cues from the video and audio streams. In addition
to being 2.92x faster and 2.34x memory-efficient than long-range video-only
approaches, our method also achieves better text-to-video retrieval accuracy on
several diverse long-range video datasets such as ActivityNet, QVHighlights,
YouCook2, DiDeMo and Charades.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral project page: https://yanbo.ml/project_page/eclipse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnimeCeleb: Large-Scale Animation CelebHeads <span class="highlight-title">Dataset</span> for Head
  Reenactment <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyeol Kim, Sunghyun Park, Jaeseong Lee, Sunghyo Chung, Junsoo Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an
animation head reenactment. Different from previous animation head datasets, we
utilize 3D animation models as the controllable image samplers, which can
provide a large amount of head images with their corresponding detailed pose
annotations. To facilitate a data creation process, we build a semi-automatic
pipeline leveraging an open 3D computer graphics software with a developed
annotation system. After training with the AnimeCeleb, recent head reenactment
models produce high-quality animation head reenactment results, which are not
achievable with existing datasets. Furthermore, motivated by metaverse
application, we propose a novel pose mapping method and architecture to tackle
a cross-domain head reenactment task. During inference, a user can easily
transfer one's motion to an arbitrary animation head. Experiments demonstrate
the usefulness of the AnimeCeleb to train animation head reenactment models,
and the superiority of our cross-domain head reenactment model compared to
state-of-the-art methods. Our dataset and code are available at
https://github.com/kangyeolk/AnimeCeleb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages; Accepted to ECCV 2022; code and dataset URL added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-Actor: Text-Driven Recommendation and Stylization for Animating
  Human Meshes <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Youwang, Kim Ji-Yeon, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose CLIP-Actor, a text-driven motion recommendation and neural mesh
stylization system for human mesh animation. CLIP-Actor animates a 3D human
mesh to conform to a text prompt by recommending a motion sequence and
optimizing mesh style attributes. We build a text-driven human motion
recommendation system by leveraging a large-scale human motion dataset with
language labels. Given a natural language prompt, CLIP-Actor suggests a
text-conforming human motion in a coarse-to-fine manner. Then, our novel
zero-shot neural style optimization detailizes and texturizes the recommended
mesh sequence to conform to the prompt in a temporally-consistent and
pose-agnostic manner. This is distinctive in that prior work fails to generate
plausible results when the pose of an artist-designed mesh does not conform to
the text from the beginning. We further propose the spatio-temporal view
augmentation and mask-weighted embedding attention, which stabilize the
optimization process by leveraging multi-frame human motion and rejecting
poorly rendered views. We demonstrate that CLIP-Actor produces plausible and
human-recognizable style 3D human mesh in motion with detailed geometry and
texture solely from a natural language prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022. [Project page] https://clip-actor.github.io
  [Code] https://github.com/postech-ami/CLIP-Actor</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Fine-Grained Audiovisual Categorization with the SSW60 <span class="highlight-title">Dataset</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grant Van Horn, Rui Qian, Kimberly Wilber, Hartwig Adam, Oisin Mac Aodha, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing
research on audiovisual fine-grained categorization. While our community has
made great strides in fine-grained visual categorization on images, the
counterparts in audio and video fine-grained categorization are relatively
unexplored. To encourage advancements in this space, we have carefully
constructed the SSW60 dataset to enable researchers to experiment with
classifying the same set of categories in three different modalities: images,
audio, and video. The dataset covers 60 species of birds and is comprised of
images from existing datasets, and brand new, expert-curated audio and video
datasets. We thoroughly benchmark audiovisual classification performance and
modality fusion experiments through the use of state-of-the-art transformer
methods. Our findings show that performance of audiovisual fusion methods is
better than using exclusively image or audio based methods for the task of
video classification. We also present interesting modality transfer
experiments, enabled by the unique construction of SSW60 to encompass three
different modalities. We hope the SSW60 dataset and accompanying baselines spur
research in this fascinating area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel Class Discovery without Forgetting <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K J Joseph, Sujoy Paul, Gaurav Aggarwal, Soma Biswas, Piyush Rai, Kai Han, Vineeth N Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess an innate ability to identify and differentiate instances that
they are not familiar with, by leveraging and adapting the knowledge that they
have acquired so far. Importantly, they achieve this without deteriorating the
performance on their earlier learning. Inspired by this, we identify and
formulate a new, pragmatic problem setting of NCDwF: Novel Class Discovery
without Forgetting, which tasks a machine learning model to incrementally
discover novel categories of instances from unlabeled data, while maintaining
its performance on the previously seen categories. We propose 1) a method to
generate pseudo-latent representations which act as a proxy for (no longer
available) labeled data, thereby alleviating forgetting, 2) a
mutual-information based regularizer which enhances unsupervised discovery of
novel classes, and 3) a simple Known Class Identifier which aids generalized
inference when the testing data contains instances form both seen and unseen
categories. We introduce experimental protocols based on CIFAR-10, CIFAR-100
and ImageNet-1000 to measure the trade-off between knowledge retention and
novel class discovery. Our extensive evaluations reveal that existing models
catastrophically forget previously seen categories while identifying novel
categories, while our method is able to effectively balance between the
competing objectives. We hope our work will attract further research into this
newly identified pragmatic problem setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning of Radiative Atmospheric Transfer with an Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abigail Basener, Bill Basener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As electro-optical energy from the sun propagates through the atmosphere it
is affected by radiative transfer effects including absorption, emission, and
scattering. Modeling these affects is essential for scientific remote sensing
measurements of the earth and atmosphere. For example, hyperspectral imagery is
a form of digital imagery collected with many, often hundreds, of wavelengths
of light in pixel. The amount of light measured at the sensor is the result of
emitted sunlight, atmospheric radiative transfer, and the reflectance off the
materials on the ground, all of which vary per wavelength resulting from
multiple physical phenomena. Therefore measurements of the ground spectra or
atmospheric constituents requires separating these different contributions per
wavelength. In this paper, we create an autoencoder similar to denoising
autoencoders treating the atmospheric affects as 'noise' and ground reflectance
as truth per spectrum. We generate hundreds of thousands of training samples by
taking random samples of spectra from laboratory measurements and adding
atmospheric affects using physics-based modelling via MODTRAN
(http://modtran.spectral.com/modtran\_home) by varying atmospheric inputs. This
process ideally could create an autoencoder that would separate atmospheric
effects and ground reflectance in hyperspectral imagery, a process called
atmospheric compensation which is difficult and time-consuming requiring a
combination of heuristic approximations, estimates of physical quantities, and
physical modelling. While the accuracy of our method is not as good as other
methods in the field, this an important first step in applying the growing
field of deep learning of physical principles to atmospheric compensation in
hyperspectral imagery and remote sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Basener, Alexey Castrodad, David Messinger, Jennifer Mahle, Paul Prue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a new dynamical systems algorithm for clustering in
hyperspectral images. The main idea of the algorithm is that data points are
\`pushed\' in the direction of increasing density and groups of pixels that end
up in the same dense regions belong to the same class. This is essentially a
numerical solution of the differential equation defined by the gradient of the
density of data points on the data manifold. The number of classes is automated
and the resulting clustering can be extremely accurate. In addition to
providing a accurate clustering, this algorithm presents a new tool for
understanding hyperspectral data in high dimensions. We evaluate the algorithm
on the Urban (Available at www.tec.ary.mil/Hypercube/) scene comparing
performance against the k-means algorithm using pre-identified classes of
materials as ground truth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised <span class="highlight-title">pre-train</span>ing of graph <span class="highlight-title">transformer</span>s on patient population
  graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Nassir Navab, Anees Kazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training has shown success in different areas of machine learning, such
as Computer Vision, Natural Language Processing (NLP), and medical imaging.
However, it has not been fully explored for clinical data analysis. An immense
amount of clinical records are recorded, but still, data and labels can be
scarce for data collected in small hospitals or dealing with rare diseases. In
such scenarios, pre-training on a larger set of unlabelled clinical data could
improve performance. In this paper, we propose novel unsupervised pre-training
techniques designed for heterogeneous, multi-modal clinical data for patient
outcome prediction inspired by masked language modeling (MLM), by leveraging
graph deep learning over population graphs. To this end, we further propose a
graph-transformer-based network, designed to handle heterogeneous clinical
data. By combining masking-based pre-training with a transformer-based network,
we translate the success of masking-based pre-training in other domains to
heterogeneous clinical data. We show the benefit of our pre-training method in
a self-supervised and a transfer learning setting, utilizing three medical
datasets TADPOLE, MIMIC-III, and a Sepsis Prediction Dataset. We find that our
proposed pre-training methods help in modeling the data at a patient and
population level and improve performance in different fine-tuning tasks on all
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures, 20 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Careful What You Wish For: on the Extraction of Adversarially Trained
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kacem Khaled, Gabriela Nicolescu, Felipe Gohring de Magalhães
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent attacks on Machine Learning (ML) models such as evasion attacks with
adversarial examples and models stealing through extraction attacks pose
several security and privacy threats. Prior work proposes to use adversarial
training to secure models from adversarial examples that can evade the
classification of a model and deteriorate its performance. However, this
protection technique affects the model's decision boundary and its prediction
probabilities, hence it might raise model privacy risks. In fact, a malicious
user using only a query access to the prediction output of a model can extract
it and obtain a high-accuracy and high-fidelity surrogate model. To have a
greater extraction, these attacks leverage the prediction probabilities of the
victim model. Indeed, all previous work on extraction attacks do not take into
consideration the changes in the training process for security purposes. In
this paper, we propose a framework to assess extraction attacks on
adversarially trained models with vision datasets. To the best of our
knowledge, our work is the first to perform such evaluation. Through an
extensive empirical study, we demonstrate that adversarially trained models are
more vulnerable to extraction attacks than models obtained under natural
training circumstances. They can achieve up to $\times1.2$ higher accuracy and
agreement with a fraction lower than $\times0.75$ of the queries. We
additionally find that the adversarial robustness capability is transferable
through extraction attacks, i.e., extracted Deep Neural Networks (DNNs) from
robust models show an enhanced accuracy to adversarial examples compared to
extracted DNNs from naturally trained (i.e. standard) models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the proceedings of the 19th Annual International
  Conference on Privacy, Security & Trust (PST 2022). The conference
  proceedings will be included in IEEE Xplore as in previous editions of the
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The MABe22 Benchmarks for Representation Learning of Multi-Agent
  Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer J. Sun, Andrew Ulmer, Dipam Chakraborty, Brian Geuther, Edward Hayes, Heng Jia, Vivek Kumar, Zachary Partridge, Alice Robie, Catherine E. Schretter, Chao Sun, Keith Sheppard, Param Uttarwar, Pietro Perona, Yisong Yue, Kristin Branson, Ann Kennedy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world behavior is often shaped by complex interactions between multiple
agents. To scalably study multi-agent behavior, advances in unsupervised and
self-supervised learning have enabled a variety of different behavioral
representations to be learned from trajectory data. To date, there does not
exist a unified set of benchmarks that can enable comparing methods
quantitatively and systematically across a broad set of behavior analysis
settings. We aim to address this by introducing a large-scale, multi-agent
trajectory dataset from real-world behavioral neuroscience experiments that
covers a range of behavior analysis tasks. Our dataset consists of trajectory
data from common model organisms, with 9.6 million frames of mouse data and 4.4
million frames of fly data, in a variety of experimental settings, such as
different strains, lengths of interaction, and optogenetic stimulation. A
subset of the frames also consist of expert-annotated behavior labels.
Improvements on our dataset corresponds to behavioral representations that work
across multiple organisms and is able to capture differences for common
behavior analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website:
  https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Primer on Topological Data Analysis to Support Image Analysis Tasks in
  Environmental Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lander Ver Hoef, Henry Adams, Emily J. King, Imme Ebert-Uphoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological data analysis (TDA) is a tool from data science and mathematics
that is beginning to make waves in environmental science. In this work, we seek
to provide an intuitive and understandable introduction to a tool from TDA that
is particularly useful for the analysis of imagery, namely persistent homology.
We briefly discuss the theoretical background but focus primarily on
understanding the output of this tool and discussing what information it can
glean. To this end, we frame our discussion around a guiding example of
classifying satellite images from the Sugar, Fish, Flower, and Gravel Dataset
produced for the study of mesocale organization of clouds by Rasp et. al. in
2020 (arXiv:1906:01906). We demonstrate how persistent homology and its
vectorization, persistence landscapes, can be used in a workflow with a simple
machine learning algorithm to obtain good results, and explore in detail how we
can explain this behavior in terms of image-level features. One of the core
strengths of persistent homology is how interpretable it can be, so throughout
this paper we discuss not just the patterns we find, but why those results are
to be expected given what we know about the theory of persistent homology. Our
goal is that a reader of this paper will leave with a better understanding of
TDA and persistent homology, be able to identify problems and datasets of their
own for which persistent homology could be helpful, and gain an understanding
of results they obtain from applying the included GitHub example code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to Artificial Intelligence for the Earth
  Systems (AIES). Copyright in this work may be transferred without further
  notice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Scaling Laws vs Model Architectures: How does Inductive Bias Influence
  Scaling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, <span class="highlight-author">William Fedus</span>, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been a lot of interest in the scaling properties of Transformer
models. However, not much has been done on the front of investigating the
effect of scaling properties of different inductive biases and model
architectures. Do model architectures scale differently? If so, how does
inductive bias affect scaling behaviour? How does this influence upstream
(pretraining) and downstream (transfer)? This paper conducts a systematic study
of scaling behaviour of ten diverse model architectures such as Transformers,
Switch Transformers, Universal Transformers, Dynamic convolutions, Performers,
and recently proposed MLP-Mixers. Via extensive experiments, we show that (1)
architecture is an indeed an important consideration when performing scaling
and (2) the best performing model can fluctuate at different scales. We believe
that the findings outlined in this work has significant implications to how
model architectures are currently evaluated in the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal precision for GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning disconnected distributions, Generative adversarial networks
(GANs) are known to face model misspecification. Indeed, a continuous mapping
from a unimodal latent distribution to a disconnected one is impossible, so
GANs necessarily generate samples outside of the support of the target
distribution. This raises a fundamental question: what is the latent space
partition that minimizes the measure of these areas? Building on a recent
result of geometric measure theory, we prove that an optimal GANs must
structure its latent space as a 'simplicial cluster' - a Voronoi partition
where cells are convex cones - when the dimension of the latent space is larger
than the number of modes. In this configuration, each Voronoi cell maps to a
distinct mode of the data. We derive both an upper and a lower bound on the
optimal precision of GANs learning disconnected manifolds. Interestingly, these
two bounds have the same order of decrease: $\sqrt{\log m}$, $m$ being the
number of modes. Finally, we perform several experiments to exhibit the
geometry of the latent space and experimentally show that GANs have a geometry
with similar properties to the theoretical one.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Learning of Chemical Bond Representations in Spectral
  Indices and Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bill Basener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate neural networks for classification in
hyperspectral imaging with a focus on connecting the architecture of the
network with the physics of the sensing and materials present. Spectroscopy is
the process of measuring light reflected or emitted by a material as a function
wavelength. Molecular bonds present in the material have vibrational
frequencies which affect the amount of light measured at each wavelength. Thus
the measured spectrum contains information about the particular chemical
constituents and types of bonds. For example, chlorophyll reflects more light
in the near-IR rage (800-900nm) than in the red (625-675nm) range, and this
difference can be measured using a normalized vegetation difference index
(NDVI), which is commonly used to detect vegetation presence, health, and type
in imagery collected at these wavelengths. In this paper we show that the
weights in a Neural Network trained on different vegetation classes learn to
measure this difference in reflectance. We then show that a Neural Network
trained on a more complex set of ten different polymer materials will learn
spectral 'features' evident in the weights for the network, and these features
can be used to reliably distinguish between the different types of polymers.
Examination of the weights provides a human-interpretable understanding of the
network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MQRetNN: Multi-Horizon Time Series Forecasting with Retrieval
  Augmentation <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitan Yang, Carson Eisenach, Dhruv Madeka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-horizon probabilistic time series forecasting has wide applicability to
real-world tasks such as demand forecasting. Recent work in neural time-series
forecasting mainly focus on the use of Seq2Seq architectures. For example,
MQTransformer - an improvement of MQCNN - has shown the state-of-the-art
performance in probabilistic demand forecasting. In this paper, we consider
incorporating cross-entity information to enhance model performance by adding a
cross-entity attention mechanism along with a retrieval mechanism to select
which entities to attend over. We demonstrate how our new neural architecture,
MQRetNN, leverages the encoded contexts from a pretrained baseline model on the
entire population to improve forecasting accuracy. Using MQCNN as the baseline
model (due to computational constraints, we do not use MQTransformer), we first
show on a small demand forecasting dataset that it is possible to achieve ~3%
improvement in test loss by adding a cross-entity attention mechanism where
each entity attends to all others in the population. We then evaluate the model
with our proposed retrieval methods - as a means of approximating an attention
over a large population - on a large-scale demand forecasting application with
over 2 million products and observe ~1% performance gain over the MQCNN
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 8th SIGKDD International Workshop on Mining and Learning
  from Time Series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Forgotten Danger in DNN Supervision Testing: Generating and Detecting
  True Ambiguity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Weiss, André García Gómez, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are becoming a crucial component of modern
software systems, but they are prone to fail under conditions that are
different from the ones observed during training (out-of-distribution inputs)
or on inputs that are truly ambiguous, i.e., inputs that admit multiple classes
with nonzero probability in their ground truth labels. Recent work proposed DNN
supervisors to detect high-uncertainty inputs before their possible
misclassification leads to any harm. To test and compare the capabilities of
DNN supervisors, researchers proposed test generation techniques, to focus the
testing effort on high-uncertainty inputs that should be recognized as
anomalous by supervisors. However, existing test generators can only produce
out-of-distribution inputs. No existing model- and supervisor-independent
technique supports the generation of truly ambiguous test inputs.
  In this paper, we propose a novel way to generate ambiguous inputs to test
DNN supervisors and used it to empirically compare several existing supervisor
techniques. In particular, we propose AmbiGuess to generate ambiguous samples
for image classification problems. AmbiGuess is based on gradient-guided
sampling in the latent space of a regularized adversarial autoencoder.
Moreover, we conducted what is - to the best of our knowledge - the most
extensive comparative study of DNN supervisors, considering their capabilities
to detect 4 distinct types of high-uncertainty inputs, including truly
ambiguous ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metropolis Monte Carlo sampling: convergence, localization transition
  and optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexei D. Chepelianskii, Satya N. Majumdar, Hendrik Schawe, Emmanuel Trizac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among random sampling methods, Markov Chain Monte Carlo algorithms are
foremost. Using a combination of analytical and numerical approaches, we study
their convergence properties towards the steady state, within a random walk
Metropolis scheme. We show that the deviations from the target steady-state
distribution feature a localization transition as a function of the
characteristic length of the attempted jumps defining the random walk. This
transition changes drastically the error which is introduced by incomplete
convergence, and discriminates two regimes where the relaxation mechanism is
limited respectively by diffusion and by rejection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Recurrent Units and the Forward-Backward Algorithm <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Bittar, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using Bayes's theorem, we derive a unit-wise recurrence as well as a backward
recursion similar to the forward-backward algorithm. The resulting Bayesian
recurrent units can be integrated as recurrent neural networks within deep
learning frameworks, while retaining a probabilistic interpretation from the
direct correspondence with hidden Markov models. Whilst the contribution is
mainly theoretical, experiments on speech recognition indicate that adding the
derived units at the end of state-of-the-art recurrent architectures can
improve the performance at a very low cost in terms of trainable parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LPYOLO: Low Precision YOLO for Face Detection on FPGA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bestami Günay, Sefa Burak Okcu, Hasan Şakir Bilge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, number of edge computing devices and artificial intelligence
applications on them have advanced excessively. In edge computing, decision
making processes and computations are moved from servers to edge devices.
Hence, cheap and low power devices are required. FPGAs are very low power,
inclined to do parallel operations and deeply suitable devices for running
Convolutional Neural Networks (CNN) which are the fundamental unit of an
artificial intelligence application. Face detection on surveillance systems is
the most expected application on the security market. In this work, TinyYolov3
architecture is redesigned and deployed for face detection. It is a CNN based
object detection method and developed for embedded systems. PYNQ-Z2 is selected
as a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on
it. Redesigned TinyYolov3 model is defined in numerous bit width precisions
with Brevitas library which brings fundamental CNN layers and activations in
integer quantized form. Then, the model is trained in a quantized structure
with WiderFace dataset. In order to decrease latency and power consumption,
onchip memory of the FPGA is configured as a storage of whole network
parameters and the last activation function is modified as rescaled HardTanh
instead of Sigmoid. Also, high degree of parallelism is applied to logical
resources of the FPGA. The model is converted to an HLS based application with
using FINN framework and FINN-HLS library which includes the layer definitions
in C++. Later, the model is synthesized and deployed. CPU of the SoC is
employed with multithreading mechanism and responsible for preprocessing,
postprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total
board power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP
accuracy rate on Easy category of the WiderFace are achieved with 4 bits
precision model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MVML2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Data Driven Estimation of Cluster Number in Multiplex Images using
  Embedded Density Outliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer A. Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The usage of chemical imaging technologies is becoming a routine
accompaniment to traditional methods in pathology. Significant technological
advances have developed these next generation techniques to provide rich,
spatially resolved, multidimensional chemical images. The rise of digital
pathology has significantly enhanced the synergy of these imaging modalities
with optical microscopy and immunohistochemistry, enhancing our understanding
of the biological mechanisms and progression of diseases. Techniques such as
imaging mass cytometry provide labelled multidimensional (multiplex) images of
specific components used in conjunction with digital pathology techniques.
These powerful techniques generate a wealth of high dimensional data that
create significant challenges in data analysis. Unsupervised methods such as
clustering are an attractive way to analyse these data, however, they require
the selection of parameters such as the number of clusters. Here we propose a
methodology to estimate the number of clusters in an automatic data-driven
manner using a deep sparse autoencoder to embed the data into a lower
dimensional space. We compute the density of regions in the embedded space, the
majority of which are empty, enabling the high density regions to be detected
as outliers and provide an estimate for the number of clusters. This framework
provides a fully unsupervised and data-driven method to analyse
multidimensional data. In this work we demonstrate our method using 45
multiplex imaging mass cytometry datasets. Moreover, our model is trained using
only one of the datasets and the learned embedding is applied to the remaining
44 images providing an efficient process for data analysis. Finally, we
demonstrate the high computational efficiency of our method which is two orders
of magnitude faster than estimating via computing the sum squared distances as
a function of cluster number.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of Non-Crossing Quantile Regression Process with Deep ReQU
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Shen, Yuling Jiao, Yuanyuan Lin, Joel L. Horowitz, Jian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a penalized nonparametric approach to estimating the quantile
regression process (QRP) in a nonseparable model using rectifier quadratic unit
(ReQU) activated deep neural networks and introduce a novel penalty function to
enforce non-crossing of quantile regression curves. We establish the
non-asymptotic excess risk bounds for the estimated QRP and derive the mean
integrated squared error for the estimated QRP under mild smoothness and
regularity conditions. To establish these non-asymptotic risk and estimation
error bounds, we also develop a new error bound for approximating $C^s$ smooth
functions with $s >0$ and their derivatives using ReQU activated neural
networks. This is a new approximation result for ReQU networks and is of
independent interest and may be useful in other problems. Our numerical
experiments demonstrate that the proposed method is competitive with or
outperforms two existing methods, including methods using reproducing kernels
and random forests, for nonparametric quantile regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Audio Waveform Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnon Turetzky, Tzvi Michelson, Yossi Adi, Shmuel Peleg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks contain strong priors for generating natural
looking images [1]. These priors enable image denoising, super resolution, and
inpainting in an unsupervised manner. Previous attempts to demonstrate similar
ideas in audio, namely deep audio priors, (i) use hand picked architectures
such as harmonic convolutions, (ii) only work with spectrogram input, and (iii)
have been used mostly for eliminating Gaussian noise [2]. In this work we show
that existing SOTA architectures for audio source separation contain deep
priors even when working with the raw waveform. Deep priors can be discovered
by training a neural network to generate a single corrupted signal when given
white noise as input. A network with relevant deep priors is likely to generate
a cleaner version of the signal before converging on the corrupted signal. We
demonstrate this restoration effect with several corruptions: background noise,
reverberations, and a gap in the signal (audio inpainting).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Neural Race Reduction: Dynamics of Abstraction in Gated Networks <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew M. Saxe, Shagun Sodhani, Sam Lewallen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022; 23 pages; 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Log Barriers for Safe Black-box Optimization with Application to Safe
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilnura Usmanova, Yarden As, Maryam Kamgarpour, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing noisy functions online, when evaluating the objective requires
experiments on a deployed system, is a crucial task arising in manufacturing,
robotics and many others. Often, constraints on safe inputs are unknown ahead
of time, and we only obtain noisy information, indicating how close we are to
violating the constraints. Yet, safety must be guaranteed at all times, not
only for the final output of the algorithm.
  We introduce a general approach for seeking a stationary point in high
dimensional non-linear stochastic optimization problems in which maintaining
safety during learning is crucial. Our approach called LB-SGD is based on
applying stochastic gradient descent (SGD) with a carefully chosen adaptive
step size to a logarithmic barrier approximation of the original problem. We
provide a complete convergence analysis of non-convex, convex, and
strongly-convex smooth constrained problems, with first-order and zeroth-order
feedback. Our approach yields efficient updates and scales better with
dimensionality compared to existing approaches.
  We empirically compare the sample complexity and the computational cost of
our method with existing safe learning approaches. Beyond synthetic benchmarks,
we demonstrate the effectiveness of our approach on minimizing constraint
violation in policy search tasks in safe reinforcement learning (RL).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 8 pages of appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequence Models for Drone vs Bird Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Cagatay Akyon, Erdem Akagunduz, Sinan Onur Altinuc, Alptekin Temizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone detection has become an essential task in object detection as drone
costs have decreased and drone technology has improved. It is, however,
difficult to detect distant drones when there is weak contrast, long range, and
low visibility. In this work, we propose several sequence classification
architectures to reduce the detected false-positive ratio of drone tracks.
Moreover, we propose a new drone vs. bird sequence classification dataset to
train and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer
based sequence classification architectures have been trained on the proposed
dataset to show the effectiveness of the proposed idea. As experiments show,
using sequence information, bird classification and overall F1 scores can be
increased by up to 73% and 35%, respectively. Among all sequence classification
models, R(2+1)D-based fully convolutional model yields the best transfer
learning and fine-tuning results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AVSS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error Compensation Framework for Flow-Guided Video Inpainting <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyeon Kang, Seoung Wug Oh, Seon Joo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -> pixel
propagation -> synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Implementation of a Reinforcement Learning-based Capacity Sharing
  Algorithm in O-RAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Vilà, Oriol Sallent, Jordi Pérez-Romero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capacity sharing problem in Radio Access Network (RAN) slicing deals with
the distribution of the capacity available in each RAN node among various RAN
slices to satisfy their traffic demands and efficiently use the radio
resources. While several capacity sharing algorithmic solutions have been
proposed in the literature, their practical implementation still remains as a
gap. In this paper, the implementation of a Reinforcement Learning-based
capacity sharing algorithm over the O-RAN architecture is discussed, providing
insights into the operation of the involved interfaces and the containerization
of the solution. Moreover, the description of the testbed implemented to
validate the solution is included and some performance and validation results
are presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE GLOBECOM 2022 workshop on NextGenRAN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and Preventing Shortcut Learning for Fair Medical AI using
  Shortcut Testing (ShorT) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Brown, Nenad Tomasev, Jan Freyberg, Yuan Liu, Alan Karthikesalingam, Jessica Schrouff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) holds great promise for improving healthcare, but it is
critical to ensure that its use will not propagate or amplify health
disparities. An important step is to characterize the (un)fairness of ML models
- their tendency to perform differently across subgroups of the population -
and to understand its underlying mechanisms. One potential driver of
algorithmic unfairness, shortcut learning, arises when ML models base
predictions on improper correlations in the training data. However, diagnosing
this phenomenon is difficult, especially when sensitive attributes are causally
linked with disease. Using multi-task learning, we propose the first method to
assess and mitigate shortcut learning as a part of the fairness assessment of
clinical ML systems, and demonstrate its application to clinical tasks in
radiology and dermatology. Finally, our approach reveals instances when
shortcutting is not responsible for unfairness, highlighting the need for a
holistic approach to fairness mitigation in medical AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Asset Closed-Loop Reservoir Management Using Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Nasir, Louis J. Durlofsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Closed-loop reservoir management (CLRM), in which history matching and
production optimization are performed multiple times over the life of an asset,
can provide significant improvement in the specified objective. These
procedures are computationally expensive due to the large number of flow
simulations required for data assimilation and optimization. Existing CLRM
procedures are applied asset by asset, without utilizing information that could
be useful over a range assets. Here, we develop a CLRM framework for multiple
assets with varying numbers of wells. We use deep reinforcement learning to
train a single global control policy that is applicable for all assets
considered. The new framework is an extension of a recently introduced control
policy methodology for individual assets. Embedding layers are incorporated
into the representation to handle the different numbers of decision variables
that arise for the different assets. Because the global control policy learns a
unified representation of useful features from multiple assets, it is less
expensive to construct than asset-by-asset training (we observe about 3x
speedup in our examples). The production optimization problem includes a
relative-change constraint on the well settings, which renders the results
suitable for practical use. We apply the multi-asset CLRM framework to 2D and
3D water-flooding examples. In both cases, four assets with different well
counts, well configurations, and geostatistical descriptions are considered.
Numerical experiments demonstrate that the global control policy provides
objective function values, for both the 2D and 3D cases, that are nearly
identical to those from control policies trained individually for each asset.
This promising finding suggests that multi-asset CLRM may indeed represent a
viable practical strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unimodal vs. <span class="highlight-title">Multimodal</span> Siamese Networks for Outfit Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Hendriksen, Viggo Overes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of online fashion shopping continues to grow. The ability to
offer an effective recommendation to customers is becoming increasingly
important. In this work, we focus on Fashion Outfits Challenge, part of SIGIR
2022 Workshop on eCommerce. The challenge is centered around Fill in the Blank
(FITB) task that implies predicting the missing outfit, given an incomplete
outfit and a list of candidates. In this paper, we focus on applying siamese
networks on the task. More specifically, we explore how combining information
from multiple modalities (textual and visual modality) impacts the performance
of the model on the task. We evaluate our model on the test split provided by
the challenge organizers and the test split with gold assignments that we
created during the development phase. We discover that using both visual, and
visual and textual data demonstrates promising results on the task. We conclude
by suggesting directions for further improvement of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Search of Multiple Neural Architectures with Different
  Complexities via Importance Sampling <span class="chip">ICANN 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhei Noda, Shota Saito, Shinichi Shirakawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architecture search (NAS) aims to automate architecture design
processes and improve the performance of deep neural networks. Platform-aware
NAS methods consider both performance and complexity and can find
well-performing architectures with low computational resources. Although
ordinary NAS methods result in tremendous computational costs owing to the
repetition of model training, one-shot NAS, which trains the weights of a
supernetwork containing all candidate architectures only once during the search
process, has been reported to result in a lower search cost. This study focuses
on the architecture complexity-aware one-shot NAS that optimizes the objective
function composed of the weighted sum of two metrics, such as the predictive
performance and number of parameters. In existing methods, the architecture
search process must be run multiple times with different coefficients of the
weighted sum to obtain multiple architectures with different complexities. This
study aims at reducing the search cost associated with finding multiple
architectures. The proposed method uses multiple distributions to generate
architectures with different complexities and updates each distribution using
the samples obtained from multiple distributions based on importance sampling.
The proposed method allows us to obtain multiple architectures with different
complexities in a single architecture search, resulting in reducing the search
cost. The proposed method is applied to the architecture search of
convolutional neural networks on the CIAFR-10 and ImageNet datasets.
Consequently, compared with baseline methods, the proposed method finds
multiple architectures with varying complexities while requiring less
computational effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at the 31st International Conference
  on Artificial Neural Networks (ICANN 2022). The final authenticated
  publication will be available in the Springer Lecture Notes in Computer
  Science (LNCS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Generative Model for Weakly Supervised Chest Anomaly
  Localization via Pseudo-paired Registration with Bilaterally Symmetrical Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyung-Su Kim, Seong Je Oh, Tae Uk Kim, Myung Jin Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image translation based on a generative adversarial network (GAN-IT) is a
promising method for precise localization of abnormal regions in chest X-ray
images (AL-CXR). However, heterogeneous unpaired datasets undermine existing
methods to extract key features and distinguish normal from abnormal cases,
resulting in inaccurate and unstable AL-CXR. To address this problem, we
propose an improved two-stage GAN-IT involving registration and data
augmentation. For the first stage, we introduce an invertible
deep-learning-based registration technique that virtually and reasonably
converts unpaired data into paired data for learning registration maps. This
novel approach achieves high registration performance. For the second stage, we
apply data augmentation to diversify anomaly locations by swapping the left and
right lung regions on the uniform registered frames, further improving the
performance by alleviating imbalance in data distribution showing left and
right lung lesions. Our method is intended for application to existing GAN-IT
models, allowing existing architecture to benefit from key features for
translation. By showing that the AL-CXR performance is uniformly improved when
applying the proposed method, we believe that GAN-IT for AL-CXR can be deployed
in clinical environments, even if learning data are scarce.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Kyung-Su Kim and Seong Je Oh have contributed equally to this work as
  the co-first author. Kyung-Su Kim (kskim.doc@gmail.com) and Myung Jin Chung
  (mj1.chung@samsung.com) have contributed equally to this work as the
  co-corresponding author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniFed: A Benchmark for Federated Learning Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyuan Liu, Tianneng Shi, Chulin Xie, Qinbin Li, Kangping Hu, Haoyu Kim, Xiaojun Xu, Bo Li, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has become a practical and popular paradigm in
machine learning. However, currently, there is no systematic solution that
covers diverse use cases. Practitioners often face the challenge of how to
select a matching FL framework for their use case. In this work, we present
UniFed, the first unified benchmark for standardized evaluation of the existing
open-source FL frameworks. With 15 evaluation scenarios, we present both
qualitative and quantitative evaluation results of nine existing popular
open-sourced FL frameworks, from the perspectives of functionality, usability,
and system performance. We also provide suggestions on framework selection
based on the benchmark conclusions and point out future improvement directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/AI-secure/FLBenchmark-toolkit Website:
  https://unifedbenchmark.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-enhanced Black-box Attacks for Recommendations <span class="chip">KDD'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing Li, Yihua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that deep neural networks-based recommender systems
are vulnerable to adversarial attacks, where attackers can inject carefully
crafted fake user profiles (i.e., a set of items that fake users have
interacted with) into a target recommender system to achieve malicious
purposes, such as promote or demote a set of target items. Due to the security
and privacy concerns, it is more practical to perform adversarial attacks under
the black-box setting, where the architecture/parameters and training data of
target systems cannot be easily accessed by attackers. However, generating
high-quality fake user profiles under black-box setting is rather challenging
with limited resources to target systems. To address this challenge, in this
work, we introduce a novel strategy by leveraging items' attribute information
(i.e., items' knowledge graph), which can be publicly accessible and provide
rich auxiliary knowledge to enhance the generation of fake user profiles. More
specifically, we propose a knowledge graph-enhanced black-box attacking
framework (KGAttack) to effectively learn attacking policies through deep
reinforcement learning techniques, in which knowledge graph is seamlessly
integrated into hierarchical policy networks to generate fake user profiles for
performing adversarial black-box attacks. Comprehensive experiments on various
real-world datasets demonstrate the effectiveness of the proposed attacking
framework under the black-box setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the KDD'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Matching via Query-Conditioned Subgraph Matching Neural
  Networks and Bi-Level Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunsheng Bai, Derek Xu, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have shown the success of using reinforcement learning and
search to solve NP-hard graph-related tasks, such as Traveling Salesman
Optimization, Graph Edit Distance computation, etc. However, it remains unclear
how one can efficiently and accurately detect the occurrences of a small query
graph in a large target graph, which is a core operation in graph database
search, biomedical analysis, social group finding, etc. This task is called
Subgraph Matching which essentially performs subgraph isomorphism check between
a query graph and a large target graph. One promising approach to this
classical problem is the "learning-to-search" paradigm, where a reinforcement
learning (RL) agent is designed with a learned policy to guide a search
algorithm to quickly find the solution without any solved instances for
supervision. However, for the specific task of Subgraph Matching, though the
query graph is usually small given by the user as input, the target graph is
often orders-of-magnitude larger. It poses challenges to the neural network
design and can lead to solution and reward sparsity. In this paper, we propose
N-BLS with two innovations to tackle the challenges: (1) A novel
encoder-decoder neural network architecture to dynamically compute the matching
information between the query and the target graphs at each search state; (2) A
Monte Carlo Tree Search enhanced bi-level search framework for training the
policy and value networks. Experiments on five large real-world target graphs
show that N-BLS can significantly improve the subgraph matching performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action2Score: An Embedding Approach To Score Player Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Jang, Ji Young Woo, Huy Kang Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiplayer Online Battle Arena (MOBA) is one of the most successful game
genres. MOBA games such as League of Legends have competitive environments
where players race for their rank. In most MOBA games, a player's rank is
determined by the match result (win or lose). It seems natural because of the
nature of team play, but in some sense, it is unfair because the players who
put a lot of effort lose their rank just in case of loss and some players even
get free-ride on teammates' efforts in case of a win. To reduce the
side-effects of the team-based ranking system and evaluate a player's
performance impartially, we propose a novel embedding model that converts a
player's actions into quantitative scores based on the actions' respective
contribution to the team's victory. Our model is built using a sequence-based
deep learning model with a novel loss function working on the team match. The
sequence-based deep learning model process the action sequence from the game
start to the end of a player in a team play using a GRU unit that takes a
hidden state from the previous step and the current input selectively. The loss
function is designed to help the action score to reflect the final score and
the success of the team. We showed that our model can evaluate a player's
individual performance fairly and analyze the contributions of the player's
respective actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 4 tables; accepted to ACM CHIPLAY 2022, and PACM
  on Human-Computer Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Villaflor, Zhe Huang, Swapnil Pande, John Dolan, Jeff Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive results in natural language processing (NLP) based on the
Transformer neural network architecture have inspired researchers to explore
viewing offline reinforcement learning (RL) as a generic sequence modeling
problem. Recent works based on this paradigm have achieved state-of-the-art
results in several of the mostly deterministic offline Atari and D4RL
benchmarks. However, because these methods jointly model the states and actions
as a single sequencing problem, they struggle to disentangle the effects of the
policy and world dynamics on the return. Thus, in adversarial or stochastic
environments, these methods lead to overly optimistic behavior that can be
dangerous in safety-critical systems like autonomous driving. In this work, we
propose a method that addresses this optimism bias by explicitly disentangling
the policy and world models, which allows us at test time to search for
policies that are robust to multiple possible futures in the environment. We
demonstrate our method's superior performance on a variety of autonomous
driving tasks in simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comprehensive study of non-adaptive and residual-based adaptive
  sampling for physics-informed neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, Lu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have shown to be an effective tool
for solving forward and inverse problems of partial differential equations
(PDEs). PINNs embed the PDEs into the loss of the neural network, and this PDE
loss is evaluated at a set of scattered residual points. The distribution of
these points are highly important to the performance of PINNs. However, in the
existing studies on PINNs, only a few simple residual point sampling methods
have mainly been used. Here, we present a comprehensive study of two categories
of sampling: non-adaptive uniform sampling and adaptive nonuniform sampling. We
consider six uniform sampling, including (1) equispaced uniform grid, (2)
uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence,
(5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling
strategy for uniform sampling. To improve the sampling efficiency and the
accuracy of PINNs, we propose two new residual-based adaptive sampling methods:
residual-based adaptive distribution (RAD) and residual-based adaptive
refinement with distribution (RAR-D), which dynamically improve the
distribution of residual points based on the PDE residuals during training.
Hence, we have considered a total of 10 different sampling methods, including
six non-adaptive uniform sampling, uniform sampling with resampling, two
proposed adaptive sampling, and an existing adaptive sampling. We extensively
tested the performance of these sampling methods for four forward problems and
two inverse problems in many setups. Our numerical results presented in this
study are summarized from more than 6000 simulations of PINNs. We show that the
proposed adaptive sampling methods of RAD and RAR-D significantly improve the
accuracy of PINNs with fewer residual points. The results obtained in this
study can also be used as a practical guideline in choosing sampling methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Study on Supervised versus Semi-supervised Machine Learning
  for Anomaly Detection of In-vehicle CAN Network <span class="chip">SC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Dong, Kejia Chen, Yinxuan Peng, Zhiyuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the central nerve of the intelligent vehicle control system, the
in-vehicle network bus is crucial to the security of vehicle driving. One of
the best standards for the in-vehicle network is the Controller Area Network
(CAN bus) protocol. However, the CAN bus is designed to be vulnerable to
various attacks due to its lack of security mechanisms. To enhance the security
of in-vehicle networks and promote the research in this area, based upon a
large scale of CAN network traffic data with the extracted valuable features,
this study comprehensively compared fully-supervised machine learning with
semi-supervised machine learning methods for CAN message anomaly detection.
Both traditional machine learning models (including single classifier and
ensemble models) and neural network based deep learning models are evaluated.
Furthermore, this study proposed a deep autoencoder based semi-supervised
learning method applied for CAN message anomaly detection and verified its
superiority over other semi-supervised methods. Extensive experiments show that
the fully-supervised methods generally outperform semi-supervised ones as they
are using more information as inputs. Typically the developed XGBoost based
model obtained state-of-the-art performance with the best accuracy (98.65%),
precision (0.9853), and ROC AUC (0.9585) beating other methods reported in the
literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted by the 25th IEEE International
  Conference on Intelligent Transportation Systems (IEEE ITSC 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Visual Representations with Texts for Domain Generalization <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonwoo Min, Nokyung Park, Siwon Kim, Seunghyun Park, Jinkyu Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the representational discrepancy between source and target domains
is a key component to maximize the model generalization. In this work, we
advocate for leveraging natural language supervision for the domain
generalization task. We introduce two modules to ground visual representations
with texts containing typical reasoning of humans: (1) Visual and Textual Joint
Embedder and (2) Textual Explanation Generator. The former learns the
image-text joint embedding space where we can ground high-level
class-discriminative information into the model. The latter leverages an
explainable model and generates explanations justifying the rationale behind
its decision. To the best of our knowledge, this is the first work to leverage
the vision-and-language cross-modality approach for the domain generalization
task. Our experiments with a newly created CUB-DG benchmark dataset demonstrate
that cross-modality supervision can be successfully used to ground
domain-invariant visual representations and improve the model generalization.
Furthermore, in the large-scale DomainBed benchmark, our proposed method
achieves state-of-the-art results and ranks 1st in average performance for five
multi-domain datasets. The dataset and codes are available at
https://github.com/mswzeus/GVRT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages (including Supplementary Materials), ECCV 2022 camera ready
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Resolution Analysis (MRA) for Approximate Self-Attention <span class="chip">ICML2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have emerged as a preferred model for many tasks in natural
langugage processing and vision. Recent efforts on training and deploying
Transformers more efficiently have identified many strategies to approximate
the self-attention matrix, a key module in a Transformer architecture.
Effective ideas include various prespecified sparsity patterns, low-rank basis
expansions and combinations thereof. In this paper, we revisit classical
Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value
in this setting remains underexplored thus far. We show that simple
approximations based on empirical feedback and design choices informed by
modern hardware and implementation challenges, eventually yield a MRA-based
approach for self-attention with an excellent performance profile across most
criteria of interest. We undertake an extensive set of experiments and
demonstrate that this multi-resolution scheme outperforms most efficient
self-attention proposals and is favorable for both short and long sequences.
Code is available at \url{https://github.com/mlpen/mra-attention}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for
  Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sekitoshi Kanai, Shin'ya Yamaguchi, Masanori Yamada, Hiroshi Takahashi, Yasutoshi Ida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defending deep neural networks against adversarial examples is a key
challenge for AI safety. To improve the robustness effectively, recent methods
focus on important data points near the decision boundary in adversarial
training. However, these methods are vulnerable to Auto-Attack, which is an
ensemble of parameter-free attacks for reliable evaluation. In this paper, we
experimentally investigate the causes of their vulnerability and find that
existing methods reduce margins between logits for the true label and the other
labels while keeping their gradient norms non-small values. Reduced margins and
non-small gradient norms cause their vulnerability since the largest logit can
be easily flipped by the perturbation. Our experiments also show that the
histogram of the logit margins has two peaks, i.e., small and large logit
margins. From the observations, we propose switching one-versus-the-rest loss
(SOVR), which uses one-versus-the-rest loss when data have small logit margins
so that it increases the margins. We find that SOVR increases logit margins
more than existing methods while keeping gradient norms small and outperforms
them in terms of the robustness against Auto-Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProMix: Combating Label Noise via Maximizing Clean Sample Utility <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to train deep neural networks under label noise is appealing, as
imperfectly annotated data are relatively cheaper to obtain. State-of-the-art
approaches are based on semi-supervised learning(SSL), which selects small loss
examples as clean and then applies SSL techniques for boosted performance.
However, the selection step mostly provides a medium-sized and decent-enough
clean subset, which overlooks a rich set of clean samples. In this work, we
propose a novel noisy label learning framework ProMix that attempts to maximize
the utility of clean samples for boosted performance. Key to our method, we
propose a matched high-confidence selection technique that selects those
examples having high confidence and matched prediction with its given labels.
Combining with the small-loss selection, our method is able to achieve a
precision of 99.27 and a recall of 98.22 in detecting clean samples on the
CIFAR-10N dataset. Based on such a large set of clean data, ProMix improves the
best baseline method by +2.67% on CIFAR-10N and +1.61% on CIFAR-100N datasets.
The code and data are available at https://github.com/Justherozen/ProMix
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Winner of the 1st Learning and Mining with Noisy Labels Challenge in
  IJCAI-ECAI 2022 (an informal technical report)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOCUS: Fairness via Agent-Awareness for Federated Learning on
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenda Chu, Chulin Xie, Boxin Wang, Linyi Li, Lang Yin, Han Zhao, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) provides an effective paradigm to train machine
learning models over distributed data with privacy protection. However, recent
studies show that FL is subject to various security, privacy, and fairness
threats due to the potentially malicious and heterogeneous local agents. For
instance, it is vulnerable to local adversarial agents who only contribute
low-quality data, with the goal of harming the performance of those with
high-quality data. This kind of attack hence breaks existing definitions of
fairness in FL that mainly focus on a certain notion of performance parity. In
this work, we aim to address this limitation and propose a formal definition of
fairness via agent-awareness for FL (FAA), which takes the heterogeneous data
contributions of local agents into account. In addition, we propose a fair FL
training algorithm based on agent clustering (FOCUS) to achieve FAA.
Theoretically, we prove the convergence and optimality of FOCUS under mild
conditions for linear models and general convex loss functions with bounded
smoothness. We also prove that FOCUS always achieves higher fairness measured
by FAA compared with standard FedAvg protocol under both linear models and
general convex loss functions. Empirically, we evaluate FOCUS on four datasets,
including synthetic data, images, and texts under different settings, and we
show that FOCUS achieves significantly higher fairness based on FAA while
maintaining similar or even higher prediction accuracy compared with FedAvg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplitMixer: Fat Trimmed From MLP-like Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Borji, Sikun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SplitMixer, a simple and lightweight isotropic MLP-like
architecture, for visual recognition. It contains two types of interleaving
convolutional operations to mix information across spatial locations (spatial
mixing) and channels (channel mixing). The first one includes sequentially
applying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial
information. The second one is splitting the channels into overlapping or
non-overlapping segments, with or without shared parameters, and applying our
proposed channel mixing approaches or 3D convolution to mix channel
information. Depending on design choices, a number of SplitMixer variants can
be constructed to balance accuracy, the number of parameters, and speed. We
show, both theoretically and experimentally, that SplitMixer performs on par
with the state-of-the-art MLP-like models while having a significantly lower
number of parameters and FLOPS. For example, without strong data augmentation
and optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only
0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M
parameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On
CIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with
ConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our
results spark further research towards finding more efficient vision
architectures and facilitate the development of MLP-like models. Code is
available at https://github.com/aliborji/splitmixer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBDF: Gender Balanced DeepFake <span class="highlight-title">Dataset</span> Towards Fair DeepFake Detection <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Varma Nadimpalli, Ajita Rattani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial forgery by deepfakes has raised severe societal concerns. Several
solutions have been proposed by the vision community to effectively combat the
misinformation on the internet via automated deepfake detection systems. Recent
studies have demonstrated that facial analysis-based deep learning models can
discriminate based on protected attributes. For the commercial adoption and
massive roll-out of the deepfake detection technology, it is vital to evaluate
and understand the fairness (the absence of any prejudice or favoritism) of
deepfake detectors across demographic variations such as gender and race. As
the performance differential of deepfake detectors between demographic
subgroups would impact millions of people of the deprived sub-group. This paper
aims to evaluate the fairness of the deepfake detectors across males and
females. However, existing deepfake datasets are not annotated with demographic
labels to facilitate fairness analysis. To this aim, we manually annotated
existing popular deepfake datasets with gender labels and evaluated the
performance differential of current deepfake detectors across gender. Our
analysis on the gender-labeled version of the datasets suggests (a) current
deepfake datasets have skewed distribution across gender, and (b) commonly
adopted deepfake detectors obtain unequal performance across gender with mostly
males outperforming females. Finally, we contributed a gender-balanced and
annotated deepfake dataset, GBDF, to mitigate the performance differential and
to promote research and development towards fairness-aware deep fake detectors.
The GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26th International Conference on Pattern Recognition (ICPR) 2022|
  Montreal, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Legendre-Galerkin Neural Network for Stiff Partial
  Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Choi, Namjung Kim, Youngjoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods have been lately used to solve differential
equations and dynamical systems. These approaches have been developed into a
novel research field known as scientific machine learning in which techniques
such as deep neural networks and statistical learning are applied to classical
problems of applied mathematics. Because neural networks provide an
approximation capability, computational parameterization through machine
learning and optimization methods achieve noticeable performance when solving
various partial differential equations (PDEs). In this paper, we develop a
novel numerical algorithm that incorporates machine learning and artificial
intelligence to solve PDEs. In particular, we propose an unsupervised machine
learning algorithm based on the Legendre-Galerkin neural network to find an
accurate approximation to the solution of different types of PDEs. The proposed
neural network is applied to the general 1D and 2D PDEs as well as singularly
perturbed PDEs that possess boundary layer behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Level Set Theory for Neural Implicit Evolution under Explicit Flows <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishit Mehta, Manmohan Chandraker, Ravi Ramamoorthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordinate-based neural networks parameterizing implicit surfaces have
emerged as efficient representations of geometry. They effectively act as
parametric level sets with the zero-level set defining the surface of interest.
We present a framework that allows applying deformation operations defined for
triangle meshes onto such implicit surfaces. Several of these operations can be
viewed as energy-minimization problems that induce an instantaneous flow field
on the explicit surface. Our method uses the flow field to deform parametric
implicit surfaces by extending the classical theory of level sets. We also
derive a consolidated view for existing methods on differentiable surface
extraction and rendering, by formalizing connections to the level-set theory.
We show that these methods drift from the theory and that our approach exhibits
improvements for applications like surface smoothing, mean-curvature flow,
inverse rendering and user-defined editing on implicit geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 (Oral); Project Page at https://ishit.github.io/nie</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Machine Learning: A <span class="highlight-title">Survey</span> and Open Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, Ricardo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal Machine Learning (CausalML) is an umbrella term for machine learning
methods that formalize the data-generation process as a structural causal model
(SCM). This perspective enables us to reason about the effects of changes to
this process (interventions) and what would have happened in hindsight
(counterfactuals). We categorize work in CausalML into five groups according to
the problems they address: (1) causal supervised learning, (2) causal
generative modeling, (3) causal explanations, (4) causal fairness, and (5)
causal reinforcement learning. We systematically compare the methods in each
category and point out open problems. Further, we review data-modality-specific
applications in computer vision, natural language processing, and graph
representation learning. Finally, we provide an overview of causal benchmarks
and a critical discussion of the state of this nascent field, including
recommendations for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>191 pages. v02. Work in progress. Feedback and comments are highly
  appreciated!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locally Random P-adic Alloy Codes with Channel Coding Theorems for
  Distributed Coded Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03469v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03469v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Soto, Haibin Guan, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensors, i.e., multi-linear functions, are a fundamental building block of
machine learning algorithms. In order to train on large data-sets, it is common
practice to distribute the computation amongst workers. However, stragglers and
other faults can severely impact the performance and overall training time. A
novel strategy to mitigate these failures is the use of coded computation. We
introduce a new metric for analysis called the typical recovery threshold,
which focuses on the most likely event and provide a novel construction of
distributed coded tensor operations which are optimal with this measure. We
show that our general framework encompasses many other computational schemes
and metrics as a special case. In particular, we prove that the recovery
threshold and the tensor rank can be recovered as a special case of the typical
recovery threshold when the probability of noise, i.e., a fault, is equal to
zero, thereby providing a noisy generalization of noiseless computation as a
serendipitous result. Far from being a purely theoretical construction, these
definitions lead us to practical random code constructions, i.e., locally
random p-adic alloy codes, which are optimal with respect to the measures. We
analyze experiments conducted on Amazon EC2 and establish that they are faster
and more numerically stable than many other benchmark computation schemes in
practice, as is predicted by theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Deep Learning Architectures for Intelligent Reflecting
  Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.02540v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.02540v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet M. Elbir, Kumar Vijay Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent reflecting surfaces (IRSs) have recently received significant
attention for wireless communications because it reduces the hardware
complexity, physical size, weight, and cost of conventional large arrays.
However, deployment of IRS entails dealing with multiple channel links between
the base station (BS) and the users. Further, the BS and IRS beamformers
require a joint design, wherein the IRS elements must be rapidly reconfigured.
Data-driven techniques, such as deep learning (DL), are critical in addressing
these challenges. The lower computation time and model-free nature of DL makes
it robust against the data imperfections and environmental changes. At the
physical layer, DL has been shown to be effective for IRS signal detection,
channel estimation and active/passive beamforming using architectures such as
supervised, unsupervised and reinforcement learning. This article provides a
synopsis of these techniques for designing DL-based IRS-assisted wireless
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages and 5 figures. This work has been submitted to the IEEE for
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Generalization in Federated Learning by Seeking Flat Minima <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11834v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11834v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Caldarola, Barbara Caputo, Marco Ciccone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knee arthritis severity measurement using deep learning: a publicly
  available algorithm with a multi-institutional validation showing
  radiologist-level performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxue Gu, Keyu Li, Roy J. Colglazier, Jichen Yang, Michael Lebhar, Jonathan O'Donnell, William A. Jiranek, Richard C. Mather, Rob J. French, Nicholas Said, Jikai Zhang, Christine Park, Maciej A. Mazurowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a
central criteria for the use of total knee arthroplasty. However, this
assessment suffers from imprecise standards and a remarkably high inter-reader
variability. An algorithmic, automated assessment of KOA severity could improve
overall outcomes of knee replacement procedures by increasing the
appropriateness of its use. We propose a novel deep learning-based five-step
algorithm to automatically grade KOA from posterior-anterior (PA) views of
radiographs: (1) image preprocessing (2) localization of knees joints in the
image using the YOLO v3-Tiny model, (3) initial assessment of the severity of
osteoarthritis using a convolutional neural network-based classifier, (4)
segmentation of the joints and calculation of the joint space narrowing (JSN),
and (5), a combination of the JSN and the initial assessment to determine a
final Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation
masks used to make the assessment, our algorithm demonstrates a higher degree
of transparency compared to typical "black box" deep learning classifiers. We
perform a comprehensive evaluation using two public datasets and one dataset
from our institution, and show that our algorithm reaches state-of-the art
performance. Moreover, we also collected ratings from multiple radiologists at
our institution and showed that our algorithm performs at the radiologist
level.
  The software has been made publicly available at
https://github.com/MaciejMazurowski/osteoarthritis-classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Regression in Materials Science: Discovering Interatomic
  Potentials from Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bogdan Burlacu, Michael Kommenda, Gabriel Kronberger, Stephan Winkler, Michael Affenzeller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Particle-based modeling of materials at atomic scale plays an important role
in the development of new materials and understanding of their properties. The
accuracy of particle simulations is determined by interatomic potentials, which
allow to calculate the potential energy of an atomic system as a function of
atomic coordinates and potentially other properties. First-principles-based ab
initio potentials can reach arbitrary levels of accuracy, however their
aplicability is limited by their high computational cost.
  Machine learning (ML) has recently emerged as an effective way to offset the
high computational costs of ab initio atomic potentials by replacing expensive
models with highly efficient surrogates trained on electronic structure data.
Among a plethora of current methods, symbolic regression (SR) is gaining
traction as a powerful "white-box" approach for discovering functional forms of
interatomic potentials.
  This contribution discusses the role of symbolic regression in Materials
Science (MS) and offers a comprehensive overview of current methodological
challenges and state-of-the-art results. A genetic programming-based approach
for modeling atomic potentials from raw data (consisting of snapshots of atomic
positions and associated potential energy) is presented and empirically
validated on ab initio electronic structure data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the GPTP XIX Workshop, June 2-4 2022, University of
  Michigan, Ann Arbor, Michigan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Hierarchical Bayesian Tucker Decomposition for Genetic Data
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.12426v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.12426v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Sandler, Diego Klabjan, Yuan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop methods for reducing the dimensionality of large data sets, common
in biomedical applications. Learning about patients using genetic data often
includes more features than observations, which makes direct supervised
learning difficult. One method of reducing the feature space is to use latent
Dirichlet allocation to group genetic variants in an unsupervised manner.
Latent Dirichlet allocation describes a patient as a mixture of topics
corresponding to genetic variants. This can be generalized as a Bayesian tensor
decomposition to account for multiple feature variables. Our most significant
contributions are with hierarchical topic modeling. We design distinct methods
of incorporating hierarchical topic modeling, based on nested Chinese
restaurant processes and Pachinko Allocation Machine, into Bayesian tensor
decomposition. We apply these models to examine patients with one of four
common types of cancer (breast, lung, prostate, and colorectal) and siblings
with and without autism spectrum disorder. We linked the genes with their
biological pathways and combine this information into a tensor of patients,
counts of their genetic variants, and the genes' membership in pathways. We
find that our trained models outperform baseline models, with respect to
coherence, by up to 40%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCR-free Document Understanding <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.15664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.15664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding document images (e.g., invoices) is a core but challenging task
since it requires complex functions such as reading text and a holistic
understanding of the document. Current Visual Document Understanding (VDU)
methods outsource the task of reading text to off-the-shelf Optical Character
Recognition (OCR) engines and focus on the understanding task with the OCR
outputs. Although such OCR-based approaches have shown promising performance,
they suffer from 1) high computational costs for using OCR; 2) inflexibility of
OCR models on languages or types of document; 3) OCR error propagation to the
subsequent process. To address these issues, in this paper, we introduce a
novel OCR-free VDU model named Donut, which stands for Document understanding
transformer. As the first step in OCR-free VDU research, we propose a simple
architecture (i.e., Transformer) with a pre-training objective (i.e.,
cross-entropy loss). Donut is conceptually simple yet effective. Through
extensive experiments and analyses, we show a simple OCR-free VDU model, Donut,
achieves state-of-the-art performances on various VDU tasks in terms of both
speed and accuracy. In addition, we offer a synthetic data generator that helps
the model pre-training to be flexible in various languages and domains. The
code, trained model and synthetic data are available at
https://github.com/clovaai/donut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Learning the <span class="highlight-title">Transformer</span> Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankalan Pal Chowdhury, Adamos Solomou, Avinava Dubey, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data
driven framework for learning the kernel function in Transformers. Our
framework approximates the Transformer kernel as a dot product between spectral
feature maps and learns the kernel by learning the spectral distribution. This
not only helps in learning a generic kernel end-to-end, but also reduces the
time and space complexity of Transformers from quadratic to linear. We show
that KERNELIZED TRANSFORMERS achieve performance comparable to existing
efficient Transformer architectures, both in terms of accuracy as well as
computational efficiency. Our study also demonstrates that the choice of the
kernel has a substantial impact on performance, and kernel learning variants
are competitive alternatives to fixed kernel Transformers, both in long as well
as short sequence tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric <span class="highlight-title">Multimodal</span> <span class="highlight-title">Contrastive</span> Representation Learning <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petra Poklukar, Miguel Vasco, Hang Yin, Francisco S. Melo, Ana Paiva, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations of multimodal data that are both informative and
robust to missing modalities at test time remains a challenging problem due to
the inherent heterogeneity of data obtained from different channels. To address
it, we present a novel Geometric Multimodal Contrastive (GMC) representation
learning method consisting of two main components: i) a two-level architecture
consisting of modality-specific base encoders, allowing to process an arbitrary
number of modalities to an intermediate representation of fixed dimensionality,
and a shared projection head, mapping the intermediate representations to a
latent representation space; ii) a multimodal contrastive loss function that
encourages the geometric alignment of the learned representations. We
experimentally demonstrate that GMC representations are semantically rich and
achieve state-of-the-art performance with missing modality information on three
different learning problems including prediction and reinforcement learning
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022 Camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Guided Evolutionary Fuzzing for Finding Traffic
  Violations of Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06126v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06126v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Zhong, Gail Kaiser, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-driving cars and trucks, autonomous vehicles (AVs), should not be
accepted by regulatory bodies and the public until they have much higher
confidence in their safety and reliability -- which can most practically and
convincingly be achieved by testing. But existing testing methods are
inadequate for checking the end-to-end behaviors of AV controllers against
complex, real-world corner cases involving interactions with multiple
independent agents such as pedestrians and human-driven vehicles. While
test-driving AVs on streets and highways fails to capture many rare events,
existing simulation-based testing methods mainly focus on simple scenarios and
do not scale well for complex driving situations that require sophisticated
awareness of the surroundings. To address these limitations, we propose a new
fuzz testing technique, called AutoFuzz, which can leverage widely-used AV
simulators' API grammars to generate semantically and temporally valid complex
driving scenarios (sequences of scenes). To efficiently search for traffic
violations-inducing scenarios in a large search space, we propose a constrained
neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation
of our prototype on one state-of-the-art learning-based controller, two
rule-based controllers, and one industrial-grade controller in five scenarios
shows that AutoFuzz efficiently finds hundreds of traffic violations in
high-fidelity simulation environments. For each scenario, AutoFuzz can find on
average 10-39% more unique traffic violations than the best-performing baseline
method. Further, fine-tuning the learning-based controller with the traffic
violations found by AutoFuzz successfully reduced the traffic violations found
in the new version of the AV controller software.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic encoding of protected characteristics in image-based models
  for disease detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14755v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14755v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Glocker, Charles Jones, Melanie Bernhardt, Stefan Winzeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been rightfully emphasized that the use of AI for clinical decision
making could amplify health disparities. An algorithm may encode protected
characteristics, and then use this information for making predictions due to
undesirable correlations in the (historical) training data. It remains unclear
how we can establish whether such information is actually used. Besides the
scarcity of data from underserved populations, very little is known about how
dataset biases manifest in predictive models and how this may result in
disparate performance. This article aims to shed some light on these issues by
exploring new methodology for subgroup analysis in image-based disease
detection models. We utilize two publicly available chest X-ray datasets,
CheXpert and MIMIC-CXR, to study performance disparities across race and
biological sex in deep learning models. We explore test set resampling,
transfer learning, multitask learning, and model inspection to assess the
relationship between the encoding of protected characteristics and disease
detection performance across subgroups. We confirm subgroup disparities in
terms of shifted true and false positive rates which are partially removed
after correcting for population and prevalence shifts in the test sets. We
further find a previously used transfer learning method to be insufficient for
establishing whether specific patient information is used for making
predictions. The proposed combination of test-set resampling, multitask
learning, and model inspection reveals valuable new insights about the way
protected characteristics are encoded in the feature representations of deep
neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available on https://github.com/biomedia-mira/chexploration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attack Agnostic <span class="highlight-title">Dataset</span>: Towards Generalization and Stabilization of
  Audio DeepFake Detection <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Kawa, Marcin Plata, Piotr Syga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio DeepFakes allow the creation of high-quality, convincing utterances and
therefore pose a threat due to its potential applications such as impersonation
or fake news. Methods for detecting these manipulations should be characterized
by good generalization and stability leading to robustness against attacks
conducted with techniques that are not explicitly included in the training. In
this work, we introduce Attack Agnostic Dataset - a combination of two audio
DeepFakes and one anti-spoofing datasets that, thanks to the disjoint use of
attacks, can lead to better generalization of detection methods. We present a
thorough analysis of current DeepFake detection methods and consider different
audio features (front-ends). In addition, we propose a model based on LCNN with
LFCC and mel-spectrogram front-end, which not only is characterized by a good
generalization and stability results but also shows improvement over LFCC-based
mode - we decrease standard deviation on all folds and EER in two folds by up
to 5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of INTERSPEECH 2022 (Updated version: corrected ASVspoof
  dataset description)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Networks for Labeled Acceleration Data
  Augmentation for Structural Damage Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03478v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03478v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Luleci, F. Necati Catbas, Onur Avci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a major advance in the field of Data Science in the last few
decades, and these have been utilized for different engineering disciplines and
applications. Artificial Intelligence (AI), Machine Learning (ML) and Deep
Learning (DL) algorithms have been utilized for civil Structural Health
Monitoring (SHM) especially for damage detection applications using sensor
data. Although ML and DL methods show superior learning skills for complex data
structures, they require plenty of data for training. However, in SHM, data
collection from civil structures can be expensive and time taking; particularly
getting useful data (damage associated data) can be challenging. The objective
of this study is to address the data scarcity problem for damage detection
applications. This paper employs 1-D Wasserstein Deep Convolutional Generative
Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) for synthetic
labelled acceleration data generation. Then, the generated data is augmented
with varying ratios for the training dataset of a 1-D Deep Convolutional Neural
Network (1-D DCNN) for damage detection application. The damage detection
results show that the 1-D WDCGAN-GP can be successfully utilized to tackle data
scarcity in vibration-based damage detection applications of civil structures.
  Keywords: Structural Health Monitoring (SHM), Structural Damage Detection,
1-D Deep Convolutional Neural Networks (1-D DCNN), 1-D Generative Adversarial
Networks (1-D GAN), Wasserstein Generative Adversarial Networks with Gradient
Penalty (WGAN-GP)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for
  Robotic Bin Picking <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Rui Cao, Stephen James, Yichuan Li, Yun-Hui Liu, Pieter Abbeel, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an iterative self-training framework for
sim-to-real 6D object pose estimation to facilitate cost-effective robotic
grasping. Given a bin-picking scenario, we establish a photo-realistic
simulator to synthesize abundant virtual data, and use this to train an initial
pose estimation network. This network then takes the role of a teacher model,
which generates pose predictions for unlabeled real data. With these
predictions, we further design a comprehensive adaptive selection scheme to
distinguish reliable results, and leverage them as pseudo labels to update a
student model for pose estimation on real data. To continuously improve the
quality of pseudo labels, we iterate the above steps by taking the trained
student model as a new teacher and re-label real data using the refined teacher
model. We evaluate our method on a public benchmark and our newly-released
dataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.
Our method is also able to improve robotic bin-picking success by 19.54%,
demonstrating the potential of iterative sim-to-real solutions for robotic
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Prediction of Future Lesion Activity and Treatment Effect
  in Multiple Sclerosis from Baseline MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Durso-Finley, Jean-Pierre R. Falet, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision medicine for chronic diseases such as multiple sclerosis (MS)
involves choosing a treatment which best balances efficacy and side
effects/preferences for individual patients. Making this choice as early as
possible is important, as delays in finding an effective therapy can lead to
irreversible disability accrual. To this end, we present the first deep neural
network model for individualized treatment decisions from baseline magnetic
resonance imaging (MRI) (with clinical information if available) for MS
patients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)
lesion counts on follow-up MRI on multiple treatments and (b) estimates the
conditional average treatment effect (CATE), as defined by the predicted future
suppression of NE-T2 lesions, between different treatment options relative to
placebo. Our model is validated on a proprietary federated dataset of 1817
multi-sequence MRIs acquired from MS patients during four multi-centre
randomized clinical trials. Our framework achieves high average precision in
the binarized regression of future NE-T2 lesions on five different treatments,
identifies heterogeneous treatment effects, and provides a personalized
treatment recommendation that accounts for treatment-associated risk (e.g. side
effects, patient preference, administration difficulties).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MIDL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edi<span class="highlight-title">BERT</span>, a generative model for image editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.15264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.15264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in computer vision are pushing the limits of im-age manipulation,
with generative models sampling detailed images on various tasks. However, a
specialized model is often developed and trained for each specific task, even
though many image edition tasks share similarities. In denoising, inpainting,
or image compositing, one always aims at generating a realistic image from a
low-quality one. In this paper, we aim at making a step towards a unified
approach for image editing. To do so, we propose EdiBERT, a bi-directional
transformer trained in the discrete latent space built by a vector-quantized
auto-encoder. We argue that such a bidirectional model is suited for image
manipulation since any patch can be re-sampled conditionally to the whole
image. Using this unique and straightforward training objective, we show that
the resulting model matches state-of-the-art performances on a wide variety of
tasks: image denoising, image completion, and image composition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multigraph Topology Design for Cross-Silo Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh X. Nguyen, Tuong Do, Hien Nguyen, Vuong Pham, Toan Tran, Erman Tjiputra, Quang Tran, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-silo federated learning utilizes a few hundred reliable data silos with
high-speed access links to jointly train a model. While this approach becomes a
popular setting in federated learning, designing a robust topology to reduce
the training time is still an open problem. In this paper, we present a new
multigraph topology for cross-silo federated learning. We first construct the
multigraph using the overlay graph. We then parse this multigraph into
different simple graphs with isolated nodes. The existence of isolated nodes
allows us to perform model aggregation without waiting for other nodes, hence
reducing the training time. We further propose a new distributed learning
algorithm to use with our multigraph topology. The intensive experiments on
public datasets show that our proposed method significantly reduces the
training time compared with recent state-of-the-art topologies while ensuring
convergence and maintaining the model's accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFactorGNNs: Revisiting Factorisation-based Models from a
  Message-Passing Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and to generalise to unseen nodes in inductive settings. Our work
bridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new
architecture draws upon both modelling paradigms, which previously were largely
thought of as disjoint. Concretely, using a message-passing formalism, we show
how FMs can be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactorGNNs. Across a
multitude of well-established KGC benchmarks, our ReFactorGNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On learning parametric distributions from quantized samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.12019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.12019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Septimia Sarbu, Abdellatif Zaidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning parametric distributions from their
quantized samples in a network. Specifically, $n$ agents or sensors observe
independent samples of an unknown parametric distribution; and each of them
uses $k$ bits to describe its observed sample to a central processor whose goal
is to estimate the unknown distribution. First, we establish a generalization
of the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in
terms of Generalized Fisher information. Then, we develop minimax lower bounds
on the estimation error for two losses: general $L_p$-norms and the related
Wasserstein loss from optimal transport.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short version accepted for publication at the IEEE Information Theory
  Symposium (ISIT) 2021; this version contains the detailed proofs with some
  minor corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Dimensional $L_2$Boosting: Rate of Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1602.08927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1602.08927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Luo, Martin Spindler, Jannis Kück
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Boosting is one of the most significant developments in machine learning.
This paper studies the rate of convergence of $L_2$Boosting, which is tailored
for regression, in a high-dimensional setting. Moreover, we introduce so-called
\textquotedblleft post-Boosting\textquotedblright. This is a post-selection
estimator which applies ordinary least squares to the variables selected in the
first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal
Boosting\textquotedblright\ where after each step an orthogonal projection is
conducted. We show that both post-$L_2$Boosting and the orthogonal boosting
achieve the same rate of convergence as LASSO in a sparse, high-dimensional
setting. We show that the rate of convergence of the classical $L_2$Boosting
depends on the design matrix described by a sparse eigenvalue constant. To show
the latter results, we derive new approximation results for the pure greedy
algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also
introduce feasible rules for early stopping, which can be easily implemented
and used in applied work. Our results also allow a direct comparison between
LASSO and boosting which has been missing from the literature. Finally, we
present simulation studies and applications to illustrate the relevance of our
theoretical results and to provide insights into the practical aspects of
boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms
LASSO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,
  62J07, 41A25; secondary 49M15, 68Q32</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Solve Soft-Constrained Vehicle Routing Problems with
  Lagrangian Relaxation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyue Tang, Yangzhe Kong, Lemeng Pan, Choonmeng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle Routing Problems (VRPs) in real-world applications often come with
various constraints, therefore bring additional computational challenges to
exact solution methods or heuristic search approaches. The recent idea to learn
heuristic move patterns from sample data has become increasingly promising to
reduce solution developing costs. However, using learning-based approaches to
address more types of constrained VRP remains a challenge. The difficulty lies
in controlling for constraint violations while searching for optimal solutions.
To overcome this challenge, we propose a Reinforcement Learning based method to
solve soft-constrained VRPs by incorporating the Lagrangian relaxation
technique and using constrained policy optimization. We apply the method on
three common types of VRPs, the Travelling Salesman Problem with Time Windows
(TSPTW), the Capacitated VRP (CVRP) and the Capacitated VRP with Time Windows
(CVRPTW), to show the generalizability of the proposed method. After comparing
to existing RL-based methods and open-source heuristic solvers, we demonstrate
its competitive performance in finding solutions with a good balance in travel
distance, constraint violations and inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Encrypted Internet traffic classification using a supervised Spiking
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.09818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.09818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Rasteh, Florian Delpech, Carlos Aguilar-Melchor, Romain Zimmer, Saeed Bagheri Shouraki, Timothée Masquelier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet traffic recognition is an essential tool for access providers since
recognizing traffic categories related to different data packets transmitted on
a network help them define adapted priorities. That means, for instance, high
priority requirements for an audio conference and low ones for a file transfer,
to enhance user experience. As internet traffic becomes increasingly encrypted,
the mainstream classic traffic recognition technique, payload inspection, is
rendered ineffective. This paper uses machine learning techniques for encrypted
traffic classification, looking only at packet size and time of arrival.
Spiking neural networks (SNN), largely inspired by how biological neurons
operate, were used for two reasons. Firstly, they are able to recognize
time-related data packet features. Secondly, they can be implemented
efficiently on neuromorphic hardware with a low energy footprint. Here we used
a very simple feedforward SNN, with only one fully-connected hidden layer, and
trained in a supervised manner using the newly introduced method known as
Surrogate Gradient Learning. Surprisingly, such a simple SNN reached an
accuracy of 95.9% on ISCX datasets, outperforming previous approaches. Besides
better accuracy, there is also a very significant improvement on simplicity:
input size, number of neurons, trainable parameters are all reduced by one to
four orders of magnitude. Next, we analyzed the reasons for this good accuracy.
It turns out that, beyond spatial (i.e. packet size) features, the SNN also
exploits temporal ones, mostly the nearly synchronous (within a 200ms range)
arrival times of packets with certain sizes. Taken together, these results show
that SNNs are an excellent fit for encrypted internet traffic classification:
they can be more accurate than conventional artificial neural networks (ANN),
and they could be implemented efficiently on low power embedded systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures. Neurocomputing (2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational quantum algorithm for Gaussian discrete solitons and their
  boson sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.12379v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.12379v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Conti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of quantum information, highly nonlinear regimes, such as
those supporting solitons, are marginally investigated. We miss general methods
for quantum solitons, although they can act as entanglement generators or as
self-organized quantum processors. We develop a computational approach that
uses a neural network as a variational ansatz for quantum solitons in an array
of waveguides. By training the resulting phase-space quantum machine learning
model, we find different soliton solutions varying the number of particles and
interaction strength. We consider Gaussian states that enable measuring the
degree of entanglement and sampling the probability distribution of
many-particle events. We also determine the probability of generating particle
pairs and unveil that soliton bound states emit correlated pairs. These results
may have a role in boson sampling with nonlinear systems and in quantum
processors for entangled nonlinear waves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor changes. 21 figures and 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive Learning</span> with Complex Heterogeneity <span class="chip">KDD22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.09401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.09401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lecheng Zheng, Jinjun Xiong, Yada Zhu, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and are characterized with multiple
labels, thus exhibiting the co-existence of multiple types of heterogeneity.
Although state-of-the-art techniques are good at modeling complex heterogeneity
with sufficient label information, such label information can be quite
expensive to obtain in real applications. Recently, researchers pay great
attention to contrastive learning due to its prominent performance by utilizing
rich unlabeled data. However, existing work on contrastive learning is not able
to address the problem of false negative pairs, i.e., some `negative' pairs may
have similar representations if they have the same label. To overcome the
issues, in this paper, we propose a unified heterogeneous learning framework,
which combines both the weighted unsupervised contrastive loss and the weighted
supervised contrastive loss to model multiple types of heterogeneity. We first
provide a theoretical analysis showing that the vanilla contrastive learning
loss easily leads to the sub-optimal solution in the presence of false negative
pairs, whereas the proposed weighted loss could automatically adjust the weight
based on the similarity of the learned representations to mitigate this issue.
Experimental results on real-world data sets demonstrate the effectiveness and
the efficiency of the proposed framework modeling multiple types of
heterogeneity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning with Non-IID Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1806.00582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1806.00582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, Vikas Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning enables resource-constrained edge compute devices, such as
mobile phones and IoT devices, to learn a shared model for prediction, while
keeping the training data local. This decentralized approach to train models
provides privacy, security, regulatory and economic benefits. In this work, we
focus on the statistical challenge of federated learning when local data is
non-IID. We first show that the accuracy of federated learning reduces
significantly, by up to 55% for neural networks trained for highly skewed
non-IID data, where each client device trains only on a single class of data.
We further show that this accuracy reduction can be explained by the weight
divergence, which can be quantified by the earth mover's distance (EMD) between
the distribution over classes on each device and the population distribution.
As a solution, we propose a strategy to improve training on non-IID data by
creating a small subset of data which is globally shared between all the edge
devices. Experiments show that accuracy can be increased by 30% for the
CIFAR-10 dataset with only 5% globally shared data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Intra-Risk and Contagion Risk for Enterprise Bankruptcy
  Prediction Using Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Shaopeng Wei, Yu Guo, Qing Yang, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu, Gang Kou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the bankruptcy risk of small and medium-sized enterprises (SMEs)
is an important step for financial institutions when making decisions about
loans. Existing studies in both finance and AI research fields, however, tend
to only consider either the intra-risk or contagion risk of enterprises,
ignoring their interactions and combinatorial effects. This study for the first
time considers both types of risk and their joint effects in bankruptcy
prediction. Specifically, we first propose an enterprise intra-risk encoder
based on statistically significant enterprise risk indicators for its
intra-risk learning. Then, we propose an enterprise contagion risk encoder
based on enterprise relation information from an enterprise knowledge graph for
its contagion risk embedding. In particular, the contagion risk encoder
includes both the newly proposed Hyper-Graph Neural Networks and Heterogeneous
Graph Neural Networks, which can model contagion risk in two different aspects,
i.e. common risk factors based on hyperedges and direct diffusion risk from
neighbors, respectively. To evaluate the model, we collect real-world
multi-sources data on SMEs and build a novel benchmark dataset called SMEsD. We
provide open access to the dataset, which is expected to further promote
research on financial risk analysis. Experiments on SMEsD against twelve
state-of-the-art baselines demonstrate the effectiveness of the proposed model
for bankruptcy prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Storehouse: a Reinforcement Learning Environment for Optimizing
  Warehouse Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julen Cestero, Marco Quartulli, Alberto Maria Metelli, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warehouse Management Systems have been evolving and improving thanks to new
Data Intelligence techniques. However, many current optimizations have been
applied to specific cases or are in great need of manual interaction. Here is
where Reinforcement Learning techniques come into play, providing
automatization and adaptability to current optimization policies. In this
paper, we present Storehouse, a customizable environment that generalizes the
definition of warehouse simulations for Reinforcement Learning. We also
validate this environment against state-of-the-art reinforcement learning
algorithms and compare these results to human and random policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, accepted in WCCI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable concept learning for interpretable predictions using
  variational autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armeen Taeb, Nicolo Ruggeri, Carina Schnuck, Fanny Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In safety-critical applications, practitioners are reluctant to trust neural
networks when no interpretable explanations are available. Many attempts to
provide such explanations revolve around pixel-based attributions or use
previously known concepts. In this paper we aim to provide explanations by
provably identifying \emph{high-level, previously unknown ground-truth
concepts}. To this end, we propose a probabilistic modeling framework to derive
(C)oncept (L)earning and (P)rediction (CLAP) -- a VAE-based classifier that
uses visually interpretable concepts as predictors for a simple classifier.
Assuming a generative model for the ground-truth concepts, we prove that CLAP
is able to identify them while attaining optimal classification accuracy. Our
experiments on synthetic datasets verify that CLAP identifies distinct
ground-truth concepts on synthetic datasets and yields promising results on the
medical Chest X-Ray dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual-Learning-as-a-Service (CLaaS): On-Demand Efficient Adaptation
  of Predictive Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudy Semola, Vincenzo Lomonaco, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive machine learning models nowadays are often updated in a stateless
and expensive way. The two main future trends for companies that want to build
machine learning-based applications and systems are real-time inference and
continual updating. Unfortunately, both trends require a mature infrastructure
that is hard and costly to realize on-premise. This paper defines a novel
software service and model delivery infrastructure termed Continual
Learning-as-a-Service (CLaaS) to address these issues. Specifically, it
embraces continual machine learning and continuous integration techniques. It
provides support for model updating and validation tools for data scientists
without an on-premise solution and in an efficient, stateful and easy-to-use
manner. Finally, this CL model service is easy to encapsulate in any machine
learning infrastructure or cloud system. This paper presents the design and
implementation of a CLaaS instantiation, called LiquidBrain, evaluated in two
real-world scenarios. The former is a robotic object recognition setting using
the CORe50 dataset while the latter is a named category and attribute
prediction using the DeepFashion-C dataset in the fashion domain. Our
preliminary results suggest the usability and efficiency of the Continual
Learning model services and the effectiveness of the solution in addressing
real-world use-cases regardless of where the computation happens in the
continuum Edge-Cloud.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of Macromolecule Type Based on Sequences of Amino Acids
  Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1907.03532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1907.03532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarwar Khan, Faisal Ghaffar, Imad ali, qazi mazhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of amino acids and their sequence analysis plays a vital
role in life sciences and is a challenging task. This article uses and compares
state-of-the-art deep learning models like convolution neural networks (CNN),
long short-term memory (LSTM), and gated recurrent units (GRU) to solve
macromolecule classification problems using amino acids. These models have
efficient frameworks for solving a broad spectrum of complex learning problems
compared to traditional machine learning techniques. We use word embedding to
represent the amino acid sequences as vectors. The CNN extracts features from
amino acid sequences, which are treated as vectors, then fed to the models
mentioned above to train a robust classifier. Our results show that word2vec as
embedding combined with VGG-16 performs better than LSTM and GRU. The proposed
approach gets an error rate of 1.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Tangent Kernel Beyond the Infinite-Width Limit: Effects of Depth
  and Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariia Seleznova, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Tangent Kernel (NTK) is widely used to analyze overparametrized neural
networks due to the famous result by Jacot et al. (2018): in the infinite-width
limit, the NTK is deterministic and constant during training. However, this
result cannot explain the behavior of deep networks, since it generally does
not hold if depth and width tend to infinity simultaneously. In this paper, we
study the NTK of fully-connected ReLU networks with depth comparable to width.
We prove that the NTK properties depend significantly on the depth-to-width
ratio and the distribution of parameters at initialization. In fact, our
results indicate the importance of the three phases in the hyperparameter space
identified in Poole et al. (2016): ordered, chaotic and the edge of chaos
(EOC). We derive exact expressions for the NTK dispersion in the
infinite-depth-and-width limit in all three phases and conclude that the NTK
variability grows exponentially with depth at the EOC and in the chaotic phase
but not in the ordered phase. We also show that the NTK of deep networks may
stay constant during training only in the ordered phase and discuss how the
structure of the NTK matrix changes during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-term Spatio-temporal Forecasting via Dynamic Multiple-Graph
  Attention <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11008v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11008v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shao, Zhiling Jin, Shuo Wang, Yufan Kang, Xiao Xiao, Hamid Menouar, Zhaofeng Zhang, Junshan Zhang, Flora Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world ubiquitous applications, such as parking recommendations and
air pollution monitoring, benefit significantly from accurate long-term
spatio-temporal forecasting (LSTF). LSTF makes use of long-term dependency
between spatial and temporal domains, contextual information, and inherent
pattern in the data. Recent studies have revealed the potential of multi-graph
neural networks (MGNNs) to improve prediction performance. However, existing
MGNN methods cannot be directly applied to LSTF due to several issues: the low
level of generality, insufficient use of contextual information, and the
imbalanced graph fusion approach. To address these issues, we construct new
graph models to represent the contextual information of each node and the
long-term spatio-temporal data dependency structure. To fuse the information
across multiple graphs, we propose a new dynamic multi-graph fusion module to
characterize the correlations of nodes within a graph and the nodes across
graphs via the spatial attention and graph attention mechanisms. Furthermore,
we introduce a trainable weight tensor to indicate the importance of each node
in different graphs. Extensive experiments on two large-scale datasets
demonstrate that our proposed approaches significantly improve the performance
of existing graph neural network models in LSTF prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 31st International Joint Conference on Artificial
  Intelligence and the 25th European Conference on Artificial Intelligence
  (IJCAI-ECAI 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuralNEB -- Neural Networks can find Reaction Paths Fast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Schreiner, Arghya Bhowmik, Tejs Vegge, Ole Winther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum mechanical methods like Density Functional Theory (DFT) are used with
great success alongside efficient search algorithms for studying kinetics of
reactive systems. However, DFT is prohibitively expensive for large scale
exploration. Machine Learning (ML) models have turned out to be excellent
emulators of small molecule DFT calculations and could possibly replace DFT in
such tasks. For kinetics, success relies primarily on the models capability to
accurately predict the Potential Energy Surface (PES) around transition-states
and Minimal Energy Paths (MEPs). Previously this has not been possible due to
scarcity of relevant data in the literature. In this paper we train state of
the art equivariant Graph Neural Network (GNN)-based models on around 10.000
elementary reactions from the Transition1x dataset. We apply the models as
potentials for the Nudged Elastic Band (NEB) algorithm and achieve a Mean
Average Error (MAE) of 0.13+/-0.03 eV on barrier energies on unseen reactions.
We compare the results against equivalent models trained on QM9 and ANI1x. We
also compare with and outperform Density Functional based Tight Binding (DFTB)
on both accuracy and computational resource. The implication is that ML models,
given relevant data, are now at a level where they can be applied for
downstream tasks in quantum chemistry transcending prediction of simple
molecular features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APPTeK: Agent-Based Predicate Prediction in Temporal Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian M. M. Frey, Yunpu Ma, Matthias Schubert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In temporal Knowledge Graphs (tKGs), the temporal dimension is attached to
facts in a knowledge base resulting in quadruples between entities such as
(Nintendo, released, Super Mario, Sep-13-1985), where the predicate holds
within a time interval or at a timestamp. We propose a reinforcement learning
agent gathering temporal relevant information about the query entities'
neighborhoods, simultaneously. We refer to the encodings of the explored graph
structures as fingerprints which are used as input to a Q-network. Our agent
decides sequentially which relation type needs to be explored next to expand
the local subgraphs of the query entities. Our evaluation shows that the
proposed method yields competitive results compared to state-of-the-art
embedding algorithms for tKGs, and we additionally gain information about the
relevant structures between subjects and objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying partial mouse brain microscopy images from Allen reference
  atlas using a <span class="highlight-title">contrastive</span>ly learned semantic space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justinas Antanavicius, Roberto Leiras, Raghavendra Selvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise identification of mouse brain microscopy images is a crucial first
step when anatomical structures in the mouse brain are to be registered to a
reference atlas. Practitioners usually rely on manual comparison of images or
tools that assume the presence of complete images. This work explores Siamese
Networks as the method for finding corresponding 2D reference atlas plates for
given partial 2D mouse brain images. Siamese networks are a class of
convolutional neural networks (CNNs) that use weight-shared paths to obtain low
dimensional embeddings of pairs of input images. The correspondence between the
partial mouse brain image and reference atlas plate is determined based on the
distance between low dimensional embeddings of brain slices and atlas plates
that are obtained from Siamese networks using contrastive learning. Experiments
showed that Siamese CNNs can precisely identify brain slices using the Allen
mouse brain atlas when training and testing images come from the same source.
They achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking
only 7.2 seconds to identify 29 images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Proceedings of International Workshop on Biomedical
  Image Registration (WBIR-2022). Source code available at
  https://github.com/Justinas256/2d-mouse-brain-identification. 12 pages, 6
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> An Explanation of In-context Learning as Implicit Bayesian Inference <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02080v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02080v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang Michael Xie, Aditi Raghunathan, <span class="highlight-author">Percy Liang</span>, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is
unclear what enables in-context learning. In this paper, we study how
in-context learning can emerge when pretraining documents have long-range
coherence. Here, the LM must infer a latent document-level concept to generate
coherent next tokens during pretraining. At test time, in-context learning
occurs when the LM also infers a shared latent concept between examples in a
prompt. We prove when this occurs despite a distribution mismatch between
prompts and pretraining data in a setting where the pretraining distribution is
a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs
capable of in-context learning, we generate a small-scale synthetic dataset
(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond
the theory, experiments on GINC exhibit large-scale real-world phenomena
including improved in-context performance with model scaling (despite the same
pretraining loss), sensitivity to example order, and instances where zero-shot
is better than few-shot in-context learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.01955v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.01955v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement
learning algorithm but is significantly less utilized than off-policy learning
algorithms in multi-agent settings. This is often due to the belief that PPO is
significantly less sample efficient than off-policy methods in multi-agent
systems. In this work, we carefully study the performance of PPO in cooperative
multi-agent settings. We show that PPO-based multi-agent algorithms achieve
surprisingly strong performance in four popular multi-agent testbeds: the
particle-world environments, the StarCraft multi-agent challenge, the Hanabi
challenge, and Google Research Football, with minimal hyperparameter tuning and
without any domain-specific algorithmic modifications or architectures.
Importantly, compared to strong off-policy methods, PPO often achieves
competitive or superior results in both final rewards and sample efficiency.
Finally, through ablation studies, we analyze implementation and hyperparameter
factors that are critical to PPO's empirical performance, and give concrete
practical suggestions regarding these factors. Our results show that when using
these practices, simple PPO-based methods are a strong baseline in cooperative
multi-agent reinforcement learning. Source code is released at
https://github.com/marlbenchmark/on-policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TANDEM: Learning Joint Exploration and Decision Making with Tactile
  Sensors <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxi Xu, Shuran Song, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the human ability to perform complex manipulation in the complete
absence of vision (like retrieving an object from a pocket), the robotic
manipulation field is motivated to develop new methods for tactile-based object
interaction. However, tactile sensing presents the challenge of being an active
sensing modality: a touch sensor provides sparse, local data, and must be used
in conjunction with effective exploration strategies in order to collect
information. In this work, we focus on the process of guiding tactile
exploration, and its interplay with task-related decision making. We propose
TANDEM (TActile exploration aNd DEcision Making), an architecture to learn
efficient exploration strategies in conjunction with decision making. Our
approach is based on separate but co-trained modules for exploration and
discrimination. We demonstrate this method on a tactile object recognition
task, where a robot equipped with a touch sensor must explore and identify an
object from a known set based on binary contact signals alone. TANDEM achieves
higher accuracy with fewer actions than alternative methods and is also shown
to be more robust to sensor noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Robotics and Automation Letters (RA-L) and International
  Conference on Intelligent Robots and Systems (IROS) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViewFormer: NeRF-free Neural Rendering from Few Images Using
  <span class="highlight-title">Transformer</span>s <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonáš Kulhánek, Erik Derner, Torsten Sattler, Robert Babuška
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Field (NeRF), and while achieving impressive results, the
methods suffer from long training times as they require evaluating millions of
3D point samples via a neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning explicitly in 3D, and it is
faster to train.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Graph Neural Network with Multi-view Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Shao, Yongjun Xu, Wei Wei, Fei Wang, Zhao Zhang, Feida Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks for heterogeneous graph embedding is to project nodes
into a low-dimensional space by exploring the heterogeneity and semantics of
the heterogeneous graph. However, on the one hand, most of existing
heterogeneous graph embedding methods either insufficiently model the local
structure under specific semantic, or neglect the heterogeneity when
aggregating information from it. On the other hand, representations from
multiple semantics are not comprehensively integrated to obtain versatile node
embeddings. To address the problem, we propose a Heterogeneous Graph Neural
Network with Multi-View Representation Learning (named MV-HetGNN) for
heterogeneous graph embedding by introducing the idea of multi-view
representation learning. The proposed model consists of node feature
transformation, view-specific ego graph encoding and auto multi-view fusion to
thoroughly learn complex structural and semantic information for generating
comprehensive node representations. Extensive experiments on three real-world
heterogeneous graph datasets show that the proposed MV-HetGNN model
consistently outperforms all the state-of-the-art GNN baselines in various
downstream tasks, e.g., node classification, node clustering, and link
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Denoised MDPs: Learning World Models Better Than the World Itself 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzhou Wang, Simon S. Du, Antonio Torralba, <span class="highlight-author">Phillip Isola</span>, Amy Zhang, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to separate signal from noise, and reason with clean
abstractions, is critical to intelligence. With this ability, humans can
efficiently perform real world tasks without considering all possible nuisance
factors.How can artificial agents do the same? What kind of information can
agents safely discard as noises?
  In this work, we categorize information out in the wild into four types based
on controllability and relation with reward, and formulate useful information
as that which is both controllable and reward-relevant. This framework
clarifies the kinds information removed by various prior work on representation
learning in reinforcement learning (RL), and leads to our proposed approach of
learning a Denoised MDP that explicitly factors out certain noise distractors.
Extensive experiments on variants of DeepMind Control Suite and RoboDesk
demonstrate superior performance of our denoised world model over using raw
observations alone, and over prior works, across policy optimization control
tasks as well as the non-control task of joint position regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ssnl.github.io/denoised_mdp/ Code:
  https://github.com/facebookresearch/denoised_mdp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Every Preference Changes Differently: Neural Multi-Interest Preference
  Model with Temporal Dynamics for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shi, Yupeng Gu, Yitong Zhou, Bo Zhao, Sicun Gao, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings (vectorized representations of a user) are essential in
recommendation systems. Numerous approaches have been proposed to construct a
representation for the user in order to find similar items for retrieval tasks,
and they have been proven effective in industrial recommendation systems as
well. Recently people have discovered the power of using multiple embeddings to
represent a user, with the hope that each embedding represents the user's
interest in a certain topic. With multi-interest representation, it's important
to model the user's preference over the different topics and how the preference
change with time. However, existing approaches either fail to estimate the
user's affinity to each interest or unreasonably assume every interest of every
user fades with an equal rate with time, thus hurting the recall of candidate
retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,
an approach that not only produces multi-interest for users by using the user's
sequential engagement more effectively but also automatically learns a set of
weights to represent the preference over each embedding so that the candidates
can be retrieved from each interest proportionally. Extensive experiments have
been done on various industrial-scale datasets to demonstrate the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping a User-Centered <span class="highlight-title">Task-Oriented</span> <span class="highlight-title">Dialogue</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Lingbo Mo, Samuel Stevens, Zhen Wang, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TacoBot, a task-oriented dialogue system built for the inaugural
Alexa Prize TaskBot Challenge, which assists users in completing multi-step
cooking and home improvement tasks. TacoBot is designed with a user-centered
principle and aspires to deliver a collaborative and accessible dialogue
experience. Towards that end, it is equipped with accurate language
understanding, flexible dialogue management, and engaging response generation.
Furthermore, TacoBot is backed by a strong search engine and an automated
end-to-end test suite. In bootstrapping the development of TacoBot, we explore
a series of data augmentation strategies to train advanced neural language
processing models and continuously improve the dialogue experience with
collected real conversations. At the end of the semifinals, TacoBot achieved an
average rating of 3.55/5.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 1st Proceedings of Alexa Prize TaskBot (Alexa Prize
  2021). TacoBot won 3rd place in the challenge. See project website
  https://sunlab-osu.github.io/tacobot/ for details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RADAMS: Resilient and Adaptive Alert and Attention Management Strategy
  against Informational Denial-of-Service (IDoS) Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linan Huang, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attacks exploiting human attentional vulnerability have posed severe threats
to cybersecurity. In this work, we identify and formally define a new type of
proactive attentional attacks called Informational Denial-of-Service (IDoS)
attacks that generate a large volume of feint attacks to overload human
operators and hide real attacks among feints. We incorporate human factors
(e.g., levels of expertise, stress, and efficiency) and empirical psychological
results (e.g., the Yerkes-Dodson law and the sunk cost fallacy) to model the
operators' attention dynamics and their decision-making processes along with
the real-time alert monitoring and inspection. To assist human operators in
dismissing the feints and escalating the real attacks timely and accurately, we
develop a Resilient and Adaptive Data-driven alert and Attention Management
Strategy (RADAMS) that de-emphasizes alerts selectively based on the abstracted
category labels of the alerts. RADAMS uses reinforcement learning to achieve a
customized and transferable design for various human operators and evolving
IDoS attacks. The integrated modeling and theoretical analysis lead to the
Product Principle of Attention (PPoA), fundamental limits, and the tradeoff
among crucial human and economic factors. Experimental results corroborate that
the proposed strategy outperforms the default strategy and can reduce the IDoS
risk by as much as 20%. Besides, the strategy is resilient to large variations
of costs, attack frequencies, and human attention capacities. We have
recognized interesting phenomena such as attentional risk equivalency,
attacker's dilemma, and the half-truth optimal attack strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLOWGEN: Fast and slow graph generation <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Madaan, Yiming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLOWGEN, a graph-generation model inspired by the dual-process
theory of mind that generates large graphs incrementally. Depending on the
difficulty of completing the graph at the current step, graph generation is
routed to either a fast~(weaker) or a slow~(stronger) model. fast and slow
models have identical architectures, but vary in the number of parameters and
consequently the strength. Experiments on real-world graphs show that ours can
successfully generate graphs similar to those generated by a single large model
in a fraction of time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version to be presented at Dynn workshop @ ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis of Regularized Learning for Generalized Data in Banach Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03159v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03159v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we study the whole theory of regularized learning for
generalized data in Banach spaces including representer theorems, approximation
theorems, and convergence theorems. The generalized input data are composed of
linear functionals in the predual spaces of the Banach spaces to represent the
discrete local information of different engineering and physics models. The
generalized data and the multi-loss functions are used to compute the empirical
risks, and the regularized learning is to minimize the regularized empirical
risks over the Banach spaces. Even if the original problems are unknown or
unformulated, then the exact solutions of the original problems are
approximated globally by the regularized learning. In the proof of the
convergence theorems, the strong convergence condition is replaced to the weak
convergence condition with the additional checkable condition which is
independent of the original problems. The theorems of the regularized learning
can be used to solve many problems of machine learning such as support vector
machines and neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyu Ding, Jihan Yang, Li Jiang, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code
is available at https://github.com/CVMI-Lab/DODA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Robust Landmark-based Stent Tracking in X-ray Fluoroscopy <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luojie Huang, Yi<span class="highlight-author">kang Liu</span>, Li Chen, Eric Z. Chen, Xiao Chen, Shanhui Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient and Adaptive Granular-ball Generation Method in
  Classification Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyin Xia, Xiaochuan Dai, Guoyin Wang, Xinbo Gao, Elisabeth Giem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Granular-ball computing is an efficient, robust, and scalable learning method
for granular computing. The basis of granular-ball computing is the
granular-ball generation method. This paper proposes a method for accelerating
the granular-ball generation using the division to replace $k$-means. It can
greatly improve the efficiency of granular-ball generation while ensuring the
accuracy similar to the existing method. Besides, a new adaptive method for the
granular-ball generation is proposed by considering granular-ball's overlap
eliminating and some other factors. This makes the granular-ball generation
process of parameter-free and completely adaptive in the true sense. In
addition, this paper first provides the mathematical models for the
granular-ball covering. The experimental results on some real data sets
demonstrate that the proposed two granular-ball generation methods have similar
accuracies with the existing method while adaptiveness or acceleration is
realized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Does <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing Perform with Streaming Data? <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.12081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.12081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dapeng Hu, Shipeng Yan, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang, Zhenguo Li, Xinchao Wang, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior works on self-supervised pre-training focus on the joint training
scenario, where massive unlabeled data are assumed to be given as input all at
once, and only then is a learner trained. Unfortunately, such a problem setting
is often impractical if not infeasible since many real-world tasks rely on
sequential learning, e.g., data are decentralized or collected in a streaming
fashion. In this paper, we conduct the first thorough and dedicated
investigation on self-supervised pre-training with streaming data, aiming to
shed light on the model behavior under this overlooked setup. Specifically, we
pre-train over 500 models on four categories of pre-training streaming data
from ImageNet and DomainNet and evaluate them on three types of downstream
tasks and 12 different downstream datasets. Our studies show that, somehow
beyond our expectation, with simple data replay or parameter regularization,
sequential self-supervised pre-training turns out to be an efficient
alternative for joint pre-training, as the performances of the former are
mostly on par with those of the latter. Moreover, catastrophic forgetting, a
common issue in sequential supervised learning, is much alleviated in
sequential self-supervised learning (SSL), which is well justified through our
comprehensive empirical analysis on representations and the sharpness of minima
in the loss landscape. Our findings, therefore, suggest that, in practice, for
SSL, the cumbersome joint training can be replaced mainly by sequential
learning, which in turn enables a much broader spectrum of potential
application scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution Approximation and Statistical Estimation Guarantees of
  Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.03938v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.03938v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minshuo Chen, Wenjing Liao, Hongyuan Zha, Tuo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have achieved a great success in
unsupervised learning. Despite its remarkable empirical performance, there are
limited theoretical studies on the statistical properties of GANs. This paper
provides approximation and statistical guarantees of GANs for the estimation of
data distributions that have densities in a H\"{o}lder space. Our main result
shows that, if the generator and discriminator network architectures are
properly chosen, GANs are consistent estimators of data distributions under
strong discrepancy metrics, such as the Wasserstein-1 distance. Furthermore,
when the data distribution exhibits low-dimensional structures, we show that
GANs are capable of capturing the unknown low-dimensional structures in data
and enjoy a fast statistical convergence, which is free of curse of the ambient
dimensionality. Our analysis for low-dimensional data builds upon a universal
approximation theory of neural networks with Lipschitz continuity guarantees,
which may be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version two extends to low-dimensional linear and mixture
  distributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FELARE: Fair Scheduling of Machine Learning Tasks on Heterogeneous Edge
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Mokhtari, Md Abir Hossen, Pooyan Jamshidi, Mohsen Amini Salehi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing enables smart IoT-based systems via concurrent and continuous
execution of latency-sensitive machine learning (ML) applications. These
edge-based machine learning systems are often battery-powered (i.e.,
energy-limited). They use heterogeneous resources with diverse computing
performance (e.g., CPU, GPU, and/or FPGAs) to fulfill the latency constraints
of ML applications. The challenge is to allocate user requests for different ML
applications on the Heterogeneous Edge Computing Systems (HEC) with respect to
both the energy and latency constraints of these systems. To this end, we study
and analyze resource allocation solutions that can increase the on-time task
completion rate while considering the energy constraint. Importantly, we
investigate edge-friendly (lightweight) multi-objective mapping heuristics that
do not become biased toward a particular application type to achieve the
objectives; instead, the heuristics consider "fairness" across the concurrent
ML applications in their mapping decisions. Performance evaluations demonstrate
that the proposed heuristic outperforms widely-used heuristics in heterogeneous
systems in terms of the latency and energy objectives, particularly, at low to
moderate request arrival rates. We observed 8.9% improvement in on-time task
completion rate and 12.6% in energy-saving without imposing any significant
overhead on the edge system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomal-E: A <span class="highlight-title">Self-Supervised</span> Network Intrusion Detection System based on
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Caville, Wai Weng Lo, Siamak Layeghy, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Graph Neural Networks (GNNs) application for
self-supervised network intrusion and anomaly detection. GNNs are a deep
learning approach for graph-based data that incorporate graph structures into
learning to generalise graph representations and output embeddings. As network
flows are naturally graph-based, GNNs are a suitable fit for analysing and
learning network behaviour. The majority of current implementations of
GNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled
network traffic which can not only restrict the amount and structure of input
traffic, but also the NIDSs potential to adapt to unseen attacks. To overcome
these restrictions, we present Anomal-E, a GNN approach to intrusion and
anomaly detection that leverages edge features and graph topological structure
in a self-supervised process. This approach is, to the best our knowledge, the
first successful and practical approach to network intrusion detection that
utilises network flows in a self-supervised, edge leveraging GNN. Experimental
results on two modern benchmark NIDS datasets not only clearly display the
improvement of using Anomal-E embeddings rather than raw features, but also the
potential Anomal-E has for detection on wild network traffic.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unimodal vs. <span class="highlight-title">Multimodal</span> Siamese Networks for Outfit Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Hendriksen, Viggo Overes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of online fashion shopping continues to grow. The ability to
offer an effective recommendation to customers is becoming increasingly
important. In this work, we focus on Fashion Outfits Challenge, part of SIGIR
2022 Workshop on eCommerce. The challenge is centered around Fill in the Blank
(FITB) task that implies predicting the missing outfit, given an incomplete
outfit and a list of candidates. In this paper, we focus on applying siamese
networks on the task. More specifically, we explore how combining information
from multiple modalities (textual and visual modality) impacts the performance
of the model on the task. We evaluate our model on the test split provided by
the challenge organizers and the test split with gold assignments that we
created during the development phase. We discover that using both visual, and
visual and textual data demonstrates promising results on the task. We conclude
by suggesting directions for further improvement of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-enhanced Black-box Attacks for Recommendations <span class="chip">KDD'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing Li, Yihua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that deep neural networks-based recommender systems
are vulnerable to adversarial attacks, where attackers can inject carefully
crafted fake user profiles (i.e., a set of items that fake users have
interacted with) into a target recommender system to achieve malicious
purposes, such as promote or demote a set of target items. Due to the security
and privacy concerns, it is more practical to perform adversarial attacks under
the black-box setting, where the architecture/parameters and training data of
target systems cannot be easily accessed by attackers. However, generating
high-quality fake user profiles under black-box setting is rather challenging
with limited resources to target systems. To address this challenge, in this
work, we introduce a novel strategy by leveraging items' attribute information
(i.e., items' knowledge graph), which can be publicly accessible and provide
rich auxiliary knowledge to enhance the generation of fake user profiles. More
specifically, we propose a knowledge graph-enhanced black-box attacking
framework (KGAttack) to effectively learn attacking policies through deep
reinforcement learning techniques, in which knowledge graph is seamlessly
integrated into hierarchical policy networks to generate fake user profiles for
performing adversarial black-box attacks. Comprehensive experiments on various
real-world datasets demonstrate the effectiveness of the proposed attacking
framework under the black-box setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the KDD'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Graph Learning for Occasional Group Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02274v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02274v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Hao, Hongzhi Yin, Cuiping Li, Hong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an important branch in Recommender System, occasional group recommendation
has received more and more attention. In this scenario, each occasional group
(cold-start group) has no or few historical interacted items. As each
occasional group has extremely sparse interactions with items, traditional
group recommendation methods can not learn high-quality group representations.
The recent proposed Graph Neural Networks (GNNs), which incorporate the
high-order neighbors of the target occasional group, can alleviate the above
problem in some extent. However, these GNNs still can not explicitly strengthen
the embedding quality of the high-order neighbors with few interactions.
Motivated by the Self-supervised Learning technique, which is able to find the
correlations within the data itself, we propose a self-supervised graph
learning framework, which takes the user/item/group embedding reconstruction as
the pretext task to enhance the embeddings of the cold-start
users/items/groups. In order to explicitly enhance the high-order cold-start
neighbors' embedding quality, we further introduce an embedding enhancer, which
leverages the self-attention mechanism to improve the embedding quality for
them. Comprehensive experiments show the advantages of our proposed framework
than the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper uses self-supervised learning technique to enhance the
  embeddings of users/groups/items, the idea is novel in group recommendation
  scenario. However, some presentations need to be revised, so as to let the
  readers understand</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch
  Attribution <span class="chip">KDD 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.00689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.00689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Yao, Chang Gong, Lei Zhang, Sheng Chen, Jingping Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-touch attribution (MTA), aiming to estimate the contribution of each
advertisement touchpoint in conversion journeys, is essential for budget
allocation and automatically advertising. Existing methods first train a model
to predict the conversion probability of the advertisement journeys with
historical data and calculate the attribution of each touchpoint using
counterfactual predictions. An assumption of these works is the conversion
prediction model is unbiased, i.e., it can give accurate predictions on any
randomly assigned journey, including both the factual and counterfactual ones.
Nevertheless, this assumption does not always hold as the exposed
advertisements are recommended according to user preferences. This confounding
bias of users would lead to an out-of-distribution (OOD) problem in the
counterfactual prediction and cause concept drift in attribution. In this
paper, we define the causal MTA task and propose CausalMTA to eliminate the
influence of user preferences. It systemically eliminates the confounding bias
from both static and dynamic preferences to learn the conversion prediction
model using historical data. We also provide a theoretical analysis to prove
CausalMTA can learn an unbiased prediction model with sufficient data.
Extensive experiments on both public datasets and the impression data in an
e-commerce company show that CausalMTA not only achieves better prediction
performance than the state-of-the-art method but also generates meaningful
attribution credits across different advertising channels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures This paper has been accepted in KDD 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Every Preference Changes Differently: Neural Multi-Interest Preference
  Model with Temporal Dynamics for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shi, Yupeng Gu, Yitong Zhou, Bo Zhao, Sicun Gao, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings (vectorized representations of a user) are essential in
recommendation systems. Numerous approaches have been proposed to construct a
representation for the user in order to find similar items for retrieval tasks,
and they have been proven effective in industrial recommendation systems as
well. Recently people have discovered the power of using multiple embeddings to
represent a user, with the hope that each embedding represents the user's
interest in a certain topic. With multi-interest representation, it's important
to model the user's preference over the different topics and how the preference
change with time. However, existing approaches either fail to estimate the
user's affinity to each interest or unreasonably assume every interest of every
user fades with an equal rate with time, thus hurting the recall of candidate
retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,
an approach that not only produces multi-interest for users by using the user's
sequential engagement more effectively but also automatically learns a set of
weights to represent the preference over each embedding so that the candidates
can be retrieved from each interest proportionally. Extensive experiments have
been done on various industrial-scale datasets to demonstrate the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unimodal vs. <span class="highlight-title">Multimodal</span> Siamese Networks for Outfit Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Hendriksen, Viggo Overes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of online fashion shopping continues to grow. The ability to
offer an effective recommendation to customers is becoming increasingly
important. In this work, we focus on Fashion Outfits Challenge, part of SIGIR
2022 Workshop on eCommerce. The challenge is centered around Fill in the Blank
(FITB) task that implies predicting the missing outfit, given an incomplete
outfit and a list of candidates. In this paper, we focus on applying siamese
networks on the task. More specifically, we explore how combining information
from multiple modalities (textual and visual modality) impacts the performance
of the model on the task. We evaluate our model on the test split provided by
the challenge organizers and the test split with gold assignments that we
created during the development phase. We discover that using both visual, and
visual and textual data demonstrates promising results on the task. We conclude
by suggesting directions for further improvement of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud
  Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Zhi Liu, Bo Han, Pan Hui, Pengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud video transmission is challenging due to high encoding/decoding
complexity, high video bitrate, and low latency requirement. Consequently,
conventional adaptive streaming methodologies often find themselves
unsatisfactory to meet the requirements in threefold: 1) current algorithms
reuse existing quality of experience (QoE) definitions while overlooking the
unique features of point cloud video thus failing to provide optimal user
experience, 2) most deep learning approaches require long-span data collections
to learn sufficiently varied network conditions and result in long training
period and capacity occupation, 3) cloud training approaches pose privacy risks
caused by leakage of user reported service usage and networking conditions.
  To overcome the limitations, we present FRAS, the first federated
reinforcement learning framework, to the best of our knowledge, for adaptive
point cloud video streaming. We define a new QoE model which takes the unique
features of point cloud video into account. Each client uses reinforcement
learning (RL) to train encoding rate selection with the objective of optimizing
the user's QoE under multiple constraints. Then, a federated learning framework
is integrated with the RL algorithm to enhance training performance with
privacy preservation. Extensive simulations using real point cloud videos and
network traces reveal the superiority of the proposed scheme over baseline
schemes. We also implement a prototype that demonstrates the performance of
FRAS via real-world tests.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-20T00:00:00Z">2022-07-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFACTOR GNNS: Revisiting Factorisation-based Models from a
  Message-Passing Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and to generalise to unseen nodes in inductive settings. Our work
bridges the gap between FMs and GNNs by proposing REFACTOR GNNS. This new
architecture draws upon both modelling paradigms, which previously were largely
thought of as disjoint. Concretely, using a message-passing formalism, we show
how FMs can be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our REFACTOR GNNS. Across
a multitude of well-established KGC benchmarks, our REFACTOR GNNS achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Is TTS Augmentation Through a Pivot Language Useful? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Robinson, Perez Ogayo, Swetha Gangu, David R. Mortensen, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing Automatic Speech Recognition (ASR) for low-resource languages is a
challenge due to the small amount of transcribed audio data. For many such
languages, audio and text are available separately, but not audio with
transcriptions. Using text, speech can be synthetically produced via
text-to-speech (TTS) systems. However, many low-resource languages do not have
quality TTS systems either. We propose an alternative: produce synthetic audio
by running text from the target language through a trained TTS system for a
higher-resource pivot language. We investigate when and how this technique is
most effective in low-resource settings. In our experiments, using several
thousand synthetic TTS text-speech pairs and duplicating authentic data to
balance yields optimal results. Our findings suggest that searching over a set
of candidate pivot languages can lead to marginal improvements and that,
surprisingly, ASR performance can by harmed by increases in measured TTS
quality. Application of these findings improves ASR by 64.5\% and 45.0\%
character error reduction rate (CERR) respectively for two low-resource
languages: Guaran\'i and Suba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Auxiliary Text Query-modifier to Content-based Audio
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The amount of audio data available on public websites is growing rapidly, and
an efficient mechanism for accessing the desired data is necessary. We propose
a content-based audio retrieval method that can retrieve a target audio that is
similar to but slightly different from the query audio by introducing auxiliary
textual information which describes the difference between the query and target
audio. While the range of conventional content-based audio retrieval is limited
to audio that is similar to the query audio, the proposed method can adjust the
retrieval range by adding an embedding of the auxiliary text query-modifier to
the embedding of the query sample audio in a shared latent space. To evaluate
our method, we built a dataset comprising two different audio clips and the
text that describes the difference. The experimental results show that the
proposed method retrieves the paired audio more accurately than the baseline.
We also confirmed based on visualization that the proposed method obtains the
shared latent space in which the audio difference and the corresponding text
are represented as similar embedding vectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Data Driven Inverse Text Normalization using Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laxmi Pandey, Debjyoti Paul, Pooja Chitkara, Yutong Pang, Xuedong Zhang, Kjell Schubert, Mark Chou, Shu Liu, Yatharth Saraf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRIT: Faster and Better Image captioning <span class="highlight-title">Transformer</span> Using Dual Visual
  Features <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van-Quang Nguyen, Masanori Suganuma, Takayuki Okatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art methods for image captioning employ region-based
features, as they provide object-level information that is essential to
describe the content of images; they are usually extracted by an object
detector such as Faster R-CNN. However, they have several issues, such as lack
of contextual information, the risk of inaccurate detection, and the high
computational cost. The first two could be resolved by additionally using
grid-based features. However, how to extract and fuse these two types of
features is uncharted. This paper proposes a Transformer-only neural
architecture, dubbed GRIT (Grid- and Region-based Image captioning
Transformer), that effectively utilizes the two visual features to generate
better captions. GRIT replaces the CNN-based detector employed in previous
methods with a DETR-based one, making it computationally faster. Moreover, its
monolithic design consisting only of Transformers enables end-to-end training
of the model. This innovative design and the integration of the dual visual
features bring about significant performance improvement. The experimental
results on several image captioning benchmarks show that GRIT outperforms
previous methods in inference accuracy and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022; 14 pages with appendix; Code:
  https://github.com/davidnvq/grit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Linguistic Theory and Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models have recently achieved remarkable results
in many natural language tasks. However, performance on leaderboards is
generally achieved by leveraging massive amounts of training data, and rarely
by encoding explicit linguistic knowledge into neural models. This has led many
to question the relevance of linguistics for modern natural language
processing. In this dissertation, I present several case studies to illustrate
how theoretical linguistics and neural language models are still relevant to
each other. First, language models are useful to linguists by providing an
objective tool to measure semantic distance, which is difficult to do using
traditional methods. On the other hand, linguistic theory contributes to
language modelling research by providing frameworks and sources of data to
probe our language models for specific aspects of language understanding.
  This thesis contributes three studies that explore different aspects of the
syntax-semantics interface in language models. In the first part of my thesis,
I apply language models to the problem of word class flexibility. Using mBERT
as a source of semantic distance measurements, I present evidence in favour of
analyzing word class flexibility as a directional process. In the second part
of my thesis, I propose a method to measure surprisal at intermediate layers of
language models. My experiments show that sentences containing morphosyntactic
anomalies trigger surprisals earlier in language models than semantic and
commonsense anomalies. Finally, in the third part of my thesis, I adapt several
psycholinguistic studies to show that language models contain knowledge of
argument structure constructions. In summary, my thesis develops new
connections between natural language processing, linguistic theory, and
psycholinguistics to provide fresh perspectives for the interpretation of
language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD dissertation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doge Tickets: Uncovering Domain-general Language Models by Playing
  Lottery Tickets <span class="chip">NLPCC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yang, Chen Zhang, Benyou Wang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-parameterized models, typically pre-trained language models (LMs), have
shown an appealing expressive power due to their small learning bias. However,
the huge learning capacity of LMs can also lead to large learning variance. In
a pilot study, we find that, when faced with multiple domains, a critical
portion of parameters behave unexpectedly in a domain-specific manner while
others behave in a domain-general one. Motivated by this phenomenon, we for the
first time posit that domain-general parameters can underpin a domain-general
LM that can be derived from the original LM. To uncover the domain-general LM,
we propose to identify domain-general parameters by playing lottery tickets
(dubbed doge tickets). In order to intervene the lottery, we propose a
domain-general score, which depicts how domain-invariant a parameter is by
associating it with the variance. Comprehensive experiments are conducted on
the Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets
obtains an improved out-of-domain generalization in comparison with a range of
competitive baselines. Analysis results further hint the existence of
domain-general parameters and the performance consistency of doge tickets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NLPCC 2022. Code is available at
  https://github.com/Ylily1015/DogeTickets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D dense captioning is a recently-proposed novel task, where point clouds
contain more geometric information than the 2D counterpart. However, it is also
more challenging due to the higher complexity and wider variety of inter-object
relations contained in point clouds. Existing methods only treat such relations
as by-products of object feature learning in graphs without specifically
encoding them, which leads to sub-optimal results. In this paper, aiming at
improving 3D dense captioning via capturing and utilizing the complex relations
in the 3D scene, we propose MORE, a Multi-Order RElation mining model, to
support generating more descriptive and comprehensive captions. Technically,
our MORE encodes object relations in a progressive manner since complex
relations can be deduced from a limited number of basic ones. We first devise a
novel Spatial Layout Graph Convolution (SLGC), which semantically encodes
several first-order relations as edges of a graph constructed over 3D object
proposals. Next, from the resulting graph, we further extract multiple triplets
which encapsulate basic first-order relations as the basic unit, and construct
several Object-centric Triplet Attention Graphs (OTAG) to infer multi-order
relations for every target object. The updated node features from OTAG are
aggregated and fed into the caption decoder to provide abundant relational
cues, so that captions including diverse relations with context objects can be
generated. Extensive experiments on the Scan2Cap dataset prove the
effectiveness of our proposed MORE and its components, and we also outperform
the current state-of-the-art method. Our code is available at
https://github.com/SxJyJay/MORE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Speech Recognition for Speech Assessment of Persian Preschool
  Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12886v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12886v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Abaskohi, Fatemeh Mortazavi, Hadi Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition(ASR) system is useless since they are
pre-trained on voices that are different from children's voices in terms of
frequency and amplitude. We constructed an ASR for our cognitive test system to
solve this issue using the Wav2Vec 2.0 model with a new pre-training objective
called Random Frequency Pitch(RFP). In addition, we used our new dataset to
fine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)
tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian
section of the CommonVoice dataset. Furthermore, our novel methodology produces
positive outcomes in zero- and few-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELECTRA is a Zero-Shot Learner, Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwen Ni, Hung-Yu Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, for few-shot or even zero-shot learning, the new paradigm
"pre-train, prompt, and predict" has achieved remarkable achievements compared
with the "pre-train, fine-tune" paradigm. After the success of prompt-based
GPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)
prompt learning methods became popular and widely used. However, another
efficient pre-trained discriminative model, ELECTRA, has probably been
neglected. In this paper, we attempt to accomplish several NLP tasks in the
zero-shot scenario using a novel our proposed replaced token detection
(RTD)-based prompt learning method. Experimental results show that ELECTRA
model based on RTD-prompt learning achieves surprisingly state-of-the-art
zero-shot performance. Numerically, compared to MLM-RoBERTa-large and
MLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%
improvement on all 15 tasks. Especially on the SST-2 task, our
RTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training
data. Overall, compared to the pre-trained masked language models, the
pre-trained replaced token detection model performs better in zero-shot
learning. The source code is available at:
https://github.com/nishiwen1214/RTD-ELECTRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code is available at:
  https://github.com/nishiwen1214/RTD-ELECTRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming
  E2E ASR via Supernet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichuan Yang, Yuan Shangguan, Dilin Wang, Meng Li, Pierce Chuang, Xiaohui Zhang, Ganesh Venkatesh, Ozlem Kalinli, Vikas Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From wearables to powerful smart devices, modern automatic speech recognition
(ASR) models run on a variety of edge devices with different computational
budgets. To navigate the Pareto front of model accuracy vs model size,
researchers are trapped in a dilemma of optimizing model accuracy by training
and fine-tuning models for each individual edge device while keeping the
training GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,
where a single neural network can be pruned to generate optimized model for a
large range of model sizes. We develop training strategies for Omni-sparsity
DNN that allows it to find models along the Pareto front of word-error-rate
(WER) vs model size while keeping the training GPU-hours to no more than that
of training one singular model. We demonstrate the Omni-sparsity DNN with
streaming E2E ASR models. Our results show great saving on training time and
resources with similar or better accuracy on LibriSpeech compared to
individually pruned sparse models: 2%-6.6% better WER on Test-other.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transgender Community Sentiment Analysis from Social Media Data: A
  Natural Language Processing Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.13062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.13062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqiao Liu, Yudan Wang, Ying Zhao, Zhixiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transgender community is experiencing a huge disparity in mental health
conditions compared with the general population. Interpreting the social medial
data posted by transgender people may help us understand the sentiments of
these sexual minority groups better and apply early interventions. In this
study, we manually categorize 300 social media comments posted by transgender
people to the sentiment of negative, positive, and neutral. 5 machine learning
algorithms and 2 deep neural networks are adopted to build sentiment analysis
classifiers based on the annotated data. Results show that our annotations are
reliable with a high Cohen's Kappa score over 0.8 across all three classes.
LSTM model yields an optimal performance of accuracy over 0.85 and AUC of
0.876. Our next step will focus on using advanced natural language processing
algorithms on a larger annotated dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PiC: A Phrase-in-Context <span class="highlight-title">Dataset</span> for Phrase Understanding and Semantic
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thang M. Pham, Seunghyun Yoon, Trung Bui, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since BERT (Devlin et al., 2018), learning contextualized word embeddings has
been a de-facto standard in NLP. However, the progress of learning
contextualized phrase embeddings is hindered by the lack of a human-annotated,
phrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of
~28K of noun phrases accompanied by their contextual Wikipedia pages and a
suite of three tasks of increasing difficulty for evaluating the quality of
phrase embeddings. We find that training on our dataset improves ranking
models' accuracy and remarkably pushes Question Answering (QA) models to
near-human accuracy which is 95% Exact Match (EM) on semantic search given a
query phrase and a passage. Interestingly, we find evidence that such
impressive performance is because the QA models learn to better capture the
common meaning of a phrase regardless of its actual context. That is, on our
Phrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially
(60% EM), failing to differentiate between two different senses of the same
phrase under two different contexts. Further results on our 3-task PiC
benchmark reveal that learning contextualized phrase embeddings remains an
interesting, open challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Textual Adversarial Examples through Randomized Substitution
  and Vote <span class="chip">UAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Wang, Yifeng Xiong, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A line of work has shown that natural text processing models are vulnerable
to adversarial examples. Correspondingly, various defense methods are proposed
to mitigate the threat of textual adversarial examples, eg, adversarial
training, input transformations, detection, etc. In this work, we treat the
optimization process for synonym substitution based textual adversarial attacks
as a specific sequence of word replacement, in which each word mutually
influences other words. We identify that we could destroy such mutual
interaction and eliminate the adversarial perturbation by randomly substituting
a word with its synonyms. Based on this observation, we propose a novel textual
adversarial example detection method, termed Randomized Substitution and Vote
(RS&V), which votes the prediction label by accumulating the logits of k
samples generated by randomly substituting the words in the input text with
synonyms. The proposed RS&V is generally applicable to any existing neural
networks without modification on the architecture or extra training, and it is
orthogonal to prior work on making the classification network itself more
robust. Empirical evaluations on three benchmark datasets demonstrate that our
RS&V could detect the textual adversarial examples more successfully than the
existing detection methods while maintaining the high classification accuracy
on benign samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by UAI 2022, code is avaliable at
  https://github.com/JHL-HUST/RSV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Split for Automatic Bias Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Bao, Regina Barzilay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifiers are biased when trained on biased datasets. As a remedy, we
propose Learning to Split (ls), an algorithm for automatic bias detection.
Given a dataset with input-label pairs, ls learns to split this dataset so that
predictors trained on the training split cannot generalize to the testing
split. This performance gap suggests that the testing split is
under-represented in the dataset, which is a signal of potential bias.
Identifying non-generalizable splits is challenging since we have no
annotations about the bias. In this work, we show that the prediction
correctness of each example in the testing split can be used as a source of
weak supervision: generalization performance will drop if we move examples that
are predicted correctly away from the testing split, leaving only those that
are mis-predicted. ls is task-agnostic and can be applied to any supervised
learning problem, ranging from natural language understanding and image
classification to molecular property prediction. Empirical results show that ls
is able to generate astonishingly challenging splits that correlate with
human-identified biases. Moreover, we demonstrate that combining robust
learning algorithms (such as group DRO) with splits identified by ls enables
automatic de-biasing. Compared to previous state-of-the-art, we substantially
improve the worst-group performance (23.4% on average) when the source of
biases is unknown during training and validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nominal Metaphor Generation with Multitask Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Li, Chenghua Lin, Frank Geurin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphor generation is a challenging task which can impact many downstream
tasks such as improving user satisfaction with dialogue systems and story
generation. This paper tackles the problem of Chinese nominal metaphor
generation by introducing a multitask metaphor generation framework with
self-training and metaphor identification mechanisms. Self-training addresses
the data scarcity issue of metaphor datasets. That is, instead of solely
relying on labelled metaphor datasets which are usually small in size,
self-training helps identify potential metaphors from a large-scale unlabelled
corpus for metaphor generation. The metaphor weighting mechanism enables our
model to focus on the metaphor-related parts of the input (e.g., the comparison
of the metaphor and comparator) during model learning and thus improves the
metaphoricity of the generated metaphors. Our model is trained on an annotated
corpus consisting of 6.3k sentences that contain diverse metaphorical
expressions. Experimental results show that our model is able to generate
metaphors with better readability and creativity compared to the baseline
models, even in the situation where training data is insufficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INLG 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discover and Mitigate Unknown Biases with Debiasing Alternate Networks <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Li, Anthony Hoogs, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep image classifiers have been found to learn biases from datasets. To
mitigate the biases, most previous methods require labels of protected
attributes (e.g., age, skin tone) as full-supervision, which has two
limitations: 1) it is infeasible when the labels are unavailable; 2) they are
incapable of mitigating unknown biases -- biases that humans do not
preconceive. To resolve those problems, we propose Debiasing Alternate Networks
(DebiAN), which comprises two networks -- a Discoverer and a Classifier. By
training in an alternate manner, the discoverer tries to find multiple unknown
biases of the classifier without any annotations of biases, and the classifier
aims at unlearning the biases identified by the discoverer. While previous
works evaluate debiasing results in terms of a single bias, we create
Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in
a multi-bias setting, which not only reveals the problems in previous methods
but also demonstrates the advantage of DebiAN in identifying and mitigating
multiple biases simultaneously. We further conduct extensive experiments on
real-world datasets, showing that the discoverer in DebiAN can identify unknown
biases that may be hard to be found by humans. Regarding debiasing, DebiAN
achieves strong bias mitigation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Is an Object-Centric Video Representation Beneficial for Transfer? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Zhang, Ankush Gupta, <span class="highlight-author">Andrew Zisserman</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this work is to learn an object-centric video
representation, with the aim of improving transferability to novel tasks, i.e.,
tasks different from the pre-training task of action classification. To this
end, we introduce a new object-centric video recognition model based on a
transformer architecture. The model learns a set of object-centric summary
vectors for the video, and uses these vectors to fuse the visual and
spatio-temporal trajectory `modalities' of the video clip. We also introduce a
novel trajectory contrast loss to further enhance objectness in these summary
vectors. With experiments on four datasets -- SomethingSomething-V2,
SomethingElse, Action Genome and EpicKitchens -- we show that the
object-centric model outperforms prior video representations (both
object-agnostic and object-aware), when: (1) classifying actions on unseen
objects and unseen environments; (2) low-shot learning to novel classes; (3)
linear probe to other downstream tasks; as well as (4) for standard action
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Semantic uncertainty intervals for disentangled latent spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swami Sankaranarayanan, Anastasios N. Angelopoulos, Stephen Bates, Yaniv Romano, <span class="highlight-author">Phillip Isola</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meaningful uncertainty quantification in computer vision requires reasoning
about semantic information -- say, the hair color of the person in a photo or
the location of a car on the street. To this end, recent breakthroughs in
generative modeling allow us to represent semantic information in disentangled
latent spaces, but providing uncertainties on the semantic latent variables has
remained challenging. In this work, we provide principled uncertainty intervals
that are guaranteed to contain the true semantic factors for any underlying
generative model. The method does the following: (1) it uses quantile
regression to output a heuristic uncertainty interval for each element in the
latent space (2) calibrates these uncertainties such that they contain the true
value of the latent for a new, unseen input. The endpoints of these calibrated
intervals can then be propagated through the generator to produce interpretable
uncertainty visualizations for each semantic factor. This technique reliably
communicates semantically meaningful, principled, and instance-adaptive
uncertainty in inverse problems like image super-resolution and image
completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/swamiviv/generative_semantic_uncertainty</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular 3D Object Reconstruction with GAN Inversion <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Zhang, Daxuan Ren, Zhongang Cai, Chai Kiat Yeo, Bo Dai, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering a textured 3D mesh from a monocular image is highly challenging,
particularly for in-the-wild objects that lack 3D ground truths. In this work,
we present MeshInversion, a novel framework to improve the reconstruction by
exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh
synthesis. Reconstruction is achieved by searching for a latent space in the 3D
GAN that best resembles the target mesh in accordance with the single view
observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms
of mesh geometry and texture, searching within the GAN manifold thus naturally
regularizes the realness and fidelity of the reconstruction. Importantly, such
regularization is directly applied in the 3D space, providing crucial guidance
of mesh parts that are unobserved in the 2D space. Experiments on standard
benchmarks show that our framework obtains faithful 3D reconstructions with
consistent geometry and texture across both observed and unobserved parts.
Moreover, it generalizes well to meshes that are less commonly seen, such as
the extended articulation of deformable objects. Code is released at
https://github.com/junzhezhang/mesh-inversion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project page:
  https://www.mmlab-ntu.com/project/meshinversion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Clothed Human Reconstruction in the Wild <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongsik Moon, Hyeongjin Nam, Takaaki Shiratori, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although much progress has been made in 3D clothed human reconstruction, most
of the existing methods fail to produce robust results from in-the-wild images,
which contain diverse human poses and appearances. This is mainly due to the
large domain gap between training datasets and in-the-wild datasets. The
training datasets are usually synthetic ones, which contain rendered images
from GT 3D scans. However, such datasets contain simple human poses and less
natural image appearances compared to those of real in-the-wild datasets, which
makes generalization of it to in-the-wild images extremely challenging. To
resolve this issue, in this work, we propose ClothWild, a 3D clothed human
reconstruction framework that firstly addresses the robustness on in-thewild
images. First, for the robustness to the domain gap, we propose a weakly
supervised pipeline that is trainable with 2D supervision targets of
in-the-wild datasets. Second, we design a DensePose-based loss function to
reduce ambiguities of the weak supervision. Extensive empirical tests on
several public in-the-wild datasets demonstrate that our proposed ClothWild
produces much more accurate and robust results than the state-of-the-art
methods. The codes are available in here:
https://github.com/hygenie1228/ClothWild_RELEASE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022, 25 pages including the supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pretrain</span>ing a Neural Network before Knowing Its Architecture <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Knyazev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large neural networks is possible by training a smaller hypernetwork
that predicts parameters for the large ones. A recently released Graph
HyperNetwork (GHN) trained this way on one million smaller ImageNet
architectures is able to predict parameters for large unseen networks such as
ResNet-50. While networks with predicted parameters lose performance on the
source task, the predicted parameters have been found useful for fine-tuning on
other tasks. We study if fine-tuning based on the same GHN is still useful on
novel strong architectures that were published after the GHN had been trained.
We found that for recent architectures such as ConvNeXt, GHN initialization
becomes less useful than for ResNet-50. One potential reason is the increased
distribution shift of novel architectures from those used to train the GHN. We
also found that the predicted parameters lack the diversity necessary to
successfully fine-tune parameters with gradient descent. We alleviate this
limitation by applying simple post-processing techniques to predicted
parameters before fine-tuning them on a target task and improve fine-tuning of
ResNet-50 and ConvNeXt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2022 Workshop on Pre-training: Perspectives,
  Pitfalls, and Paths Forward, source code is available at
  https://github.com/facebookresearch/ppuda</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Densely Constrained Depth Estimator for Monocular 3D Object Detection <span class="chip">ECCV22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyan Li, Yunchao Chen, Jiawei He, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by ECCV22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A
  New Physics-Inspired <span class="highlight-title">Transformer</span> Model <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Mao, Ajay Jaiswal, Zhangyang Wang, Stanley H. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration algorithms for atmospheric turbulence are known to be much
more challenging to design than traditional ones such as blur or noise because
the distortion caused by the turbulence is an entanglement of spatially varying
blur, geometric distortion, and sensor noise. Existing CNN-based restoration
methods built upon convolutional kernels with static weights are insufficient
to handle the spatially dynamical atmospheric turbulence effect. To address
this problem, in this paper, we propose a physics-inspired transformer model
for imaging through atmospheric turbulence. The proposed network utilizes the
power of transformer blocks to jointly extract a dynamical turbulence
distortion map and restore a turbulence-free image. In addition, recognizing
the lack of a comprehensive dataset, we collect and present two new real-world
turbulence datasets that allow for evaluation with both classical objective
metrics (e.g., PSNR and SSIM) and a new task-driven metric using text
recognition accuracy. Both real testing sets and all related code will be made
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted as a poster at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Sparse 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lue Fan, Feng Wang, Naiyan Wang, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the perception range of LiDAR increases, LiDAR-based 3D object detection
becomes a dominant task in the long-range perception task of autonomous
driving. The mainstream 3D object detectors usually build dense feature maps in
the network backbone and prediction head. However, the computational and
spatial costs on the dense feature map are quadratic to the perception range,
which makes them hardly scale up to the long-range setting. To enable efficient
long-range LiDAR-based object detection, we build a fully sparse 3D object
detector (FSD). The computational and spatial cost of FSD is roughly linear to
the number of points and independent of the perception range. FSD is built upon
the general sparse voxel encoder and a novel sparse instance recognition (SIR)
module. SIR first groups the points into instances and then applies
instance-wise feature extraction and prediction. In this way, SIR resolves the
issue of center feature missing, which hinders the design of the fully sparse
architecture for all center-based or anchor-based detectors. Moreover, SIR
avoids the time-consuming neighbor queries in previous point-based methods by
grouping points into instances. We conduct extensive experiments on the
large-scale Waymo Open Dataset to reveal the working mechanism of FSD, and
state-of-the-art performance is reported. To demonstrate the superiority of FSD
in long-range detection, we also conduct experiments on Argoverse 2 Dataset,
which has a much larger perception range ($200m$) than Waymo Open Dataset
($75m$). On such a large perception range, FSD achieves state-of-the-art
performance and is 2.4$\times$ faster than the dense counterpart.Codes will be
released at https://github.com/TuSimple/SST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOTCOM: The Multi-Object Tracking <span class="highlight-title">Dataset</span> Complexity Metric <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Pedersen, Joakim Bruslund Haurum, Patrick Dendorfer, Thomas B. Moeslund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There exists no comprehensive metric for describing the complexity of
Multi-Object Tracking (MOT) sequences. This lack of metrics decreases
explainability, complicates comparison of datasets, and reduces the
conversation on tracker performance to a matter of leader board position. As a
remedy, we present the novel MOT dataset complexity metric (MOTCOM), which is a
combination of three sub-metrics inspired by key problems in MOT: occlusion,
erratic motion, and visual similarity. The insights of MOTCOM can open nuanced
discussions on tracker performance and may lead to a wider acknowledgement of
novel contributions developed for either less known datasets or those aimed at
solving sub-problems. We evaluate MOTCOM on the comprehensive MOT17, MOT20, and
MOTSynth datasets and show that MOTCOM is far better at describing the
complexity of MOT sequences compared to the conventional density and number of
tracks. Project page at https://vap.aau.dk/motcom
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project webpage https://vap.aau.dk/motcom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locality Guidance for Improving Vision <span class="highlight-title">Transformer</span>s on Tiny <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the Vision Transformer (VT) architecture is becoming trendy in computer
vision, pure VT models perform poorly on tiny datasets. To address this issue,
this paper proposes the locality guidance for improving the performance of VTs
on tiny datasets. We first analyze that the local information, which is of
great importance for understanding images, is hard to be learned with limited
data due to the high flexibility and intrinsic globality of the self-attention
mechanism in VTs. To facilitate local information, we realize the locality
guidance for VTs by imitating the features of an already trained convolutional
neural network (CNN), inspired by the built-in local-to-global hierarchy of
CNN. Under our dual-task learning paradigm, the locality guidance provided by a
lightweight CNN trained on low-resolution images is adequate to accelerate the
convergence and improve the performance of VTs to a large extent. Therefore,
our locality guidance approach is very simple and efficient, and can serve as a
basic performance enhancement method for VTs on tiny datasets. Extensive
experiments demonstrate that our method can significantly improve VTs when
training from scratch on tiny datasets and is compatible with different kinds
of VTs and datasets. For example, our proposed method can boost the performance
of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85%
for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing
the potential of VTs on tiny datasets. The code is available at
https://github.com/lkhl/tiny-transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Synthetic Data: Facial Expression Classification based on
  Ensemble of Multi-task Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-Yeop Jeong, Yeong-Gi Hong, JiYeon Oh, Sumin Hong, Jin-Woo Jeong, Yuchul Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression in-the-wild is essential for various interactive computing
domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic
in the facial expression recognition task. In this paper, we propose a
multi-task learning-based facial expression recognition approach which consists
of emotion and appearance learning branches that can share all face
information, and present preliminary results for the LSD challenge introduced
in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our
method achieved the mean F1 score of 0.71.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Difficulty-Aware Simulator for Open Set Recognition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WonJun Moon, Junho Park, Hyun Seok Seong, Cheol-Ho Cho, Jae-Pil Heo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open set recognition (OSR) assumes unknown instances appear out of the blue
at the inference time. The main challenge of OSR is that the response of models
for unknowns is totally unpredictable. Furthermore, the diversity of open set
makes it harder since instances have different difficulty levels. Therefore, we
present a novel framework, DIfficulty-Aware Simulator (DIAS), that generates
fakes with diverse difficulty levels to simulate the real world. We first
investigate fakes from generative adversarial network (GAN) in the classifier's
viewpoint and observe that these are not severely challenging. This leads us to
define the criteria for difficulty by regarding samples generated with GANs
having moderate-difficulty. To produce hard-difficulty examples, we introduce
Copycat, imitating the behavior of the classifier. Furthermore, moderate- and
easy-difficulty samples are also yielded by our modified GAN and Copycat,
respectively. As a result, DIAS outperforms state-of-the-art methods with both
metrics of AUROC and F-score. Our code is available at
https://github.com/wjun0830/Difficulty-Aware-Simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at
  github.com/wjun0830/Difficulty-Aware-Simulator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailoring <span class="highlight-title">Self-Supervision</span> for Supervised Learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WonJun Moon, Ji-Hwan Kim, Jae-Pil Heo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, it is shown that deploying a proper self-supervision is a
prospective way to enhance the performance of supervised learning. Yet, the
benefits of self-supervision are not fully exploited as previous pretext tasks
are specialized for unsupervised representation learning. To this end, we begin
by presenting three desirable properties for such auxiliary tasks to assist the
supervised objective. First, the tasks need to guide the model to learn rich
features. Second, the transformations involved in the self-supervision should
not significantly alter the training distribution. Third, the tasks are
preferred to be light and generic for high applicability to prior arts.
Subsequently, to show how existing pretext tasks can fulfill these and be
tailored for supervised learning, we propose a simple auxiliary
self-supervision task, predicting localizable rotation (LoRot). Our exhaustive
experiments validate the merits of LoRot as a pretext task tailored for
supervised learning in terms of robustness and generalization capability. Our
code is available at https://github.com/wjun0830/Localizable-Rotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at
  github.com/wjun0830/Localizable-Rotation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secrets of Event-Based Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shintaro Shiba, Yoshimitsu Aoki, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras respond to scene dynamics and offer advantages to estimate
motion. Following recent image-based deep-learning achievements, optical flow
estimation methods for event cameras have rushed to combine those image-based
methods with event data. However, it requires several adaptations (data
conversion, loss function, etc.) as they have very different properties. We
develop a principled method to extend the Contrast Maximization framework to
estimate optical flow from events alone. We investigate key elements: how to
design the objective function to prevent overfitting, how to warp events to
deal better with occlusions, and how to improve convergence with multi-scale
raw events. With these key elements, our method ranks first among unsupervised
methods on the MVSEC benchmark, and is competitive on the DSEC benchmark.
Moreover, our method allows us to expose the issues of the ground truth flow in
those benchmarks, and produces remarkable results when it is transferred to
unsupervised learning settings. Our code is available at
https://github.com/tub-rip/event_based_optical_flow
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 7 tables,
  https://github.com/tub-rip/event_based_optical_flow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Generative Domain Adaptation for Face Anti-Spoofing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyu Zhou, Ke-<span class="highlight-author">Yue Zhang</span>, Taiping Yao, Ran Yi, Kekai Sheng, Shouhong Ding, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing (FAS) approaches based on unsupervised domain adaption
(UDA) have drawn growing attention due to promising performances for target
scenarios. Most existing UDA FAS methods typically fit the trained models to
the target domain via aligning the distribution of semantic high-level
features. However, insufficient supervision of unlabeled target domains and
neglect of low-level feature alignment degrade the performances of existing
methods. To address these issues, we propose a novel perspective of UDA FAS
that directly fits the target data to the models, i.e., stylizes the target
data to the source-domain style via image translation, and further feeds the
stylized data into the well-trained source model for classification. The
proposed Generative Domain Adaptation (GDA) framework combines two carefully
designed consistency constraints: 1) Inter-domain neural statistic consistency
guides the generator in narrowing the inter-domain gap. 2) Dual-level semantic
consistency ensures the semantic quality of stylized images. Besides, we
propose intra-domain spectrum mixup to further expand target data distributions
to ensure generalization and reduce the intra-domain gap. Extensive experiments
and visualizations demonstrate the effectiveness of our method against the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyan Li, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimal solutions for relative rotation and translation estimation tasks have
been explored in different scenarios, typically relying on the so-called
co-visibility graph. However, how to build direct rotation relationships
between two frames without overlap is still an open topic, which, if solved,
could greatly improve the accuracy of visual odometry.
  In this paper, a new minimal solution is proposed to solve relative rotation
estimation between two images without overlapping areas by exploiting a new
graph structure, which we call Extensibility Graph (E-Graph). Differently from
a co-visibility graph, high-level landmarks, including vanishing directions and
plane normals, are stored in our E-Graph, which are geometrically extensible.
Based on E-Graph, the rotation estimation problem becomes simpler and more
elegant, as it can deal with pure rotational motion and requires fewer
assumptions, e.g. Manhattan/Atlanta World, planar/vertical motion. Finally, we
embed our rotation estimation strategy into a complete camera tracking and
mapping system which obtains 6-DoF camera poses and a dense 3D mesh model.
  Extensive experiments on public benchmarks demonstrate that the proposed
method achieves state-of-the-art tracking performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BYEL : Bootstrap on Your Emotion Latent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungjun Lee, Hwangyu Lim, Sejoon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the problem of dataset construction cost for training in deep
learning and the development of generative models, more and more researches are
being conducted to train with synthetic data and to inference using real data.
We propose emotion aware Self-Supervised Learning using ABAW's Learning
Synthetic Data (LSD) dataset. We pre-train our method to LSD dataset as a
self-supervised learning and then use the same LSD dataset to do downstream
training on the emotion classification task as a supervised learning. As a
result, a higher result(0.63) than baseline(0.5) was obtained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ABAW4th competition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming Shortcut Learning in a Target Domain by Generalizing Basic
  Visual Factors from a Source Domain <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyapat Saranrittichai, Chaithanya Kumar Mummadi, Claudia Blaiotta, Mauricio Munoz, Volker Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning occurs when a deep neural network overly relies on spurious
correlations in the training dataset in order to solve downstream tasks. Prior
works have shown how this impairs the compositional generalization capability
of deep learning models. To address this problem, we propose a novel approach
to mitigate shortcut learning in uncontrolled target domains. Our approach
extends the training set with an additional dataset (the source domain), which
is specifically designed to facilitate learning independent representations of
basic visual factors. We benchmark our idea on synthetic target domains where
we explicitly control shortcut opportunities as well as real-world target
domains. Furthermore, we analyze the effect of different specifications of the
source domain and the network architecture on compositional generalization. Our
main finding is that leveraging data from a source domain is an effective way
to mitigate shortcut learning. By promoting independence across different
factors of variation in the learned representations, networks can learn to
consider only predictive factors and ignore potential shortcut factors during
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at European Conference on Computer Vision
  (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lai, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation in semantic segmentation has been raised to
alleviate the reliance on expensive pixel-wise annotations. It leverages a
labeled source domain dataset as well as unlabeled target domain images to
learn a segmentation network. In this paper, we observe two main issues of the
existing domain-invariant learning framework. (1) Being distracted by the
feature distribution alignment, the network cannot focus on the segmentation
task. (2) Fitting source domain data well would compromise the target domain
performance. To address these issues, we propose DecoupleNet that alleviates
source domain overfitting and enables the final model to focus more on the
segmentation task. Furthermore, we put forward Self-Discrimination (SD) and
introduce an auxiliary classifier to learn more discriminative target domain
features with pseudo labels. Finally, we propose Online Enhanced Self-Training
(OEST) to contextually enhance the quality of pseudo labels in an online
manner. Experiments show our method outperforms existing state-of-the-art
methods, and extensive ablation studies verify the effectiveness of each
component. Code is available at https://github.com/dvlab-research/DecoupleNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at
  https://github.com/dvlab-research/DecoupleNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation
  on Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Sun, Daniel Rebain, Renjie Liao, Vladimir Tankovich, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for instance proposal generation for 3D point clouds.
Existing techniques typically directly regress proposals in a single
feed-forward step, leading to inaccurate estimation. We show that this serves
as a critical bottleneck, and propose a method based on iterative bilateral
filtering with learned kernels. Following the spirit of bilateral filtering, we
consider both the deep feature embeddings of each point, as well as their
locations in the 3D space. We show via synthetic experiments that our method
brings drastic improvements when generating instance proposals for a given
point of interest. We further validate our method on the challenging ScanNet
benchmark, achieving the best instance segmentation performance amongst the
sub-category of top-down methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://neuralbf.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal and <span class="highlight-title">cross-modal</span> attention for audio-visual zero-shot learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Otniel-Bogdan Mercea, Thomas Hummel, A. Sophia Koepke, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual generalised zero-shot learning for video classification requires
understanding the relations between the audio and visual information in order
to be able to recognise samples from novel, previously unseen classes at test
time. The natural semantic and temporal alignment between audio and visual data
in video data can be exploited to learn powerful representations that
generalise to unseen classes at test time. We propose a multi-modal and
Temporal Cross-attention Framework (\modelName) for audio-visual generalised
zero-shot learning. Its inputs are temporally aligned audio and visual features
that are obtained from pre-trained networks. Encouraging the framework to focus
on cross-modal correspondence across time instead of self-attention within the
modalities boosts the performance significantly. We show that our proposed
framework that ingests temporal features yields state-of-the-art performance on
the \ucf, \vgg, and \activity benchmarks for (generalised) zero-shot learning.
Code for reproducing all results is available at
\url{https://github.com/ExplainableML/TCAF-GZSL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2-Net: Multi-stages Specular Highlight Detection and Removal in
  Multi-scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyangfan Huang, Kun Hu, Xingjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel uniformity framework for highlight
detection and removal in multi-scenes, including synthetic images, face images,
natural images, and text images. The framework consists of three main
components, highlight feature extractor module, highlight coarse removal
module, and highlight refine removal module. Firstly, the highlight feature
extractor module can directly separate the highlight feature and non-highlight
feature from the original highlight image. Then highlight removal image is
obtained using a coarse highlight removal network. To further improve the
highlight removal effect, the refined highlight removal image is finally
obtained using refine highlight removal module based on contextual highlight
attention mechanisms. Extensive experimental results in multiple scenes
indicate that the proposed framework can obtain excellent visual effects of
highlight removal and achieve state-of-the-art results in several quantitative
evaluation metrics. Our algorithm is applied for the first time in video
highlight removal with promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Few-Shot Class-Incremental Learning with Open-Set Hypothesis
  in Hyperbolic Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yawen Cui, Zitong Yu, Wei Peng, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Class-Incremental Learning (FSCIL) aims at incrementally learning
novel classes from a few labeled samples by avoiding the overfitting and
catastrophic forgetting simultaneously. The current protocol of FSCIL is built
by mimicking the general class-incremental learning setting, while it is not
totally appropriate due to the different data configuration, i.e., novel
classes are all in the limited data regime. In this paper, we rethink the
configuration of FSCIL with the open-set hypothesis by reserving the
possibility in the first session for incoming categories. To assign better
performances on both close-set and open-set recognition to the model,
Hyperbolic Reciprocal Point Learning module (Hyper-RPL) is built on Reciprocal
Point Learning (RPL) with hyperbolic neural networks. Besides, for learning
novel categories from limited labeled data, we incorporate a hyperbolic metric
learning (Hyper-Metric) module into the distillation-based framework to
alleviate the overfitting issue and better handle the trade-off issue between
the preservation of old knowledge and the acquisition of new knowledge. The
comprehensive assessments of the proposed configuration and modules on three
benchmark datasets are executed to validate the effectiveness concerning three
evaluation indicators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Transactions on Cybernetics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Model Performance under Domain Shifts with Class-Specific
  Confidence Scores <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Li, Konstantinos Kamnitsas, Mobarakol Islam, Chen Chen, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are typically deployed in a test setting that differs
from the training setting, potentially leading to decreased model performance
because of domain shift. If we could estimate the performance that a
pre-trained model would achieve on data from a specific deployment setting, for
example a certain clinic, we could judge whether the model could safely be
deployed or if its performance degrades unacceptably on the specific data.
Existing approaches estimate this based on the confidence of predictions made
on unlabeled test data from the deployment's domain. We find existing methods
struggle with data that present class imbalance, because the methods used to
calibrate confidence do not account for bias induced by class imbalance,
consequently failing to estimate class-wise accuracy. Here, we introduce
class-wise calibration within the framework of performance estimation for
imbalanced datasets. Specifically, we derive class-specific modifications of
state-of-the-art confidence-based model evaluation methods including
temperature scaling (TS), difference of confidences (DoC), and average
thresholded confidence (ATC). We also extend the methods to estimate Dice
similarity coefficient (DSC) in image segmentation. We conduct experiments on
four tasks and find the proposed modifications consistently improve the
estimation accuracy for imbalanced datasets. Our methods improve accuracy
estimation by 18\% in classification under natural domain shifts, and double
the estimation accuracy on segmentation tasks, when compared with prior
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Telepresence Video Quality Assessment <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqiang Ying, Deepti Ghadiyaram, Alan Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video conferencing, which includes both video and audio content, has
contributed to dramatic increases in Internet traffic, as the COVID-19 pandemic
forced millions of people to work and learn from home. Global Internet traffic
of video conferencing has dramatically increased Because of this, efficient and
accurate video quality tools are needed to monitor and perceptually optimize
telepresence traffic streamed via Zoom, Webex, Meet, etc. However, existing
models are limited in their prediction capabilities on multi-modal, live
streaming telepresence content. Here we address the significant challenges of
Telepresence Video Quality Assessment (TVQA) in several ways. First, we
mitigated the dearth of subjectively labeled data by collecting ~2k
telepresence videos from different countries, on which we crowdsourced ~80k
subjective quality labels. Using this new resource, we created a
first-of-a-kind online video quality prediction framework for live streaming,
using a multi-modal learning framework with separate pathways to compute visual
and audio quality predictions. Our all-in-one model is able to provide accurate
quality predictions at the patch, frame, clip, and audiovisual levels. Our
model achieves state-of-the-art performance on both existing quality databases
and our new TVQA database, at a considerably lower computational expense,
making it an attractive solution for mobile and embedded systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Pedestrian Group Representations for <span class="highlight-title">Multi-modal</span> Trajectory
  Prediction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Jin-Hwi Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling the dynamics of people walking is a problem of long-standing
interest in computer vision. Many previous works involving pedestrian
trajectory prediction define a particular set of individual actions to
implicitly model group actions. In this paper, we present a novel architecture
named GP-Graph which has collective group representations for effective
pedestrian trajectory prediction in crowded environments, and is compatible
with all types of existing approaches. A key idea of GP-Graph is to model both
individual-wise and group-wise relations as graph representations. To do this,
GP-Graph first learns to assign each pedestrian into the most likely behavior
group. Using this assignment information, GP-Graph then forms both intra- and
inter-group interactions as graphs, accounting for human-human relations within
a group and group-group relations, respectively. To be specific, for the
intra-group interaction, we mask pedestrian graph edges out of an associated
group. We also propose group pooling&unpooling operations to represent a group
with multiple pedestrians as one graph node. Lastly, GP-Graph infers a
probability map for socially-acceptable future trajectories from the integrated
features of both group interactions. Moreover, we introduce a group-level
latent vector sampling to ensure collective inferences over a set of possible
future trajectories. Extensive experiments are conducted to validate the
effectiveness of our architecture, which demonstrates consistent performance
improvements with publicly available benchmarks. Code is publicly available at
https://github.com/inhwanbae/GPGraph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Su, Chunyu Wang, Xiaoxuan Ma, Wenjun Zeng, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While monocular 3D pose estimation seems to have achieved very accurate
results on the public datasets, their generalization ability is largely
overlooked. In this work, we perform a systematic evaluation of the existing
methods and find that they get notably larger errors when tested on different
cameras, human poses and appearance. To address the problem, we introduce
VirtualPose, a two-stage learning framework to exploit the hidden "free lunch"
specific to this task, i.e. generating infinite number of poses and cameras for
training models at no cost. To that end, the first stage transforms images to
abstract geometry representations (AGR), and then the second maps them to 3D
poses. It addresses the generalization issue from two aspects: (1) the first
stage can be trained on diverse 2D datasets to reduce the risk of over-fitting
to limited appearance; (2) the second stage can be trained on diverse AGR
synthesized from a large number of virtual cameras and poses. It outperforms
the SOTA methods without using any paired images and 3D poses from the
benchmarks, which paves the way for practical applications. Code is available
at https://github.com/wkom/VirtualPose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probable Domain Generalization via Quantile Risk Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kügelgen, Hamed Hassani, George J. Pappas, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) seeks predictors which perform well on unseen test
distributions by leveraging labeled training data from multiple related
distributions or domains. To achieve this, the standard formulation optimizes
for worst-case performance over the set of all possible domains. However, with
worst-case shifts very unlikely in practice, this generally leads to
overly-conservative solutions. In fact, a recent study found that no DG
algorithm outperformed empirical risk minimization in terms of average
performance. In this work, we argue that DG is neither a worst-case problem nor
an average-case problem, but rather a probabilistic one. To this end, we
propose a probabilistic framework for DG, which we call Probable Domain
Generalization, wherein our key idea is that distribution shifts seen during
training should inform us of probable shifts at test time. To realize this, we
explicitly relate training and test domains as draws from the same underlying
meta-distribution, and propose a new optimization problem -- Quantile Risk
Minimization (QRM) -- which requires that predictors generalize with high
probability. We then prove that QRM: (i) produces predictors that generalize to
new domains with a desired probability, given sufficiently many domains and
samples; and (ii) recovers the causal predictor as the desired probability of
generalization approaches one. In our experiments, we introduce a more holistic
quantile-focused evaluation protocol for DG, and show that our algorithms
outperform state-of-the-art baselines on real and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient and Scale-Robust Ultra-High-Definition Image
  Demoireing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Jiajun Shen, Jia Li, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of mobile devices, modern widely-used mobile
phones typically allow users to capture 4K resolution (i.e.,
ultra-high-definition) images. However, for image demoireing, a challenging
task in low-level vision, existing works are generally carried out on
low-resolution or synthetic images. Hence, the effectiveness of these methods
on 4K resolution images is still unknown. In this paper, we explore moire
pattern removal for ultra-high-definition images. To this end, we propose the
first ultra-high-definition demoireing dataset (UHDM), which contains 5,000
real-world 4K resolution image pairs, and conduct a benchmark study on current
state-of-the-art methods. Further, we present an efficient baseline model
ESDNet for tackling 4K moire images, wherein we build a semantic-aligned
scale-aware module to address the scale variation of moire patterns. Extensive
experiments manifest the effectiveness of our approach, which outperforms
state-of-the-art methods by a large margin while being much more lightweight.
Code and dataset are available at https://xinyu-andy.github.io/uhdm-page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepIPC: Deeply Integrated Perception and Control for Mobile Robot in
  Real Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Natan, Jun Miura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DeepIPC, an end-to-end multi-task model that handles both
perception and control tasks in driving a mobile robot autonomously. The model
consists of two main parts, perception and controller modules. The perception
module takes RGB image and depth map to perform semantic segmentation and
bird's eye view (BEV) semantic mapping along with providing their encoded
features. Meanwhile, the controller module processes these features with the
measurement of GNSS locations and angular speed to estimate waypoints that come
with latent features. Then, two different agents are used to translate
waypoints and latent features into a set of navigational controls to drive the
robot. The model is evaluated by predicting driving records and performing
automated driving under various conditions in the real environment. Based on
the experimental results, DeepIPC achieves the best drivability and multi-task
performance even with fewer parameters compared to the other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted to a journal or conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Robust Landmark-based Stent Tracking in X-ray Fluoroscopy <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luojie Huang, Yi<span class="highlight-author">kang Liu</span>, Li Chen, Eric Z Chen, Xiao Chen, Shanhui Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViGAT: Bottom-up event recognition and explanation in video using
  factorized graph attention network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Gkalelis, Dimitrios Daskalakis, Vasileios Mezaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Framework for Few-shot Skeleton-based Temporal Action
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leiyang Xu, Qiang Wang, Xiaotian Lin, Lin Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal action segmentation (TAS) aims to classify and locate actions in the
long untrimmed action sequence. With the success of deep learning, many deep
models for action segmentation have emerged. However, few-shot TAS is still a
challenging problem. This study proposes an efficient framework for the
few-shot skeleton-based TAS, including a data augmentation method and an
improved model. The data augmentation approach based on motion interpolation is
presented here to solve the problem of insufficient data, and can increase the
number of samples significantly by synthesizing action sequences. Besides, we
concatenate a Connectionist Temporal Classification (CTC) layer with a network
designed for skeleton-based TAS to obtain an optimized model. Leveraging CTC
can enhance the temporal alignment between prediction and ground truth and
further improve the segment-wise metrics of segmentation results. Extensive
experiments on both public and self-constructed datasets, including two
small-scale datasets and one large-scale dataset, show the effectiveness of two
proposed methods in improving the performance of the few-shot skeleton-based
TAS task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on the variation of geometric functionals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nir Sochen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calculus of Variation combined with Differential Geometry as tools of
modelling and solving problems in image processing and computer vision were
introduced in the late 80's and the 90s of the 20th century. The beginning of
an extensive work in these directions was marked by works such as Geodesic
Active Contours (GAC), the Beltrami framework, level set method of Osher and
Sethian the works of Charpiat et al. and the works by Chan and Vese to name
just a few. In many cases the optimization of these functional are done by the
gradient descent method via the calculation of the Euler-Lagrange equations.
Straightforward use of the resulted EL equations in the gradient descent scheme
leads to non-geometric and in some cases non sensical equations. It is
costumary to modify these EL equations or even the functional itself in order
to obtain geometric and/or sensical equations. The aim of this note is to point
to the correct way to derive the EL and the gradient descent equations such
that the resulted gradient descent equation is geometric and makes sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Labeling instructions matter in biomedical image analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Rädsch, Annika Reinke, Vivienn Weru, Minu D. Tizabi, Nicholas Schreck, A. Emre Kavur, Bünyamin Pekdemir, Tobias Roß, Annette Kopp-Schneider, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical image analysis algorithm validation depends on high-quality
annotation of reference datasets, for which labeling instructions are key.
Despite their importance, their optimization remains largely unexplored. Here,
we present the first systematic study of labeling instructions and their impact
on annotation quality in the field. Through comprehensive examination of
professional practice and international competitions registered at the MICCAI
Society, we uncovered a discrepancy between annotators' needs for labeling
instructions and their current quality and availability. Based on an analysis
of 14,040 images annotated by 156 annotators from four professional companies
and 708 Amazon Mechanical Turk (MTurk) crowdworkers using instructions with
different information density levels, we further found that including exemplary
images significantly boosts annotation performance compared to text-only
descriptions, while solely extending text descriptions does not. Finally,
professional annotators constantly outperform MTurk crowdworkers. Our study
raises awareness for the need of quality standards in biomedical image analysis
labeling instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Samples are at Large: Leveraging Hard-distance Elastic Loss for
  Re-identification <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungtae Lee, Sungmin Eum, Heesung Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Momentum Re-identification (MoReID) framework that can leverage
a very large number of negative samples in training for general
re-identification task. The design of this framework is inspired by Momentum
Contrast (MoCo), which uses a dictionary to store current and past batches to
build a large set of encoded samples. As we find it less effective to use past
positive samples which may be highly inconsistent to the encoded feature
property formed with the current positive samples, MoReID is designed to use
only a large number of negative samples stored in the dictionary. However, if
we train the model using the widely used Triplet loss that uses only one sample
to represent a set of positive/negative samples, it is hard to effectively
leverage the enlarged set of negative samples acquired by the MoReID framework.
To maximize the advantage of using the scaled-up negative sample set, we newly
introduce Hard-distance Elastic loss (HE loss), which is capable of using more
than one hard sample to represent a large number of samples. Our experiments
demonstrate that a large number of negative samples provided by MoReID
framework can be utilized at full capacity only with the HE loss, achieving the
state-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and
VeRi-Wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Neural Network Training Method for Autonomous Driving Using
  Semi-Pseudo-Labels and 3D Data Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamas Matuszka, Daniel Kozma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training neural networks to perform 3D object detection for autonomous
driving requires a large amount of diverse annotated data. However, obtaining
training data with sufficient quality and quantity is expensive and sometimes
impossible due to human and sensor constraints. Therefore, a novel solution is
needed for extending current training methods to overcome this limitation and
enable accurate 3D object detection. Our solution for the above-mentioned
problem combines semi-pseudo-labeling and novel 3D augmentations. For
demonstrating the applicability of the proposed method, we have designed a
convolutional neural network for 3D object detection which can significantly
increase the detection range in comparison with the training data distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Adaptive Mixture of Experts Learning for Generalizable Face
  Anti-Spoofing <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyu Zhou, Ke-<span class="highlight-author">Yue Zhang</span>, Taiping Yao, Ran Yi, Shouhong Ding, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With various face presentation attacks emerging continually, face
anti-spoofing (FAS) approaches based on domain generalization (DG) have drawn
growing attention. Existing DG-based FAS approaches always capture the
domain-invariant features for generalizing on the various unseen domains.
However, they neglect individual source domains' discriminative characteristics
and diverse domain-specific information of the unseen domains, and the trained
model is not sufficient to be adapted to various unseen domains. To address
this issue, we propose an Adaptive Mixture of Experts Learning (AMEL)
framework, which exploits the domain-specific information to adaptively
establish the link among the seen source domains and unseen target domains to
further improve the generalization. Concretely, Domain-Specific Experts (DSE)
are designed to investigate discriminative and unique domain-specific features
as a complement to common domain-invariant features. Moreover, Dynamic Expert
Aggregation (DEA) is proposed to adaptively aggregate the complementary
information of each source expert based on the domain relevance to the unseen
target domain. And combined with meta-learning, these modules work
collaboratively to adaptively aggregate meaningful domain-specific information
for the various unseen target domains. Extensive experiments and visualizations
demonstrate the effectiveness of our method against the state-of-the-art
competitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete-Constrained Regression for Local Counting Models <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Xiong, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local counts, or the number of objects in a local area, is a continuous value
by nature. Yet recent state-of-the-art methods show that formulating counting
as a classification task performs better than regression. Through a series of
experiments on carefully controlled synthetic data, we show that this
counter-intuitive result is caused by imprecise ground truth local counts.
Factors such as biased dot annotations and incorrectly matched Gaussian kernels
used to generate ground truth counts introduce deviations from the true local
counts. Standard continuous regression is highly sensitive to these errors,
explaining the performance gap between classification and regression. To
mitigate the sensitivity, we loosen the regression formulation from a
continuous scale to a discrete ordering and propose a novel
discrete-constrained (DC) regression. Applied to crowd counting, DC-regression
is more accurate than both classification and standard regression on three
public benchmarks. A similar advantage also holds for the age estimation task,
verifying the overall effectiveness of DC-regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Stability of Deep Image Quality Assessment With Respect
  to Image Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koki Tsubota, Hiroaki Akutsu, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) is a fundamental metric for image processing
tasks (e.g., compression). With full-reference IQAs, traditional IQAs, such as
PSNR and SSIM, have been used. Recently, IQAs based on deep neural networks
(deep IQAs), such as LPIPS and DISTS, have also been used. It is known that
image scaling is inconsistent among deep IQAs, as some perform down-scaling as
pre-processing, whereas others instead use the original image size. In this
paper, we show that the image scale is an influential factor that affects deep
IQA performance. We comprehensively evaluate four deep IQAs on the same five
datasets, and the experimental results show that image scale significantly
influences IQA performance. We found that the most appropriate image scale is
often neither the default nor the original size, and the choice differs
depending on the methods and datasets used. We visualized the stability and
found that PieAPP is the most stable among the four deep IQAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEICE Transactions on Information and Systems (Letter)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Everything is There in Latent Space: Attribute Editing and Attribute
  Style Manipulation by StyleGAN Latent Space Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishubh Parihar, Ankit Dhiman, Tejan Karmali, R. Venkatesh Babu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unconstrained Image generation with high realism is now possible using recent
Generative Adversarial Networks (GANs). However, it is quite challenging to
generate images with a given set of attributes. Recent methods use style-based
GAN models to perform image editing by leveraging the semantic hierarchy
present in the layers of the generator. We present Few-shot Latent-based
Attribute Manipulation and Editing (FLAME), a simple yet effective framework to
perform highly controlled image editing by latent space manipulation.
Specifically, we estimate linear directions in the latent space (of a
pre-trained StyleGAN) that controls semantic attributes in the generated image.
In contrast to previous methods that either rely on large-scale attribute
labeled datasets or attribute classifiers, FLAME uses minimal supervision of a
few curated image pairs to estimate disentangled edit directions. FLAME can
perform both individual and sequential edits with high precision on a diverse
set of images while preserving identity. Further, we propose a novel task of
Attribute Style Manipulation to generate diverse styles for attributes such as
eyeglass and hair. We first encode a set of synthetic images of the same
identity but having different attribute styles in the latent space to estimate
an attribute style manifold. Sampling a new latent from this manifold will
result in a new attribute style in the generated image. We propose a novel
sampling method to sample latent from the manifold, enabling us to generate a
diverse set of attribute styles beyond the styles present in the training set.
FLAME can generate diverse attribute styles in a disentangled manner. We
illustrate the superior performance of FLAME against previous image editing
methods by extensive qualitative and quantitative comparisons. FLAME also
generalizes well on multiple datasets such as cars and churches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sites.google.com/view/flamelatentediting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Embedded Monocular Vision Approach for Ground-Aware Objects Detection
  and Position Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João G. Melo, Edna Barros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the RoboCup Small Size League (SSL), teams are encouraged to propose
solutions for executing basic soccer tasks inside the SSL field using only
embedded sensing information. Thus, this work proposes an embedded monocular
vision approach for detecting objects and estimating relative positions inside
the soccer field. Prior knowledge from the environment is exploited by assuming
objects lay on the ground, and the onboard camera has its position fixed on the
robot. We implemented the proposed method on an NVIDIA Jetson Nano and employed
SSD MobileNet v2 for 2D Object Detection with TensorRT optimization, detecting
balls, robots, and goals with distances up to 3.5 meters. Ball localization
evaluation shows that the proposed solution overcomes the currently used SSL
vision system for positions closer than 1 meter to the onboard camera with a
Root Mean Square Error of 14.37 millimeters. In addition, the proposed method
achieves real-time performance with an average processing speed of 30 frames
per second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, submitted to RoboCup Symposium 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Yang, Wanrong He, Yingqing Xu, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing methods view makeup transfer as transferring color
distributions of different facial regions and ignore details such as eye
shadows and blushes. Besides, they only achieve controllable transfer within
predefined fixed regions. This paper emphasizes the transfer of makeup details
and steps towards more flexible controls. To this end, we propose Exquisite and
locally editable GAN for makeup transfer (EleGANt). It encodes facial
attributes into pyramidal feature maps to preserves high-frequency information.
It uses attention to extract makeup features from the reference and adapt them
to the source face, and we introduce a novel Sow-Attention Module that applies
attention within shifted overlapped windows to reduce the computational cost.
Moreover, EleGANt is the first to achieve customized local editing within
arbitrary areas by corresponding editing on the feature maps. Extensive
experiments demonstrate that EleGANt generates realistic makeup faces with
exquisite details and achieves state-of-the-art performance. The code is
available at https://github.com/Chenyu-Yang-2000/EleGANt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UNIF: United Neural Implicit Functions for Clothed Human Reconstruction
  and Animation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenhan Qian, Jiale Xu, Ziwei Liu, Liqian Ma, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose united implicit functions (UNIF), a part-based method for clothed
human reconstruction and animation with raw scans and skeletons as the input.
Previous part-based methods for human reconstruction rely on ground-truth part
labels from SMPL and thus are limited to minimal-clothed humans. In contrast,
our method learns to separate parts from body motions instead of part
supervision, thus can be extended to clothed humans and other articulated
objects. Our Partition-from-Motion is achieved by a bone-centered
initialization, a bone limit loss, and a section normal loss that ensure stable
part division even when the training poses are limited. We also present a
minimal perimeter loss for SDF to suppress extra surfaces and part overlapping.
Another core of our method is an adjacent part seaming algorithm that produces
non-rigid deformations to maintain the connection between parts which
significantly relieves the part-based artifacts. Under this algorithm, we
further propose "Competing Parts", a method that defines blending weights by
the relative position of a point to bones instead of the absolute position,
avoiding the generalization problem of neural implicit functions with inverse
LBS (linear blend skinning). We demonstrate the effectiveness of our method by
clothed human body reconstruction and animation on the CAPE and the ClothSeq
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> NUWA-Infinity: Autoregressive over Autoregressive Generation for
  Infinite Visual Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenfei Wu, Jian Liang, Xiaowei Hu, <span class="highlight-author">Zhe Gan</span>, Jianfeng Wang, <span class="highlight-author">Lijuan Wang</span>, Zicheng Liu, Yuejian Fang, <span class="highlight-author">Nan Duan</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present NUWA-Infinity, a generative model for infinite
visual synthesis, which is defined as the task of generating arbitrarily-sized
high-resolution images or long-duration videos. An autoregressive over
autoregressive generation mechanism is proposed to deal with this variable-size
generation task, where a global patch-level autoregressive model considers the
dependencies between patches, and a local token-level autoregressive model
considers dependencies between visual tokens within each patch. A Nearby
Context Pool (NCP) is introduced to cache-related patches already generated as
the context for the current patch being generated, which can significantly save
computation costs without sacrificing patch-level dependency modeling. An
Arbitrary Direction Controller (ADC) is used to decide suitable generation
orders for different visual synthesis tasks and learn order-aware positional
embeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate
high-resolution images with arbitrary sizes and support long-duration video
generation additionally. Compared to NUWA, which also covers images and videos,
NUWA-Infinity has superior visual synthesis capabilities in terms of resolution
and variable-size generation. The GitHub link is
https://github.com/microsoft/NUWA. The homepage link is
https://nuwa-infinity.microsoft.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Anatomy of Video Editing: A <span class="highlight-title">Dataset</span> and Benchmark Suite for
  AI-Assisted Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawit Mureja Argaw, Fabian Caba Heilbron, Joon-Young Lee, Markus Woodson, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is transforming the video editing industry. Recent advances
in computer vision have leveled-up video editing tasks such as intelligent
reframing, rotoscoping, color grading, or applying digital makeups. However,
most of the solutions have focused on video manipulation and VFX. This work
introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster
research in AI-assisted video editing. Our benchmark suite focuses on video
editing tasks, beyond visual effects, such as automatic footage organization
and assisted video assembling. To enable research on these fronts, we annotate
more than 1.5M tags, with relevant concepts to cinematography, from 196176
shots sampled from movie scenes. We establish competitive baseline methods and
detailed analyses for each of the tasks. We hope our work sparks innovative
research towards underexplored areas of AI-assisted video editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> <span class="highlight-title">Transformer</span> for Automatic 3D Annotation and Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Xiaoyan Qian, Binxiao Huang, Xiaojuan Qi, Edmund Lam, Siew-Chong Tan, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a growing number of datasets being collected for training 3D object
detection models, significant human effort is still required to annotate 3D
boxes on LiDAR scans. To automate the annotation and facilitate the production
of various customized datasets, we propose an end-to-end multimodal transformer
(MTrans) autolabeler, which leverages both LiDAR scans and images to generate
precise 3D box annotations from weak 2D bounding boxes. To alleviate the
pervasive sparsity problem that hinders existing autolabelers, MTrans densifies
the sparse point clouds by generating new 3D points based on 2D image
information. With a multi-task design, MTrans segments the
foreground/background, densifies LiDAR point clouds, and regresses 3D boxes
simultaneously. Experimental results verify the effectiveness of the MTrans for
improving the quality of the generated labels. By enriching the sparse point
clouds, our method achieves 4.48\% and 4.03\% better 3D AP on KITTI moderate
and hard samples, respectively, versus the state-of-the-art autolabeler. MTrans
can also be extended to improve the accuracy for 3D object detection, resulting
in a remarkable 89.45\% AP on KITTI hard samples. Codes are at
\url{https://github.com/Cliu2/MTrans}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EASNet: Searching Elastic and Accurate Network Architecture for Stereo
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Wang, Shaohuai Shi, Kaiyong Zhao, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advanced studies have spent considerable human efforts on optimizing
network architectures for stereo matching but hardly achieved both high
accuracy and fast inference speed. To ease the workload in network design,
neural architecture search (NAS) has been applied with great success to various
sparse prediction tasks, such as image classification and object detection.
However, existing NAS studies on the dense prediction task, especially stereo
matching, still cannot be efficiently and effectively deployed on devices of
different computing capabilities. To this end, we propose to train an elastic
and accurate network for stereo matching (EASNet) that supports various 3D
architectural settings on devices with different computing capabilities. Given
the deployment latency constraint on the target device, we can quickly extract
a sub-network from the full EASNet without additional training while the
accuracy of the sub-network can still be maintained. Extensive experiments show
that our EASNet outperforms both state-of-the-art human-designed and NAS-based
architectures on Scene Flow and MPI Sintel datasets in terms of model accuracy
and inference speed. Particularly, deployed on an inference GPU, EASNet
achieves a new SOTA 0.73 EPE on the Scene Flow dataset with 100 ms, which is
4.5$\times$ faster than LEAStereo with a better quality model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Industrial Anomaly Detection via Pattern Generative and
  <span class="highlight-title">Contrastive</span> Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Huang, Chenyang Li, Yimin Lin, Shiguo Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is hard to collect enough flaw images for training deep learning network
in industrial production. Therefore, existing industrial anomaly detection
methods prefer to use CNN-based unsupervised detection and localization network
to achieve this task. However, these methods always fail when there are
varieties happened in new signals since traditional end-to-end networks suffer
barriers of fitting nonlinear model in high-dimensional space. Moreover, they
have a memory library by clustering the feature of normal images essentially,
which cause it is not robust to texture change. To this end, we propose the
Vision Transformer based (VIT-based) unsupervised anomaly detection network. It
utilizes a hierarchical task learning and human experience to enhance its
interpretability. Our network consists of pattern generation and comparison
networks. Pattern generation network uses two VIT-based encoder modules to
extract the feature of two consecutive image patches, then uses VIT-based
decoder module to learn the human designed style of these features and predict
the third image patch. After this, we use the Siamese-based network to compute
the similarity of the generation image patch and original image patch. Finally,
we refine the anomaly localization by the bi-directional inference strategy.
Comparison experiments on public dataset MVTec dataset show our method achieves
99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we
give a qualitative illustration on our own leather and cloth datasets. The
accurate segment results strongly prove the accuracy of our method in anomaly
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceFormer: Scale-aware Blind Face Restoration with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aijin Li, Gen Li, Lei Sun, Xintao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind face restoration usually encounters with diverse scale face inputs,
especially in the real world. However, most of the current works support
specific scale faces, which limits its application ability in real-world
scenarios. In this work, we propose a novel scale-aware blind face restoration
framework, named FaceFormer, which formulates facial feature restoration as
scale-aware transformation. The proposed Facial Feature Up-sampling (FFUP)
module dynamically generates upsampling filters based on the original
scale-factor priors, which facilitate our network to adapt to arbitrary face
scales. Moreover, we further propose the facial feature embedding (FFE) module
which leverages transformer to hierarchically extract diversity and robustness
of facial latent. Thus, our FaceFormer achieves fidelity and robustness
restored faces, which possess realistic and symmetrical details of facial
components. Extensive experiments demonstrate that our proposed method trained
with synthetic dataset generalizes better to a natural low quality images than
current state-of-the-arts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Elisa Ricci, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D LiDAR semantic segmentation is fundamental for autonomous driving. Several
Unsupervised Domain Adaptation (UDA) methods for point cloud data have been
recently proposed to improve model generalization for different sensors and
environments. Researchers working on UDA problems in the image domain have
shown that sample mixing can mitigate domain shift. We propose a new approach
of sample mixing for point cloud UDA, namely Compositional Semantic Mix
(CoSMix), the first UDA approach for point cloud segmentation based on sample
mixing. CoSMix consists of a two-branch symmetric network that can process
labelled synthetic data (source) and real-world unlabelled point clouds
(target) concurrently. Each branch operates on one domain by mixing selected
pieces of data from the other one, and by using the semantic information
derived from source labels and target pseudo-labels. We evaluate CoSMix on two
large-scale datasets, showing that it outperforms state-of-the-art methods by a
large margin. Our code is available at
https://github.com/saltoricristiano/cosmix-uda.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AU-Supervised Convolutional Vision <span class="highlight-title">Transformer</span>s for Synthetic Facial
  Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Mao, Xinpeng Li, Junyao Chen, Xiaojiang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper describes our proposed methodology for the six basic expression
classification track of Affective Behavior Analysis in-the-wild (ABAW)
Competition 2022. In Learing from Synthetic Data(LSD) task, facial expression
recognition (FER) methods aim to learn the representation of expression from
the artificially generated data and generalise to real data. Because of the
ambiguous of the synthetic data and the objectivity of the facial Action Unit
(AU), we resort to the AU information for performance boosting, and make
contributions as follows. First, to adapt the model to synthetic scenarios, we
use the knowledge from pre-trained large-scale face recognition data. Second,
we propose a conceptually-new framework, termed as AU-Supervised Convolutional
Vision Transformers (AU-CVT), which clearly improves the performance of FER by
jointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT
achieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The
source code of our work is publicly available online:
https://github.com/msy1412/ABAW4
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More Practical Scenario of Open-set Object Detection: Open at Category
  Level and Closed at Super-category Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Hosoya, Masanori Suganuma, Takayuki Okatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set object detection (OSOD) has recently attracted considerable
attention. It is to detect unknown objects while correctly
detecting/classifying known objects. We first point out that the scenario of
OSOD considered in recent studies, which considers an unlimited variety of
unknown objects similar to open-set recognition (OSR), has a fundamental issue.
That is, we cannot determine what to detect and what not for such unlimited
unknown objects, which is necessary for detection tasks. This issue leads to
difficulty with the evaluation of methods' performance on unknown object
detection. We then introduce a novel scenario of OSOD, which deals with only
unknown objects that share the super-category with known objects. It has many
real-world applications, e.g., detecting an increasing number of fine-grained
objects. This new setting is free from the above issue and evaluation
difficulty. Moreover, it makes detecting unknown objects more realistic owing
to the visual similarity between known and unknown objects. We show through
experimental results that a simple method based on the uncertainty of class
prediction from standard detectors outperforms the current state-of-the-art
OSOD methods tested in the previous setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drivable Volumetric Avatars using Texel-Aligned Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Tomas Simon, Chenglei Wu, Shih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, Yaser Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic telepresence requires both high-fidelity body modeling and
faithful driving to enable dynamically synthesized appearance that is
indistinguishable from reality. In this work, we propose an end-to-end
framework that addresses two core challenges in modeling and driving full-body
avatars of real people. One challenge is driving an avatar while staying
faithful to details and dynamics that cannot be captured by a global
low-dimensional parameterization such as body pose. Our approach supports
driving of clothed avatars with wrinkles and motion that a real driving
performer exhibits beyond the training corpus. Unlike existing global state
representations or non-parametric screen-space approaches, we introduce
texel-aligned features -- a localised representation which can leverage both
the structural prior of a skeleton-based parametric model and observed sparse
image signals at the same time. Another challenge is modeling a temporally
coherent clothed avatar, which typically requires precise surface tracking. To
circumvent this, we propose a novel volumetric avatar representation by
extending mixtures of volumetric primitives to articulated objects. By
explicitly incorporating articulation, our approach naturally generalizes to
unseen poses. We also introduce a localized viewpoint conditioning, which leads
to a large improvement in generalization of view-dependent appearance. The
proposed volumetric representation does not require high-quality mesh tracking
as a prerequisite and brings significant quality improvements compared to
mesh-based counterparts. In our experiments, we carefully examine our design
choices and demonstrate the efficacy of our approach, outperforming the
state-of-the-art methods on challenging driving scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localization supervision of chest x-ray classifiers using label-specific
  eye-tracking annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have been successfully applied to chest
x-ray (CXR) images. Moreover, annotated bounding boxes have been shown to
improve the interpretability of a CNN in terms of localizing abnormalities.
However, only a few relatively small CXR datasets containing bounding boxes are
available, and collecting them is very costly. Opportunely, eye-tracking (ET)
data can be collected in a non-intrusive way during the clinical workflow of a
radiologist. We use ET data recorded from radiologists while dictating CXR
reports to train CNNs. We extract snippets from the ET data by associating them
with the dictation of keywords and use them to supervise the localization of
abnormalities. We show that this method improves a model's interpretability
without impacting its image-level classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Convolutional Neural Network with Meta Feature Learning for
  Abnormality Detection in Wireless Capsule Endoscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Jain, Ayan Seal, Aparajita Ojha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless Capsule Endoscopy is one of the most advanced non-invasive methods
for the examination of gastrointestinal tracts. An intelligent computer-aided
diagnostic system for detecting gastrointestinal abnormalities like polyp,
bleeding, inflammation, etc. is highly exigent in wireless capsule endoscopy
image analysis. Abnormalities greatly differ in their shape, size, color, and
texture, and some appear to be visually similar to normal regions. This poses a
challenge in designing a binary classifier due to intra-class variations. In
this study, a hybrid convolutional neural network is proposed for abnormality
detection that extracts a rich pool of meaningful features from wireless
capsule endoscopy images using a variety of convolution operations. It consists
of three parallel convolutional neural networks, each with a distinctive
feature learning capability. The first network utilizes depthwise separable
convolution, while the second employs cosine normalized convolution operation.
A novel meta-feature extraction mechanism is introduced in the third network,
to extract patterns from the statistical information drawn over the features
generated from the first and second networks and its own previous layer. The
network trio effectively handles intra-class variance and efficiently detects
gastrointestinal abnormalities. The proposed hybrid convolutional neural
network model is trained and tested on two widely used publicly available
datasets. The test results demonstrate that the proposed model outperforms six
state-of-the-art methods with 97\% and 98\% classification accuracy on KID and
Kvasir-Capsule datasets respectively. Cross dataset evaluation results also
demonstrate the generalization performance of the proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborating Domain-shared and Target-specific Feature Clustering for
  Cross-domain 3D Action Recognition <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinying Liu, Zilei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the problem of cross-domain 3D action recognition
in the open-set setting, which has been rarely explored before. Specifically,
there is a source domain and a target domain that contain the skeleton
sequences with different styles and categories, and our purpose is to cluster
the target data by utilizing the labeled source data and unlabeled target data.
For such a challenging task, this paper presents a novel approach dubbed CoDT
to collaboratively cluster the domain-shared features and target-specific
features. CoDT consists of two parallel branches. One branch aims to learn
domain-shared features with supervised learning in the source domain, while the
other is to learn target-specific features using contrastive learning in the
target domain. To cluster the features, we propose an online clustering
algorithm that enables simultaneous promotion of robust pseudo label generation
and feature clustering. Furthermore, to leverage the complementarity of
domain-shared features and target-specific features, we propose a novel
collaborative clustering strategy to enforce pair-wise relationship consistency
between the two branches. We conduct extensive experiments on multiple
cross-domain 3D action recognition datasets, and the results demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D
  LiDAR Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Saltori, Evgeny Krivosheev, Stéphane Lathuilière, Nicu Sebe, Fabio Galasso, Giuseppe Fiameni, Elisa Ricci, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud semantic segmentation is fundamental for autonomous driving.
Most approaches in the literature neglect an important aspect, i.e., how to
deal with domain shift when handling dynamic scenes. This can significantly
hinder the navigation capabilities of self-driving vehicles. This paper
advances the state of the art in this research field. Our first contribution
consists in analysing a new unexplored scenario in point cloud segmentation,
namely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We
experimentally show that state-of-the-art methods have a rather limited ability
to adapt pre-trained deep network models to unseen domains in an online manner.
Our second contribution is an approach that relies on adaptive self-training
and geometric-feature propagation to adapt a pre-trained source model online
without requiring either source data or target labels. Our third contribution
is to study SF-OUDA in a challenging setup where source data is synthetic and
target data is point clouds captured in the real world. We use the recent
SynLiDAR dataset as a synthetic source and introduce two new synthetic (source)
datasets, which can stimulate future synthetic-to-real autonomous driving
research. Our experiments show the effectiveness of our segmentation approach
on thousands of real-world point clouds. Code and synthetic datasets are
available at https://github.com/saltoricristiano/gipso-sfouda.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action
  Recognition <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huabin Liu, Weixian Lv, John See, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A primary challenge faced in few-shot action recognition is inadequate video
data for training. To address this issue, current methods in this field mainly
focus on devising algorithms at the feature level while little attention is
paid to processing input video data. Moreover, existing frame sampling
strategies may omit critical action information in temporal and spatial
dimensions, which further impacts video utilization efficiency. In this paper,
we propose a novel video frame sampler for few-shot action recognition to
address this issue, where task-specific spatial-temporal frame sampling is
achieved via a temporal selector (TS) and a spatial amplifier (SA).
Specifically, our sampler first scans the whole video at a small computational
cost to obtain a global perception of video frames. The TS plays its role in
selecting top-T frames that contribute most significantly and subsequently. The
SA emphasizes the discriminative information of each frame by amplifying
critical regions with the guidance of saliency maps. We further adopt
task-adaptive learning to dynamically adjust the sampling strategy according to
the episode task at hand. Both the implementations of TS and SA are
differentiable for end-to-end optimization, facilitating seamless integration
of our proposed sampler with most few-shot action recognition methods.
Extensive experiments show a significant boost in the performances on various
benchmarks including long-term videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Affect Analysis: Learning from Synthetic Data & Multi-Task
  Learning Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Li, Yifan Xu, Huanyu Wu, Dongrui Wu, Yingjie Yin, Jiajiong Cao, Jingting Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial affect analysis remains a challenging task with its setting
transitioned from lab-controlled to in-the-wild situations. In this paper, we
present novel frameworks to handle the two challenges in the 4th Affective
Behavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL)
Challenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL
challenge, we adopt the SMM-EmotionNet with a better ensemble strategy of
feature vectors. For LSD challenge, we propose respective methods to combat the
problems of single labels, imbalanced distribution, fine-tuning limitations,
and choice of model architectures. Experimental results on the official
validation sets from the competition demonstrated that our proposed approaches
outperformed baselines by a large margin. The code is available at
https://github.com/sylyoung/ABAW4-HUST-ANT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Latent Spaces of Generative Models for Medical Images using
  Unsupervised Methods <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Schön, Raghavendra Selvan, Jens Petersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as Generative Adversarial Networks (GANs) and
Variational Autoencoders (VAEs) play an increasingly important role in medical
image analysis. The latent spaces of these models often show semantically
meaningful directions corresponding to human-interpretable image
transformations. However, until now, their exploration for medical images has
been limited due to the requirement of supervised data. Several methods for
unsupervised discovery of interpretable directions in GAN latent spaces have
shown interesting results on natural images. This work explores the potential
of applying these techniques on medical images by training a GAN and a VAE on
thoracic CT scans and using an unsupervised method to discover interpretable
directions in the resulting latent space. We find several directions
corresponding to non-trivial image transformations, such as rotation or breast
size. Furthermore, the directions show that the generative models capture 3D
structure despite being presented only with 2D data. The results show that
unsupervised methods to discover interpretable directions in GANs generalize to
VAEs and can be applied to medical images. This opens a wide array of future
work using these methods in medical image analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at DGM4MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point
  Clouds <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Anh Vu, Duc Thanh Nguyen, Binh-Son Hua, Quang-Hieu Pham, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object reconstruction from 3D point clouds has achieved impressive progress
in the computer vision and computer graphics research field. However,
reconstruction from time-varying point clouds (a.k.a. 4D point clouds) is
generally overlooked. In this paper, we propose a new network architecture,
namely RFNet-4D, that jointly reconstruct objects and their motion flows from
4D point clouds. The key insight is that simultaneously performing both tasks
via learning spatial and temporal features from a sequence of point clouds can
leverage individual tasks, leading to improved overall performance. To prove
this ability, we design a temporal vector field learning module using
unsupervised learning approach for flow estimation, leveraged by supervised
learning of spatial structures for object reconstruction. Extensive experiments
and analyses on benchmark dataset validated the effectiveness and efficiency of
our method. As shown in experimental results, our method achieves
state-of-the-art performance on both flow estimation and object reconstruction
while performing much faster than existing methods in both training and
inference. Our code and data are available at
https://github.com/hkust-vgd/RFNet-4D
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bringing Rolling Shutter Images Alive with Dual Reversed Distortion <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Zhong, Mingdeng Cao, Xiao Sun, Zhirong Wu, Zhongyi Zhou, Yinqiang Zheng, Stephen Lin, Imari Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rolling shutter (RS) distortion can be interpreted as the result of picking a
row of pixels from instant global shutter (GS) frames over time during the
exposure of the RS camera. This means that the information of each instant GS
frame is partially, yet sequentially, embedded into the row-dependent
distortion. Inspired by this fact, we address the challenging task of reversing
this process, i.e., extracting undistorted GS frames from images suffering from
RS distortion. However, since RS distortion is coupled with other factors such
as readout settings and the relative velocity of scene elements to the camera,
models that only exploit the geometric correlation between temporally adjacent
images suffer from poor generality in processing data with different readout
settings and dynamic scenes with both camera motion and object motion. In this
paper, instead of two consecutive frames, we propose to exploit a pair of
images captured by dual RS cameras with reversed RS directions for this highly
challenging task. Grounded on the symmetric and complementary nature of dual
reversed distortion, we develop a novel end-to-end model, IFED, to generate
dual optical flow sequence through iterative learning of the velocity field
during the RS time. Extensive experimental results demonstrate that IFED is
superior to naive cascade schemes, as well as the state-of-the-art which
utilizes adjacent RS images. Most importantly, although it is trained on a
synthetic dataset, IFED is shown to be effective at retrieving GS frame
sequences from real-world RS distorted images of dynamic scenes. Code is
available at https://github.com/zzh-tech/Dual-Reversed-RS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00486v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00486v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Difei Gao, Licheng Yu, Stan Weixian Lei, Matt Feiszli, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive science has shown that humans perceive videos in terms of events
separated by the state changes of dominant subjects. State changes trigger new
events and are one of the most useful among the large amount of redundant
information perceived. However, previous research focuses on the overall
understanding of segments without evaluating the fine-grained status changes
inside. In this paper, we introduce a new dataset called Kinetic-GEB+. The
dataset consists of over 170k boundaries associated with captions describing
status changes in the generic events in 12K videos. Upon this new dataset, we
propose three tasks supporting the development of a more fine-grained, robust,
and human-like understanding of videos through status changes. We evaluate many
representative baselines in our dataset, where we also design a new TPD
(Temporal-based Pairwise Difference) Modeling method for visual difference and
achieve significant performance improvements. Besides, the results show there
are still formidable challenges for current methods in the utilization of
different granularities, representation of visual difference, and the accurate
localization of status changes. Further analysis shows that our dataset can
drive developing more powerful methods to understand status changes and thus
improve video level comprehension. The dataset is available at
https://github.com/Yuxuan-W/GEB-Plus
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the European Conference on Computer Vision 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaCLR: Motion-aware <span class="highlight-title">Contrastive Learning</span> of Representations for Videos <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.09703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.09703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanyi Xiao, Joseph Tighe, Davide Modolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MaCLR, a novel method to explicitly perform cross-modal
self-supervised video representations learning from visual and motion
modalities. Compared to previous video representation learning methods that
mostly focus on learning motion cues implicitly from RGB inputs, MaCLR enriches
standard contrastive learning objectives for RGB video clips with a cross-modal
learning objective between a Motion pathway and a Visual pathway. We show that
the representation learned with our MaCLR method focuses more on foreground
motion regions and thus generalizes better to downstream tasks. To demonstrate
this, we evaluate MaCLR on five datasets for both action recognition and action
detection, and demonstrate state-of-the-art self-supervised performance on all
datasets. Furthermore, we show that MaCLR representation can be as effective as
representations learned with full supervision on UCF101 and HMDB51 action
recognition, and even outperform the supervised representation for action
recognition on VidSitu and SSv2, and action detection on AVA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D dense captioning is a recently-proposed novel task, where point clouds
contain more geometric information than the 2D counterpart. However, it is also
more challenging due to the higher complexity and wider variety of inter-object
relations contained in point clouds. Existing methods only treat such relations
as by-products of object feature learning in graphs without specifically
encoding them, which leads to sub-optimal results. In this paper, aiming at
improving 3D dense captioning via capturing and utilizing the complex relations
in the 3D scene, we propose MORE, a Multi-Order RElation mining model, to
support generating more descriptive and comprehensive captions. Technically,
our MORE encodes object relations in a progressive manner since complex
relations can be deduced from a limited number of basic ones. We first devise a
novel Spatial Layout Graph Convolution (SLGC), which semantically encodes
several first-order relations as edges of a graph constructed over 3D object
proposals. Next, from the resulting graph, we further extract multiple triplets
which encapsulate basic first-order relations as the basic unit, and construct
several Object-centric Triplet Attention Graphs (OTAG) to infer multi-order
relations for every target object. The updated node features from OTAG are
aggregated and fed into the caption decoder to provide abundant relational
cues, so that captions including diverse relations with context objects can be
generated. Extensive experiments on the Scan2Cap dataset prove the
effectiveness of our proposed MORE and its components, and we also outperform
the current state-of-the-art method. Our code is available at
https://github.com/SxJyJay/MORE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Visual Tracking by Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Paul, Martin Danelljan, Christoph Mayer, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the target extent poses a fundamental challenge in visual object
tracking. Typically, trackers are box-centric and fully rely on a bounding box
to define the target in the scene. In practice, objects often have complex
shapes and are not aligned with the image axis. In these cases, bounding boxes
do not provide an accurate description of the target and often contain a
majority of background pixels. We propose a segmentation-centric tracking
pipeline that not only produces a highly accurate segmentation mask, but also
internally works with segmentation masks instead of bounding boxes. Thus, our
tracker is able to better learn a target representation that clearly
differentiates the target in the scene from background content. In order to
achieve the necessary robustness for the challenging tracking scenario, we
propose a separate instance localization component that is used to condition
the segmentation decoder when producing the output mask. We infer a bounding
box from the segmentation mask, validate our tracker on challenging tracking
datasets and achieve the new state of the art on LaSOT with a success AUC score
of 69.7%. Since most tracking datasets do not contain mask annotations, we
cannot use them to evaluate predicted segmentation masks. Instead, we validate
our segmentation quality on two popular video object segmentation datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022. Code and trained models are available at:
  https://github.com/visionml/pytracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphVid: It Only Takes a Few Nodes to Understand a Video <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eitan Kosman, Dotan Di Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a concise representation of videos that encode perceptually
meaningful features into graphs. With this representation, we aim to leverage
the large amount of redundancies in videos and save computations. First, we
construct superpixel-based graph representations of videos by considering
superpixels as graph nodes and create spatial and temporal connections between
adjacent superpixels. Then, we leverage Graph Convolutional Networks to process
this representation and predict the desired output. As a result, we are able to
train models with much fewer parameters, which translates into short training
periods and a reduction in computation resource requirements. A comprehensive
experimental study on the publicly available datasets Kinetics-400 and Charades
shows that the proposed method is highly cost-effective and uses limited
commodity hardware during training and inference. It reduces the computational
requirements 10-fold while achieving results that are comparable to
state-of-the-art methods. We believe that the proposed approach is a promising
direction that could open the door to solving video understanding more
efficiently and enable more resource limited users to thrive in this research
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV2022 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Connect, Not Collapse: Explaining <span class="highlight-title">Contrastive Learning</span> for Unsupervised
  Domain Adaptation <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kendrick Shen, Robbie Jones, Ananya Kumar, Sang Michael Xie, Jeff Z. HaoChen, Tengyu Ma, <span class="highlight-author">Percy Liang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider unsupervised domain adaptation (UDA), where labeled data from a
source domain (e.g., photographs) and unlabeled data from a target domain
(e.g., sketches) are used to learn a classifier for the target domain.
Conventional UDA methods (e.g., domain adversarial training) learn
domain-invariant features to improve generalization to the target domain. In
this paper, we show that contrastive pre-training, which learns features on
unlabeled source and target data and then fine-tunes on labeled source data, is
competitive with strong UDA methods. However, we find that contrastive
pre-training does not learn domain-invariant features, diverging from
conventional UDA intuitions. We show theoretically that contrastive
pre-training can learn features that vary subtantially across domains but still
generalize to the target domain, by disentangling domain and class information.
Our results suggest that domain invariance is not necessary for UDA. We
empirically validate our theory on benchmark vision datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022 (Long Talk)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual <span class="highlight-title">Prompt</span> Tuning <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current modus operandi in adapting pre-trained models involves updating
all the backbone parameters, ie, full fine-tuning. This paper introduces Visual
Prompt Tuning (VPT) as an efficient and effective alternative to full
fine-tuning for large-scale Transformer models in vision. Taking inspiration
from recent advances in efficiently tuning large language models, VPT
introduces only a small amount (less than 1% of model parameters) of trainable
parameters in the input space while keeping the model backbone frozen. Via
extensive experiments on a wide variety of downstream recognition tasks, we
show that VPT achieves significant performance gains compared to other
parameter efficient tuning protocols. Most importantly, VPT even outperforms
full fine-tuning in many cases across model capacities and training data
scales, while reducing per-task storage cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AssistQ: Affordance-centric Question-driven Task Completion for
  Egocentric Assistant <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04203v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04203v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benita Wong, Joya Chen, You Wu, Stan Weixian Lei, Dongxing Mao, Difei Gao, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as "how
can I run the microwave for 1 minute?". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos to provide step-by-step help in the
user's view. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples from 100 newly filmed instructional
videos. We also developed a novel Question-to-Actions (Q2A) model to address
the AQTC task and validate it on the AssistQ dataset. The results show that our
model significantly outperforms several VQA-related baselines while still
having large room for improvement. We expect our task and dataset to advance
Egocentric AI Assistant's development. Our project page is available at:
https://showlab.github.io/assistq/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. Equal contribution: Benita Wong, Joya Chen,
  You Wu; Corresponding author: Mike Zheng Shou</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointMixer: MLP-Mixer for Point Cloud Understanding <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11187v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11187v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesung Choe, Chunghyun Park, Francois Rameau, Jaesik Park, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MLP-Mixer has newly appeared as a new challenger against the realm of CNNs
and transformer. Despite its simplicity compared to transformer, the concept of
channel-mixing MLPs and token-mixing MLPs achieves noticeable performance in
visual recognition tasks. Unlike images, point clouds are inherently sparse,
unordered and irregular, which limits the direct use of MLP-Mixer for point
cloud understanding. In this paper, we propose PointMixer, a universal point
set operator that facilitates information sharing among unstructured 3D points.
By simply replacing token-mixing MLPs with a softmax function, PointMixer can
"mix" features within/between point sets. By doing so, PointMixer can be
broadly used in the network as inter-set mixing, intra-set mixing, and pyramid
mixing. Extensive experiments show the competitive or superior performance of
PointMixer in semantic segmentation, classification, and point reconstruction
against transformer-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15781v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15781v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose L. Gómez, Gabriel Villalonga, Antonio M. López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image segmentation is a central and challenging task in autonomous
driving, addressed by training deep models. Since this training draws to a
curse of human-based image labeling, using synthetic images with automatically
generated labels together with unlabeled real-world images is a promising
alternative. This implies to address an unsupervised domain adaptation (UDA)
problem. In this paper, we propose a new co-training procedure for
synth-to-real UDA of semantic segmentation models. It consists of a
self-training stage, which provides two domain-adapted models, and a model
collaboration loop for the mutual improvement of these two models. These models
are then used to provide the final semantic segmentation labels (pseudo-labels)
for the real-world images. The overall procedure treats the deep models as
black boxes and drives their collaboration at the level of pseudo-labeled
target images, i.e., neither modifying loss functions is required, nor explicit
feature alignment. We test our proposal on standard synthetic and real-world
datasets for on-board semantic segmentation. Our procedure shows improvements
ranging from ~13 to ~26 mIoU points over baselines, so establishing new
state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-Deform-Subtract: An Interventional Framework for Explaining Object
  Differences <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cian Eastwood, Li Nanbo, Christopher K. I. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two object images, how can we explain their differences in terms of the
underlying object properties? To address this question, we propose
Align-Deform-Subtract (ADS) -- an interventional framework for explaining
object differences. By leveraging semantic alignments in image-space as
counterfactual interventions on the underlying object properties, ADS
iteratively quantifies and removes differences in object properties. The result
is a set of "disentangled" error measures which explain object differences in
terms of the underlying properties. Experiments on real and synthetic data
illustrate the efficacy of the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 Workshop on Objects, Structure and Causality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning for Real-World Super-Resolution from Dual
  Zoomed Observations <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilu Zhang, Ruohao Wang, Hongzhi Zhang, Yunjin Chen, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider two challenging issues in reference-based
super-resolution (RefSR), (i) how to choose a proper reference image, and (ii)
how to learn real-world RefSR in a self-supervised manner. Particularly, we
present a novel self-supervised learning approach for real-world image SR from
observations at dual camera zooms (SelfDZSR). Considering the popularity of
multiple cameras in modern smartphones, the more zoomed (telephoto) image can
be naturally leveraged as the reference to guide the SR of the lesser zoomed
(short-focus) image. Furthermore, SelfDZSR learns a deep network to obtain the
SR result of short-focus image to have the same resolution as the telephoto
image. For this purpose, we take the telephoto image instead of an additional
high-resolution image as the supervision information and select a center patch
from it as the reference to super-resolve the corresponding short-focus image
patch. To mitigate the effect of the misalignment between short-focus
low-resolution (LR) image and telephoto ground-truth (GT) image, we design an
auxiliary-LR generator and map the GT to an auxiliary-LR while keeping the
spatial position unchanged. Then the auxiliary-LR can be utilized to deform the
LR features by the proposed adaptive spatial transformer networks (AdaSTN), and
match the Ref features to GT. During testing, SelfDZSR can be directly deployed
to super-solve the whole short-focus image with the reference of telephoto
image. Experiments show that our method achieves better quantitative and
qualitative performance against state-of-the-arts. Codes are available at
https://github.com/cszhilu1998/SelfDZSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoGS: Controllable Generation and Search from Sketch and Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cusuh Ham, Gemma Canet Tarres, Tu Bui, James Hays, Zhe Lin, John Collomosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CoGS, a novel method for the style-conditioned, sketch-driven
synthesis of images. CoGS enables exploration of diverse appearance
possibilities for a given sketched object, enabling decoupled control over the
structure and the appearance of the output. Coarse-grained control over object
structure and appearance are enabled via an input sketch and an exemplar
"style" conditioning image to a transformer-based sketch and style encoder to
generate a discrete codebook representation. We map the codebook representation
into a metric space, enabling fine-grained control over selection and
interpolation between multiple synthesis options before generating the image
via a vector quantized GAN (VQGAN) decoder. Our framework thereby unifies
search and synthesis tasks, in that a sketch and style pair may be used to run
an initial synthesis which may be refined via combination with similar results
in a search corpus to produce an image more closely matching the user's intent.
We show that our model, trained on the 125 object classes of our newly created
Pseudosketches dataset, is capable of producing a diverse gamut of semantic
content and appearance styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised
  Object Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Li, Xiang Li, Yujie Wang, Yichao Wu, Ding Liang, Shanshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we delve into two key techniques in Semi-Supervised Object
Detection (SSOD), namely pseudo labeling and consistency training. We observe
that these two techniques currently neglect some important properties of object
detection, hindering efficient learning on unlabeled data. Specifically, for
pseudo labeling, existing works only focus on the classification score yet fail
to guarantee the localization precision of pseudo boxes; For consistency
training, the widely adopted random-resize training only considers the
label-level consistency but misses the feature-level one, which also plays an
important role in ensuring the scale invariance. To address the problems
incurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that
includes Prediction-guided Label Assignment (PLA) and Positive-proposal
Consistency Voting (PCV). PLA relies on model predictions to assign labels and
makes it robust to even coarse pseudo boxes; while PCV leverages the regression
consistency of positive proposals to reflect the localization quality of pseudo
boxes. Furthermore, in consistency training, we propose Multi-view
Scale-invariant Learning (MSL) that includes mechanisms of both label- and
feature-level consistency, where feature consistency is achieved by aligning
shifted feature pyramids between two images with identical content but varied
scales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency
training (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points
under 1%, 5%, and 10% labelling ratios, respectively. It also significantly
improves the learning efficiency for SSOD, e.g., PseCo halves the training time
of the SOTA approach but achieves even better performance. Code is available at
https://github.com/ligang-cs/PseCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RankSeg: Adaptive Pixel Classification with Image Category Ranking for
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodi He, Yuhui Yuan, Xiangyu Yue, Han Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation task has traditionally been formulated as a complete-label
pixel classification task to predict a class for each pixel from a fixed number
of predefined semantic categories shared by all images or videos. Yet,
following this formulation, standard architectures will inevitably encounter
various challenges under more realistic settings where the scope of categories
scales up (e.g., beyond the level of 1k). On the other hand, in a typical image
or video, only a few categories, i.e., a small subset of the complete label are
present. Motivated by this intuition, in this paper, we propose to decompose
segmentation into two sub-problems: (i) image-level or video-level multi-label
classification and (ii) pixel-level rank-adaptive selected-label
classification. Given an input image or video, our framework first conducts
multi-label classification over the complete label, then sorts the complete
label and selects a small subset according to their class confidence scores. We
then use a rank-adaptive pixel classifier to perform the pixel-wise
classification over only the selected labels, which uses a set of rank-oriented
learnable temperature parameters to adjust the pixel classifications scores.
Our approach is conceptually general and can be used to improve various
existing segmentation frameworks by simply using a lightweight multi-label
classification head and rank-adaptive pixel classifier. We demonstrate the
effectiveness of our framework with competitive experimental results across
four tasks, including image semantic segmentation, image panoptic segmentation,
video instance segmentation, and video semantic segmentation. Especially, with
our RankSeg, Mask2Former gains +0.8%/+0.7%/+0.7% on ADE20K panoptic
segmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic
segmentation benchmarks respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022, Code will be available at:
  https://github.com/openseg-group/RankSeg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via
  Speech-Visage Feature Selection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to reconstruct speech from a silent talking face
video. Recent studies have shown impressive performance on synthesizing speech
from silent talking face videos. However, they have not explicitly considered
on varying identity characteristics of different speakers, which place a
challenge in the video-to-speech synthesis, and this becomes more critical in
unseen-speaker settings. Our approach is to separate the speech content and the
visage-style from a given silent talking face video. By guiding the model to
independently focus on modeling the two representations, we can obtain the
speech of high intelligibility from the model even when the input video of an
unseen subject is given. To this end, we introduce speech-visage selection that
separates the speech content and the speaker identity from the visual features
of the input video. The disentangled representations are jointly incorporated
to synthesize speech through visage-style based synthesizer which generates
speech by coating the visage-styles while maintaining the speech content. Thus,
the proposed framework brings the advantage of synthesizing the speech
containing the right content even with the silent talking face video of an
unseen subject. We validate the effectiveness of the proposed framework on the
GRID, TCD-TIMIT volunteer, and LRW datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-Aware Observer Network for Out-of-Distribution Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Besnier, Andrei Bursuc, David Picard, Alexandre Briot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on Observer Network has shown promising results on
Out-Of-Distribution (OOD) detection for semantic segmentation. These methods
have difficulty in precisely locating the point of interest in the image, i.e,
the anomaly. This limitation is due to the difficulty of fine-grained
prediction at the pixel level. To address this issue, we provide instance
knowledge to the observer. We extend the approach of ObsNet by harnessing an
instance-wise mask prediction. We use an additional, class agnostic, object
detector to filter and aggregate observer predictions. Finally, we predict an
unique anomaly score for each instance in the image. We show that our proposed
method accurately disentangle in-distribution objects from Out-Of-Distribution
objects on three datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DFNet: Enhance Absolute Pose Regression with Direct Feature Matching <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00559v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00559v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Chen, Xinghui Li, Zirui Wang, Victor Adrian Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Code released at https://github.com/ActiveVisionLab/DFNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Uncertain Single-View Depths in Colonoscopies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Rodríguez-Puigvert, David Recasens, Javier Civera, Rubén Martínez-Cantín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating depth information from endoscopic images is a prerequisite for a
wide set of AI-assisted technologies, such as accurate localization and
measurement of tumors, or identification of non-inspected areas. As the domain
specificity of colonoscopies -- deformable low-texture environments with
fluids, poor lighting conditions and abrupt sensor motions -- pose challenges
to multi-view 3D reconstructions, single-view depth learning stands out as a
promising line of research. Depth learning can be extended in a Bayesian
setting, which enables continual learning, improves decision making and can be
used to compute confidence intervals or quantify uncertainty for in-body
measurements. In this paper, we explore for the first time Bayesian deep
networks for single-view depth estimation in colonoscopies. Our specific
contribution is two-fold: 1) an exhaustive analysis of scalable Bayesian
networks for depth learning in different datasets, highlighting challenges and
conclusions regarding synthetic-to-real domain changes and supervised vs.
self-supervised methods; and 2) a novel teacher-student approach to deep depth
learning that takes into account the teacher uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poseur: Direct Human Pose Regression with <span class="highlight-title">Transformer</span>s <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, Anton van den Hengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a direct, regression-based approach to 2D human pose estimation
from single images. We formulate the problem as a sequence prediction task,
which we solve using a Transformer network. This network directly learns a
regression mapping from images to the keypoint coordinates, without resorting
to intermediate representations such as heatmaps. This approach avoids much of
the complexity associated with heatmap-based approaches. To overcome the
feature misalignment issues of previous regression-based methods, we propose an
attention mechanism that adaptively attends to the features that are most
relevant to the target keypoints, considerably improving the accuracy.
Importantly, our framework is end-to-end differentiable, and naturally learns
to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,
two predominant pose-estimation datasets, demonstrate that our method
significantly improves upon the state-of-the-art in regression-based pose
estimation. More notably, ours is the first regression-based approach to
perform favorably compared to the best heatmap-based pose estimation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proc. Eur. Conf. Comp. Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Open-Vocabulary Object Detection with Vision <span class="highlight-title">Transformer</span>s <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing strategies and <span class="highlight-title">dataset</span>s for facial representation learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.16554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.16554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Bulat, Shiyang Cheng, Jing Yang, Andrew Garbett, Enrique Sanchez, Georgios Tzimiropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What is the best way to learn a universal face representation? Recent work on
Deep Learning in the area of face analysis has focused on supervised learning
for specific tasks of interest (e.g. face recognition, facial landmark
localization etc.) but has overlooked the overarching question of how to find a
facial representation that can be readily adapted to several facial analysis
tasks and datasets. To this end, we make the following 4 contributions: (a) we
introduce, for the first time, a comprehensive evaluation benchmark for facial
representation learning consisting of 5 important face analysis tasks. (b) We
systematically investigate two ways of large-scale representation learning
applied to faces: supervised and unsupervised pre-training. Importantly, we
focus our evaluations on the case of few-shot facial learning. (c) We
investigate important properties of the training datasets including their size
and quality (labelled, unlabelled or even uncurated). (d) To draw our
conclusions, we conducted a very large number of experiments. Our main two
findings are: (1) Unsupervised pre-training on completely in-the-wild,
uncurated data provides consistent and, in some cases, significant accuracy
improvements for all facial tasks considered. (2) Many existing facial video
datasets seem to have a large amount of redundancy. We will release code, and
pre-trained models to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextformer: A <span class="highlight-title">Transformer</span> with Spatio-Channel Attention for Context
  Modeling in Learned Image Compression <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Burakhan Koyuncu, Han Gao, Atanas Boev, Georgii Gaikov, Elena Alshina, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entropy modeling is a key component for high-performance image compression
algorithms. Recent developments in autoregressive context modeling helped
learning-based methods to surpass their classical counterparts. However, the
performance of those models can be further improved due to the underexploited
spatio-channel dependencies in latent space, and the suboptimal implementation
of context adaptivity. Inspired by the adaptive characteristics of the
transformers, we propose a transformer-based context model, named
Contextformer, which generalizes the de facto standard attention mechanism to
spatio-channel attention. We replace the context model of a modern compression
framework with the Contextformer and test it on the widely used Kodak,
CLIC2020, and Tecnick image datasets. Our experimental results show that the
proposed model provides up to 11% rate savings compared to the standard
Versatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various
learning-based models in terms of PSNR and MS-SSIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022; 31 pages (14 main paper + References + 13
  Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-rigid Point Cloud Registration with Neural Deformation Pyramid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Tatsuya Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-rigid point cloud registration is a key component in many computer vision
and computer graphics applications. The high complexity of the unknown
non-rigid motion make this task a challenging problem. In this paper, we break
down this problem via hierarchical motion decomposition. Our method called
Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid
architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),
takes as input a sinusoidally encoded 3D point and outputs its motion
increments from the previous level. The sinusoidal function starts with a low
input frequency and gradually increases when the pyramid level goes down. This
allows a multi-level rigid to nonrigid motion decomposition and also speeds up
the solving by 50 times compared to the existing MLP-based approach. Our method
achieves advanced partialto-partial non-rigid point cloud registration results
on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/rabbityl/DeformationPyramid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOSTER: Feature Boosting and Compression for Class-Incremental Learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to learn new concepts continually is necessary in this
ever-changing world. However, deep neural networks suffer from catastrophic
forgetting when learning new categories. Many works have been proposed to
alleviate this phenomenon, whereas most of them either fall into the
stability-plasticity dilemma or take too much computation or storage overhead.
Inspired by the gradient boosting algorithm to gradually fit the residuals
between the target model and the previous ensemble model, we propose a novel
two-stage learning paradigm FOSTER, empowering the model to learn new
categories adaptively. Specifically, we first dynamically expand new modules to
fit the residuals between the target and the output of the original model.
Next, we remove redundant parameters and feature dimensions through an
effective distillation strategy to maintain the single backbone model. We
validate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different
settings. Experimental results show that our method achieves state-of-the-art
performance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at:
  https://github.com/G-U-N/ECCV22-FOSTER</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Responsive Listening Head Generation: A Benchmark <span class="highlight-title">Dataset</span> and Baseline <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.13548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.13548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new listening head generation benchmark, for synthesizing
responsive feedbacks of a listener (e.g., nod, smile) during a face-to-face
conversation. As the indispensable complement to talking heads generation,
listening head generation has seldomly been studied in literature.
Automatically synthesizing listening behavior that actively responds to a
talking head, is critical to applications such as digital human, virtual agents
and social robots. In this work, we propose a novel dataset "ViCo",
highlighting the listening head generation during a face-to-face conversation.
A total number of 92 identities (67 speakers and 76 listeners) are involved in
ViCo, featuring 483 clips in a paired "speaking-listening" pattern, where
listeners show three listening styles based on their attitudes: positive,
neutral, negative. Different from traditional speech-to-gesture or talking-head
generation, listening head generation takes as input both the audio and visual
signals from the speaker, and gives non-verbal feedbacks (e.g., head motions,
facial expressions) in a real-time manner. Our dataset supports a wide range of
applications such as human-to-human interaction, video-to-video translation,
cross-modal understanding and generation. To encourage further research, we
also release a listening head generation baseline, conditioning on different
listening attitudes. Code & ViCo dataset: https://project.mhzhou.com/vico.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space-Partitioning RANSAC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Barath, Gabor Valasek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new algorithm is proposed to accelerate RANSAC model quality calculations.
The method is based on partitioning the joint correspondence space, e.g., 2D-2D
point correspondences, into a pair of regular grids. The grid cells are mapped
by minimal sample models, estimated within RANSAC, to reject correspondences
that are inconsistent with the model parameters early. The proposed technique
is general. It works with arbitrary transformations even if a point is mapped
to a point set, e.g., as a fundamental matrix maps to epipolar lines. The
method is tested on thousands of image pairs from publicly available datasets
on fundamental and essential matrix, homography and radially distorted
homography estimation. On average, it reduces the RANSAC run-time by 41% with
provably no deterioration in the accuracy. It can be straightforwardly plugged
into state-of-the-art RANSAC frameworks, e.g. VSAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2C-SR: A Divergence to Convergence Approach for Real-World Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.14373v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.14373v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youwei Li, Haibin Huang, Lanpeng Jia, Haoqiang Fan, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present D2C-SR, a novel framework for the task of
real-world image super-resolution. As an ill-posed problem, the key challenge
in super-resolution related tasks is there can be multiple predictions for a
given low-resolution input. Most classical deep learning based approaches
ignored the fundamental fact and lack explicit modeling of the underlying
high-frequency distribution which leads to blurred results. Recently, some
methods of GAN-based or learning super-resolution space can generate simulated
textures but do not promise the accuracy of the textures which have low
quantitative performance. Rethinking both, we learn the distribution of
underlying high-frequency details in a discrete form and propose a two-stage
pipeline: divergence stage to convergence stage. At divergence stage, we
propose a tree-based structure deep network as our divergence backbone.
Divergence loss is proposed to encourage the generated results from the
tree-based network to diverge into possible high-frequency representations,
which is our way of discretely modeling the underlying high-frequency
distribution. At convergence stage, we assign spatial weights to fuse these
divergent predictions to obtain the final output with more accurate details.
Our approach provides a convenient end-to-end manner to inference. We conduct
evaluations on several real-world benchmarks, including a new proposed
D2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that
D2C-SR achieves better accuracy and visual improvements against
state-of-the-art methods, with a significantly less parameters number and our
D2C structure can also be applied as a generalized structure to some other
methods to obtain improvement. Our codes and dataset are available at
https://github.com/megvii-research/D2C-SR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManiFest: Manifold Deformation for Few-shot Image Translation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Pizzati, Jean-François Lalonde, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most image-to-image translation methods require a large number of training
images, which restricts their applicability. We instead propose ManiFest: a
framework for few-shot image translation that learns a context-aware
representation of a target domain from a few images only. To enforce feature
consistency, our framework learns a style manifold between source and proxy
anchor domains (assumed to be composed of large numbers of images). The learned
manifold is interpolated and deformed towards the few-shot target domain via
patch-based adversarial and feature statistics alignment losses. All of these
components are trained simultaneously during a single end-to-end loop. In
addition to the general few-shot translation task, our approach can
alternatively be conditioned on a single exemplar image to reproduce its
specific style. Extensive experiments demonstrate the efficacy of ManiFest on
multiple tasks, outperforming the state-of-the-art on all metrics and in both
the general- and exemplar-based scenarios. Our code is available at
https://github.com/cv-rits/Manifest .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonizer: Learning to Perform White-Box Image and Video Harmonization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on image harmonization solve the problem as a pixel-wise image
translation task via large autoencoders. They have unsatisfactory performances
and slow inference speeds when dealing with high-resolution images. In this
work, we observe that adjusting the input arguments of basic image filters,
e.g., brightness and contrast, is sufficient for humans to produce realistic
images from the composite ones. Hence, we frame image harmonization as an
image-level regression problem to learn the arguments of the filters that
humans use for the task. We present a Harmonizer framework for image
harmonization. Unlike prior methods that are based on black-box autoencoders,
Harmonizer contains a neural network for filter argument prediction and several
white-box filters (based on the predicted arguments) for image harmonization.
We also introduce a cascade regressor and a dynamic loss strategy for
Harmonizer to learn filter arguments more stably and precisely. Since our
network only outputs image-level arguments and the filters we used are
efficient, Harmonizer is much lighter and faster than existing methods.
Comprehensive experiments demonstrate that Harmonizer surpasses existing
methods notably, especially with high-resolution inputs. Finally, we apply
Harmonizer to video harmonization, which achieves consistent results across
frames and 56 fps at 1080P resolution. Code and models are available at:
https://github.com/ZHKKKe/Harmonizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TO-Scene: A Large-scale <span class="highlight-title">Dataset</span> for Understanding 3D Tabletop Scenes <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian Xu, Pei Chen, Haolin Liu, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many basic indoor activities such as eating or writing are always conducted
upon different tabletops (e.g., coffee tables, writing desks). It is
indispensable to understanding tabletop scenes in 3D indoor scene parsing
applications. Unfortunately, it is hard to meet this demand by directly
deploying data-driven algorithms, since 3D tabletop scenes are rarely available
in current datasets. To remedy this defect, we introduce TO-Scene, a
large-scale dataset focusing on tabletop scenes, which contains 20,740 scenes
with three variants. To acquire the data, we design an efficient and scalable
framework, where a crowdsourcing UI is developed to transfer CAD objects from
ModelNet and ShapeNet onto tables from ScanNet, then the output tabletop scenes
are simulated into real scans and annotated automatically.
  Further, a tabletop-aware learning strategy is proposed for better perceiving
the small-sized tabletop instances. Notably, we also provide a real scanned
test set TO-Real to verify the practical value of TO-Scene. Experiments show
that the algorithms trained on TO-Scene indeed work on the realistic test data,
and our proposed tabletop-aware learning strategy greatly improves the
state-of-the-art results on both 3D semantic segmentation and object detection
tasks. Dataset and code are available at
https://github.com/GAP-LAB-CUHK-SZ/TO-Scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled
  Conditions <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based virtual try-on aims to synthesize an image of a person wearing a
given clothing item. To solve the task, the existing methods warp the clothing
item to fit the person's body and generate the segmentation map of the person
wearing the item before fusing the item with the person. However, when the
warping and the segmentation generation stages operate individually without
information exchange, the misalignment between the warped clothes and the
segmentation map occurs, which leads to the artifacts in the final image. The
information disconnection also causes excessive warping near the clothing
regions occluded by the body parts, so-called pixel-squeezing artifacts. To
settle the issues, we propose a novel try-on condition generator as a unified
module of the two stages (i.e., warping and segmentation generation stages). A
newly proposed feature fusion block in the condition generator implements the
information exchange, and the condition generator does not create any
misalignment or pixel-squeezing artifacts. We also introduce discriminator
rejection that filters out the incorrect segmentation map predictions and
assures the performance of virtual try-on frameworks. Experiments on a
high-resolution dataset demonstrate that our model successfully handles the
misalignment and occlusion, and significantly outperforms the baselines. Code
is available at https://github.com/sangyun884/HR-VITON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Image Transformations for Transfer-based Adversarial Attack <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Jie Zhang, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks provide a good way to study the robustness of deep
learning models. One category of methods in transfer-based black-box attack
utilizes several image transformation operations to improve the transferability
of adversarial examples, which is effective, but fails to take the specific
characteristic of the input image into consideration. In this work, we propose
a novel architecture, called Adaptive Image Transformation Learner (AITL),
which incorporates different image transformation operations into a unified
framework to further improve the transferability of adversarial examples.
Unlike the fixed combinational transformations used in existing works, our
elaborately designed transformation learner adaptively selects the most
effective combination of image transformations specific to the input image.
Extensive experiments on ImageNet demonstrate that our method significantly
improves the attack success rates on both normally trained models and defense
models under various settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 7 figures, 11 tables. Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NARRATE: A Normal Assisted Free-View Portrait Stylizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, Jingyi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose NARRATE, a novel pipeline that enables
simultaneously editing portrait lighting and perspective in a photorealistic
manner. As a hybrid neural-physical face model, NARRATE leverages complementary
benefits of geometry-aware generative approaches and normal-assisted physical
face models. In a nutshell, NARRATE first inverts the input portrait to a
coarse geometry and employs neural rendering to generate images resembling the
input, as well as producing convincing pose changes. However, inversion step
introduces mismatch, bringing low-quality images with less facial details. As
such, we further estimate portrait normal to enhance the coarse geometry,
creating a high-fidelity physical face model. In particular, we fuse the neural
and physical renderings to compensate for the imperfect inversion, resulting in
both realistic and view-consistent novel perspective images. In relighting
stage, previous works focus on single view portrait relighting but ignoring
consistency between different perspectives as well, leading unstable and
inconsistent lighting effects for view changes. We extend Total Relighting to
fix this problem by unifying its multi-view input normal maps with the physical
face model. NARRATE conducts relighting with consistent normal maps, imposing
cross-view constraints and exhibiting stable and coherent illumination effects.
We experimentally demonstrate that NARRATE achieves more photorealistic,
reliable results over prior works. We further bridge NARRATE with animation and
style transfer tools, supporting pose change, light change, facial animation,
and style transfer, either separately or in combination, all at a photographic
quality. We showcase vivid free-view facial animations as well as 3D-aware
relightable stylization, which help facilitate various AR/VR applications like
virtual cinematography, 3D video conferencing, and post-production.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,13 figures https://youtu.be/mP4FV3evmyw</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An application of Pixel Interval Down-sampling (PID) for dense tiny
  microorganism counting on environmental microorganism images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhang, Ning Xu, Chen Li, Md Mamunur Rahaman, Yu-Dong Yao, Yu-Hao Lin, Jinghua Zhang, Tao Jiang, Wenjun Qin, Marcin Grzegorzek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel pixel interval down-sampling network (PID-Net)
for dense tiny object (yeast cells) counting tasks with higher accuracy. The
PID-Net is an end-to-end convolutional neural network (CNN) model with an
encoder--decoder architecture. The pixel interval down-sampling operations are
concatenated with max-pooling operations to combine the sparse and dense
features. This addresses the limitation of contour conglutination of dense
objects while counting. The evaluation was conducted using classical
segmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as
counting metrics. The experimental results show that the proposed PID-Net had
the best performance and potential for dense tiny object counting tasks, which
achieved 96.97\% counting accuracy on the dataset with 2448 yeast cell images.
By comparing with the state-of-the-art approaches, such as Attention U-Net,
Swin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects
with clearer boundaries and fewer incorrect debris, which shows the great
potential of PID-Net in the task of accurate counting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Fine-Grained Scene Graph Generation with Data Transfer <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Zhang, Yuan Yao, Qianyu Chen, Wei Ji, <span class="highlight-author">Zhiyuan Liu</span>, Maosong Sun, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discover and Mitigate Unknown Biases with Debiasing Alternate Networks <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Li, Anthony Hoogs, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep image classifiers have been found to learn biases from datasets. To
mitigate the biases, most previous methods require labels of protected
attributes (e.g., age, skin tone) as full-supervision, which has two
limitations: 1) it is infeasible when the labels are unavailable; 2) they are
incapable of mitigating unknown biases -- biases that humans do not
preconceive. To resolve those problems, we propose Debiasing Alternate Networks
(DebiAN), which comprises two networks -- a Discoverer and a Classifier. By
training in an alternate manner, the discoverer tries to find multiple unknown
biases of the classifier without any annotations of biases, and the classifier
aims at unlearning the biases identified by the discoverer. While previous
works evaluate debiasing results in terms of a single bias, we create
Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in
a multi-bias setting, which not only reveals the problems in previous methods
but also demonstrates the advantage of DebiAN in identifying and mitigating
multiple biases simultaneously. We further conduct extensive experiments on
real-world datasets, showing that the discoverer in DebiAN can identify unknown
biases that may be hard to be found by humans. Regarding debiasing, DebiAN
achieves strong bias mitigation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Semantic uncertainty intervals for disentangled latent spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swami Sankaranarayanan, Anastasios N. Angelopoulos, Stephen Bates, Yaniv Romano, <span class="highlight-author">Phillip Isola</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meaningful uncertainty quantification in computer vision requires reasoning
about semantic information -- say, the hair color of the person in a photo or
the location of a car on the street. To this end, recent breakthroughs in
generative modeling allow us to represent semantic information in disentangled
latent spaces, but providing uncertainties on the semantic latent variables has
remained challenging. In this work, we provide principled uncertainty intervals
that are guaranteed to contain the true semantic factors for any underlying
generative model. The method does the following: (1) it uses quantile
regression to output a heuristic uncertainty interval for each element in the
latent space (2) calibrates these uncertainties such that they contain the true
value of the latent for a new, unseen input. The endpoints of these calibrated
intervals can then be propagated through the generator to produce interpretable
uncertainty visualizations for each semantic factor. This technique reliably
communicates semantically meaningful, principled, and instance-adaptive
uncertainty in inverse problems like image super-resolution and image
completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/swamiviv/generative_semantic_uncertainty</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DataPerf: Benchmarks for Data-Centric AI Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karlaš, William Gaviria Rojas, Sudnya Diamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera, Juan Ciro, Lora Aroyo, Bilge Acun, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman, Tariq Kane, Christine R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen Paritosh, Ce Zhang, James Zou, Carole-Jean Wu, Cody Coleman, <span class="highlight-author">Andrew Ng</span>, Peter Mattson, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) research has generally focused on models, while the
most prominent datasets have been employed for everyday ML tasks without regard
for the breadth, difficulty, and faithfulness of these datasets to the
underlying problem. Neglecting the fundamental importance of datasets has
caused major problems involving data cascades in real-world applications and
saturation of dataset-driven criteria for model quality, hindering research
growth. To solve this problem, we present DataPerf, a benchmark package for
evaluating ML datasets and dataset-working algorithms. We intend it to enable
the "data ratchet," in which training sets will aid in evaluating test sets on
the same problems, and vice versa. Such a feedback-driven strategy will
generate a virtuous loop that will accelerate development of data-centric AI.
The MLCommons Association will maintain DataPerf.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discriminator-Weighted Offline Imitation Learning from Suboptimal
  Demonstrations <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Xianyuan Zhan, Honglei Yin, Huiling Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of offline Imitation Learning (IL) where an agent aims
to learn an optimal expert behavior policy without additional online
environment interactions. Instead, the agent is provided with a supplementary
offline dataset from suboptimal behaviors. Prior works that address this
problem either require that expert data occupies the majority proportion of the
offline dataset, or need to learn a reward function and perform offline
reinforcement learning (RL) afterwards. In this paper, we aim to address the
problem without additional steps of reward learning and offline RL training for
the case when demonstrations contain a large proportion of suboptimal data.
Built upon behavioral cloning (BC), we introduce an additional discriminator to
distinguish expert and non-expert data. We propose a cooperation framework to
boost the learning of both tasks, Based on this framework, we design a new IL
algorithm, where the outputs of discriminator serve as the weights of the BC
loss. Experimental results show that our proposed algorithm achieves higher
returns and faster training speed compared to baseline algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022, code at https://github.com/ryanxhr/DWBC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pretrain</span>ing a Neural Network before Knowing Its Architecture <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Knyazev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large neural networks is possible by training a smaller hypernetwork
that predicts parameters for the large ones. A recently released Graph
HyperNetwork (GHN) trained this way on one million smaller ImageNet
architectures is able to predict parameters for large unseen networks such as
ResNet-50. While networks with predicted parameters lose performance on the
source task, the predicted parameters have been found useful for fine-tuning on
other tasks. We study if fine-tuning based on the same GHN is still useful on
novel strong architectures that were published after the GHN had been trained.
We found that for recent architectures such as ConvNeXt, GHN initialization
becomes less useful than for ResNet-50. One potential reason is the increased
distribution shift of novel architectures from those used to train the GHN. We
also found that the predicted parameters lack the diversity necessary to
successfully fine-tune parameters with gradient descent. We alleviate this
limitation by applying simple post-processing techniques to predicted
parameters before fine-tuning them on a target task and improve fine-tuning of
ResNet-50 and ConvNeXt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2022 Workshop on Pre-training: Perspectives,
  Pitfalls, and Paths Forward, source code is available at
  https://github.com/facebookresearch/ppuda</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Step-Size Methods for Compressed SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh M. Subramaniam, Akshayaa Magesh, Venugopal V. Veeravalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressed Stochastic Gradient Descent (SGD) algorithms have been recently
proposed to address the communication bottleneck in distributed and
decentralized optimization problems, such as those that arise in federated
machine learning. Existing compressed SGD algorithms assume the use of
non-adaptive step-sizes(constant or diminishing) to provide theoretical
convergence guarantees. Typically, the step-sizes are fine-tuned in practice to
the dataset and the learning algorithm to provide good empirical performance.
Such fine-tuning might be impractical in many learning scenarios, and it is
therefore of interest to study compressed SGD using adaptive step-sizes.
Motivated by prior work on adaptive step-size methods for SGD to train neural
networks efficiently in the uncompressed setting, we develop an adaptive
step-size method for compressed SGD. In particular, we introduce a scaling
technique for the descent step in compressed SGD, which we use to establish
order-optimal convergence rates for convex-smooth and strong convex-smooth
objectives under an interpolation condition and for non-convex objectives under
a strong growth condition. We also show through simulation examples that
without this scaling, the algorithm can fail to converge. We present
experimental results on deep neural networks for real-world datasets, and
compare the performance of our proposed algorithm with previously proposed
compressed SGD methods in literature, and demonstrate improved performance on
ResNet-18, ResNet-34 and DenseNet architectures for CIFAR-100 and CIFAR-10
datasets at various levels of compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MANI-Rank: Multiple Attribute and Intersectional Group Fairness for
  Consensus Ranking <span class="chip">ICDE 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathleen Cachel, Elke Rundensteiner, Lane Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining the preferences of many rankers into one single consensus ranking
is critical for consequential applications from hiring and admissions to
lending. While group fairness has been extensively studied for classification,
group fairness in rankings and in particular rank aggregation remains in its
infancy. Recent work introduced the concept of fair rank aggregation for
combining rankings but restricted to the case when candidates have a single
binary protected attribute, i.e., they fall into two groups only. Yet it
remains an open problem how to create a consensus ranking that represents the
preferences of all rankers while ensuring fair treatment for candidates with
multiple protected attributes such as gender, race, and nationality. In this
work, we are the first to define and solve this open Multi-attribute Fair
Consensus Ranking (MFCR) problem. As a foundation, we design novel group
fairness criteria for rankings, called MANI-RANK, ensuring fair treatment of
groups defined by individual protected attributes and their intersection.
Leveraging the MANI-RANK criteria, we develop a series of algorithms that for
the first time tackle the MFCR problem. Our experimental study with a rich
variety of consensus scenarios demonstrates our MFCR methodology is the only
approach to achieve both intersectional and protected attribute fairness while
also representing the preferences expressed through many base rankings. Our
real-world case study on merit scholarships illustrates the effectiveness of
our MFCR methods to mitigate bias across multiple protected attributes and
their intersections. This is an extended version of "MANI-Rank: Multiple
Attribute and Intersectional Group Fairness for Consensus Ranking", to appear
in ICDE 2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE ICDE 2022. 15 pages, and 7
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Algorithmic Bias with Limited Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanchu Wang, Mengnan Du, Ninghao Liu, Na Zou, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing work on fairness modeling commonly assumes that sensitive attributes
for all instances are fully available, which may not be true in many real-world
applications due to the high cost of acquiring sensitive information. When
sensitive attributes are not disclosed or available, it is needed to manually
annotate a small part of the training data to mitigate bias. However, the
skewed distribution across different sensitive groups preserves the skewness of
the original dataset in the annotated subset, which leads to non-optimal bias
mitigation. To tackle this challenge, we propose Active Penalization Of
Discrimination (APOD), an interactive framework to guide the limited
annotations towards maximally eliminating the effect of algorithmic bias. The
proposed APOD integrates discrimination penalization with active instance
selection to efficiently utilize the limited annotation budget, and it is
theoretically proved to be capable of bounding the algorithmic bias. According
to the evaluation on five benchmark datasets, APOD outperforms the
state-of-the-arts baseline methods under the limited annotation budget, and
shows comparable performance to fully annotated bias mitigation, which
demonstrates that APOD could benefit real-world applications when sensitive
information is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Object-Centric Process Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Rohrer, Anahita Farhang Ghahfarokhi, Mohamed Behery, Gerhard Lakemeyer, Wil M. P. van der Aalst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automation and digitalization of business processes has resulted in large
amounts of data captured in information systems, which can aid businesses in
understanding their processes better, improve workflows, or provide operational
support. By making predictions about ongoing processes, bottlenecks can be
identified and resources reallocated, as well as insights gained into the state
of a process instance (case). Traditionally, data is extracted from systems in
the form of an event log with a single identifying case notion, such as an
order id for an Order to Cash (O2C) process. However, real processes often have
multiple object types, for example, order, item, and package, so a format that
forces the use of a single case notion does not reflect the underlying
relations in the data. The Object-Centric Event Log (OCEL) format was
introduced to correctly capture this information. The state-of-the-art
predictive methods have been tailored to only traditional event logs. This
thesis shows that a prediction method utilizing Generative Adversarial Networks
(GAN), Long Short-Term Memory (LSTM) architectures, and Sequence to Sequence
models (Seq2seq), can be augmented with the rich data contained in OCEL.
Objects in OCEL can have attributes that are useful in predicting the next
event and timestamp, such as a priority class attribute for an object type
package indicating slower or faster processing. In the metrics of sequence
similarity of predicted remaining events and mean absolute error (MAE) of the
timestamp, the approach in this thesis matches or exceeds previous research,
depending on whether selected object attributes are useful features for the
model. Additionally, this thesis provides a web interface to predict the next
sequence of activities from user input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BYEL : Bootstrap on Your Emotion Latent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungjun Lee, Hwangyu Lim, Sejoon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the problem of dataset construction cost for training in deep
learning and the development of generative models, more and more researches are
being conducted to train with synthetic data and to inference using real data.
We propose emotion aware Self-Supervised Learning using ABAW's Learning
Synthetic Data (LSD) dataset. We pre-train our method to LSD dataset as a
self-supervised learning and then use the same LSD dataset to do downstream
training on the emotion classification task as a supervised learning. As a
result, a higher result(0.63) than baseline(0.5) was obtained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ABAW4th competition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin-based Intrusion Detection for Industrial Control Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seba Anna Varghese, Alireza Dehlaghi Ghadim, Ali Balador, Zahra Alimadadi, Panos Papadimitratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital twins have recently gained significant interest in simulation,
optimization, and predictive maintenance of Industrial Control Systems (ICS).
Recent studies discuss the possibility of using digital twins for intrusion
detection in industrial systems. Accordingly, this study contributes to a
digital twin-based security framework for industrial control systems, extending
its capabilities for simulation of attacks and defense mechanisms. Four types
of process-aware attack scenarios are implemented on a standalone open-source
digital twin of an industrial filling plant: command injection, network Denial
of Service (DoS), calculated measurement modification, and naive measurement
modification. A stacked ensemble classifier is proposed as the real-time
intrusion detection, based on the offline evaluation of eight supervised
machine learning algorithms. The designed stacked model outperforms previous
methods in terms of F1-Score and accuracy, by combining the predictions of
various algorithms, while it can detect and classify intrusions in near
real-time (0.1 seconds). This study also discusses the practicality and
benefits of the proposed digital twin-based security framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 3 tables, workshop paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFACTOR GNNS: Revisiting Factorisation-based Models from a
  Message-Passing Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and to generalise to unseen nodes in inductive settings. Our work
bridges the gap between FMs and GNNs by proposing REFACTOR GNNS. This new
architecture draws upon both modelling paradigms, which previously were largely
thought of as disjoint. Concretely, using a message-passing formalism, we show
how FMs can be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our REFACTOR GNNS. Across
a multitude of well-established KGC benchmarks, our REFACTOR GNNS achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralNEB -- Neural Networks can find Reaction Paths Fast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Schreiner, Arghya Bhowmik, Tejs Vegge, Ole Winther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) models have, in contrast to their usefulness in
molecular dynamics studies, had limited success as surrogate potentials for
reaction barrier search. It is due to the scarcity of training data in relevant
transition state regions of chemical space. Currently, available datasets for
training ML models on small molecular systems almost exclusively contain
configurations at or near equilibrium. In this work, we present the dataset
Transition1x containing 9.6 million Density Functional Theory (DFT)
calculations of forces and energies of molecular configurations on and around
reaction pathways at the wB97x/6-31G(d) level of theory. The data was generated
by running Nudged Elastic Band (NEB) calculations with DFT on 10k reactions
while saving intermediate calculations. We train state-of-the-art equivariant
graph message-passing neural network models on Transition1x and cross-validate
on the popular ANI1x and QM9 datasets. We show that ML models cannot learn
features in transition-state regions solely by training on hitherto popular
benchmark datasets. Transition1x is a new challenging benchmark that will
provide an important step towards developing next-generation ML force fields
that also work far away from equilibrium configurations and reactive systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and signing fairness as performance under multiple stakeholder
  distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Lopez-Paz, Diane Bouchacourt, Levent Sagun, Nicolas Usunier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As learning machines increase their influence on decisions concerning human
lives, analyzing their fairness properties becomes a subject of central
importance. Yet, our best tools for measuring the fairness of learning systems
are rigid fairness metrics encapsulated as mathematical one-liners, offer
limited power to the stakeholders involved in the prediction task, and are easy
to manipulate when we exhort excessive pressure to optimize them. To advance
these issues, we propose to shift focus from shaping fairness metrics to
curating the distributions of examples under which these are computed. In
particular, we posit that every claim about fairness should be immediately
followed by the tagline "Fair under what examples, and collected by whom?". By
highlighting connections to the literature in domain generalization, we propose
to measure fairness as the ability of the system to generalize under multiple
stress tests -- distributions of examples with social relevance. We encourage
each stakeholder to curate one or multiple stress tests containing examples
reflecting their (possibly conflicting) interests. The machine passes or fails
each stress test by falling short of or exceeding a pre-defined metric value.
The test results involve all stakeholders in a discussion about how to improve
the learning system, and provide flexible assessments of fairness dependent on
context and based on interpretable data. We provide full implementation
guidelines for stress testing, illustrate both the benefits and shortcomings of
this framework, and introduce a cryptographic scheme to enable a degree of
prediction accountability from system providers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploration of Parameter Spaces Assisted by Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Hammad, Myeonghun Park, Raymundo Ramos, Pankaj Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We showcase a variety of functions and classes that implement sampling
procedures with improved exploration of the parameter space assisted by machine
learning. Special attention is paid to setting sane defaults with the objective
that adjustments required by different problems remain minimal. This collection
of routines can be employed for different types of analysis, from finding
bounds on the parameter space to accumulating samples in areas of interest. In
particular, we discuss two methods assisted by incorporating different machine
learning models: regression and classification. We show that a machine learning
classifier can provide higher efficiency for exploring the parameter space.
Also, we introduce a boosting technique to improve the slow convergence at the
start of the process. The use of these routines is better explained with the
help of a few examples that illustrate the type of results one can obtain. We
also include examples of the code used to obtain the examples as well as
descriptions of the adjustments that can be made to adapt the calculation to
other problems. We finalize by showing the impact of these techniques when
exploring the parameter space of the two Higgs doublet model that matches the
measured Higgs Boson signal strength. The code used for this paper and
instructions on how to use it are available on the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures. Code and instructions are available on
  https://github.com/AHamamd150/MLscanner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Model Performance under Domain Shifts with Class-Specific
  Confidence Scores <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Li, Konstantinos Kamnitsas, Mobarakol Islam, Chen Chen, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are typically deployed in a test setting that differs
from the training setting, potentially leading to decreased model performance
because of domain shift. If we could estimate the performance that a
pre-trained model would achieve on data from a specific deployment setting, for
example a certain clinic, we could judge whether the model could safely be
deployed or if its performance degrades unacceptably on the specific data.
Existing approaches estimate this based on the confidence of predictions made
on unlabeled test data from the deployment's domain. We find existing methods
struggle with data that present class imbalance, because the methods used to
calibrate confidence do not account for bias induced by class imbalance,
consequently failing to estimate class-wise accuracy. Here, we introduce
class-wise calibration within the framework of performance estimation for
imbalanced datasets. Specifically, we derive class-specific modifications of
state-of-the-art confidence-based model evaluation methods including
temperature scaling (TS), difference of confidences (DoC), and average
thresholded confidence (ATC). We also extend the methods to estimate Dice
similarity coefficient (DSC) in image segmentation. We conduct experiments on
four tasks and find the proposed modifications consistently improve the
estimation accuracy for imbalanced datasets. Our methods improve accuracy
estimation by 18\% in classification under natural domain shifts, and double
the estimation accuracy on segmentation tasks, when compared with prior
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operation-Level Performance Benchmarking of Graph Neural Networks for
  Scientific Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryien Hosseini, Filippo Simini, Venkatram Vishwanath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Graph Neural Networks (GNNs) increase in popularity for scientific machine
learning, their training and inference efficiency is becoming increasingly
critical. Additionally, the deep learning field as a whole is trending towards
wider and deeper networks, and ever increasing data sizes, to the point where
hard hardware bottlenecks are often encountered. Emerging specialty hardware
platforms provide an exciting solution to this problem. In this paper, we
systematically profile and select low-level operations pertinent to GNNs for
scientific computing implemented in the Pytorch Geometric software framework.
These are then rigorously benchmarked on NVIDIA A100 GPUs for several various
combinations of input values, including tensor sparsity. We then analyze these
results for each operation. At a high level, we conclude that on NVIDIA
systems: (1) confounding bottlenecks such as memory inefficiency often dominate
runtime costs moreso than data sparsity alone, (2) native Pytorch operations
are often as or more competitive than their Pytorch Geometric equivalents,
especially at low to moderate levels of input data sparsity, and (3) many
operations central to state-of-the-art GNN architectures have little to no
optimization for sparsity. We hope that these results serve as a baseline for
those developing these operations on specialized hardware and that our
subsequent analysis helps to facilitate future software and hardware based
optimizations of these operations and thus scalable GNN performance as a whole.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as workshop paper at MLSys 2022 (MLBench)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Pedestrian Group Representations for <span class="highlight-title">Multi-modal</span> Trajectory
  Prediction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Jin-Hwi Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling the dynamics of people walking is a problem of long-standing
interest in computer vision. Many previous works involving pedestrian
trajectory prediction define a particular set of individual actions to
implicitly model group actions. In this paper, we present a novel architecture
named GP-Graph which has collective group representations for effective
pedestrian trajectory prediction in crowded environments, and is compatible
with all types of existing approaches. A key idea of GP-Graph is to model both
individual-wise and group-wise relations as graph representations. To do this,
GP-Graph first learns to assign each pedestrian into the most likely behavior
group. Using this assignment information, GP-Graph then forms both intra- and
inter-group interactions as graphs, accounting for human-human relations within
a group and group-group relations, respectively. To be specific, for the
intra-group interaction, we mask pedestrian graph edges out of an associated
group. We also propose group pooling&unpooling operations to represent a group
with multiple pedestrians as one graph node. Lastly, GP-Graph infers a
probability map for socially-acceptable future trajectories from the integrated
features of both group interactions. Moreover, we introduce a group-level
latent vector sampling to ensure collective inferences over a set of possible
future trajectories. Extensive experiments are conducted to validate the
effectiveness of our architecture, which demonstrates consistent performance
improvements with publicly available benchmarks. Code is publicly available at
https://github.com/inhwanbae/GPGraph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Market Making Under a Hawkes
  Process-Based Limit Order Book Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Gašperov, Zvonko Kostanjčar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The stochastic control problem of optimal market making is among the central
problems in quantitative finance. In this paper, a deep reinforcement
learning-based controller is trained on a weakly consistent, multivariate
Hawkes process-based limit order book simulator to obtain market making
controls. The proposed approach leverages the advantages of Monte Carlo
backtesting and contributes to the line of research on market making under
weakly consistent limit order book models. The ensuing deep reinforcement
learning controller is compared to multiple market making benchmarks, with the
results indicating its superior performance with respect to various risk-reward
metrics, even under significant transaction costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixed Points of Cone Mapping with the Application to Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grzegorz Gabor, Krzysztof Rykaczewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive conditions for the existence of fixed points of cone mappings
without assuming scalability of functions. Monotonicity and scalability are
often inseparable in the literature in the context of searching for fixed
points of interference mappings. In applications, such mappings are
approximated by non-negative neural networks. It turns out, however, that the
process of training non-negative networks requires imposing an artificial
constraint on the weights of the model. However, in the case of specific
non-negative data, it cannot be said that if the mapping is non-negative, it
has only non-negative weights. Therefore, we considered the problem of the
existence of fixed points for general neural networks, assuming the conditions
of tangency conditions with respect to specific cones. This does not relax the
physical assumptions, because even assuming that the input and output are to be
non-negative, the weights can have (small, but) less than zero values. Such
properties (often found in papers on the interpretability of weights of neural
networks) lead to the weakening of the assumptions about the monotonicity or
scalability of the mapping associated with the neural network. To the best of
our knowledge, this paper is the first to study this phenomenon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probable Domain Generalization via Quantile Risk Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kügelgen, Hamed Hassani, George J. Pappas, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) seeks predictors which perform well on unseen test
distributions by leveraging labeled training data from multiple related
distributions or domains. To achieve this, the standard formulation optimizes
for worst-case performance over the set of all possible domains. However, with
worst-case shifts very unlikely in practice, this generally leads to
overly-conservative solutions. In fact, a recent study found that no DG
algorithm outperformed empirical risk minimization in terms of average
performance. In this work, we argue that DG is neither a worst-case problem nor
an average-case problem, but rather a probabilistic one. To this end, we
propose a probabilistic framework for DG, which we call Probable Domain
Generalization, wherein our key idea is that distribution shifts seen during
training should inform us of probable shifts at test time. To realize this, we
explicitly relate training and test domains as draws from the same underlying
meta-distribution, and propose a new optimization problem -- Quantile Risk
Minimization (QRM) -- which requires that predictors generalize with high
probability. We then prove that QRM: (i) produces predictors that generalize to
new domains with a desired probability, given sufficiently many domains and
samples; and (ii) recovers the causal predictor as the desired probability of
generalization approaches one. In our experiments, we introduce a more holistic
quantile-focused evaluation protocol for DG, and show that our algorithms
outperform state-of-the-art baselines on real and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Preconditioners and their application to seismic wavefield
  processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Ravasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seismic data processing heavily relies on the solution of physics-driven
inverse problems. In the presence of unfavourable data acquisition conditions
(e.g., regular or irregular coarse sampling of sources and/or receivers), the
underlying inverse problem becomes very ill-posed and prior information is
required to obtain a satisfactory solution. Sparsity-promoting inversion,
coupled with fixed-basis sparsifying transforms, represent the go-to approach
for many processing tasks due to its simplicity of implementation and proven
successful application in a variety of acquisition scenarios. Leveraging the
ability of deep neural networks to find compact representations of complex,
multi-dimensional vector spaces, we propose to train an AutoEncoder network to
learn a direct mapping between the input seismic data and a representative
latent manifold. The trained decoder is subsequently used as a nonlinear
preconditioner for the physics-driven inverse problem at hand. Synthetic and
field data are presented for a variety of seismic processing tasks and the
proposed nonlinear, learned transformations are shown to outperform fixed-basis
transforms and convergence faster to the sought solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Robust Landmark-based Stent Tracking in X-ray Fluoroscopy <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luojie Huang, Yi<span class="highlight-author">kang Liu</span>, Li Chen, Eric Z Chen, Xiao Chen, Shanhui Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical procedures of angioplasty (i.e., open clogged coronary arteries),
devices such as balloons and stents need to be placed and expanded in arteries
under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,
the resulting images are often noisy. To check the correct placement of these
devices, typically multiple motion-compensated frames are averaged to enhance
the view. Therefore, device tracking is a necessary procedure for this purpose.
Even though angioplasty devices are designed to have radiopaque markers for the
ease of tracking, current methods struggle to deliver satisfactory results due
to the small marker size and complex scenes in angioplasty. In this paper, we
propose an end-to-end deep learning framework for single stent tracking, which
consists of three hierarchical modules: U-Net based landmark detection, ResNet
based stent proposal and feature extraction, and graph convolutional neural
network (GCN) based stent tracking that temporally aggregates both spatial
information and appearance features. The experiments show that our method
performs significantly better in detection compared with the state-of-the-art
point-based tracking models. In addition, its fast inference speed satisfies
clinical requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViGAT: Bottom-up event recognition and explanation in video using
  factorized graph attention network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Gkalelis, Dimitrios Daskalakis, Vasileios Mezaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Scale Radio Frequency Signal Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Boegner, Manbir Gulati, Garrett Vanhoy, Phillip Vallance, Bradley Comar, Silvija Kokalj-Filipovic, Craig Lennon, Robert D. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing datasets used to train deep learning models for narrowband radio
frequency (RF) signal classification lack enough diversity in signal types and
channel impairments to sufficiently assess model performance in the real world.
We introduce the Sig53 dataset consisting of 5 million synthetically-generated
samples from 53 different signal classes and expertly chosen impairments. We
also introduce TorchSig, a signals processing machine learning toolkit that can
be used to generate this dataset. TorchSig incorporates data handling
principles that are common to the vision domain, and it is meant to serve as an
open-source foundation for future signals machine learning research. Initial
experiments using the Sig53 dataset are conducted using state of the art (SoTA)
convolutional neural networks (ConvNets) and Transformers. These experiments
reveal Transformers outperform ConvNets without the need for additional
regularization or a ConvNet teacher, which is contrary to results from the
vision domain. Additional experiments demonstrate that TorchSig's
domain-specific data augmentations facilitate model training, which ultimately
benefits model performance. Finally, TorchSig supports on-the-fly synthetic
data creation at training time, thus enabling massive scale training sessions
with virtually unlimited datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stream-based active learning with linear models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of automated data collection schemes and the advances in
sensorics are increasing the amount of data we are able to monitor in
real-time. However, given the high annotation costs and the time required by
quality inspections, data is often available in an unlabeled form. This is
fostering the use of active learning for the development of soft sensors and
predictive models. In production, instead of performing random inspections to
obtain product information, labels are collected by evaluating the information
content of the unlabeled data. Several query strategy frameworks for regression
have been proposed in the literature but most of the focus has been dedicated
to the static pool-based scenario. In this work, we propose a new strategy for
the stream-based scenario, where instances are sequentially offered to the
learner, which must instantaneously decide whether to perform the quality check
to obtain the label or discard the instance. The approach is inspired by the
optimal experimental design theory and the iterative aspect of the
decision-making process is tackled by setting a threshold on the
informativeness of the unlabeled data points. The proposed approach is
evaluated using numerical simulations and the Tennessee Eastman Process
simulator. The results confirm that selecting the examples suggested by the
proposed algorithm allows for a faster reduction in the prediction error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Neural Network Training Method for Autonomous Driving Using
  Semi-Pseudo-Labels and 3D Data Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamas Matuszka, Daniel Kozma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training neural networks to perform 3D object detection for autonomous
driving requires a large amount of diverse annotated data. However, obtaining
training data with sufficient quality and quantity is expensive and sometimes
impossible due to human and sensor constraints. Therefore, a novel solution is
needed for extending current training methods to overcome this limitation and
enable accurate 3D object detection. Our solution for the above-mentioned
problem combines semi-pseudo-labeling and novel 3D augmentations. For
demonstrating the applicability of the proposed method, we have designed a
convolutional neural network for 3D object detection which can significantly
increase the detection range in comparison with the training data distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Solve Soft-Constrained Vehicle Routing Problems with
  Lagrangian Relaxation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyue Tang, Yangzhe Kong, Lemeng Pan, Choonmeng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle Routing Problems (VRPs) in real-world applications often come with
various constraints, therefore bring additional computational challenges to
exact solution methods or heuristic search approaches. The recent idea to learn
heuristic move patterns from sample data has become increasingly promising to
reduce solution developing costs. However, using learning-based approaches to
address more types of constrained VRP remains a challenge. The difficulty lies
in controlling for constraint violations while searching for optimal solutions.
To overcome this challenge, we propose a Reinforcement Learning based method to
solve soft-constrained VRPs by incorporating the Lagrangian relaxation
technique and using constrained policy optimization. We apply the method on
three common types of VRPs, the Travelling Salesman Problem with Time Windows
(TSPTW), the Capacitated VRP (CVRP) and the Capacitated VRP with Time Windows
(CVRPTW), to show the generalizability of the proposed method. After comparing
to existing RL-based methods and open-source heuristic solvers, we demonstrate
its competitive performance in finding solutions with a good balance in travel
distance, constraint violations and inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniHPF : Universal Healthcare Predictive Framework with Zero Domain
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyunghoon Hur, Jungwoo Oh, Junu Kim, Min Jae Lee, Eunbyeol Choi, Jiyoun Kim, Seong-Eun Moon, Young-Hak Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the abundance of Electronic Healthcare Records (EHR), its
heterogeneity restricts the utilization of medical data in building predictive
models. To address this challenge, we propose Universal Healthcare Predictive
Framework (UniHPF), which requires no medical domain knowledge and minimal
pre-processing for multiple prediction tasks. Experimental results demonstrate
that UniHPF is capable of building large-scale EHR models that can process any
form of medical data from distinct EHR systems. Our framework significantly
outperforms baseline models in multi-source learning tasks, including transfer
and pooled learning, while also showing comparable results when trained on a
single medical dataset. To empirically demonstrate the efficacy of our work, we
conducted extensive experiments using various datasets, model structures, and
tasks. We believe that our findings can provide helpful insights for further
research on the multi-source learning of EHRs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper (11pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated machine learning for borehole resistivity measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Shahriari, D. Pardo, S. Kargaran, T. Teijeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) offer a real-time solution for the inversion of
borehole resistivity measurements to approximate forward and inverse operators.
It is possible to use extremely large DNNs to approximate the operators, but it
demands a considerable training time. Moreover, evaluating the network after
training also requires a significant amount of memory and processing power. In
addition, we may overfit the model. In this work, we propose a scoring function
that accounts for the accuracy and size of the DNNs compared to a reference DNN
that provides a good approximation for the operators. Using this scoring
function, we use DNN architecture search algorithms to obtain a quasi-optimal
DNN smaller than the reference network; hence, it requires less computational
effort during training and evaluation. The quasi-optimal DNN delivers
comparable accuracy to the original large DNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Effect of Feedback Frequency in Interactive
  Reinforcement Learning for Robotic Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Harnack, Julie Pivin-Bachler, Nicolás Navarro-Guerrero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has become widely adopted in robot control.
Despite many successes, one major persisting problem can be very low data
efficiency. One solution is interactive feedback, which has been shown to speed
up RL considerably. As a result, there is an abundance of different strategies,
which are, however, primarily tested on discrete grid-world and small scale
optimal control scenarios. In the literature, there is no consensus about which
feedback frequency is optimal or at which time the feedback is most beneficial.
To resolve these discrepancies we isolate and quantify the effect of feedback
frequency in robotic tasks with continuous state and action spaces. The
experiments encompass inverse kinematics learning for robotic manipulator arms
of different complexity. We show that seemingly contradictory reported
phenomena occur at different complexity levels. Furthermore, our results
suggest that no single ideal feedback frequency exists. Rather that feedback
frequency should be changed as the agent's proficiency in the task increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neural Computing and Applications. Special Issue on Human-aligned
  Reinforcement Learning for Autonomous Agents and Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Fairness: from Principles to Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Bateni, Matthew C. Chan, Ray Eitel-Porter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper summarizes and evaluates various approaches, methods, and
techniques for pursuing fairness in artificial intelligence (AI) systems. It
examines the merits and shortcomings of these measures and proposes practical
guidelines for defining, measuring, and preventing bias in AI. In particular,
it cautions against some of the simplistic, yet common, methods for evaluating
bias in AI systems, and offers more sophisticated and effective alternatives.
The paper also addresses widespread controversies and confusions in the field
by providing a common language among different stakeholders of high-impact AI
systems. It describes various trade-offs involving AI fairness, and provides
practical recommendations for balancing them. It offers techniques for
evaluating the costs and benefits of fairness targets, and defines the role of
human judgment in setting these targets. This paper provides discussions and
guidelines for AI practitioners, organization leaders, and policymakers, as
well as various links to additional materials for a more technical audience.
Numerous real-world examples are provided to clarify the concepts, challenges,
and recommendations from a practical perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Journal Impact Factor and Peer <span class="highlight-title">Review</span> Thoroughness and Helpfulness: A
  Supervised Machine Learning Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Severin, Michaela Strinzel, Matthias Egger, Tiago Barros, Alexander Sokolov, Julia Vilstrup Mouatt, Stefan Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The journal impact factor (JIF) is often equated with journal quality and the
quality of the peer review of the papers submitted to the journal. We examined
the association between the content of peer review and JIF by analysing 10,000
peer review reports submitted to 1,644 medical and life sciences journals. Two
researchers hand-coded a random sample of 2,000 sentences. We then trained
machine learning models to classify all 187,240 sentences as contributing or
not contributing to content categories. We examined the association between ten
groups of journals defined by JIF deciles and the content of peer reviews using
linear mixed-effects models, adjusting for the length of the review. The JIF
ranged from 0.21 to 74.70. The length of peer reviews increased from the lowest
(median number of words 185) to the JIF group (387 words). The proportion of
sentences allocated to different content categories varied widely, even within
JIF groups. For thoroughness, sentences on 'Materials and Methods' were more
common in the highest JIF journals than in the lowest JIF group (difference of
7.8 percentage points; 95% CI 4.9 to 10.7%). The trend for 'Presentation and
Reporting' went in the opposite direction, with the highest JIF journals giving
less emphasis to such content (difference -8.9%; 95% CI -11.3 to -6.5%). For
helpfulness, reviews for higher JIF journals devoted less attention to
'Suggestion and Solution' and provided fewer Examples than lower impact factor
journals. No, or only small differences were evident for other content
categories. In conclusion, peer review in journals with higher JIF tends to be
more thorough in discussing the methods used but less helpful in terms of
suggesting solutions and providing examples. Differences were modest and
variability high, indicating that the JIF is a bad predictor for the quality of
peer review of an individual manuscript.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operating Envelopes under Probabilistic Electricity Demand and Solar
  Generation Forecasts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yi, Gregor Verbic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing penetration of distributed energy resources in low-voltage
networks is turning end-users from consumers to prosumers. However, the
incomplete smart meter rollout and paucity of smart meter data due to the
regulatory separation between retail and network service provision make active
distribution network management difficult. Furthermore, distribution network
operators oftentimes do not have access to real-time smart meter data, which
creates an additional challenge. For the lack of better solutions, they use
blanket rooftop solar export limits, leading to suboptimal outcomes. To address
this, we designed a conditional generative adversarial network (CGAN)-based
model to forecast household solar generation and electricity demand, which
serves as an input to chance-constrained optimal power flow used to compute
fair operating envelopes under uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the 11th Bulk Power Systems Dynamics and Control
  Symposium (IREP 2022), July 25-30, 2022, Banff, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Uniform Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as one of the most promising frameworks for
deep generative modeling. In this work, we explore the potential of non-uniform
diffusion models. We show that non-uniform diffusion leads to multi-scale
diffusion models which have similar structure to this of multi-scale
normalizing flows. We experimentally find that in the same or less training
time, the multi-scale diffusion model achieves better FID score than the
standard uniform diffusion model. More importantly, it generates samples $4.4$
times faster in $128\times 128$ resolution. The speed-up is expected to be
higher in higher resolutions where more scales are used. Moreover, we show that
non-uniform diffusion leads to a novel estimator for the conditional score
function which achieves on par performance with the state-of-the-art
conditional denoising estimator. Our theoretical and experimental findings are
accompanied by an open source library MSDiff which can facilitate further
research of non-uniform diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2111.13606</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised energy disaggregation via convolutional sparse coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Aarset, Andreas Habring, Martin Holler, Mario Mitter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, a method for unsupervised energy disaggregation in private
households equipped with smart meters is proposed. This method aims to classify
power consumption as active or passive, granting the ability to report on the
residents' activity and presence without direct interaction. This lays the
foundation for applications like non-intrusive health monitoring of private
homes.
  The proposed method is based on minimizing a suitable energy functional, for
which the iPALM (inertial proximal alternating linearized minimization)
algorithm is employed, demonstrating that various conditions guaranteeing
convergence are satisfied.
  In order to confirm feasibility of the proposed method, experiments on
semi-synthetic test data sets and a comparison to existing, supervised methods
are provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cancer Subtyping by Improved Transcriptomic Features Using Vector
  Quantized Variational Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Ziwei Yang, Lingwei Zhu, Guang Shi, Kun Yue, Takashi Matsubara, Shigehiko Kanaya, MD Altaf-Ul-Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defining and separating cancer subtypes is essential for facilitating
personalized therapy modality and prognosis of patients. The definition of
subtypes has been constantly recalibrated as a result of our deepened
understanding. During this recalibration, researchers often rely on clustering
of cancer data to provide an intuitive visual reference that could reveal the
intrinsic characteristics of subtypes. The data being clustered are often omics
data such as transcriptomics that have strong correlations to the underlying
biological mechanism. However, while existing studies have shown promising
results, they suffer from issues associated with omics data: sample scarcity
and high dimensionality. As such, existing methods often impose unrealistic
assumptions to extract useful features from the data while avoiding overfitting
to spurious correlations. In this paper, we propose to leverage a recent strong
generative model, Vector Quantized Variational AutoEncoder (VQ-VAE), to tackle
the data issues and extract informative latent features that are crucial to the
quality of subsequent clustering by retaining only information relevant to
reconstructing the input. VQ-VAE does not impose strict assumptions and hence
its latent features are better representations of the input, capable of
yielding superior clustering performance with any mainstream clustering method.
Extensive experiments and medical analysis on multiple datasets comprising 10
distinct cancers demonstrate the VQ-VAE clustering results can significantly
and robustly improve prognosis over prevalent subtyping systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Elisa Ricci, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D LiDAR semantic segmentation is fundamental for autonomous driving. Several
Unsupervised Domain Adaptation (UDA) methods for point cloud data have been
recently proposed to improve model generalization for different sensors and
environments. Researchers working on UDA problems in the image domain have
shown that sample mixing can mitigate domain shift. We propose a new approach
of sample mixing for point cloud UDA, namely Compositional Semantic Mix
(CoSMix), the first UDA approach for point cloud segmentation based on sample
mixing. CoSMix consists of a two-branch symmetric network that can process
labelled synthetic data (source) and real-world unlabelled point clouds
(target) concurrently. Each branch operates on one domain by mixing selected
pieces of data from the other one, and by using the semantic information
derived from source labels and target pseudo-labels. We evaluate CoSMix on two
large-scale datasets, showing that it outperforms state-of-the-art methods by a
large margin. Our code is available at
https://github.com/saltoricristiano/cosmix-uda.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Counterfactually Invariant Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Quinzan, Cecilia Casolo, Krikamol Muandet, Niki Kilbertus, Yucen Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to learn predictors that are invariant under
counterfactual changes of certain covariates. This method is useful when the
prediction target is causally influenced by covariates that should not affect
the predictor output. For instance, an object recognition model may be
influenced by position, orientation, or scale of the object itself. We address
the problem of training predictors that are explicitly counterfactually
invariant to changes of such covariates. We propose a model-agnostic
regularization term based on conditional kernel mean embeddings, to enforce
counterfactual invariance during training. We prove the soundness of our
method, which can handle mixed categorical and continuous multi-variate
attributes. Empirical results on synthetic and real-world data demonstrate the
efficacy of our method in a variety of settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ApHMM: Accelerating Profile Hidden Markov Models for Fast and
  Energy-Efficient Genome Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Firtina, Kamlesh Pillai, Gurpreet S. Kalsi, Bharathwaj Suresh, Damla Senol Cali, Jeremie Kim, Taha Shahroodi, Meryem Banu Cavlak, Joel Lindegger, Mohammed Alser, Juan Gómez Luna, Sreenivas Subramoney, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Profile hidden Markov models (pHMMs) are widely used in many bioinformatics
applications to accurately identify similarities between biological sequences
(e.g., DNA or protein sequences). PHMMs use a commonly-adopted and
highly-accurate method, called the Baum-Welch algorithm, to calculate these
similarities. However, the Baum-Welch algorithm is computationally expensive,
and existing works provide either software- or hardware-only solutions for a
fixed pHMM design. When we analyze the state-of-the-art works, we find that
there is a pressing need for a flexible, high-performant, and energy-efficient
hardware-software co-design to efficiently and effectively solve all the major
inefficiencies in the Baum-Welch algorithm for pHMMs.
  We propose ApHMM, the first flexible acceleration framework that can
significantly reduce computational and energy overheads of the Baum-Welch
algorithm for pHMMs. ApHMM leverages hardware-software co-design to solve the
major inefficiencies in the Baum-Welch algorithm by 1) designing a flexible
hardware to support different pHMMs designs, 2) exploiting the predictable data
dependency pattern in an on-chip memory with memoization techniques, 3) quickly
eliminating negligible computations with a hardware-based filter, and 4)
minimizing the redundant computations. We implement our 1) hardware-software
optimizations on a specialized hardware and 2) software optimizations for GPUs
to provide the first flexible Baum-Welch accelerator for pHMMs. ApHMM provides
significant speedups of 15.55x-260.03x, 1.83x-5.34x, and 27.97x compared to
CPU, GPU, and FPGA implementations of the Baum-Welch algorithm, respectively.
ApHMM outperforms the state-of-the-art CPU implementations of three important
bioinformatics applications, 1) error correction, 2) protein family search, and
3) multiple sequence alignment, by 1.29x-59.94x, 1.03x-1.75x, and 1.03x-1.95x,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D
  LiDAR Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Saltori, Evgeny Krivosheev, Stéphane Lathuilière, Nicu Sebe, Fabio Galasso, Giuseppe Fiameni, Elisa Ricci, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud semantic segmentation is fundamental for autonomous driving.
Most approaches in the literature neglect an important aspect, i.e., how to
deal with domain shift when handling dynamic scenes. This can significantly
hinder the navigation capabilities of self-driving vehicles. This paper
advances the state of the art in this research field. Our first contribution
consists in analysing a new unexplored scenario in point cloud segmentation,
namely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We
experimentally show that state-of-the-art methods have a rather limited ability
to adapt pre-trained deep network models to unseen domains in an online manner.
Our second contribution is an approach that relies on adaptive self-training
and geometric-feature propagation to adapt a pre-trained source model online
without requiring either source data or target labels. Our third contribution
is to study SF-OUDA in a challenging setup where source data is synthetic and
target data is point clouds captured in the real world. We use the recent
SynLiDAR dataset as a synthetic source and introduce two new synthetic (source)
datasets, which can stimulate future synthetic-to-real autonomous driving
research. Our experiments show the effectiveness of our segmentation approach
on thousands of real-world point clouds. Code and synthetic datasets are
available at https://github.com/saltoricristiano/gipso-sfouda.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Temporally and Spatially Local Spike-based Backpropagation Algorithm
  to Enable Training in Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Biswas, Vivek Saraswat, Udayan Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have emerged as a hardware efficient
architecture for classification tasks. The penalty of spikes-based encoding has
been the lack of a universal training mechanism performed entirely using
spikes. There have been several attempts to adopt the powerful backpropagation
(BP) technique used in non-spiking artificial neural networks (ANN): (1) SNNs
can be trained by externally computed numerical gradients. (2) A major
advancement toward native spike-based learning has been the use of approximate
Backpropagation using spike-time-dependent plasticity (STDP) with phased
forward/backward passes. However, the transfer of information between such
phases necessitates external memory and computational access. This is a
challenge for neuromorphic hardware implementations. In this paper, we propose
a stochastic SNN-based Back-Prop (SSNN-BP) algorithm that utilizes a composite
neuron to simultaneously compute the forward pass activations and backward pass
gradients explicitly with spikes. Although signed gradient values are a
challenge for spike-based representation, we tackle this by splitting the
gradient signal into positive and negative streams. The composite neuron
encodes information in the form of stochastic spike-trains and converts
Backpropagation weight updates into temporally and spatially local discrete
STDP-like spike coincidence updates compatible with hardware-friendly Resistive
Processing Units (RPUs). Furthermore, our method approaches BP ANN baseline
with sufficiently long spike-trains. Finally, we show that softmax
cross-entropy loss function can be implemented through inhibitory lateral
connections enforcing a Winner Take All (WTA) rule. Our SNN shows excellent
generalization through comparable performance to ANNs on the MNIST,
Fashion-MNIST and Extended MNIST datasets. Thus, SSNN-BP enables BP compatible
with purely spike-based neuromorphic hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Affect Analysis: Learning from Synthetic Data & Multi-Task
  Learning Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Li, Yifan Xu, Huanyu Wu, Dongrui Wu, Yingjie Yin, Jiajiong Cao, Jingting Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial affect analysis remains a challenging task with its setting
transitioned from lab-controlled to in-the-wild situations. In this paper, we
present novel frameworks to handle the two challenges in the 4th Affective
Behavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL)
Challenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL
challenge, we adopt the SMM-EmotionNet with a better ensemble strategy of
feature vectors. For LSD challenge, we propose respective methods to combat the
problems of single labels, imbalanced distribution, fine-tuning limitations,
and choice of model architectures. Experimental results on the official
validation sets from the competition demonstrated that our proposed approaches
outperformed baselines by a large margin. The code is available at
https://github.com/sylyoung/ABAW4-HUST-ANT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Latent Spaces of Generative Models for Medical Images using
  Unsupervised Methods <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Schön, Raghavendra Selvan, Jens Petersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as Generative Adversarial Networks (GANs) and
Variational Autoencoders (VAEs) play an increasingly important role in medical
image analysis. The latent spaces of these models often show semantically
meaningful directions corresponding to human-interpretable image
transformations. However, until now, their exploration for medical images has
been limited due to the requirement of supervised data. Several methods for
unsupervised discovery of interpretable directions in GAN latent spaces have
shown interesting results on natural images. This work explores the potential
of applying these techniques on medical images by training a GAN and a VAE on
thoracic CT scans and using an unsupervised method to discover interpretable
directions in the resulting latent space. We find several directions
corresponding to non-trivial image transformations, such as rotation or breast
size. Furthermore, the directions show that the generative models capture 3D
structure despite being presented only with 2D data. The results show that
unsupervised methods to discover interpretable directions in GANs generalize to
VAEs and can be applied to medical images. This opens a wide array of future
work using these methods in medical image analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at DGM4MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Auxiliary Text Query-modifier to Content-based Audio
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The amount of audio data available on public websites is growing rapidly, and
an efficient mechanism for accessing the desired data is necessary. We propose
a content-based audio retrieval method that can retrieve a target audio that is
similar to but slightly different from the query audio by introducing auxiliary
textual information which describes the difference between the query and target
audio. While the range of conventional content-based audio retrieval is limited
to audio that is similar to the query audio, the proposed method can adjust the
retrieval range by adding an embedding of the auxiliary text query-modifier to
the embedding of the query sample audio in a shared latent space. To evaluate
our method, we built a dataset comprising two different audio clips and the
text that describes the difference. The experimental results show that the
proposed method retrieves the paired audio more accurately than the baseline.
We also confirmed based on visualization that the proposed method obtains the
shared latent space in which the audio difference and the corresponding text
are represented as similar embedding vectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting data augmentation for subspace clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Abdolali, Nicolas Gillis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subspace clustering is the classical problem of clustering a collection of
data samples that approximately lie around several low-dimensional subspaces.
The current state-of-the-art approaches for this problem are based on the
self-expressive model which represents the samples as linear combination of
other samples. However, these approaches require sufficiently well-spread
samples for accurate representation which might not be necessarily accessible
in many applications. In this paper, we shed light on this commonly neglected
issue and argue that data distribution within each subspace plays a critical
role in the success of self-expressive models. Our proposed solution to tackle
this issue is motivated by the central role of data augmentation in the
generalization power of deep neural networks. We propose two subspace
clustering frameworks for both unsupervised and semi-supervised settings that
use augmented samples as an enlarged dictionary to improve the quality of the
self-expressive representation. We present an automatic augmentation strategy
using a few labeled samples for the semi-supervised problem relying on the fact
that the data samples lie in the union of multiple linear subspaces.
Experimental results confirm the effectiveness of data augmentation, as it
significantly improves the performance of general self-expressive models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages (including 10 of supplementary material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Agent-based Epidemiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Chopra, Alexander Rodríguez, Jayakumar Subramanian, Balaji Krishnamurthy, B. Aditya Prakash, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanistic simulators are an indispensable tool for epidemiology to explore
the behavior of complex, dynamic infections under varying conditions and
navigate uncertain environments. ODE-based models are the dominant paradigm
that enable fast simulations and are tractable to gradient-based optimization,
but make simplifying assumptions about population homogeneity. Agent-based
models (ABMs) are an increasingly popular alternative paradigm that can
represent the heterogeneity of contact interactions with granular detail and
agency of individual behavior. However, conventional ABM frameworks are not
differentiable and present challenges in scalability; due to which it is
non-trivial to connect them to auxiliary data sources easily. In this paper we
introduce GradABM which is a new scalable, fast and differentiable design for
ABMs. GradABM runs simulations in few seconds on commodity hardware and enables
fast forward and differentiable inverse simulations. This makes it amenable to
be merged with deep neural networks and seamlessly integrate heterogeneous data
sources to help with calibration, forecasting and policy evaluation. We
demonstrate the efficacy of GradABM via extensive experiments with real
COVID-19 and influenza datasets. We are optimistic this work will bring ABM and
AI communities closer together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Sequence Representations by Non-local Recurrent Neural Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Pei, Xin Feng, Canmiao Fu, Qiong Cao, Guangming Lu, Yu-Wing Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key challenge of sequence representation learning is to capture the
long-range temporal dependencies. Typical methods for supervised sequence
representation learning are built upon recurrent neural networks to capture
temporal dependencies. One potential limitation of these methods is that they
only model one-order information interactions explicitly between adjacent time
steps in a sequence, hence the high-order interactions between nonadjacent time
steps are not fully exploited. It greatly limits the capability of modeling the
long-range temporal dependencies since the temporal features learned by
one-order interactions cannot be maintained for a long term due to temporal
information dilution and gradient vanishing. To tackle this limitation, we
propose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence
representation learning, which performs non-local operations \MR{by means of
self-attention mechanism} to learn full-order interactions within a sliding
temporal memory block and models global interactions between memory blocks in a
gated recurrent manner. Consequently, our model is able to capture long-range
dependencies. Besides, the latent high-level features contained in high-order
interactions can be distilled by our model. We validate the effectiveness and
generalization of our NRNM on three types of sequence applications across
different modalities, including sequence classification, step-wise sequential
prediction and sequence similarity learning. Our model compares favorably
against other state-of-the-art methods specifically designed for each of these
sequence applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared in International Journal of Computer Vision (IJCV).
  arXiv admin note: substantial text overlap with arXiv:1908.09535</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correntropy-Based Logistic Regression with Automatic Relevance
  Determination for Robust Sparse Brain Activity Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Li, Badong Chen, Yuxi Shi, Natsue Yoshimura, Yasuharu Koike
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have utilized sparse classifications to predict categorical
variables from high-dimensional brain activity signals to expose human's
intentions and mental states, selecting the relevant features automatically in
the model training process. However, existing sparse classification models will
likely be prone to the performance degradation which is caused by noise
inherent in the brain recordings. To address this issue, we aim to propose a
new robust and sparse classification algorithm in this study. To this end, we
introduce the correntropy learning framework into the automatic relevance
determination based sparse classification model, proposing a new
correntropy-based robust sparse logistic regression algorithm. To demonstrate
the superior brain activity decoding performance of the proposed algorithm, we
evaluate it on a synthetic dataset, an electroencephalogram (EEG) dataset, and
a functional magnetic resonance imaging (fMRI) dataset. The extensive
experimental results confirm that not only the proposed method can achieve
higher classification accuracy in a noisy and high-dimensional classification
task, but also it would select those more informative features for the decoding
scenarios. Integrating the correntropy learning approach with the automatic
relevance determination technique will significantly improve the robustness
with respect to the noise, leading to more adequate robust sparse brain
decoding algorithm. It provides a more powerful approach in the real-world
brain activity decoding and the brain-computer interfaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic dimension estimation for discrete metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iuri Macocco, Aldo Glielmo, Jacopo Grilli, Alessandro Laio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real world-datasets characterized by discrete features are ubiquitous: from
categorical surveys to clinical questionnaires, from unweighted networks to DNA
sequences. Nevertheless, the most common unsupervised dimensional reduction
methods are designed for continuous spaces, and their use for discrete spaces
can lead to errors and biases. In this letter we introduce an algorithm to
infer the intrinsic dimension (ID) of datasets embedded in discrete spaces. We
demonstrate its accuracy on benchmark datasets, and we apply it to analyze a
metagenomic dataset for species fingerprinting, finding a surprisingly small
ID, of order 2. This suggests that evolutive pressure acts on a low-dimensional
manifold despite the high-dimensionality of sequences' space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RevTeX4.2, 12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Quantized Training of Gradient Boosting Decision Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shi, Guolin Ke, Zhuoming Chen, Shuxin Zheng, <span class="highlight-author">Tie-Yan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed significant success in Gradient Boosting Decision
Trees (GBDT) for a wide range of machine learning applications. Generally, a
consensus about GBDT's training algorithms is gradients and statistics are
computed based on high-precision floating points. In this paper, we investigate
an essentially important question which has been largely ignored by the
previous literature: how many bits are needed for representing gradients in
training GBDT? To solve this mystery, we propose to quantize all the
high-precision gradients in a very simple yet effective way in the GBDT's
training algorithm. Surprisingly, both our theoretical analysis and empirical
studies show that the necessary precisions of gradients without hurting any
performance can be quite low, e.g., 2 or 3 bits. With low-precision gradients,
most arithmetic operations in GBDT training can be replaced by integer
operations of 8, 16, or 32 bits. Promisingly, these findings may pave the way
for much more efficient training of GBDT from several aspects: (1) speeding up
the computation of gradient statistics in histograms; (2) compressing the
communication cost of high-precision statistical information during distributed
training; (3) the inspiration of utilization and development of hardware
architectures which well support low-precision computation for GBDT training.
Benchmarked on CPU, GPU, and distributed clusters, we observe up to 2$\times$
speedup of our simple quantization strategy compared with SOTA GBDT systems on
extensive datasets, demonstrating the effectiveness and potential of the
low-precision training of GBDT. The code will be released to the official
repository of LightGBM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Data Driven Inverse Text Normalization using Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laxmi Pandey, Debjyoti Paul, Pooja Chitkara, Yutong Pang, Xuedong Zhang, Kjell Schubert, Mark Chou, Shu Liu, Yatharth Saraf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable and Robust Deep Learning Algorithm for Atrial Fibrillation
  Diagnosis Across Ethnicities, Ages and Sexes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shany Biton, Mohsin Aldhafeeri, Erez Marcusohn, Kenta Tsutsui, Tom Szwagier, Adi Elias, Julien Oster, Jean Marc Sellal, Mahmoud Suleiman, Joachim A. Behar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To drive health innovation that meets the needs of all and democratize
healthcare, there is a need to assess the generalization performance of deep
learning (DL) algorithms across various distribution shifts to ensure that
these algorithms are robust. This retrospective study is, to the best of our
knowledge, the first to develop and assess the generalization performance of a
deep learning (DL) model for AF events detection from long term beat-to-beat
intervals across ethnicities, ages and sexes. The new recurrent DL model,
denoted ArNet2, was developed on a large retrospective dataset of 2,147
patients totaling 51,386 hours of continuous electrocardiogram (ECG). The
models generalization was evaluated on manually annotated test sets from four
centers (USA, Israel, Japan and China) totaling 402 patients. The model was
further validated on a retrospective dataset of 1,730 consecutives Holter
recordings from the Rambam Hospital Holter clinic, Haifa, Israel. The model
outperformed benchmark state-of-the-art models and generalized well across
ethnicities, ages and sexes. Performance was higher for female than male and
young adults (less than 60 years old) and showed some differences across
ethnicities. The main finding explaining these variations was an impairment in
performance in groups with a higher prevalence of atrial flutter (AFL). Our
findings on the relative performance of ArNet2 across groups may have clinical
implications on the choice of the preferred AF examination method to use
relative to the group of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExoSGAN and ExoACGAN: Exoplanet Detection using Adversarial Training
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cicy K Agnes, Akthar Naveed V, Anitha Mary M O Chacko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoplanet detection opens the door to the discovery of new habitable worlds
and helps us understand how planets were formed. With the objective of finding
earth-like habitable planets, NASA launched Kepler space telescope and its
follow up mission K2. The advancement of observation capabilities has increased
the range of fresh data available for research, and manually handling them is
both time-consuming and difficult. Machine learning and deep learning
techniques can greatly assist in lowering human efforts to process the vast
array of data produced by the modern instruments of these exoplanet programs in
an economical and unbiased manner. However, care should be taken to detect all
the exoplanets precisely while simultaneously minimizing the misclassification
of non-exoplanet stars. In this paper, we utilize two variations of generative
adversarial networks, namely semi-supervised generative adversarial networks
and auxiliary classifier generative adversarial networks, to detect transiting
exoplanets in K2 data. We find that the usage of these models can be helpful
for the classification of stars with exoplanets. Both of our techniques are
able to categorize the light curves with a recall and precision of 1.00 on the
test data. Our semi-supervised technique is beneficial to solve the cumbersome
task of creating a labeled dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multigraph Topology Design for Cross-Silo Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh X. Nguyen, Tuong Do, Hien Nguyen, Vuong Pham, Toan Tran, Erman Tjiputra, Quang Tran, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-silo federated learning utilizes a few hundred reliable data silos with
high-speed access links to jointly train a model. While this approach becomes a
popular setting in federated learning, designing a robust topology to reduce
the training time is still an open problem. In this paper, we present a new
multigraph topology for cross-silo federated learning. We first construct the
multigraph using the overlay graph. We then parse this multigraph into
different simple graphs with isolated nodes. The existence of isolated nodes
allows us to perform model aggregation without waiting for other nodes, hence
reducing the training time. We further propose a new distributed learning
algorithm to use with our multigraph topology. The intensive experiments on
public datasets show that our proposed method significantly reduces the
training time compared with recent state-of-the-art topologies while ensuring
convergence and maintaining the model's accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDM: Iterative Distribution Matching for Communication-Efficient
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu, Cho-Jui Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning~(FL) has recently attracted increasing attention from
academia and industry, with the ultimate goal of achieving collaborative
training under privacy and communication constraints. Existing iterative model
averaging based FL algorithms require a large number of communication rounds to
obtain a well-performed model due to extremely unbalanced and non-i.i.d data
partitioning among different clients. Thus, we propose FedDM to build the
global training objective from multiple local surrogate functions, which
enables the server to gain a more global view of the loss landscape. In detail,
we construct synthetic sets of data on each client to locally match the loss
landscape from original data through distribution matching. FedDM reduces
communication rounds and improves model quality by transmitting more
informative and smaller synthesized data compared with unwieldy model weights.
We conduct extensive experiments on three image classification datasets, and
results show that our method can outperform other FL counterparts in terms of
efficiency and model performance. Moreover, we demonstrate that FedDM can be
adapted to preserve differential privacy with Gaussian mechanism and train a
better model under the same privacy budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Adaptation via Conjugate Pseudo-labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachin Goyal, Mingjie Sun, Aditi Raghunathan, Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) refers to adapting neural networks to distribution
shifts, with access to only the unlabeled test samples from the new domain at
test-time. Prior TTA methods optimize over unsupervised objectives such as the
entropy of model predictions in TENT [Wang et al., 2021], but it is unclear
what exactly makes a good TTA loss. In this paper, we start by presenting a
surprising phenomenon: if we attempt to meta-learn the best possible TTA loss
over a wide class of functions, then we recover a function that is remarkably
similar to (a temperature-scaled version of) the softmax-entropy employed by
TENT. This only holds, however, if the classifier we are adapting is trained
via cross-entropy; if trained via squared loss, a different best TTA loss
emerges. To explain this phenomenon, we analyze TTA through the lens of the
training losses's convex conjugate. We show that under natural conditions, this
(unsupervised) conjugate function can be viewed as a good local approximation
to the original supervised loss and indeed, it recovers the best losses found
by meta-learning. This leads to a generic recipe that can be used to find a
good TTA loss for any given supervised training loss function of a general
class. Empirically, our approach consistently dominates other baselines over a
wide range of benchmarks. Our approach is particularly of interest when applied
to classifiers trained with novel loss functions, e.g., the recently-proposed
PolyLoss, where it differs substantially from (and outperforms) an
entropy-based loss. Further, we show that our approach can also be interpreted
as a kind of self-training using a very specific soft label, which we refer to
as the conjugate pseudolabel. Overall, our method provides a broad framework
for better understanding and improving test-time adaptation. Code is available
at https://github.com/locuslab/tta_conjugate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DC-BENCH: <span class="highlight-title">Dataset</span> Condensation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Cui, Ruochen Wang, Si Si, Cho-Jui Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset Condensation is a newly emerging technique aiming at learning a tiny
dataset that captures the rich information encoded in the original dataset. As
the size of datasets contemporary machine learning models rely on becomes
increasingly large, condensation methods become a prominent direction for
accelerating network training and reducing data storage. Despite numerous
methods have been proposed in this rapidly growing field, evaluating and
comparing different condensation methods is non-trivial and still remains an
open issue. The quality of condensed dataset are often shadowed by many
critical contributing factors to the end performance, such as data augmentation
and model architectures. The lack of a systematic way to evaluate and compare
condensation methods not only hinders our understanding of existing techniques,
but also discourages practical usage of the synthesized datasets. This work
provides the first large-scale standardized benchmark on Dataset Condensation.
It consists of a suite of evaluations to comprehensively reflect the
generability and effectiveness of condensation methods through the lens of
their generated dataset. Leveraging this benchmark, we conduct a large-scale
study of current condensation methods, and report many insightful findings that
open up new possibilities for future development. The benchmark library,
including evaluators, baseline methods, and generated datasets, is open-sourced
to facilitate future research and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doge Tickets: Uncovering Domain-general Language Models by Playing
  Lottery Tickets <span class="chip">NLPCC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yang, Chen Zhang, Benyou Wang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-parameterized models, typically pre-trained language models (LMs), have
shown an appealing expressive power due to their small learning bias. However,
the huge learning capacity of LMs can also lead to large learning variance. In
a pilot study, we find that, when faced with multiple domains, a critical
portion of parameters behave unexpectedly in a domain-specific manner while
others behave in a domain-general one. Motivated by this phenomenon, we for the
first time posit that domain-general parameters can underpin a domain-general
LM that can be derived from the original LM. To uncover the domain-general LM,
we propose to identify domain-general parameters by playing lottery tickets
(dubbed doge tickets). In order to intervene the lottery, we propose a
domain-general score, which depicts how domain-invariant a parameter is by
associating it with the variance. Comprehensive experiments are conducted on
the Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets
obtains an improved out-of-domain generalization in comparison with a range of
competitive baselines. Analysis results further hint the existence of
domain-general parameters and the performance consistency of doge tickets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NLPCC 2022. Code is available at
  https://github.com/Ylily1015/DogeTickets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVHA: Explainable Vision System for Hardware Testing and Assurance -- An
  <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mahfuz Al Hasan, Mohammad Tahsin Mostafiz, Thomas An Le, Jake Julia, Nidish Vashistha, Shayan Taheri, Navid Asadizanjani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the ever-growing demands for electronic chips in different sectors the
semiconductor companies have been mandated to offshore their manufacturing
processes. This unwanted matter has made security and trustworthiness of their
fabricated chips concerning and caused creation of hardware attacks. In this
condition, different entities in the semiconductor supply chain can act
maliciously and execute an attack on the design computing layers, from devices
to systems. Our attack is a hardware Trojan that is inserted during mask
generation/fabrication in an untrusted foundry. The Trojan leaves a footprint
in the fabricated through addition, deletion, or change of design cells. In
order to tackle this problem, we propose Explainable Vision System for Hardware
Testing and Assurance (EVHA) in this work that can detect the smallest possible
change to a design in a low-cost, accurate, and fast manner. The inputs to this
system are Scanning Electron Microscopy (SEM) images acquired from the
Integrated Circuits (ICs) under examination. The system output is determination
of IC status in terms of having any defect and/or hardware Trojan through
addition, deletion, or change in the design cells at the cell-level. This
article provides an overview on the design, development, implementation, and
analysis of our defense system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please contact Dr. Shayan Taheri for any questions and/or comments
  regarding the paper arXiv submission at: "www.shayan-taheri.com". The Paper
  Initial Submission: The ACM Journal on Emerging Technologies in Computing
  Systems (JETC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from few examples: Classifying sex from retinal images via deep
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Berk, Gulcenur Ozturan, Parsa Delavari, David Maberley, Özgür Yılmaz, Ipek Oruc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has seen tremendous interest in medical imaging, particularly
in the use of convolutional neural networks (CNNs) for developing automated
diagnostic tools. The facility of its non-invasive acquisition makes retinal
fundus imaging amenable to such automated approaches. Recent work in analyzing
fundus images using CNNs relies on access to massive data for training and
validation - hundreds of thousands of images. However, data residency and data
privacy restrictions stymie the applicability of this approach in medical
settings where patient confidentiality is a mandate. Here, we showcase results
for the performance of DL on small datasets to classify patient sex from fundus
images - a trait thought not to be present or quantifiable in fundus images
until recently. We fine-tune a Resnet-152 model whose last layer has been
modified for binary classification. In several experiments, we assess
performance in the small dataset context using one private (DOVS) and one
public (ODIR) data source. Our models, developed using approximately 2500
fundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]).
This corresponds to a mere 25% decrease in performance despite a nearly
1000-fold decrease in the dataset size compared to prior work in the
literature. Even with a hard task like sex categorization from retinal images,
we find that classification is possible with very small datasets. Additionally,
we perform domain adaptation experiments between DOVS and ODIR; explore the
effect of data curation on training and generalizability; and investigate model
ensembling to maximize CNN classifier performance in the context of small
development datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning to Estimate Permeability using Geophysical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. K. Mudunuru, E. L. D. Cromwell, H. Wang, X. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-lapse electrical resistivity tomography (ERT) is a popular geophysical
method to estimate three-dimensional (3D) permeability fields from electrical
potential difference measurements. Traditional inversion and data assimilation
methods are used to ingest this ERT data into hydrogeophysical models to
estimate permeability. Due to ill-posedness and the curse of dimensionality,
existing inversion strategies provide poor estimates and low resolution of the
3D permeability field. Recent advances in deep learning provide us with
powerful algorithms to overcome this challenge. This paper presents a deep
learning (DL) framework to estimate the 3D subsurface permeability from
time-lapse ERT data. To test the feasibility of the proposed framework, we
train DL-enabled inverse models on simulation data. Subsurface process models
based on hydrogeophysics are used to generate this synthetic data for deep
learning analyses. Results show that proposed weak supervised learning can
capture salient spatial features in the 3D permeability field. Quantitatively,
the average mean squared error (in terms of the natural log) on the strongly
labeled training, validation, and test datasets is less than 0.5. The R2-score
(global metric) is greater than 0.75, and the percent error in each cell (local
metric) is less than 10%. Finally, an added benefit in terms of computational
cost is that the proposed DL-based inverse model is at least O(104) times
faster than running a forward model. Note that traditional inversion may
require multiple forward model simulations (e.g., in the order of 10 to 1000),
which are very expensive. This computational savings (O(105) - O(107)) makes
the proposed DL-based inverse model attractive for subsurface imaging and
real-time ERT monitoring applications due to fast and yet reasonably accurate
estimations of the permeability field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Connect, Not Collapse: Explaining <span class="highlight-title">Contrastive Learning</span> for Unsupervised
  Domain Adaptation <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kendrick Shen, Robbie Jones, Ananya Kumar, Sang Michael Xie, Jeff Z. HaoChen, Tengyu Ma, <span class="highlight-author">Percy Liang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider unsupervised domain adaptation (UDA), where labeled data from a
source domain (e.g., photographs) and unlabeled data from a target domain
(e.g., sketches) are used to learn a classifier for the target domain.
Conventional UDA methods (e.g., domain adversarial training) learn
domain-invariant features to improve generalization to the target domain. In
this paper, we show that contrastive pre-training, which learns features on
unlabeled source and target data and then fine-tunes on labeled source data, is
competitive with strong UDA methods. However, we find that contrastive
pre-training does not learn domain-invariant features, diverging from
conventional UDA intuitions. We show theoretically that contrastive
pre-training can learn features that vary subtantially across domains but still
generalize to the target domain, by disentangling domain and class information.
Our results suggest that domain invariance is not necessary for UDA. We
empirically validate our theory on benchmark vision datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022 (Long Talk)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StARformer: <span class="highlight-title">Transformer</span> with State-Action-Reward Representations for
  Visual Reinforcement Learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghuan Shang, Kumara Kahatapitiya, Xiang Li, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) can be considered as a sequence modeling task:
given a sequence of past state-action-reward experiences, an agent predicts a
sequence of next actions. In this work, we propose State-Action-Reward
Transformer (StARformer) for visual RL, which explicitly models short-term
state-action-reward representations (StAR-representations), essentially
introducing a Markovian-like inductive bias to improve long-term modeling. Our
approach first extracts StAR-representations by self-attending image state
patches, action, and reward tokens within a short temporal window. These are
then combined with pure image state representations -- extracted as
convolutional features, to perform self-attention over the whole sequence. Our
experiments show that StARformer outperforms the state-of-the-art
Transformer-based method on image-based Atari and DeepMind Control Suite
benchmarks, in both offline-RL and imitation learning settings. StARformer is
also more compliant with longer sequences of inputs. Our code is available at
https://github.com/elicassion/StARformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Our code is available at
  https://github.com/elicassion/StARformer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-Deform-Subtract: An Interventional Framework for Explaining Object
  Differences <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cian Eastwood, Li Nanbo, Christopher K. I. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two object images, how can we explain their differences in terms of the
underlying object properties? To address this question, we propose
Align-Deform-Subtract (ADS) -- an interventional framework for explaining
object differences. By leveraging semantic alignments in image-space as
counterfactual interventions on the underlying object properties, ADS
iteratively quantifies and removes differences in object properties. The result
is a set of "disentangled" error measures which explain object differences in
terms of the underlying properties. Experiments on real and synthetic data
illustrate the efficacy of the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 Workshop on Objects, Structure and Causality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> 3D Equivariant Molecular Graph <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining molecular representation models without labels is fundamental to
various applications. Conventional methods mainly process 2D molecular graphs
and focus solely on 2D tasks, making their pretrained models incapable of
characterizing 3D geometry and thus defective for downstream 3D tasks. In this
work, we tackle 3D molecular pretraining in a complete and novel sense. In
particular, we first propose to adopt an equivariant energy-based model as the
backbone for pretraining, which enjoys the merit of fulfilling the symmetry of
3D space. Then we develop a node-level pretraining loss for force prediction,
where we further exploit the Riemann-Gaussian distribution to ensure the loss
to be E(3)-invariant, enabling more robustness. Moreover, a graph-level noise
scale prediction task is also leveraged to further promote the eventual
performance. We evaluate our model pretrained from a large-scale 3D dataset
GEOM-QM9 on two challenging 3D benchmarks: MD17 and QM9. The experimental
results support the better efficacy of our method against current
state-of-the-art pretraining approaches, and verify the validity of our design
for each proposed component.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Action Translator for Meta Reinforcement Learning on
  Sparse-Reward Tasks <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Guo, Qiucheng Wu, Honglak Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of
training tasks simultaneously and quickly adapting to new tasks. It requires
massive amounts of data drawn from training tasks to infer the common structure
shared among tasks. Without heavy reward engineering, the sparse rewards in
long-horizon tasks exacerbate the problem of sample efficiency in meta-RL.
Another challenge in meta-RL is the discrepancy of difficulty level among
tasks, which might cause one easy task dominating learning of the shared policy
and thus preclude policy adaptation to new tasks. This work introduces a novel
objective function to learn an action translator among training tasks. We
theoretically verify that the value of the transferred policy with the action
translator can be close to the value of the source policy and our objective
function (approximately) upper bounds the value difference. We propose to
combine the action translator with context-based meta-RL algorithms for better
data collection and more efficient exploration during meta-training. Our
approach empirically improves the sample efficiency and performance of meta-RL
algorithms on sparse-reward tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contingency-constrained economic dispatch with safe reinforcement
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Eichelbeck, Hannah Markgraf, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future power systems will rely heavily on micro grids with a high share of
decentralised renewable energy sources and energy storage systems. The high
complexity and uncertainty in this context might make conventional power
dispatch strategies infeasible. Reinforcement-learning based (RL) controllers
can address this challenge, however, cannot themselves provide safety
guarantees, preventing their deployment in practice. To overcome this
limitation, we propose a formally validated RL controller for economic
dispatch. We extend conventional constraints by a time-dependent constraint
encoding the islanding contingency. The contingency constraint is computed
using set-based backwards reachability analysis and actions of the RL agent are
verified through a safety layer. Unsafe actions are projected into the safe
action space while leveraging constrained zonotope set representations for
computational efficiency. The developed approach is demonstrated on a
residential use case using real-world measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Deep Random Vortex Method for Simulation and Inference of Navier-Stokes
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhang, Peiyan Hu, Qi Meng, Yue Wang, Rongchan Zhu, Bingguang Chen, Zhi-Ming Ma, <span class="highlight-author">Tie-Yan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navier-Stokes equations are significant partial differential equations that
describe the motion of fluids such as liquids and air. Due to the importance of
Navier-Stokes equations, the development on efficient numerical schemes is
important for both science and engineer. Recently, with the development of AI
techniques, several approaches have been designed to integrate deep neural
networks in simulating and inferring the fluid dynamics governed by
incompressible Navier-Stokes equations, which can accelerate the simulation or
inferring process in a mesh-free and differentiable way. In this paper, we
point out that the capability of existing deep Navier-Stokes informed methods
is limited to handle non-smooth or fractional equations, which are two critical
situations in reality. To this end, we propose the \emph{Deep Random Vortex
Method} (DRVM), which combines the neural network with a random vortex dynamics
system equivalent to the Navier-Stokes equation. Specifically, the random
vortex dynamics motivates a Monte Carlo based loss function for training the
neural network, which avoids the calculation of derivatives through
auto-differentiation. Therefore, DRVM not only can efficiently solve
Navier-Stokes equations involving rough path, non-differentiable initial
conditions and fractional operators, but also inherits the mesh-free and
differentiable benefits of the deep-learning-based solver. We conduct
experiments on the Cauchy problem, parametric solver learning, and the inverse
problem of both 2-d and 3-d incompressible Navier-Stokes equations. The
proposed method achieves accurate results for simulation and inference of
Navier-Stokes equations. Especially for the cases that include singular initial
conditions, DRVM significantly outperforms existing PINN method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via
  Speech-Visage Feature Selection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to reconstruct speech from a silent talking face
video. Recent studies have shown impressive performance on synthesizing speech
from silent talking face videos. However, they have not explicitly considered
on varying identity characteristics of different speakers, which place a
challenge in the video-to-speech synthesis, and this becomes more critical in
unseen-speaker settings. Our approach is to separate the speech content and the
visage-style from a given silent talking face video. By guiding the model to
independently focus on modeling the two representations, we can obtain the
speech of high intelligibility from the model even when the input video of an
unseen subject is given. To this end, we introduce speech-visage selection that
separates the speech content and the speaker identity from the visual features
of the input video. The disentangled representations are jointly incorporated
to synthesize speech through visage-style based synthesizer which generates
speech by coating the visage-styles while maintaining the speech content. Thus,
the proposed framework brings the advantage of synthesizing the speech
containing the right content even with the silent talking face video of an
unseen subject. We validate the effectiveness of the proposed framework on the
GRID, TCD-TIMIT volunteer, and LRW datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Simulation-Based Inference in Cosmology with Bayesian Neural
  Networks <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Lemos, Miles Cranmer, Muntazir Abidi, ChangHoon Hahn, Michael Eickenberg, Elena Massara, David Yallup, Shirley Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference (SBI) is rapidly establishing itself as a standard
machine learning technique for analyzing data in cosmological surveys. Despite
continual improvements to the quality of density estimation by learned models,
applications of such techniques to real data are entirely reliant on the
generalization power of neural networks far outside the training distribution,
which is mostly unconstrained. Due to the imperfections in scientist-created
simulations, and the large computational expense of generating all possible
parameter combinations, SBI methods in cosmology are vulnerable to such
generalization issues. Here, we discuss the effects of both issues, and show
how using a Bayesian neural network framework for training SBI can mitigate
biases, and result in more reliable inference outside the training set. We
introduce cosmoSWAG, the first application of Stochastic Weight Averaging to
cosmology, and apply it to SBI trained for inference on the cosmic microwave
background.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted at the ML4Astro Machine Learning for
  Astrophysics Workshop at the Thirty-ninth International Conference on Machine
  Learning (ICML 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-based Data Preparation and Data Analytics in Healthcare: The Case of
  Diabetes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marianna Maranghi, Aris Anagnostopoulos, Irene Cannistraci, Ioannis Chatzigiannakis, Federico Croce, Giulia Di Teodoro, Michele Gentile, Giorgio Grani, Maurizio Lenzerini, Stefano Leonardi, Andrea Mastropietro, Laura Palagi, Massimiliano Pappa, Riccardo Rosati, Riccardo Valentini, Paola Velardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Associazione Medici Diabetologi (AMD) collects and manages one of the
largest worldwide-available collections of diabetic patient records, also known
as the AMD database. This paper presents the initial results of an ongoing
project whose focus is the application of Artificial Intelligence and Machine
Learning techniques for conceptualizing, cleaning, and analyzing such an
important and valuable dataset, with the goal of providing predictive insights
to better support diabetologists in their diagnostic and therapeutic choices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work has been presented at the conference Ital-IA 2022
  (https://www.ital-ia2022.it/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSCALE: Latent Space Clustering-Based Active Learning for Node
  Classification <span class="chip">ECML-PKDD 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.07065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.07065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Liu, Yiwei Wang, Bryan Hooi, Renchi Yang, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Node classification on graphs is an important task in many practical domains.
It usually requires labels for training, which can be difficult or expensive to
obtain in practice. Given a budget for labelling, active learning aims to
improve performance by carefully choosing which nodes to label. Previous graph
active learning methods learn representations using labelled nodes and select
some unlabelled nodes for label acquisition. However, they do not fully utilize
the representation power present in unlabelled nodes. We argue that the
representation power in unlabelled nodes can be useful for active learning and
for further improving performance of active learning for node classification.
In this paper, we propose a latent space clustering-based active learning
framework for node classification (LSCALE), where we fully utilize the
representation power in both labelled and unlabelled nodes. Specifically, to
select nodes for labelling, our framework uses the K-Medoids clustering
algorithm on a latent space based on a dynamic combination of both unsupervised
features and supervised features. In addition, we design an incremental
clustering module to avoid redundancy between nodes selected at different
steps. Extensive experiments on five datasets show that our proposed framework
LSCALE consistently and significantly outperforms the stateof-the-art
approaches by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML-PKDD 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing strategies and <span class="highlight-title">dataset</span>s for facial representation learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.16554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.16554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Bulat, Shiyang Cheng, Jing Yang, Andrew Garbett, Enrique Sanchez, Georgios Tzimiropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What is the best way to learn a universal face representation? Recent work on
Deep Learning in the area of face analysis has focused on supervised learning
for specific tasks of interest (e.g. face recognition, facial landmark
localization etc.) but has overlooked the overarching question of how to find a
facial representation that can be readily adapted to several facial analysis
tasks and datasets. To this end, we make the following 4 contributions: (a) we
introduce, for the first time, a comprehensive evaluation benchmark for facial
representation learning consisting of 5 important face analysis tasks. (b) We
systematically investigate two ways of large-scale representation learning
applied to faces: supervised and unsupervised pre-training. Importantly, we
focus our evaluations on the case of few-shot facial learning. (c) We
investigate important properties of the training datasets including their size
and quality (labelled, unlabelled or even uncurated). (d) To draw our
conclusions, we conducted a very large number of experiments. Our main two
findings are: (1) Unsupervised pre-training on completely in-the-wild,
uncurated data provides consistent and, in some cases, significant accuracy
improvements for all facial tasks considered. (2) Many existing facial video
datasets seem to have a large amount of redundancy. We will release code, and
pre-trained models to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextformer: A <span class="highlight-title">Transformer</span> with Spatio-Channel Attention for Context
  Modeling in Learned Image Compression <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Burakhan Koyuncu, Han Gao, Atanas Boev, Georgii Gaikov, Elena Alshina, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entropy modeling is a key component for high-performance image compression
algorithms. Recent developments in autoregressive context modeling helped
learning-based methods to surpass their classical counterparts. However, the
performance of those models can be further improved due to the underexploited
spatio-channel dependencies in latent space, and the suboptimal implementation
of context adaptivity. Inspired by the adaptive characteristics of the
transformers, we propose a transformer-based context model, named
Contextformer, which generalizes the de facto standard attention mechanism to
spatio-channel attention. We replace the context model of a modern compression
framework with the Contextformer and test it on the widely used Kodak,
CLIC2020, and Tecnick image datasets. Our experimental results show that the
proposed model provides up to 11% rate savings compared to the standard
Versatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various
learning-based models in terms of PSNR and MS-SSIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022; 31 pages (14 main paper + References + 13
  Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOSTER: Feature Boosting and Compression for Class-Incremental Learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to learn new concepts continually is necessary in this
ever-changing world. However, deep neural networks suffer from catastrophic
forgetting when learning new categories. Many works have been proposed to
alleviate this phenomenon, whereas most of them either fall into the
stability-plasticity dilemma or take too much computation or storage overhead.
Inspired by the gradient boosting algorithm to gradually fit the residuals
between the target model and the previous ensemble model, we propose a novel
two-stage learning paradigm FOSTER, empowering the model to learn new
categories adaptively. Specifically, we first dynamically expand new modules to
fit the residuals between the target and the output of the original model.
Next, we remove redundant parameters and feature dimensions through an
effective distillation strategy to maintain the single backbone model. We
validate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different
settings. Experimental results show that our method achieves state-of-the-art
performance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Code is available at:
  https://github.com/G-U-N/ECCV22-FOSTER</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Discontinuity Capturing Shallow Neural Network for Elliptic Interface
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.05587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.05587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Fan Hu, Te-Sheng Lin, Ming-Chih Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a new Discontinuity Capturing Shallow Neural Network (DCSNN)
for approximating $d$-dimensional piecewise continuous functions and for
solving elliptic interface problems is developed. There are three novel
features in the present network; namely, (i) jump discontinuities are
accurately captured, (ii) it is completely shallow, comprising only one hidden
layer, (iii) it is completely mesh-free for solving partial differential
equations. The crucial idea here is that a $d$-dimensional piecewise continuous
function can be extended to a continuous function defined in
$(d+1)$-dimensional space, where the augmented coordinate variable labels the
pieces of each sub-domain. We then construct a shallow neural network to
express this new function. Since only one hidden layer is employed, the number
of training parameters (weights and biases) scales linearly with the dimension
and the neurons used in the hidden layer. For solving elliptic interface
problems, the network is trained by minimizing the mean square error loss that
consists of the residual of the governing equation, boundary condition, and the
interface jump conditions. We perform a series of numerical tests to
demonstrate the accuracy of the present network. Our DCSNN model is efficient
due to only a moderate number of parameters needed to be trained (a few hundred
parameters used throughout all numerical examples), and the results indicate
good accuracy. Compared with the results obtained by the traditional grid-based
immersed interface method (IIM), which is designed particularly for elliptic
interface problems, our network model shows a better accuracy than IIM. We
conclude by solving a six-dimensional problem to demonstrate the capability of
the present network for high-dimensional applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Regular Conditional Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.07743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.07743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasis Kratsios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a deep learning model which can generically approximate regular
conditional distributions (RCDs). The proposed model operates in three phases:
first linearizes inputs from a given metric space $\mathcal{X}$ to
$\mathbb{R}^d$ via a feature map then, these linearized features are processed
by a deep feedforward neural network, and the network's outputs are then
translated to the $1$-Wasserstein space $\mathcal{P}_1(\mathbb{R}^D)$ via a
probabilistic extension of the attention mechanism introduced by Bahdanau et
al. (2014). We find that the models built using our framework can approximate
any continuous function from $\mathbb{R}^d$ to $\mathcal{P}_1(\mathbb{R}^D)$
uniformly on compact sets, quantitatively. We identify two ways of avoiding the
curse of dimensionality when approximating $\mathcal{P}_1(\mathbb{R}^D)$-valued
functions. The first strategy describes functions in
$C(\mathbb{R}^d,\mathcal{P}_1(\mathbb{R}^D))$ which can be efficiently
approximated on any compact subset of $\mathbb{R}^d$. The second approach
describes compact subsets of $\mathbb{R}^d$, on which any most in
$C(\mathbb{R}^d,\mathcal{P}_1(\mathbb{R}^D))$ can be efficiently approximated.
The results are verified experimentally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Regular Conditional Distributions, Geometric Deep Learning,
  Computational Optimal Transport, Measure-Valued Neural Networks, Universal
  Approximation, Transformers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManiFest: Manifold Deformation for Few-shot Image Translation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Pizzati, Jean-François Lalonde, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most image-to-image translation methods require a large number of training
images, which restricts their applicability. We instead propose ManiFest: a
framework for few-shot image translation that learns a context-aware
representation of a target domain from a few images only. To enforce feature
consistency, our framework learns a style manifold between source and proxy
anchor domains (assumed to be composed of large numbers of images). The learned
manifold is interpolated and deformed towards the few-shot target domain via
patch-based adversarial and feature statistics alignment losses. All of these
components are trained simultaneously during a single end-to-end loop. In
addition to the general few-shot translation task, our approach can
alternatively be conditioned on a single exemplar image to reproduce its
specific style. Extensive experiments demonstrate the efficacy of ManiFest on
multiple tasks, outperforming the state-of-the-art on all metrics and in both
the general- and exemplar-based scenarios. Our code is available at
https://github.com/cv-rits/Manifest .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Object-Centered Autotelic Behaviors with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.05141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.05141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Akakzia, Olivier Sigaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although humans live in an open-ended world and endlessly face new
challenges, they do not have to learn from scratch each time they face the next
one. Rather, they have access to a handful of previously learned skills, which
they rapidly adapt to new situations. In artificial intelligence, autotelic
agents, which are intrinsically motivated to represent and set their own goals,
exhibit promising skill adaptation capabilities. However, these capabilities
are highly constrained by their policy and goal space representations. In this
paper, we propose to investigate the impact of these representations on the
learning and transfer capabilities of autotelic agents. We study different
implementations of autotelic agents using four types of Graph Neural Networks
policy representations and two types of goal spaces, either geometric or
predicate-based. By testing agents on unseen goals, we show that combining
object-centered architectures that are expressive enough with semantic
relational goals helps learning to reach more difficult goals. We also release
our graph-based implementations to encourage further research in this
direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, published at the Conference on Lifelong
  Learning Agents COLLAS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Normalizing Flows via Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Hagemann, Johannes Hertrich, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flows, diffusion normalizing flows and variational autoencoders
are powerful generative models. This chapter provides a unified framework to
handle these approaches via Markov chains. We consider stochastic normalizing
flows as a pair of Markov chains fulfilling some properties and show how many
state-of-the-art models for data generation fit into this framework. Indeed
numerical simulations show that including stochastic layers improves the
expressivity of the network and allows for generating multimodal distributions
from unimodal ones. The Markov chains point of view enables us to couple both
deterministic layers as invertible neural networks and stochastic layers as
Metropolis-Hasting layers, Langevin layers, variational autoencoders and
diffusion normalizing flows in a mathematically sound way. Our framework
establishes a useful mathematical tool to combine the various approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2109.11375</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adaptive Human Driver Model for Realistic Race Car Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Löckel, Siwei Ju, Maximilian Schaller, Peter van Vliet, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Engineering a high-performance race car requires a direct consideration of
the human driver using real-world tests or Human-Driver-in-the-Loop
simulations. Apart from that, offline simulations with human-like race driver
models could make this vehicle development process more effective and efficient
but are hard to obtain due to various challenges. With this work, we intend to
provide a better understanding of race driver behavior and introduce an
adaptive human race driver model based on imitation learning. Using existing
findings and an interview with a professional race engineer, we identify
fundamental adaptation mechanisms and how drivers learn to optimize lap time on
a new track. Subsequently, we use these insights to develop generalization and
adaptation techniques for a recently presented probabilistic driver modeling
approach and evaluate it using data from professional race drivers and a
state-of-the-art race car simulator. We show that our framework can create
realistic driving line distributions on unseen race tracks with almost
human-like performance. Moreover, our driver model optimizes its driving lap by
lap, correcting driving errors from previous laps while achieving faster lap
times. This work contributes to a better understanding and modeling of the
human driver, aiming to expedite simulation methods in the modern vehicle
development process and potentially supporting automated driving and racing
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-series image denoising of pressure-sensitive paint data by
  projected multivariate singular spectrum analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Ohmichi, Kohmi Takahashi, Kazuyuki Nakakita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-series data, such as unsteady pressure-sensitive paint (PSP) measurement
data, may contain a significant amount of random noise. Thus, in this study, we
investigated a noise-reduction method that combines multivariate singular
spectrum analysis (MSSA) with low-dimensional data representation. MSSA is a
state-space reconstruction technique that utilizes time-delay embedding, and
the low-dimensional representation is achieved by projecting data onto the
singular value decomposition (SVD) basis. The noise-reduction performance of
the proposed method for unsteady PSP data, i.e., the projected MSSA, is
compared with that of the truncated SVD method, one of the most employed
noise-reduction methods. The result shows that the projected MSSA exhibits
better performance in reducing random noise than the truncated SVD method.
Additionally, in contrast to that of the truncated SVD method, the performance
of the projected MSSA is less sensitive to the truncation rank. Furthermore,
the projected MSSA achieves denoising effectively by extracting smooth
trajectories in a state space from noisy input data. Expectedly, the projected
MSSA will be effective for reducing random noise in not only PSP measurement
data, but also various high-dimensional time-series data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error-free approximation of explicit linear MPC through lattice
  piecewise affine expression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.00201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.00201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Xu, Yunjiang Lou, Bart De Schutter, Zhenhua Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, the disjunctive and conjunctive lattice piecewise affine (PWA)
approximations of explicit linear model predictive control (MPC) are proposed.
The training data are generated uniformly in the domain of interest, consisting
of the state samples and corresponding affine control laws, based on which the
lattice PWA approximations are constructed. Re-sampling of data is also
proposed to guarantee that the lattice PWA approximations are identical to
explicit MPC control law in the unique order (UO) regions containing the sample
points as interior points. Additionally, under mild assumptions, the
equivalence of the two lattice PWA approximations guarantees that the
approximations are error-free in the domain of interest. The algorithms for
deriving statistically error-free approximation to the explicit linear MPC are
proposed and the complexity of the entire procedure is analyzed, which is
polynomial with respect to the number of samples. The performance of the
proposed approximation strategy is tested through two simulation examples, and
the result shows that with a moderate number of sample points, we can construct
lattice PWA approximations that are equivalent to optimal control law of the
explicit linear MPC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Robust Batch Contextual Bandits <span class="chip">ICML 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.05630v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.05630v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nian Si, Fan Zhang, Zhengyuan Zhou, Jose Blanchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data -- an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributionally robust policy with
incomplete observational data. We first present a policy evaluation procedure
that allows us to assess how well the policy does under the worst-case
environment shift. We then establish a central limit theorem type guarantee for
this proposed policy evaluation scheme. Leveraging this evaluation scheme, we
further propose a novel learning algorithm that is able to learn a policy that
is robust to adversarial perturbations and unknown covariate shifts with a
performance guarantee based on the theory of uniform convergence. Finally, we
empirically test the effectiveness of our proposed algorithm in synthetic
datasets and demonstrate that it provides the robustness that is missing using
standard policy learning algorithms. We conclude the paper by providing a
comprehensive application of our methods in the context of a real-world voting
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The short version has been accepted in ICML 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Network Clustering by Embedding of Attribute-augmented Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09367v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09367v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasqua D'Ambra, Panayot S. Vassilevski, Luisa Cutillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a new approach to detect clusters in undirected
graphs with attributed vertices. The aim is to group vertices which are similar
not only in terms of structural connectivity but also in terms of attribute
values. We incorporate structural and attribute similarities between the
vertices in an augmented graph by creating additional vertices and edges as
proposed in [6,38]. The augmented graph is then embedded in a Euclidean space
associated to its Laplacian where a modified K-means algorithm is applied to
identify clusters. The modified K-means relies on a vector distance measure
where to each original vertex we assign a suitable vector-valued set of
coordinates depending on both structural connectivity and attribute
similarities, so that each original graph vertex is thought as representative
of $m+1$ vertices of the augmented graph, if $m$ is the number of vertex
attributes. To define the coordinate vectors we employ our recently proposed
algorithm based on an adaptive AMG (Algebraic MultiGrid) method, which
identifies the coordinate directions in the embedding Euclidean space in terms
of algebraically smooth vectors with respect to the augmented graph Laplacian,
and thus extending our previous result for graphs without attributes.
  We analyze the effectiveness of our proposed clustering method by comparison
with some well known methods, whose software implementation is freely
available, and also with results reported in the literature, on two different
types of widely used synthetic graphs and on some real-world attributed graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 12 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Algebraic Representation for Systematic Generalization in
  Abstract Reasoning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Zhang, Sirui Xie, Baoxiong Jia, Ying Nian Wu, Song-Chun Zhu, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Is intelligence realized by connectionist or classicist? While connectionist
approaches have achieved superhuman performance, there has been growing
evidence that such task-specific superiority is particularly fragile in
systematic generalization. This observation lies in the central debate between
connectionist and classicist, wherein the latter continually advocates an
algebraic treatment in cognitive architectures. In this work, we follow the
classicist's call and propose a hybrid approach to improve systematic
generalization in reasoning. Specifically, we showcase a prototype with
algebraic representation for the abstract spatial-temporal reasoning task of
Raven's Progressive Matrices (RPM) and present the ALgebra-Aware
Neuro-Semi-Symbolic (ALANS) learner. The ALANS learner is motivated by abstract
algebra and the representation theory. It consists of a neural visual
perception frontend and an algebraic abstract reasoning backend: the frontend
summarizes the visual information from object-based representation, while the
backend transforms it into an algebraic structure and induces the hidden
operator on the fly. The induced operator is later executed to predict the
answer's representation, and the choice most similar to the prediction is
selected as the solution. Extensive experiments show that by incorporating an
algebraic treatment, the ALANS learner outperforms various pure connectionist
models in domains requiring systematic generalization. We further show the
generative nature of the learned algebraic representation; it can be decoded by
isomorphism to generate an answer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 paper. Supplementary:
  http://wellyzhang.github.io/attach/eccv22zhang_alans_supp.pdf Project:
  http://wellyzhang.github.io/project/alans.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of Quality Measures for GANs <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motasem Alfarra, Juan C. Pérez, Anna Frühstück, Philip H. S. Torr, Peter Wonka, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work evaluates the robustness of quality measures of generative models
such as Inception Score (IS) and Fr\'echet Inception Distance (FID). Analogous
to the vulnerability of deep models against a variety of adversarial attacks,
we show that such metrics can also be manipulated by additive pixel
perturbations. Our experiments indicate that one can generate a distribution of
images with very high scores but low perceptual quality. Conversely, one can
optimize for small imperceptible perturbations that, when added to real world
images, deteriorate their scores. We further extend our evaluation to
generative models themselves, including the state of the art network
StyleGANv2. We show the vulnerability of both the generative model and the FID
against additive perturbations in the latent space. Finally, we show that the
FID can be robustified by simply replacing the standard Inception with a robust
Inception. We validate the effectiveness of the robustified metric through
extensive experiments, showing it is more robust against manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the European Conference in Computer Vision (ECCV 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomal-E: A <span class="highlight-title">Self-Supervised</span> Network Intrusion Detection System based on
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Caville, Wai Weng Lo, Siamak Layeghy, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Graph Neural Networks (GNNs) application for
self-supervised network intrusion and anomaly detection. GNNs are a deep
learning approach for graph-based data that incorporate graph structures into
learning to generalise graph representations and output embeddings. As network
flows are naturally graph-based, GNNs are a suitable fit for analysing and
learning network behaviour. The majority of current implementations of
GNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled
network traffic which can not only restrict the amount and structure of input
traffic, but also the NIDSs potential to adapt to unseen attacks. To overcome
these restrictions, we present Anomal-E, a GNN approach to intrusion and
anomaly detection that leverages edge features and graph topological structure
in a self-supervised process. This approach is, to the best our knowledge, the
first successful and practical approach to network intrusion detection that
utilises network flows in a self-supervised, edge leveraging GNN. Experimental
results on two modern benchmark NIDS datasets not only clearly display the
improvement of using Anomal-E embeddings rather than raw features, but also the
potential Anomal-E has for detection on wild network traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flood Inflow Forecast Using L2-norm Ensemble Weighting Sea Surface
  Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takato Yasuno, Masazumi Amakata, Junichiro Fujii, Masahiro Okano, Riku Ogata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is important to forecast dam inflow for flood damage mitigation. The
hydrograph provides critical information such as the start time, peak level,
and volume. Particularly, dam management requires a 6-h lead time of the dam
inflow forecast based on a future hydrograph. The authors propose novel target
inflow weights to create an ocean feature vector extracted from the analyzed
images of the sea surface. We extracted 4,096 elements of the dimension vector
in the fc6 layer of the pre-trained VGG16 network. Subsequently, we reduced it
to three dimensions of t-SNE. Furthermore, we created the principal component
of the sea temperature weights using PCA. We found that these weights
contribute to the stability of predictor importance by numerical experiments.
As base regression models, we calibrate the least squares with kernel
expansion, the quantile random forest minimized out-of bag error, and the
support vector regression with a polynomial kernel. When we compute the
predictor importance, we visualize the stability of each variable importance
introduced by our proposed weights, compared with other results without
weights. We apply our method to a dam at Kanto region in Japan and focus on the
trained term from 2007 to 2018, with a limited flood term from June to October.
We test the accuracy over the 2019 flood term. Finally, we present the applied
results and further statistical learning for unknown flood forecast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> learning methods and applications in medical imaging
  analysis: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.08685v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.08685v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Shurrab, Rehab Duwairi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of high-quality annotated medical imaging datasets is a major
problem that collides with machine learning applications in the field of
medical imaging analysis and impedes its advancement. Self-supervised learning
is a recent training paradigm that enables learning robust representations
without the need for human annotation which can be considered an effective
solution for the scarcity of annotated medical data. This article reviews the
state-of-the-art research directions in self-supervised learning approaches for
image data with a concentration on their applications in the field of medical
imaging analysis. The article covers a set of the most recent self-supervised
learning methods from the computer vision field as they are applicable to the
medical imaging analysis and categorize them as predictive, generative, and
contrastive approaches. Moreover, the article covers 40 of the most recent
research papers in the field of self-supervised learning in medical imaging
analysis aiming at shedding the light on the recent innovation in the field.
Finally, the article concludes with possible future research directions in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Centric Epidemic Forecasting: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rodríguez, Harshavardhan Kamarthi, Pulak Agarwal, Javen Ho, Mira Patel, Suchet Sapre, B. Aditya Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has brought forth the importance of epidemic
forecasting for decision makers in multiple domains, ranging from public health
to the economy as a whole. While forecasting epidemic progression is frequently
conceptualized as being analogous to weather forecasting, however it has some
key differences and remains a non-trivial task. The spread of diseases is
subject to multiple confounding factors spanning human behavior, pathogen
dynamics, weather and environmental conditions. Research interest has been
fueled by the increased availability of rich data sources capturing previously
unobservable facets and also due to initiatives from government public health
and funding agencies. This has resulted, in particular, in a spate of work on
'data-centered' solutions which have shown potential in enhancing our
forecasting capabilities by leveraging non-traditional data sources as well as
recent innovations in AI and machine learning. This survey delves into various
data-driven methodological and practical advancements and introduces a
conceptual framework to navigate through them. First, we enumerate the large
number of epidemiological datasets and novel data streams that are relevant to
epidemic forecasting, capturing various factors like symptomatic online
surveys, retail and commerce, mobility, genomics data and more. Next, we
discuss methods and modeling paradigms focusing on the recent data-driven
statistical and deep-learning based methods as well as on the novel class of
hybrid models that combine domain knowledge of mechanistic models with the
effectiveness and flexibility of statistical approaches. We also discuss
experiences and challenges that arise in real-world deployment of these
forecasting systems including decision-making informed by forecasts. Finally,
we highlight some challenges and open problems found across the forecasting
pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Stochastic Optimization for Global <span class="highlight-title">Contrastive Learning</span>: Small
  Batch Does Not Harm Performance <span class="chip">ICML2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Yuan, Yuexin Wu, Zi-Hao Qiu, Xianzhi Du, Lijun Zhang, Denny Zhou, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study contrastive learning from an optimization
perspective, aiming to analyze and address a fundamental issue of existing
contrastive learning methods that either rely on a large batch size or a large
dictionary of feature vectors. We consider a global objective for contrastive
learning, which contrasts each positive pair with all negative pairs for an
anchor point. From the optimization perspective, we explain why existing
methods such as SimCLR require a large batch size in order to achieve a
satisfactory result. In order to remove such requirement, we propose a
memory-efficient Stochastic Optimization algorithm for solving the Global
objective of Contrastive Learning of Representations, named SogCLR. We show
that its optimization error is negligible under a reasonable condition after a
sufficient number of iterations or is diminishing for a slightly different
global contrastive objective. Empirically, we demonstrate that SogCLR with
small batch size (e.g., 256) can achieve similar performance as SimCLR with
large batch size (e.g., 8192) on self-supervised learning task on ImageNet-1K.
We also attempt to show that the proposed optimization technique is generic and
can be applied to solving other contrastive losses, e.g., two-way contrastive
losses for bimodal contrastive learning. The proposed method is implemented in
our open-sourced library LibAUC (www.libauc.org).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TREND: Truncated Generalized Normal Density Estimation of Inception
  Embeddings for GAN Evaluation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.14767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.14767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyuk Lee, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating image generation models such as generative adversarial networks
(GANs) is a challenging problem. A common approach is to compare the
distributions of the set of ground truth images and the set of generated test
images. The Frech\'et Inception distance is one of the most widely used metrics
for evaluation of GANs, which assumes that the features from a trained
Inception model for a set of images follow a normal distribution. In this
paper, we argue that this is an over-simplified assumption, which may lead to
unreliable evaluation results, and more accurate density estimation can be
achieved using a truncated generalized normal distribution. Based on this, we
propose a novel metric for accurate evaluation of GANs, named TREND (TRuncated
gEneralized Normal Density estimation of inception embeddings). We demonstrate
that our approach significantly reduces errors of density estimation, which
consequently eliminates the risk of faulty evaluation results. Furthermore, we
show that the proposed metric significantly improves robustness of evaluation
results against variation of the number of image samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoDES: AutoML Pipeline Generation of Classification with Dynamic
  Ensemble Strategy Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.00207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.00207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpu Zhao, Rui Zhang, Xiaqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating machine learning has achieved remarkable technological
developments in recent years, and building an automated machine learning
pipeline is now an essential task. The model ensemble is the technique of
combining multiple models to get a better and more robust model. However,
existing automated machine learning tends to be simplistic in handling the
model ensemble, where the ensemble strategy is fixed, such as stacked
generalization. There have been many techniques on different ensemble methods,
especially ensemble selection, and the fixed ensemble strategy limits the upper
limit of the model's performance. In this article, we present a novel framework
for automated machine learning. Our framework incorporates advances in dynamic
ensemble selection, and to our best knowledge, our approach is the first in the
field of AutoML to search and optimize ensemble strategies. In the comparison
experiments, our method outperforms the state-of-the-art automated machine
learning frameworks with the same CPU time in 42 classification datasets from
the OpenML platform. Ablation experiments on our framework validate the
effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconfigurable Intelligent Surface Empowered Over-the-Air Federated Edge
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Liu, Zehong Lin, Xiaojun Yuan, Ying-Jun Angela Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated edge learning (FEEL) has emerged as a revolutionary paradigm to
develop AI services at the edge of 6G wireless networks as it supports
collaborative model training at a massive number of mobile devices. However,
model communication over wireless channels, especially in uplink model
uploading of FEEL, has been widely recognized as a bottleneck that critically
limits the efficiency of FEEL. Although over-the-air computation can alleviate
the excessive cost of radio resources in FEEL model uploading, practical
implementations of over-the-air FEEL still suffer from several challenges,
including strong straggler issues, large communication overheads, and potential
privacy leakage. In this article, we study these challenges in over-the-air
FEEL and leverage reconfigurable intelligent surface (RIS), a key enabler of
future wireless systems, to address these challenges. We study the
state-of-the-art solutions on RIS-empowered FEEL and explore the promising
research opportunities for adopting RIS to enhance FEEL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Textual Adversarial Examples through Randomized Substitution
  and Vote <span class="chip">UAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Wang, Yifeng Xiong, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A line of work has shown that natural text processing models are vulnerable
to adversarial examples. Correspondingly, various defense methods are proposed
to mitigate the threat of textual adversarial examples, eg, adversarial
training, input transformations, detection, etc. In this work, we treat the
optimization process for synonym substitution based textual adversarial attacks
as a specific sequence of word replacement, in which each word mutually
influences other words. We identify that we could destroy such mutual
interaction and eliminate the adversarial perturbation by randomly substituting
a word with its synonyms. Based on this observation, we propose a novel textual
adversarial example detection method, termed Randomized Substitution and Vote
(RS&V), which votes the prediction label by accumulating the logits of k
samples generated by randomly substituting the words in the input text with
synonyms. The proposed RS&V is generally applicable to any existing neural
networks without modification on the architecture or extra training, and it is
orthogonal to prior work on making the classification network itself more
robust. Empirical evaluations on three benchmark datasets demonstrate that our
RS&V could detect the textual adversarial examples more successfully than the
existing detection methods while maintaining the high classification accuracy
on benign samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by UAI 2022, code is avaliable at
  https://github.com/JHL-HUST/RSV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FORML: Learning to Reweight Data for Fairness <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.01719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.01719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bobby Yan, Skyler Seto, Nicholas Apostoloff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are trained to minimize the mean loss for a single
metric, and thus typically do not consider fairness and robustness. Neglecting
such metrics in training can make these models prone to fairness violations
when training data are imbalanced or test distributions differ. This work
introduces Fairness Optimized Reweighting via Meta-Learning (FORML), a training
algorithm that balances fairness and robustness with accuracy by jointly
learning training sample weights and neural network parameters. The approach
increases model fairness by learning to balance the contributions from both
over- and under-represented sub-groups through dynamic reweighting of the data
learned from a user-specified held-out set representative of the distribution
under which fairness is desired. FORML improves equality of opportunity
fairness criteria on image classification tasks, reduces bias of corrupted
labels, and facilitates building more fair datasets via data condensation.
These improvements are achieved without pre-processing data or post-processing
model outputs, without learning an additional weighting function, without
changing model architecture, and while maintaining accuracy on the original
predictive metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, Presented at ICML 2022 DataPerf Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Application of QUBO solver using black-box optimization to structural
  design for resonance avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tadayoshi Matsumori, Masato Taki, Tadashi Kadowaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadratic unconstrained binary optimization (QUBO) solvers can be applied to
design an optimal structure to avoid resonance. QUBO algorithms that work on a
classical or quantum device have succeeded in some industrial applications.
However, their applications are still limited due to the difficulty of
transforming from the original optimization problem to QUBO. Recently,
black-box optimization (BBO) methods have been proposed to tackle this issue
using a machine learning technique and a Bayesian treatment for combinatorial
optimization. We employed the BBO methods to design a printed circuit board for
resonance avoidance. This design problem is formulated to maximize natural
frequency and simultaneously minimize the number of mounting points. The
natural frequency, which is the bottleneck for the QUBO formulation, is
approximated to a quadratic model in the BBO method. We demonstrated that BBO
using a factorization machine shows good performance in both the calculation
time and the success probability of finding the optimal solution. Our results
can open up QUBO solvers' potential for other applications in structural
designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of an article published in Scientific Reports. The
  final authenticated version is available online at:
  10.1038/s41598-022-16149-8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEN : Cooperatively Evolving Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02192v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02192v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ch. Sobhan Babu, Ravindra Guravannavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GANs have two competing modules: the generator module is trained to generate
new examples, and the discriminator module is trained to discriminate real
examples from generated examples. The training procedure of GAN is modeled as a
finitely repeated simultaneous game. Each module tries to increase its
performance at every repetition of the base game (at every batch of training
data) in a non-cooperative manner. We observed that each module can perform
better if training is modeled as an infinitely repeated simultaneous game. At
every repetition of the base game (at every batch of training data) the
stronger module (whose performance is increased or remains the same compared to
the previous batch of training data) cooperates with the weaker module (whose
performance is decreased compared to the previous batch of training data) and
only the weaker module is allowed to increase its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Split for Automatic Bias Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Bao, Regina Barzilay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifiers are biased when trained on biased datasets. As a remedy, we
propose Learning to Split (ls), an algorithm for automatic bias detection.
Given a dataset with input-label pairs, ls learns to split this dataset so that
predictors trained on the training split cannot generalize to the testing
split. This performance gap suggests that the testing split is
under-represented in the dataset, which is a signal of potential bias.
Identifying non-generalizable splits is challenging since we have no
annotations about the bias. In this work, we show that the prediction
correctness of each example in the testing split can be used as a source of
weak supervision: generalization performance will drop if we move examples that
are predicted correctly away from the testing split, leaving only those that
are mis-predicted. ls is task-agnostic and can be applied to any supervised
learning problem, ranging from natural language understanding and image
classification to molecular property prediction. Empirical results show that ls
is able to generate astonishingly challenging splits that correlate with
human-identified biases. Moreover, we demonstrate that combining robust
learning algorithms (such as group DRO) with splits identified by ls enables
automatic de-biasing. Compared to previous state-of-the-art, we substantially
improve the worst-group performance (23.4% on average) when the source of
biases is unknown during training and validation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Auxiliary Text Query-modifier to Content-based Audio
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The amount of audio data available on public websites is growing rapidly, and
an efficient mechanism for accessing the desired data is necessary. We propose
a content-based audio retrieval method that can retrieve a target audio that is
similar to but slightly different from the query audio by introducing auxiliary
textual information which describes the difference between the query and target
audio. While the range of conventional content-based audio retrieval is limited
to audio that is similar to the query audio, the proposed method can adjust the
retrieval range by adding an embedding of the auxiliary text query-modifier to
the embedding of the query sample audio in a shared latent space. To evaluate
our method, we built a dataset comprising two different audio clips and the
text that describes the difference. The experimental results show that the
proposed method retrieves the paired audio more accurately than the baseline.
We also confirmed based on visualization that the proposed method obtains the
shared latent space in which the audio difference and the corresponding text
are represented as similar embedding vectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duplicate Detection as a Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juliette Opdenplatz, Umutcan Şimşek, Dieter Fensel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Completeness of a knowledge graph is an important quality dimension and
factor on how well an application that makes use of it performs. Completeness
can be improved by performing knowledge enrichment. Duplicate detection aims to
find identity links between the instances of knowledge graphs and is a
fundamental subtask of knowledge enrichment. Current solutions to the problem
require expert knowledge of the tool and the knowledge graph they are applied
to. Users might not have this expert knowledge. We present our service-based
approach to the duplicate detection task that provides an easy-to-use no-code
solution that is still competitive with the state-of-the-art and has recently
been adopted in an industrial context. The evaluation will be based on several
frequently used test scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Human Values into Recommender Systems: An Interdisciplinary
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Stray, Alon Halevy, Parisa Assar, Dylan Hadfield-Menell, Craig Boutilier, Amar Ashar, Lex Beattie, Michael Ekstrand, Claire Leibowicz, Connie Moon Sehat, Sara Johansen, Lianne Kerlin, David Vickrey, Spandana Singh, Sanne Vrijenhoek, Amy Zhang, McKane Andrus, Natali Helberger, Polina Proutskova, Tanushree Mitra, Nina Vasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are the algorithms which select, filter, and personalize
content across many of the worlds largest platforms and apps. As such, their
positive and negative effects on individuals and on societies have been
extensively theorized and studied. Our overarching question is how to ensure
that recommender systems enact the values of the individuals and societies that
they serve. Addressing this question in a principled fashion requires technical
knowledge of recommender design and operation, and also critically depends on
insights from diverse fields including social science, ethics, economics,
psychology, policy and law. This paper is a multidisciplinary effort to
synthesize theory and practice from different perspectives, with the goal of
providing a shared language, articulating current design approaches, and
identifying open problems. It is not a comprehensive survey of this large
space, but a set of highlights identified by our diverse author cohort. We
collect a set of values that seem most relevant to recommender systems
operating across different domains, then examine them from the perspectives of
current industry practice, measurement, product design, and policy approaches.
Important open problems include multi-stakeholder processes for defining values
and resolving trade-offs, better values-driven measurements, recommender
controls that people use, non-behavioral algorithmic feedback, optimization for
long-term outcomes, causal inference of recommender effects, academic-industry
research collaborations, and interdisciplinary policy-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twitter Big Data as a Resource for Exoskeleton Research: A Large-Scale
  <span class="highlight-title">Dataset</span> of about 140,000 Tweets and 100 Research Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04476v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04476v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirmalya Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exoskeleton technology has been rapidly advancing in the recent past due
to its multitude of applications and diverse use-cases in assisted living,
military, healthcare, firefighting, and industry 4.0. The exoskeleton market is
projected to increase by multiple times of its current value within the next
two years. Therefore, it is crucial to study the degree and trends of user
interest, views, opinions, perspectives, attitudes, acceptance, feedback,
engagement, buying behavior, and satisfaction, towards exoskeletons, for which
the availability of Big Data of conversations about exoskeletons is necessary.
The Internet of Everything style of today's living, characterized by people
spending more time on the internet than ever before, with a specific focus on
social media platforms, holds the potential for the development of such a
dataset by the mining of relevant social media conversations. Twitter, one such
social media platform, is highly popular amongst all age groups, where the
topics found in the conversation paradigms include emerging technologies such
as exoskeletons. To address this research challenge, this work makes two
scientific contributions to this field. First, it presents an open-access
dataset of about 140,000 tweets about exoskeletons that were posted in a 5-year
period from May 21, 2017, to May 21, 2022. Second, based on a comprehensive
review of the recent works in the fields of Big Data, Natural Language
Processing, Information Retrieval, Data Mining, Pattern Recognition, and
Artificial Intelligence that may be applied to relevant Twitter data for
advancing research, innovation, and discovery in the field of exoskeleton
research, a total of 100 Research Questions are presented for researchers to
study, analyze, evaluate, ideate, and investigate based on this dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural content-aware collaborative filtering for cold-start music
  recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Magron, Cédric Févotte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art music recommender systems are based on collaborative
filtering, which builds upon learning similarities between users and songs from
the available listening data. These approaches inherently face the cold-start
problem, as they cannot recommend novel songs with no listening history.
Content-aware recommendation addresses this issue by incorporating content
information about the songs on top of collaborative filtering. However, methods
falling in this category rely on a shallow user/item interaction that
originates from a matrix factorization framework. In this work, we introduce
neural content-aware collaborative filtering, a unified framework which
alleviates these limits, and extends the recently introduced neural
collaborative filtering to its content-aware counterpart. We propose a
generative model which leverages deep learning for both extracting content
information from low-level acoustic features and for modeling the interaction
between users and songs embeddings. The deep content feature extractor can
either directly predict the item embedding, or serve as a regularization prior,
yielding two variants (strict and relaxed) of our model. Experimental results
show that the proposed method reaches state-of-the-art results for a cold-start
music recommendation task. We notably observe that exploiting deep neural
networks for learning refined user/item interactions outperforms approaches
using a more simple interaction model in a content-aware framework.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViGAT: Bottom-up event recognition and explanation in video using
  factorized graph attention network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Gkalelis, Dimitrios Daskalakis, Vasileios Mezaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face-to-Face Co-Located Human-Human Social Interaction Analysis using
  Nonverbal Cues: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cigdem Beyan, Alessandro Vinciarelli, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a systematic review of recent efforts (since 2010) aimed
at automatic analysis of nonverbal cues displayed in face-to-face co-located
human-human social interactions. The main reason for focusing on nonverbal cues
is that these are the physical, machine detectable traces of social and
psychological phenomena. Therefore, detecting and understanding nonverbal cues
means, at least to a certain extent, to detect and understand social and
psychological phenomena. The covered topics are categorized into three as: a)
modeling social traits, such as leadership, dominance, personality traits, b)
social role recognition and social relations detection and c) interaction
dynamics analysis in terms of group cohesion, empathy, rapport and so forth. We
target the co-located interactions, in which the interactants are always
humans. The survey covers a wide spectrum of settings and scenarios, including
free-standing interactions, meetings, indoor and outdoor social exchanges,
dyadic conversations, and crowd dynamics. For each of them, the survey
considers the three main elements of nonverbal cues analysis, namely data,
sensing approaches and computational methodologies. The goal is to highlight
the main advances of the last decade, to point out existing limitations, and to
outline future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Computing and Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subjective Assessment Experiments That Recruit Few Observers With
  Repetitions (FOWR) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.02618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.02618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Perez, Lucjan Janowski, Narciso Garcia, Margaret Pinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that it is possible to characterize subject bias
and variance in subjective assessment tests. Apparent differences among
subjects can, for the most part, be explained by random factors. Building on
that theory, we propose a subjective test design where three to four team
members each rate the stimuli multiple times. The results are comparable to a
high performing objective metric. This provides a quick and simple way to
analyze new technologies and perform pre-tests for subjective assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-19T00:00:00Z">2022-07-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Algorithms for <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mary Phuong, Marcus Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 15 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Urdu Speech and Text Based Sentiment Analyzer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waqar Ahmad, Maryam Edalati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering what other people think has always been a key aspect of our
information-gathering strategy. People can now actively utilize information
technology to seek out and comprehend the ideas of others, thanks to the
increased availability and popularity of opinion-rich resources such as online
review sites and personal blogs. Because of its crucial function in
understanding people's opinions, sentiment analysis (SA) is a crucial task.
Existing research, on the other hand, is primarily focused on the English
language, with just a small amount of study devoted to low-resource languages.
For sentiment analysis, this work presented a new multi-class Urdu dataset
based on user evaluations. The tweeter website was used to get Urdu dataset.
Our proposed dataset includes 10,000 reviews that have been carefully
classified into two categories by human experts: positive, negative. The
primary purpose of this research is to construct a manually annotated dataset
for Urdu sentiment analysis and to establish the baseline result. Five
different lexicon- and rule-based algorithms including Naivebayes, Stanza,
Textblob, Vader, and Flair are employed and the experimental results show that
Flair with an accuracy of 70% outperforms other tested algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sentiment Analysis, Opinion Mining, Urdu language, polarity
  assessment, lexicon-based method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the cross-lingual transferability of multilingual prototypical models
  across NLU tasks <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oralie Cattan, Christophe Servan, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised deep learning-based approaches have been applied to task-oriented
dialog and have proven to be effective for limited domain and language
applications when a sufficient number of training examples are available. In
practice, these approaches suffer from the drawbacks of domain-driven design
and under-resourced languages. Domain and language models are supposed to grow
and change as the problem space evolves. On one hand, research on transfer
learning has demonstrated the cross-lingual ability of multilingual
Transformers-based models to learn semantically rich representations. On the
other, in addition to the above approaches, meta-learning have enabled the
development of task and language learning algorithms capable of far
generalization. Through this context, this article proposes to investigate the
cross-lingual transferability of using synergistically few-shot learning with
prototypical neural networks and multilingual Transformers-based models.
Experiments in natural language understanding tasks on MultiATIS++ corpus shows
that our approach substantially improves the observed transfer learning
performances between the low and the high resource languages. More generally
our approach confirms that the meaningful latent space learned in a given
language can be can be generalized to unseen and under-resourced ones using
meta-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ACL workshop METANLP 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">Transformer</span>s-based models on French Spoken Language
  Understanding tasks <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oralie Cattan, Sahar Ghannay, Christophe Servan, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last five years, the rise of the self-attentional Transformer-based
architectures led to state-of-the-art performances over many natural language
tasks. Although these approaches are increasingly popular, they require large
amounts of data and computational resources. There is still a substantial need
for benchmarking methodologies ever upwards on under-resourced languages in
data-scarce application conditions. Most pre-trained language models were
massively studied using the English language and only a few of them were
evaluated on French. In this paper, we propose a unified benchmark, focused on
evaluating models quality and their ecological impact on two well-known French
spoken language understanding tasks. Especially we benchmark thirteen
well-established Transformer-based models on the two available spoken language
understanding tasks for French: MEDIA and ATIS-FR. Within this framework, we
show that compact models can reach comparable results to bigger ones while
their ecological impact is considerably lower. However, this assumption is
nuanced and depends on the considered compression method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper at INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Usability of <span class="highlight-title">Transformer</span>s-based models for a French
  Question-Answering task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oralie Cattan, Christophe Servan, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many tasks, state-of-the-art results have been achieved with
Transformer-based architectures, resulting in a paradigmatic shift in practices
from the use of task-specific architectures to the fine-tuning of pre-trained
language models. The ongoing trend consists in training models with an
ever-increasing amount of data and parameters, which requires considerable
resources. It leads to a strong search to improve resource efficiency based on
algorithmic and hardware improvements evaluated only for English. This raises
questions about their usability when applied to small-scale learning problems,
for which a limited amount of training data is available, especially for
under-resourced languages tasks. The lack of appropriately sized corpora is a
hindrance to applying data-driven and transfer learning-based approaches with
strong instability cases. In this paper, we establish a state-of-the-art of the
efforts dedicated to the usability of Transformer-based models and propose to
evaluate these improvements on the question-answering performances of French
language which have few resources. We address the instability relating to data
scarcity by investigating various training strategies with data augmentation,
hyperparameters optimization and cross-lingual transfer. We also introduce a
new compact model for French FrALBERT which proves to be competitive in
low-resource settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>French compact model paper: FrALBERT, Accepted to RANLP 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Bagging Methods for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranab Islam, Shaan Khosla, Arthur Lok, Mudit Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models leverage increasingly large numbers of parameters to
achieve performance on natural language understanding tasks. Ensembling these
models in specific configurations for downstream tasks show even further
performance improvements. In this paper, we perform an analysis of bagging
language models and compare single language models to bagged ensembles that are
roughly equivalent in terms of final model size. We explore an array of model
bagging configurations for natural language understanding tasks with final
ensemble sizes ranging from 300M parameters to 1.5B parameters and determine
that our ensembling methods are at best roughly equivalent to single LM
baselines. We note other positive effects of bagging and pruning in specific
scenarios according to findings in our experiments such as variance reduction
and minor performance improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MoEC: Mixture of Expert Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Xie, Shaohan Huang, Tianyu Chen, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely Mixture of Experts (MoE) has received great interest due to its
promising scaling capability with affordable computational overhead. MoE
converts dense layers into sparse experts, and utilizes a gated routing network
to make experts conditionally activated. However, as the number of experts
grows, MoE with outrageous parameters suffers from overfitting and sparse data
allocation. Such problems are especially severe on tasks with limited data,
thus hindering the progress for MoE models to improve performance by scaling
up. In this work, we propose Mixture of Expert Clusters - a general approach to
enable expert layers to learn more diverse and appropriate knowledge by
imposing variance-based constraints on the routing stage. We further propose a
cluster-level expert dropout strategy specifically designed for the expert
cluster structure. Our experiments reveal that MoEC could improve performance
on machine translation and natural language understanding tasks, and raise the
performance upper bound for scaling up experts under limited data. We also
verify that MoEC plays a positive role in mitigating overfitting and sparse
data allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can You Fool AI by Doing a 180? $\unicode{x2013}$ A Case Study on
  Authorship Analysis of Texts by Arata Osada 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jagna Nieuwazny, Karol Nowakowski, Michal Ptaszynski, Fumito Masui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is our attempt at answering a twofold question covering the areas
of ethics and authorship analysis. Firstly, since the methods used for
performing authorship analysis imply that an author can be recognized by the
content he or she creates, we were interested in finding out whether it would
be possible for an author identification system to correctly attribute works to
authors if in the course of years they have undergone a major psychological
transition. Secondly, and from the point of view of the evolution of an
author's ethical values, we checked what it would mean if the authorship
attribution system encounters difficulties in detecting single authorship. We
set out to answer those questions through performing a binary authorship
analysis task using a text classifier based on a pre-trained transformer model
and a baseline method relying on conventional similarity metrics. For the test
set, we chose works of Arata Osada, a Japanese educator and specialist in the
history of education, with half of them being books written before the World
War II and another half in the 1950s, in between which he underwent a
transformation in terms of political opinions. As a result, we were able to
confirm that in the case of texts authored by Arata Osada in a time span of
more than 10 years, while the classification accuracy drops by a large margin
and is substantially lower than for texts by other non-fiction writers,
confidence scores of the predictions remain at a similar level as in the case
of a shorter time span, indicating that the classifier was in many instances
tricked into deciding that texts written over a time span of multiple years
were actually written by two different people, which in turn leads us to
believe that such a change can affect authorship analysis, and that historical
events have great impact on a person's ethical outlook as expressed in their
writings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Future Captioning Model for Explaining Likely Collisions in
  Daily Tasks <span class="chip">ICIP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motonari Kambara, Komei Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domestic service robots that support daily tasks are a promising solution for
elderly or disabled people. It is crucial for domestic service robots to
explain the collision risk before they perform actions. In this paper, our aim
is to generate a caption about a future event. We propose the Relational Future
Captioning Model (RFCM), a crossmodal language generation model for the future
captioning task. The RFCM has the Relational Self-Attention Encoder to extract
the relationships between events more effectively than the conventional
self-attention in transformers. We conducted comparison experiments, and the
results show the RFCM outperforms a baseline method on two datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at ICIP2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILASR: Privacy-Preserving Incremental Learning for AutomaticSpeech
  Recognition at Production Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gopinath Chennupati, Milind Rao, Gurpreet Chadha, Aaron Eakin, Anirudh Raju, Gautam Tiwari, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo, Andy Oberlin, Buddha Nandanoor, Prahalad Venkataramanan, Zheng Wu, Pankaj Sitpure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental learning is one paradigm to enable model building and updating at
scale with streaming data. For end-to-end automatic speech recognition (ASR)
tasks, the absence of human annotated labels along with the need for privacy
preserving policies for model building makes it a daunting challenge. Motivated
by these challenges, in this paper we use a cloud based framework for
production systems to demonstrate insights from privacy preserving incremental
learning for automatic speech recognition (ILASR). By privacy preserving, we
mean, usage of ephemeral data which are not human annotated. This system is a
step forward for production levelASR models for incremental/continual learning
that offers near real-time test-bed for experimentation in the cloud for
end-to-end ASR, while adhering to privacy-preserving policies. We show that the
proposed system can improve the production models significantly(3%) over a new
time period of six months even in the absence of human annotated labels with
varying levels of weak supervision and large batch sizes in incremental
learning. This improvement is 20% over test sets with new words and phrases in
the new time period. We demonstrate the effectiveness of model building in a
privacy-preserving incremental fashion for ASR while further exploring the
utility of having an effective teacher model and use of large batch sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual <span class="highlight-title">Transformer</span> Encoders: a Word-Level Task-Agnostic Evaluation <span class="chip">IJCNN 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Félix Gaschi, François Plesse, Parisa Rastin, Yannick Toussaint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some Transformer-based models can perform cross-lingual transfer learning:
those models can be trained on a specific task in one language and give
relatively good results on the same task in another language, despite having
been pre-trained on monolingual tasks only. But, there is no consensus yet on
whether those transformer-based models learn universal patterns across
languages. We propose a word-level task-agnostic method to evaluate the
alignment of contextualized representations built by such models. We show that
our method provides more accurate translated word pairs than previous methods
to evaluate word-level alignment. And our results show that some inner layers
of multilingual Transformer-based models outperform other explicitly aligned
representations, and even more so according to a stricter definition of
multilingual alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IJCNN 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PiC: A Phrase-in-Context <span class="highlight-title">Dataset</span> for Phrase Understanding and Semantic
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thang M. Pham, Seunghyun Yoon, Trung Bui, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since BERT (Devlin et al., 2018), learning contextualized word embeddings has
been a de-facto standard in NLP. However, the progress of learning
contextualized phrase embeddings is hindered by the lack of a human-annotated,
phrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of
~28K of noun phrases accompanied by their contextual Wikipedia pages and a
suite of three tasks of increasing difficulty for evaluating the quality of
phrase embeddings. We find that training on our dataset improves ranking
models' accuracy and remarkably pushes Question Answering (QA) models to
near-human accuracy which is 95% Exact Match (EM) on semantic search given a
query phrase and a passage. Interestingly, we find evidence that such
impressive performance is because the QA models learn to better capture the
common meaning of a phrase regardless of its actual context. That is, on our
Phrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially
(60% EM), failing to differentiate between two different senses of the same
phrase under two different contexts. Further results on our 3-task PiC
benchmark reveal that learning contextualized phrase embeddings remains an
interesting, open challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Human-guided Collaborative Problem Solving: A Natural Language based
  Framework <span class="chip">ICAPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, <span class="highlight-author">Dan Roth</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of human-machine collaborative problem solving as a
planning task coupled with natural language communication. Our framework
consists of three components -- a natural language engine that parses the
language utterances to a formal representation and vice-versa, a concept
learner that induces generalized concepts for plans based on limited
interactions with the user, and an HTN planner that solves the task based on
human interaction. We illustrate the ability of this framework to address the
key challenges of collaborative problem solving by demonstrating it on a
collaborative building task in a Minecraft-based blocksworld domain. The
accompanied demo video is available at https://youtu.be/q1pWe4aahF0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICAPS 2021 (demo track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuoteKG: A Multilingual Knowledge Graph of Quotes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin Kuculo, Simon Gottschalk, Elena Demidova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quotes of public figures can mark turning points in history. A quote can
explain its originator's actions, foreshadowing political or personal decisions
and revealing character traits. Impactful quotes cross language barriers and
influence the general population's reaction to specific stances, always facing
the risk of being misattributed or taken out of context. The provision of a
cross-lingual knowledge graph of quotes that establishes the authenticity of
quotes and their contexts is of great importance to allow the exploration of
the lives of important people as well as topics from the perspective of what
was actually said. In this paper, we present QuoteKG, the first multilingual
knowledge graph of quotes. We propose the QuoteKG creation pipeline that
extracts quotes from Wikiquote, a free and collaboratively created collection
of quotes in many languages, and aligns different mentions of the same quote.
QuoteKG includes nearly one million quotes in $55$ languages, said by more than
$69,000$ people of public interest across a wide range of topics. QuoteKG is
publicly available and can be accessed via a SPARQL endpoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Vision-Language Pre-training, known as CLIP, has provided a new
paradigm for learning visual representations using large-scale image-text
pairs. It shows impressive performance on downstream tasks by zero-shot
knowledge transfer. To further enhance CLIP's adaption capability, existing
methods proposed to fine-tune additional learnable modules, which significantly
improves the few-shot performance but introduces extra training time and
computational resources. In this paper, we propose a training-free adaption
method for CLIP to conduct few-shot classification, termed as Tip-Adapter,
which not only inherits the training-free advantage of zero-shot CLIP but also
performs comparably to those training-required approaches. Tip-Adapter
constructs the adapter via a key-value cache model from the few-shot training
set, and updates the prior knowledge encoded in CLIP by feature retrieval. On
top of that, the performance of Tip-Adapter can be further boosted to be
state-of-the-art on ImageNet by fine-tuning the cache model for 10$\times$
fewer epochs than existing methods, which is both effective and efficient. We
conduct extensive experiments of few-shot classification on 11 datasets to
demonstrate the superiority of our proposed methods. Code is released at
https://github.com/gaopengcuhk/Tip-Adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. arXiv admin note: substantial text overlap
  with arXiv:2111.03930</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESPnet-SE++: Speech Enhancement for Robust Speech Recognition,
  Translation, and Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Ju Lu, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents recent progress on integrating speech separation and
enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE
work, numerous features have been added, including recent state-of-the-art
speech enhancement models with their respective training and evaluation
recipes. Importantly, a new interface has been designed to flexibly combine
speech enhancement front-ends with other tasks, including automatic speech
recognition (ASR), speech translation (ST), and spoken language understanding
(SLU). To showcase such integration, we performed experiments on carefully
designed synthetic datasets for noisy-reverberant multi-channel ST and SLU
tasks, which can be used as benchmark corpora for future research. In addition
to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and
single-channel SE approaches. Results show that the integration of SE
front-ends with back-end tasks is a promising research direction even for tasks
besides ASR, especially in the multi-channel scenario. The code is available
online at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU
datasets, which are another contribution of this work, are released on
HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big Data and Education: using big data analytics in language learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Ashrafimoghari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Working with big data using data mining tools is rapidly becoming a trend in
education industry. The combination of the current capacity to collect, store,
manage and process data in a timely manner, and data from online educational
platforms represents an unprecedented opportunity for educational institutes,
learners, educators, and researchers. In this position paper, we consider some
basic concepts as well as most popular tools, methods and techniques regarding
Educational Data Mining and Learning Analytics, and discuss big data
applications in language learning, in particular.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Centric Unsupervised Image Captioning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihang Meng, David Yang, Xuefei Cao, Ashish Shah, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning is a longstanding problem in the field of computer vision
and natural language processing. To date, researchers have produced impressive
state-of-the-art performance in the age of deep learning. Most of these
state-of-the-art, however, requires large volume of annotated image-caption
pairs in order to train their models. When given an image dataset of interests,
practitioner needs to annotate the caption for each image in the training set
and this process needs to happen for each newly collected image dataset. In
this paper, we explore the task of unsupervised image captioning which utilizes
unpaired images and texts to train the model so that the texts can come from
different sources than the images. A main school of research on this topic that
has been shown to be effective is to construct pairs from the images and texts
in the training set according to their overlap of objects. Unlike in the
supervised setting, these constructed pairings are however not guaranteed to
have fully overlapping set of objects. Our work in this paper overcomes this by
harvesting objects corresponding to a given sentence from the training set,
even if they don't belong to the same image. When used as input to a
transformer, such mixture of objects enables larger if not full object
coverage, and when supervised by the corresponding sentence, produced results
that outperform current state of the art unsupervised methods by a significant
margin. Building upon this finding, we further show that (1) additional
information on relationship between objects and attributes of objects also
helps in boosting performance; and (2) our method also extends well to
non-English image captioning, which usually suffers from a scarcer level of
annotations. Our findings are supported by strong empirical results. Our code
is available at https://github.com/zihangm/obj-centric-unsup-caption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Effective Few-Shot Named Entity Linking by Meta-Learning <span class="chip">ICDE 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuxing Li, Zhenyu Li, Zhengyan Zhang, Ning Liu, Haitao Yuan, Wei Zhang, <span class="highlight-author">Zhiyuan Liu</span>, Jianyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity linking aims to link ambiguous mentions to their corresponding
entities in a knowledge base, which is significant and fundamental for various
downstream applications, e.g., knowledge base completion, question answering,
and information extraction. While great efforts have been devoted to this task,
most of these studies follow the assumption that large-scale labeled data is
available. However, when the labeled data is insufficient for specific domains
due to labor-intensive annotation work, the performance of existing algorithms
will suffer an intolerable decline. In this paper, we endeavor to solve the
problem of few-shot entity linking, which only requires a minimal amount of
in-domain labeled data and is more practical in real situations. Specifically,
we firstly propose a novel weak supervision strategy to generate non-trivial
synthetic entity-mention pairs based on mention rewriting. Since the quality of
the synthetic data has a critical impact on effective model training, we
further design a meta-learning mechanism to assign different weights to each
synthetic entity-mention pair automatically. Through this way, we can
profoundly exploit rich and precious semantic information to derive a
well-trained entity linking model under the few-shot setting. The experiments
on real-world datasets show that the proposed method can extensively improve
the state-of-the-art few-shot entity linking model and achieve impressive
performance when only a small amount of labeled data is available. Moreover, we
also demonstrate the outstanding ability of the model's transferability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures. Accepted at IEEE ICDE 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synergistic Compilation Workflow for Tackling Crosstalk in Quantum
  Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Hua, Yuwei Jin, Ang Li, Yanhao Chen, Chi Zhang, Ari Hayes, Hang Gao, Eddy Z. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Near-term quantum systems tend to be noisy. Crosstalk noise has been
recognized as one of several major types of noises in superconducting Noisy
Intermediate-Scale Quantum (NISQ) devices. Crosstalk arises from the concurrent
execution of two-qubit gates on nearby qubits, such as \texttt{CX}. It might
significantly raise the error rate of gates in comparison to running them
individually. Crosstalk can be mitigated through scheduling or hardware machine
tuning. Prior scientific studies, however, manage crosstalk at a really late
phase in the compilation process, usually after hardware mapping is done. It
may miss great opportunities of optimizing algorithm logic, routing, and
crosstalk at the same time. In this paper, we push the envelope by considering
all these factors simultaneously at the very early compilation stage. We
propose a crosstalk-aware quantum program compilation framework called CQC that
can enhance crosstalk mitigation while achieving satisfactory circuit depth.
Moreover, we identify opportunities for translation from intermediate
representation to the circuit for application-specific crosstalk mitigation,
for instance, the \texttt{CX} ladder construction in variational quantum
eigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices
show that our framework can significantly reduce the error rate by up to
6$\times$, with only $\sim$60\% circuit depth compared to state-of-the-art gate
scheduling approaches. In particular, for VQE, we demonstrate 49\% circuit
depth reduction with 9.6\% fidelity improvement over prior art on the H4
molecule using IBMQ Guadalupe. Our CQC framework will be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Speech Recognition for Speech Assessment of Persian Preschool
  Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12886v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12886v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Abaskohi, Fatemeh Mortazavi, Hadi Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing Automatic Speech Recognition(ASR) system is useless, since they of
pre-trained on voices, that are different from children's voices in terms of
frequency and amplitude. We constructed an ASR for our cognitive test system to
solve this issue using the Wav2Vec 2.0 model with a new pre-training objective
called Random Frequency Pitch(RFP). In addition, we used our new dataset to
fine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)
tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian
section of the CommonVoice dataset. Furthermore, our novel methodology produces
positive outcomes in zero- and few-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FactGraph: Evaluating Factuality in Summarization with Semantic Graph
  Representations <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, <span class="highlight-author">Mohit Bansal</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent improvements in abstractive summarization, most current
approaches generate summaries that are not factually consistent with the source
document, severely restricting their trust and usage in real-world
applications. Recent works have shown promising improvements in factuality
error identification using text or dependency arc entailments; however, they do
not consider the entire semantic graph simultaneously. To this end, we propose
FactGraph, a method that decomposes the document and the summary into
structured meaning representations (MR), which are more suitable for factuality
evaluation. MRs describe core semantic concepts and their relations,
aggregating the main content in both document and summary in a canonical form,
and reducing data sparsity. FactGraph encodes such graphs using a graph encoder
augmented with structure-aware adapters to capture interactions among the
concepts based on the graph connectivity, along with text representations using
an adapter-based text encoder. Experiments on different benchmarks for
evaluating factuality show that FactGraph outperforms previous approaches by up
to 15%. Furthermore, FactGraph improves performance on identifying content
verifiability errors and better captures subsentence-level factual
inconsistencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2022 (15 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> LayoutLMv3: <span class="highlight-title">Pre-train</span>ing for Document AI with Unified Text and Image
  Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.08387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.08387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training techniques have achieved remarkable progress in
Document AI. Most multimodal pre-trained models use a masked language modeling
objective to learn bidirectional representations on the text modality, but they
differ in pre-training objectives for the image modality. This discrepancy adds
difficulty to multimodal representation learning. In this paper, we propose
\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with
unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a
word-patch alignment objective to learn cross-modal alignment by predicting
whether the corresponding image patch of a text word is masked. The simple
unified architecture and training objectives make LayoutLMv3 a general-purpose
pre-trained model for both text-centric and image-centric Document AI tasks.
Experimental results show that LayoutLMv3 achieves state-of-the-art performance
not only in text-centric tasks, including form understanding, receipt
understanding, and document visual question answering, but also in
image-centric tasks such as document image classification and document layout
analysis. The code and models are publicly available at
\url{https://aka.ms/layoutlmv3}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Rui<span class="highlight-author">yang Liu</span>, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as "semantic sensitivity". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lifelong <span class="highlight-title">Pretrain</span>ing: Continually Adapting Language Models to Emerging
  Corpora <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PTLMs) are typically learned over a large, static
corpus and further fine-tuned for various downstream tasks. However, when
deployed in the real world, a PTLM-based model must deal with data
distributions that deviate from what the PTLM was initially trained on. In this
paper, we study a lifelong language model pretraining challenge where a PTLM is
continually updated so as to adapt to emerging data. Over a domain-incremental
research paper stream and a chronologically-ordered tweet stream, we
incrementally pretrain a PTLM with different continual learning algorithms, and
keep track of the downstream task performance (after fine-tuning). We evaluate
PTLM's ability to adapt to new corpora while retaining learned knowledge in
earlier corpora. Our experiments show distillation-based approaches to be most
effective in retaining downstream performance in earlier domains. The
algorithms also improve knowledge transfer, allowing models to achieve better
downstream performance over the latest data, and improve temporal
generalization when distribution gaps exist between training and evaluation
because of time. We believe our problem formulation, methods, and analysis will
inspire future studies towards continual pretraining of language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2022; fixed Figure 7 (a)(b) in Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A pragmatic account of the weak evidence effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel A. Barnett, Thomas L. Griffiths, Robert D. Hawkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is not only used to inform. We often seek to persuade by arguing in
favor of a particular view. Persuasion raises a number of challenges for
classical accounts of belief updating, as information cannot be taken at face
value. How should listeners account for a speaker's "hidden agenda" when
incorporating new information? Here, we extend recent probabilistic models of
recursive social reasoning to allow for persuasive goals and show that our
model provides a new pragmatic explanation for why weakly favorable arguments
may backfire, a phenomenon known as the weak evidence effect. Critically, our
model predicts a relationship between belief updating and speaker expectations:
weak evidence should only backfire when speakers are expected to act under
persuasive goals, implying the absence of stronger evidence. We introduce a
simple experimental paradigm called the Stick Contest to measure the extent to
which the weak evidence effect depends on speaker expectations, and show that a
pragmatic listener model accounts for the empirical data better than
alternative models. Our findings suggest potential avenues for rational models
of social reasoning to further illuminate decision-making phenomena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in press at Open Mind</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of Topic Transition in <span class="highlight-title">Dialogue</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.14188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.14188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayank Soni, Brendan Spillane, Emer Gilmartin, Christian Saam, Benjamin R. Cowan, Vincent Wade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transitioning between topics is a natural component of human-human dialog.
Although topic transition has been studied in dialogue for decades, only a
handful of corpora based studies have been performed to investigate the
subtleties of topic transitions. Thus, this study annotates 215 conversations
from the switchboard corpus and investigates how variables such as length,
number of topic transitions, topic transitions share by participants and
turns/topic are related. This work presents an empirical study on topic
transition in switchboard corpus followed by modelling topic transition with a
precision of 83% for in-domain(id) test set and 82% on 10 out-of-domain}(ood)
test set. It is envisioned that this work will help in emulating human-human
like topic transition in open-domain dialog systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Phoneme <span class="highlight-title">BERT</span>: Improving <span class="highlight-title">BERT</span> with Mixed Phoneme and Sup-Phoneme
  Representations for Text to Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.17190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.17190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyan Zhang, Kaitao Song, Xu Tan, Daxin Tan, Yuzi Yan, Yanqing Liu, Gang Wang, Wei Zhou, Tao Qin, Tan Lee, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, leveraging BERT pre-training to improve the phoneme encoder in text
to speech (TTS) has drawn increasing attention. However, the works apply
pre-training with character-based units to enhance the TTS phoneme encoder,
which is inconsistent with the TTS fine-tuning that takes phonemes as input.
Pre-training only with phonemes as input can alleviate the input mismatch but
lack the ability to model rich representations and semantic information due to
limited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a
novel variant of the BERT model that uses mixed phoneme and sup-phoneme
representations to enhance the learning capability. Specifically, we merge the
adjacent phonemes into sup-phonemes and combine the phoneme sequence and the
merged sup-phoneme sequence as the model input, which can enhance the model
capacity to learn rich contextual representations. Experiment results
demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS
performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The
Mixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to
the previous TTS pre-trained model PnG BERT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henry Zhu, Xinchi Chen, Zhiheng Huang, Peng Xu, Andrew Arnold, <span class="highlight-author">Dan Roth</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have achieved high performance on various question
answering (QA) benchmarks, but the explainability of their output remains
elusive. Structured explanations, called entailment trees, were recently
suggested as a way to explain and inspect a QA system's answer. In order to
better generate such entailment trees, we propose an architecture called
Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a
given hypothesis by systematically generating a step-by-step explanation from
textual premises. The IRGR model iteratively searches for suitable premises,
constructing a single entailment step at a time. Contrary to previous
approaches, our method combines generation steps and retrieval of premises,
allowing the model to leverage intermediate conclusions, and mitigating the
input size limit of baseline encoder-decoder models. We conduct experiments
using the EntailmentBank dataset, where we outperform existing benchmarks on
premise retrieval and entailment tree generation, with around 300% gain in
overall correctness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-to-Robot Imitation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Bahl, Abhinav Gupta, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at RSS 2022. Demos at https://human2robot.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ShapeCrafter, a neural network for recursive text-conditioned 3D
shape generation. Existing methods to generate text-conditioned 3D shapes
consume an entire text prompt to generate a 3D shape in a single step. However,
humans tend to describe shapes recursively-we may start with an initial
description and progressively add details based on intermediate results. To
capture this recursive process, we introduce a method to generate a 3D shape
distribution, conditioned on an initial phrase, that gradually evolves as more
phrases are added. Since existing datasets are insufficient for training this
approach, we present Text2Shape++, a large dataset of 369K shape-text pairs
that supports recursive shape generation. To capture local details that are
often used to refine shape descriptions, we build on top of vector-quantized
deep implicit functions that generate a distribution of high-quality shapes.
Results show that our method can generate shapes consistent with text
descriptions, and shapes evolve gradually as more phrases are added. Our method
supports shape editing, extrapolation, and can enable new applications in
human-machine collaboration for creative design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoserNet: Refining Relative Camera Poses Exploiting Object Detections <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Taiana, Matteo Toso, Stuart James, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of the camera poses associated with a set of images commonly
relies on feature matches between the images. In contrast, we are the first to
address this challenge by using objectness regions to guide the pose estimation
problem rather than explicit semantic object detections. We propose Pose
Refiner Network (PoserNet) a light-weight Graph Neural Network to refine the
approximate pair-wise relative camera poses. PoserNet exploits associations
between the objectness regions - concisely expressed as bounding boxes - across
multiple views to globally refine sparsely connected view graphs. We evaluate
on the 7-Scenes dataset across varied sizes of graphs and show how this process
can be beneficial to optimisation-based Motion Averaging algorithms improving
the median error on the rotation by 62 degrees with respect to the initial
estimates obtained based on bounding boxes. Code and data are available at
https://github.com/IIT-PAVIS/PoserNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theseus: A Library for Differentiable Nonlinear Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Pineda, Taosha Fan, Maurizio Monge, Shobha Venkataraman, Paloma Sodhi, Ricky Chen, Joseph Ortiz, Daniel DeTone, Austin Wang, Stuart Anderson, Jing Dong, Brandon Amos, Mustafa Mukadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Theseus, an efficient application-agnostic open source library for
differentiable nonlinear least squares (DNLS) optimization built on PyTorch,
providing a common framework for end-to-end structured learning in robotics and
vision. Existing DNLS implementations are application specific and do not
always incorporate many ingredients important for efficiency. Theseus is
application-agnostic, as we illustrate with several example applications that
are built using the same underlying differentiable components, such as
second-order optimizers, standard costs functions, and Lie groups. For
efficiency, Theseus incorporates support for sparse solvers, automatic
vectorization, batching, GPU acceleration, and gradient computation with
implicit differentiation and direct loss minimization. We do extensive
performance evaluation in a set of applications, demonstrating significant
efficiency gains and better scalability when these features are incorporated.
Project page: https://sites.google.com/view/theseus-ai
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Features Informed Multi-person Human-object Interaction
  Recognition in Videos <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanqiu Qiao, Qianhui Men, Frederick W. B. Li, Yoshiki Kubotani, Shigeo Morishima, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-Object Interaction (HOI) recognition in videos is important for
analyzing human activity. Most existing work focusing on visual features
usually suffer from occlusion in the real-world scenarios. Such a problem will
be further complicated when multiple people and objects are involved in HOIs.
Consider that geometric features such as human pose and object position provide
meaningful information to understand HOIs, we argue to combine the benefits of
both visual and geometric features in HOI recognition, and propose a novel
Two-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The
geometric-level graph models the interdependency between geometric features of
humans and objects, while the fusion-level graph further fuses them with visual
features of humans and objects. To demonstrate the novelty and effectiveness of
our method in challenging scenarios, we propose a new multi-person HOI dataset
(MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120
(single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our
superior performance compared to state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SphereFed: Hyperspherical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Dong, Sai Qian Zhang, Ang Li, H. T. Kung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning aims at training a global model from multiple
decentralized devices (i.e. clients) without exchanging their private local
data. A key challenge is the handling of non-i.i.d. (independent identically
distributed) data across multiple clients that may induce disparities of their
local features. We introduce the Hyperspherical Federated Learning (SphereFed)
framework to address the non-i.i.d. issue by constraining learned
representations of data points to be on a unit hypersphere shared by clients.
Specifically, all clients learn their local representations by minimizing the
loss with respect to a fixed classifier whose weights span the unit
hypersphere. After federated training in improving the global model, this
classifier is further calibrated with a closed-form solution by minimizing a
mean squared loss. We show that the calibration solution can be computed
efficiently and distributedly without direct access of local data. Extensive
experiments indicate that our SphereFed approach is able to improve the
accuracy of multiple existing federated learning algorithms by a considerable
margin (up to 6% on challenging datasets) with enhanced computation and
communication efficiency across datasets and model architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Det6D: A Ground-Aware Full-Pose 3D Object Detector for Improving Terrain
  Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyuan Ouyang, Haoyao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object detection with LiDAR is critical for autonomous driving.
Existing research is all based on the flat-world assumption. However, the
actual road can be complex with steep sections, which breaks the premise.
Current methods suffer from performance degradation in this case due to
difficulty correctly detecting objects on sloped terrain. In this work, we
propose Det6D, the first full-degree-of-freedom 3D object detector without
spatial and postural limitations, to improve terrain robustness. We choose the
point-based framework by founding their capability of detecting objects in the
entire spatial range. To predict full-degree poses, including pitch and roll,
we design a ground-aware orientation branch that leverages the local ground
constraints. Given the difficulty of long-tail non-flat scene data collection
and 6D pose annotation, we present Slope-Aug, a data augmentation method for
synthesizing non-flat terrain from existing datasets recorded in flat scenes.
Experiments on various datasets demonstrate the effectiveness and robustness of
our method in different terrains. We further conducted an extended experiment
to explore how the network predicts the two extra poses. The proposed modules
are plug-and-play for existing point-based frameworks. The code is available at
https://github.com/HITSZ-NRSL/De6D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submit to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCLane: Relay Chain Prediction for Lane Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghua Xu, Xinyue Cai, Bin Zhao, Li Zhang, Hang Xu, Yanwei Fu, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane detection is an important component of many real-world autonomous
systems. Despite a wide variety of lane detection approaches have been
proposed, reporting steady benchmark improvements over time, lane detection
remains a largely unsolved problem. This is because most of the existing lane
detection methods either treat the lane detection as a dense prediction or a
detection task, few of them consider the unique topologies (Y-shape,
Fork-shape, nearly horizontal lane) of the lane markers, which leads to
sub-optimal solution. In this paper, we present a new method for lane detection
based on relay chain prediction. Specifically, our model predicts a
segmentation map to classify the foreground and background region. For each
pixel point in the foreground region, we go through the forward branch and
backward branch to recover the whole lane. Each branch decodes a transfer map
and a distance map to produce the direction moving to the next point, and how
many steps to progressively predict a relay station (next point). As such, our
model is able to capture the keypoints along the lanes. Despite its simplicity,
our strategy allows us to establish new state-of-the-art on four major
benchmarks including TuSimple, CULane, CurveLanes and LLAMAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Synthesis with Disentangled Attributes for Chest X-Ray Nodule
  Augmentation and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrong Shen, Xi Ouyang, Bin Xiao, Jie-Zhi Cheng, Qian Wang, Dinggang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung nodule detection in chest X-ray (CXR) images is common to early
screening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis
(CAD) systems can support radiologists for nodule screening in CXR. However, it
requires large-scale and diverse medical data with high-quality annotations to
train such robust and accurate CADs. To alleviate the limited availability of
such datasets, lung nodule synthesis methods are proposed for the sake of data
augmentation. Nevertheless, previous methods lack the ability to generate
nodules that are realistic with the size attribute desired by the detector. To
address this issue, we introduce a novel lung nodule synthesis framework in
this paper, which decomposes nodule attributes into three main aspects
including shape, size, and texture, respectively. A GAN-based Shape Generator
firstly models nodule shapes by generating diverse shape masks. The following
Size Modulation then enables quantitative control on the diameters of the
generated nodule shapes in pixel-level granularity. A coarse-to-fine gated
convolutional Texture Generator finally synthesizes visually plausible nodule
textures conditioned on the modulated shape masks. Moreover, we propose to
synthesize nodule CXR images by controlling the disentangled nodule attributes
for data augmentation, in order to better compensate for the nodules that are
easily missed in the detection task. Our experiments demonstrate the enhanced
image quality, diversity, and controllability of the proposed lung nodule
synthesis framework. We also validate the effectiveness of our data
augmentation on greatly improving nodule detection performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Recognition based on Multi-Task Learning Framework in the ABAW4
  Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tenggan Zhang, Chuanhe Liu, Xiaolong Liu, Yuchen Liu, Liyu Meng, Lei Sun, Wenqiang Jiang, Fengyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our submission to the Multi-Task Learning (MTL) Challenge
of the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Based on
visual feature representations, we utilize three types of temporal encoder to
capture the temporal context information in the video, including the
transformer based encoder, LSTM based encoder and GRU based encoder. With the
temporal context-aware representations, we employ multi-task framework to
predict the valence, arousal, expression and AU values of the images. In
addition, smoothing processing is applied to refine the initial valence and
arousal predictions, and a model ensemble strategy is used to combine multiple
results from different model setups. Our system achieves the performance of
$1.742$ on MTL Challenge validation dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and
  Editability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Mao, Liujuan Cao, Aurele T. Gnanha, Zhenguo Yang, Qing Li, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GAN inversion aims to invert an input image into the latent space of a
pre-trained GAN. Despite the recent advances in GAN inversion, there remain
challenges to mitigate the tradeoff between distortion and editability, i.e.
reconstructing the input image accurately and editing the inverted image with a
small visual quality drop. The recently proposed pivotal tuning model makes
significant progress towards reconstruction and editability, by using a
two-step approach that first inverts the input image into a latent code, called
pivot code, and then alters the generator so that the input image can be
accurately mapped into the pivot code. Here, we show that both reconstruction
and editability can be improved by a proper design of the pivot code. We
present a simple yet effective method, named cycle encoding, for a high-quality
pivot code. The key idea of our method is to progressively train an encoder in
varying spaces according to a cycle scheme: W->W+->W. This training methodology
preserves the properties of both W and W+ spaces, i.e. high editability of W
and low distortion of W+. To further decrease the distortion, we also propose
to refine the pivot code with an optimization-based method, where a
regularization term is introduced to reduce the degradation in editability.
Qualitative and quantitative comparisons to several state-of-the-art methods
demonstrate the superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computer Vision to the Rescue: Infant Postural Symmetry Estimation from
  Incongruent Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofei Huang, Michael Wan, Lingfei Luan, Bethany Tunik, Sarah Ostadabbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral postural symmetry plays a key role as a potential risk marker for
autism spectrum disorder (ASD) and as a symptom of congenital muscular
torticollis (CMT) in infants, but current methods of assessing symmetry require
laborious clinical expert assessments. In this paper, we develop a computer
vision based infant symmetry assessment system, leveraging 3D human pose
estimation for infants. Evaluation and calibration of our system against ground
truth assessments is complicated by our findings from a survey of human ratings
of angle and symmetry, that such ratings exhibit low inter-rater reliability.
To rectify this, we develop a Bayesian estimator of the ground truth derived
from a probabilistic graphical model of fallible human raters. We show that the
3D infant pose estimation model can achieve 68% area under the receiver
operating characteristic curve performance in predicting the Bayesian aggregate
labels, compared to only 61% from a 2D infant pose estimation model and 60%
from a 3D adult pose estimation model, highlighting the importance of 3D poses
and infant domain knowledge in assessing infant body symmetry. Our survey
analysis also suggests that human ratings are susceptible to higher levels of
bias and inconsistency, and hence our final 3D pose-based symmetry assessment
system is calibrated but not directly supervised by Bayesian aggregate human
ratings, yielding higher levels of consistency and lower levels of inter-limb
assessment bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Representation Learning with <span class="highlight-title">Transformer</span>: A Sequence-to-Sequence
  Perspective <span class="chip">CVPR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhang, Sixiao Zheng, Jiachen Lu, Xinxuan Zhao, Xiatian Zhu, Yanwei Fu, Tao Xiang, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning is the key of solving various vision problems.
Relying on the seminal grid structure priors, convolutional neural networks
(CNNs) have been the de facto standard architectures of most deep vision
models. For instance, classical semantic segmentation methods often adopt a
fully-convolutional network (FCN) with an encoder-decoder architecture. The
encoder progressively reduces the spatial resolution and learns more abstract
visual concepts with larger receptive fields. Since context modeling is
critical for segmentation, the latest efforts have been focused on increasing
the receptive field, through either dilated (i.e., atrous) convolutions or
inserting attention modules. However, the FCN-based architecture remains
unchanged. In this paper, we aim to provide an alternative perspective by
treating visual representation learning generally as a sequence-to-sequence
prediction task. Specifically, we deploy a pure Transformer to encode an image
as a sequence of patches, without local convolution and resolution reduction.
With the global context modeled in every layer of the Transformer, stronger
visual representation can be learned for better tackling vision tasks. In
particular, our segmentation model, termed as SEgmentation TRansformer (SETR),
excels on ADE20K (50.28% mIoU, the first position in the test leaderboard on
the day of submission), Pascal Context (55.83% mIoU) and reaches competitive
results on Cityscapes. Further, we formulate a family of Hierarchical
Local-Global (HLG) Transformers characterized by local attention within windows
and global-attention across windows in a hierarchical and pyramidal
architecture. Extensive experiments show that our method achieves appealing
performance on a variety of visual recognition tasks (e.g., image
classification, object detection and instance segmentation and semantic
segmentation).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of CVPR 2021 paper arXiv:2012.15840</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty in <span class="highlight-title">Contrastive Learning</span>: On the Predictability of Downstream
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Ardeshir, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The superior performance of some of today's state-of-the-art deep learning
models is to some extent owed to extensive (self-)supervised contrastive
pretraining on large-scale datasets. In contrastive learning, the network is
presented with pairs of positive (similar) and negative (dissimilar) datapoints
and is trained to find an embedding vector for each datapoint, i.e., a
representation, which can be further fine-tuned for various downstream tasks.
In order to safely deploy these models in critical decision-making systems, it
is crucial to equip them with a measure of their uncertainty or reliability.
However, due to the pairwise nature of training a contrastive model, and the
lack of absolute labels on the output (an abstract embedding vector), adapting
conventional uncertainty estimation techniques to such models is non-trivial.
In this work, we study whether the uncertainty of such a representation can be
quantified for a single datapoint in a meaningful way. In other words, we
explore if the downstream performance on a given datapoint is predictable,
directly from its pre-trained embedding. We show that this goal can be achieved
by directly estimating the distribution of the training data in the embedding
space and accounting for the local consistency of the representations. Our
experiments show that this notion of uncertainty for an embedding vector often
strongly correlates with its downstream accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking IoU-based Optimization for Single-stage 3D Object Detection <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hualian Sheng, Sijia Cai, Na Zhao, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Min-Jian Zhao, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since Intersection-over-Union (IoU) based optimization maintains the
consistency of the final IoU prediction metric and losses, it has been widely
used in both regression and classification branches of single-stage 2D object
detectors. Recently, several 3D object detection methods adopt IoU-based
optimization and directly replace the 2D IoU with 3D IoU. However, such a
direct computation in 3D is very costly due to the complex implementation and
inefficient backward operations. Moreover, 3D IoU-based optimization is
sub-optimal as it is sensitive to rotation and thus can cause training
instability and detection performance deterioration. In this paper, we propose
a novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the
rotation-sensitivity issue, and produce more efficient optimization objectives
compared with 3D IoU during the training stage. Specifically, our RDIoU
simplifies the complex interactions of regression parameters by decoupling the
rotation variable as an independent term, yet preserving the geometry of 3D
IoU. By incorporating RDIoU into both the regression and classification
branches, the network is encouraged to learn more precise bounding boxes and
concurrently overcome the misalignment issue between classification and
regression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset
validate that our RDIoU method can bring substantial improvement for the
single-stage 3D object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 accepted. The code is available at
  https://github.com/hlsheng1/RDIoU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Interactive Object Segmentation Through a
  Singulation-and-Grasping Approach <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houjian Yu, Changhyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance segmentation with unseen objects is a challenging problem in
unstructured environments. To solve this problem, we propose a robot learning
approach to actively interact with novel objects and collect each object's
training label for further fine-tuning to improve the segmentation model
performance, while avoiding the time-consuming process of manually labeling a
dataset. The Singulation-and-Grasping (SaG) policy is trained through
end-to-end reinforcement learning. Given a cluttered pile of objects, our
approach chooses pushing and grasping motions to break the clutter and conducts
object-agnostic grasping for which the SaG policy takes as input the visual
observations and imperfect segmentation. We decompose the problem into three
subtasks: (1) the object singulation subtask aims to separate the objects from
each other, which creates more space that alleviates the difficulty of (2) the
collision-free grasping subtask; (3) the mask generation subtask to obtain the
self-labeled ground truth masks by using an optical flow-based binary
classifier and motion cue post-processing for transfer learning. Our system
achieves 70% singulation success rate in simulated cluttered scenes. The
interactive segmentation of our system achieves 87.8%, 73.9%, and 69.3% average
precision for toy blocks, YCB objects in simulation and real-world novel
objects, respectively, which outperforms several baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-aware Scalable Deep Compressed Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Chen, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To more efficiently address image compressed sensing (CS) problems, we
present a novel content-aware scalable network dubbed CASNet which collectively
achieves adaptive sampling rate allocation, fine granular scalability and
high-quality reconstruction. We first adopt a data-driven saliency detector to
evaluate the importances of different image regions and propose a
saliency-based block ratio aggregation (BRA) strategy for sampling rate
allocation. A unified learnable generating matrix is then developed to produce
sampling matrix of any CS ratio with an ordered structure. Being equipped with
the optimization-inspired recovery subnet guided by saliency information and a
multi-block training scheme preventing blocking artifacts, CASNet jointly
reconstructs the image blocks sampled at various sampling rates with one single
model. To accelerate training convergence and improve network robustness, we
propose an SVD-based initialization scheme and a random transformation
enhancement (RTE) strategy, which are extensible without introducing extra
parameters. All the CASNet components can be combined and learned end-to-end.
We further provide a four-stage implementation for evaluation and practical
deployments. Experiments demonstrate that CASNet outperforms other CS networks
by a large margin, validating the collaboration and mutual supports among its
components and strategies. Codes are available at
https://github.com/Guaishou74851/CASNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication as a Regular paper in the IEEE Transactions
  on Image Processing (T-IP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for
  COVID-19 Screening With Chest Radiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ma, Pengcheng Xi, Karim Habashy, Ashkan Ebadi, Stéphane Tremblay, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building AI models with trustworthiness is important especially in regulated
areas such as healthcare. In tackling COVID-19, previous work uses
convolutional neural networks as the backbone architecture, which has shown to
be prone to over-caution and overconfidence in making decisions, rendering them
less trustworthy -- a crucial flaw in the context of medical imaging. In this
study, we propose a feature learning approach using Vision Transformers, which
use an attention-based mechanism, and examine the representation learning
capability of Transformers as a new backbone architecture for medical imaging.
Through the task of classifying COVID-19 chest radiographs, we investigate into
whether generalization capabilities benefit solely from Vision Transformers'
architectural advances. Quantitative and qualitative evaluations are conducted
on the trustworthiness of the models, through the use of "trust score"
computation and a visual explainability technique. We conclude that the
attention-based feature learning approach is promising in building trustworthy
deep learning models for healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 39th International Conference on Machine Learning,
  Workshop on Healthcare AI and COVID-19</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DH-AUG: DH Forward Kinematics Model Driven Augmentation for 3D Human
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linzhi Huang, Jiahao Liang, Weihong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the lack of diversity of datasets, the generalization ability of the
pose estimator is poor. To solve this problem, we propose a pose augmentation
solution via DH forward kinematics model, which we call DH-AUG. We observe that
the previous work is all based on single-frame pose augmentation, if it is
directly applied to video pose estimator, there will be several previously
ignored problems: (i) angle ambiguity in bone rotation (multiple solutions);
(ii) the generated skeleton video lacks movement continuity. To solve these
problems, we propose a special generator based on DH forward kinematics model,
which is called DH-generator. Extensive experiments demonstrate that DH-AUG can
greatly increase the generalization ability of the video pose estimator. In
addition, when applied to a single-frame 3D pose estimator, our method
outperforms the previous best pose augmentation method. The source code has
been released at
https://github.com/hlz0606/DH-AUG-DH-Forward-Kinematics-Model-Driven-Augmentation-for-3D-Human-Pose-Estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Semantic Statistics Matching (D2SM) Denoising Network <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangfu Mei, Vishal M. Patel, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ultimate aim of image restoration like denoising is to find an exact
correlation between the noisy and clear image domains. But the optimization of
end-to-end denoising learning like pixel-wise losses is performed in a
sample-to-sample manner, which ignores the intrinsic correlation of images,
especially semantics. In this paper, we introduce the Deep Semantic Statistics
Matching (D2SM) Denoising Network. It exploits semantic features of pretrained
classification networks, then it implicitly matches the probabilistic
distribution of clear images at the semantic feature space. By learning to
preserve the semantic distribution of denoised images, we empirically find our
method significantly improves the denoising capabilities of networks, and the
denoised results can be better understood by high-level vision tasks.
Comprehensive experiments conducted on the noisy Cityscapes dataset demonstrate
the superiority of our method on both the denoising performance and semantic
segmentation accuracy. Moreover, the performance improvement observed on our
extended tasks including super-resolution and dehazing experiments shows its
potentiality as a new general plug-and-play component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022, for Project Page, see https://kfmei.page/d2sm/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Caltech Fish Counting <span class="highlight-title">Dataset</span>: A Benchmark for Multiple-Object
  Tracking and Counting <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Kay, Peter Kulits, Suzanne Stathatos, Siqi Deng, Erik Young, Sara Beery, Grant Van Horn, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for
detecting, tracking, and counting fish in sonar videos. We identify sonar
videos as a rich source of data for advancing low signal-to-noise computer
vision applications and tackling domain generalization in multiple-object
tracking (MOT) and counting. In comparison to existing MOT and counting
datasets, which are largely restricted to videos of people and vehicles in
cities, CFC is sourced from a natural-world domain where targets are not easily
resolvable and appearance features cannot be easily leveraged for target
re-identification. With over half a million annotations in over 1,500 videos
sourced from seven different sonar cameras, CFC allows researchers to train MOT
and counting algorithms and evaluate generalization performance at unseen test
locations. We perform extensive baseline experiments and identify key
challenges and opportunities for advancing the state of the art in
generalization in MOT and counting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. 33 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Room Layout Estimation from a Cubemap of Panorama Image via Deep
  Manhattan Hough Transform <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Zhao, Chao Wen, Zhou Xue, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant geometric structures can be compactly described by global
wireframes in the estimation of 3D room layout from a single panoramic image.
Based on this observation, we present an alternative approach to estimate the
walls in 3D space by modeling long-range geometric patterns in a learnable
Hough Transform block. We transform the image feature from a cubemap tile to
the Hough space of a Manhattan world and directly map the feature to the
geometric output. The convolutional layers not only learn the local
gradient-like line features, but also utilize the global information to
successfully predict occluded walls with a simple network structure. Unlike
most previous work, the predictions are performed individually on each cubemap
tile, and then assembled to get the layout estimation. Experimental results
show that we achieve comparable results with recent state-of-the-art in
prediction accuracy and performance. Code is available at
https://github.com/Starrah/DMH-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Lin Zhang, Ran Song, Lin Ma, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal domain adaptation (UDA) aims to transfer the knowledge of common
classes from source domain to target domain without any prior knowledge on the
label set, which requires to distinguish the unknown samples from the known
ones in the target domain. Recent methods preferred to increase the
inter-sample affinity within a known class, while they ignored the inter-sample
affinity between the unknown samples and the known ones. This paper reveals
that exploiting such inter-sample affinity can significantly improve the
performance of UDA and proposes a knowability-aware UDA framework based on it.
First, we estimate the knowability of each target sample by searching its
neighboring samples in the source domain. Then, we propose an auto-thresholding
scheme applied to the estimated knowability to determine whether a target
sample is unknown or known. Next, in addition to increasing the inter-sample
affinity within each known class like previous methods, we design new losses
based on the estimated knowability to reduce the inter-sample affinity between
the unknown target samples and the known ones. Finally, experiments on four
public datasets demonstrate that our method significantly outperforms existing
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action Quality Assessment with Temporal Parsing <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Desen Zhou, Songyang Zhang, Jian Wang, Errui Ding, Yu Guan, Yang Long, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action Quality Assessment(AQA) is important for action understanding and
resolving the task poses unique challenges due to subtle visual differences.
Existing state-of-the-art methods typically rely on the holistic video
representations for score regression or ranking, which limits the
generalization to capture fine-grained intra-class variation. To overcome the
above limitation, we propose a temporal parsing transformer to decompose the
holistic feature into temporal part-level representations. Specifically, we
utilize a set of learnable queries to represent the atomic temporal patterns
for a specific action. Our decoding process converts the frame representations
to a fixed number of temporally ordered part representations. To obtain the
quality score, we adopt the state-of-the-art contrastive regression based on
the part representations. Since existing AQA datasets do not provide temporal
part-level labels or partitions, we propose two novel loss functions on the
cross attention responses of the decoder: a ranking loss to ensure the
learnable queries to satisfy the temporal order in cross attention and a
sparsity loss to encourage the part representations to be more discriminative.
Extensive experiments show that our proposed method outperforms prior work on
three public AQA benchmarks by a considerable margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Stop Learning: Towards Continual Learning for the CLIP Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, Haoxuan Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Super-Resolution with Deep Dictionary <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunta Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the first success of Dong et al., the deep-learning-based approach has
become dominant in the field of single-image super-resolution. This replaces
all the handcrafted image processing steps of traditional sparse-coding-based
methods with a deep neural network. In contrast to sparse-coding-based methods,
which explicitly create high/low-resolution dictionaries, the dictionaries in
deep-learning-based methods are implicitly acquired as a nonlinear combination
of multiple convolutions. One disadvantage of deep-learning-based methods is
that their performance is degraded for images created differently from the
training dataset (out-of-domain images). We propose an end-to-end
super-resolution network with a deep dictionary (SRDD), where a high-resolution
dictionary is explicitly learned without sacrificing the advantages of deep
learning. Extensive experiments show that explicit learning of high-resolution
dictionary makes the network more robust for out-of-domain test images while
maintaining the performance of the in-domain test images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KinD-LCE Curve Estimation And Retinex Fusion On Low-Light Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochun Lei, Junlin Xie, Zetao Jiang, Weiliang Mai, Zhaoting Gong, Chang Lu, Linjun Lu, Ziqi Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problems of low light image noise and chromatic aberration is a
challenging problem for tasks such as object detection, semantic segmentation,
instance segmentation, etc. In this paper, we propose the algorithm for low
illumination enhancement. KinD-LCE uses the light curve estimation module in
the network structure to enhance the illumination map in the Retinex decomposed
image, which improves the image brightness; we proposed the illumination map
and reflection map fusion module to restore the restored image details and
reduce the detail loss. Finally, we included a total variation loss function to
eliminate noise. Our method uses the GladNet dataset as the training set, and
the LOL dataset as the test set and is validated using ExDark as the dataset
for downstream tasks. Extensive Experiments on the benchmarks demonstrate the
advantages of our method and are close to the state-of-the-art results, which
achieve a PSNR of 19.7216 and SSIM of 0.8213 in terms of metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Kirch, Rafael Pagés, Sergio Arnaldo, Sergio Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VoloGAN, an adversarial domain adaptation network that translates
synthetic RGB-D images of a high-quality 3D model of a person, into RGB-D
images that could be generated with a consumer depth sensor. This system is
especially useful to generate high amount training data for single-view 3D
reconstruction algorithms replicating the real-world capture conditions, being
able to imitate the style of different sensor types, for the same high-end 3D
model database. The network uses a CycleGAN framework with a U-Net architecture
for the generator and a discriminator inspired by SIV-GAN. We use different
optimizers and learning rate schedules to train the generator and the
discriminator. We further construct a loss function that considers image
channels individually and, among other metrics, evaluates the structural
similarity. We demonstrate that CycleGANs can be used to apply adversarial
domain adaptation of synthetic 3D data to train a volumetric video generator
model having only few training samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Disentangled Content Information for Face Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liang, Huafeng Shi, Weihong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network based face forgery detection methods have
achieved remarkable results during training, but struggled to maintain
comparable performance during testing. We observe that the detector is prone to
focus more on content information than artifact traces, suggesting that the
detector is sensitive to the intrinsic bias of the dataset, which leads to
severe overfitting. Motivated by this key observation, we design an easily
embeddable disentanglement framework for content information removal, and
further propose a Content Consistency Constraint (C2C) and a Global
Representation Contrastive Constraint (GRCC) to enhance the independence of
disentangled features. Furthermore, we cleverly construct two unbalanced
datasets to investigate the impact of the content bias. Extensive
visualizations and experiments demonstrate that our framework can not only
ignore the interference of content information, but also guide the detector to
mine suspicious artifact traces and achieve competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> NDF: Neural Deformable Fields for Dynamic Human Modelling <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui<span class="highlight-author">qi Zhang</span>, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Neural Deformable Fields (NDF), a new representation for dynamic
human digitization from a multi-view video. Recent works proposed to represent
a dynamic human body with shared canonical neural radiance fields which links
to the observation space with deformation fields estimations. However, the
learned canonical representation is static and the current design of the
deformation fields is not able to represent large movements or detailed
geometry changes. In this paper, we propose to learn a neural deformable field
wrapped around a fitted parametric body model to represent the dynamic human.
The NDF is spatially aligned by the underlying reference surface. A neural
network is then learned to map pose to the dynamics of NDF. The proposed NDF
representation can synthesize the digitized performer with novel views and
novel poses with a detailed and reasonable dynamic appearance. Experiments show
that our method significantly outperforms recent human synthesis methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures. Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervision</span> Can Be a Good Few-Shot Learner <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Lu, Liangjian Wen, Jianzhuang Liu, Yajing Liu, Xinmei Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing few-shot learning (FSL) methods rely on training with a large
labeled dataset, which prevents them from leveraging abundant unlabeled data.
From an information-theoretic perspective, we propose an effective unsupervised
FSL method, learning representations with self-supervision. Following the
InfoMax principle, our method learns comprehensive representations by capturing
the intrinsic structure of the data. Specifically, we maximize the mutual
information (MI) of instances and their representations with a low-bias MI
estimator to perform self-supervised pre-training. Rather than supervised
pre-training focusing on the discriminable features of the seen classes, our
self-supervised model has less bias toward the seen classes, resulting in
better generalization for unseen classes. We explain that supervised
pre-training and self-supervised pre-training are actually maximizing different
MI objectives. Extensive experiments are further conducted to analyze their FSL
performance with various training settings. Surprisingly, the results show that
self-supervised pre-training can outperform supervised pre-training under the
appropriate conditions. Compared with state-of-the-art FSL methods, our
approach achieves comparable performance on widely used FSL benchmarks without
any labels of the base classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, code: https://github.com/bbbdylan/unisiam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Stage Framework for the 2022 Multi-Structure Segmentation for
  Renal Cancer Treatment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liu, Zhongchen Zhao, Lisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional (3D) kidney parsing on computed tomography angiography
(CTA) images is of great clinical significance. Automatic segmentation of
kidney, renal tumor, renal vein and renal artery benefits a lot on
surgery-based renal cancer treatment. In this paper, we propose a new
nnhra-unet network, and use a multi-stage framework which is based on it to
segment the multi-structure of kidney and participate in the KiPA2022
challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global and Local Features through Gaussian Mixture Models on Image
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darwin Saire, Adín Ramírez Rivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic segmentation task aims at dense classification at the pixel-wise
level. Deep models exhibited progress in tackling this task. However, one
remaining problem with these approaches is the loss of spatial precision, often
produced at the segmented objects' boundaries. Our proposed model addresses
this problem by providing an internal structure for the feature representations
while extracting a global representation that supports the former. To fit the
internal structure, during training, we predict a Gaussian Mixture Model from
the data, which, merged with the skip connections and the decoding stage, helps
avoid wrong inductive biases. Furthermore, our results show that we can improve
semantic segmentation by providing both learning representations (global and
local) with a clustering behavior and combining them. Finally, we present
results demonstrating our advances in Cityscapes and Synthia datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print to appear in IEEE Access. Code available at
  https://gitlab.com/mipl/phgmm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Single Stage Virtual Try-on via Deformable Attention Flows <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Bai, Huiling Zhou, Zhikang Li, <span class="highlight-author">Chang Zhou</span>, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual try-on aims to generate a photo-realistic fitting result given an
in-shop garment and a reference person image. Existing methods usually build up
multi-stage frameworks to deal with clothes warping and body blending
respectively, or rely heavily on intermediate parser-based labels which may be
noisy or even inaccurate. To solve the above challenges, we propose a
single-stage try-on framework by developing a novel Deformable Attention Flow
(DAFlow), which applies the deformable attention scheme to multi-flow
estimation. With pose keypoints as the guidance only, the self- and
cross-deformable attention flows are estimated for the reference person and the
garment images, respectively. By sampling multiple flow fields, the
feature-level and pixel-level information from different semantic areas are
simultaneously extracted and merged through the attention mechanism. It enables
clothes warping and body synthesizing at the same time which leads to
photo-realistic results in an end-to-end manner. Extensive experiments on two
try-on datasets demonstrate that our proposed method achieves state-of-the-art
performance both qualitatively and quantitatively. Furthermore, additional
experiments on the other two image editing tasks illustrate the versatility of
our method for multi-view synthesis and image animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedX: Unsupervised Federated Learning with Cross Knowledge <span class="highlight-title">Distillation</span> <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xing Xie, Meeyoung Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents FedX, an unsupervised federated learning framework. Our
model learns unbiased representation from decentralized and heterogeneous local
data. It employs a two-sided knowledge distillation with contrastive learning
as a core component, allowing the federated system to function without
requiring clients to share any data features. Furthermore, its adaptable
architecture can be used as an add-on module for existing unsupervised
algorithms in federated settings. Experiments show that our model improves
performance significantly (1.58--5.52pp) on five unsupervised algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and will be published at ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Mutual Modulation for <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Cross-Modal</span>
  Super-Resolution <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Dong, Naoto Yokoya, Longguang Wang, Tatsumi Uezato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised cross-modal super-resolution (SR) can overcome the difficulty
of acquiring paired training data, but is challenging because only
low-resolution (LR) source and high-resolution (HR) guide images from different
modalities are available. Existing methods utilize pseudo or weak supervision
in LR space and thus deliver results that are blurry or not faithful to the
source modality. To address this issue, we present a mutual modulation SR
(MMSR) model, which tackles the task by a mutual modulation strategy, including
a source-to-guide modulation and a guide-to-source modulation. In these
modulations, we develop cross-domain adaptive filters to fully exploit
cross-modal spatial dependency and help induce the source to emulate the
resolution of the guide and induce the guide to mimic the modality
characteristics of the source. Moreover, we adopt a cycle consistency
constraint to train MMSR in a fully self-supervised manner. Experiments on
various tasks demonstrate the state-of-the-art performance of our MMSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters for 3D Scene Flow Network <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Wang, Yunzhe Hu, Zhe Liu, Yiyang Zhou, Masayoshi Tomizuka, Wei Zhan, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene flow estimation from point clouds is a low-level 3D motion
perception task in computer vision. Flow embedding is a commonly used technique
in scene flow estimation, and it encodes the point motion between two
consecutive frames. Thus, it is critical for the flow embeddings to capture the
correct overall direction of the motion. However, previous works only search
locally to determine a soft correspondence, ignoring the distant points that
turn out to be the actual matching ones. In addition, the estimated
correspondence is usually from the forward direction of the adjacent point
clouds, and may not be consistent with the estimated correspondence acquired
from the backward direction. To tackle these problems, we propose a novel
all-to-all flow embedding layer with backward reliability validation during the
initial scene flow estimation. Besides, we investigate and compare several
design choices in key components of the 3D scene flow network, including the
point similarity calculation, input elements of predictor, and predictor &
refinement level design. After carefully choosing the most effective designs,
we are able to present a model that achieves the state-of-the-art performance
on FlyingThings3D and KITTI Scene Flow datasets. Our proposed model surpasses
all existing methods by at least 38.2% on FlyingThings3D dataset and 24.7% on
KITTI Scene Flow dataset for EPE3D metric. We release our codes at
https://github.com/IRMVLab/3DFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving
  Cameras in the Wild <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the pose of a moving camera from monocular video is a challenging
problem, especially due to the presence of moving objects in dynamic
environments, where the performance of existing camera pose estimation methods
are susceptible to pixels that are not geometrically consistent. To tackle this
challenge, we present a robust dense indirect structure-from-motion method for
videos that is based on dense correspondence initialized from pairwise optical
flow. Our key idea is to optimize long-range video correspondence as dense
point trajectories and use it to learn robust estimation of motion
segmentation. A novel neural network architecture is proposed for processing
irregular point trajectory data. Camera poses are then estimated and optimized
with global bundle adjustment over the portion of long-range point trajectories
that are classified as static. Experiments on MPI Sintel dataset show that our
system produces significantly more accurate camera trajectories compared to
existing state-of-the-art methods. In addition, our method is able to retain
reasonable accuracy of camera poses on fully static scenes, which consistently
outperforms strong state-of-the-art dense correspondence based methods with
end-to-end deep learning, demonstrating the potential of dense indirect methods
based on optical flow and point trajectories. As the point trajectory
representation is general, we further present results and comparisons on
in-the-wild monocular videos with complex motion of dynamic objects. Code is
available at https://github.com/bytedance/particle-sfm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project page: http://b1ueber2y.me/projects/ParticleSfM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shrinking the Semantic Gap: Spatial Pooling of Local Moment Invariants
  for Copy-Move Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Zhiqiu Huang, Shuren Qi, Yaoshen Yu, Guohua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Copy-move forgery is a manipulation of copying and pasting specific patches
from and to an image, with potentially illegal or unethical uses. Recent
advances in the forensic methods for copy-move forgery have shown increasing
success in detection accuracy and robustness. However, for images with high
self-similarity or strong signal corruption, the existing algorithms often
exhibit inefficient processes and unreliable results. This is mainly due to the
inherent semantic gap between low-level visual representation and high-level
semantic concept. In this paper, we present a very first study of trying to
mitigate the semantic gap problem in copy-move forgery detection, with spatial
pooling of local moment invariants for midlevel image representation. Our
detection method expands the traditional works on two aspects: 1) we introduce
the bag-of-visual-words model into this field for the first time, may meaning a
new perspective of forensic study; 2) we propose a word-to-phrase feature
description and matching pipeline, covering the spatial structure and visual
saliency information of digital images. Extensive experimental results show the
superior performance of our framework over state-of-the-art algorithms in
overcoming the related problems caused by the semantic gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Wurst, Lakshman Balasubramanian, Michael Botsch, Wolfgang Utschick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering traffic scenarios and detecting novel scenario types are required
for scenario-based testing of autonomous vehicles. These tasks benefit from
either good similarity measures or good representations for the traffic
scenarios. In this work, an expert-knowledge aided representation learning for
traffic scenarios is presented. The latent space so formed is used for
successful clustering and novel scenario type detection. Expert-knowledge is
used to define objectives that the latent representations of traffic scenarios
shall fulfill. It is presented, how the network architecture and loss is
designed from these objectives, thereby incorporating expert-knowledge. An
automatic mining strategy for traffic scenarios is presented, such that no
manual labeling is required. Results show the performance advantage compared to
baseline methods. Additionally, extensive analysis of the latent space is
performed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eCDT: Event Clustering for Simultaneous Feature Detection and Tracking- <span class="chip">IROS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin Hu, Yeeun Kim, Hyungtae Lim, Alex Junho Lee, Hyun Myung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrary to other standard cameras, event cameras interpret the world in an
entirely different manner; as a collection of asynchronous events. Despite
event camera's unique data output, many event feature detection and tracking
algorithms have shown significant progress by making detours to frame-based
data representations. This paper questions the need to do so and proposes a
novel event data-friendly method that achieve simultaneous feature detection
and tracking, called event Clustering-based Detection and Tracking (eCDT). Our
method employs a novel clustering method, named as k-NN Classifier-based
Spatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent
polarity events to retrieve event trajectories.With the aid of a Head and Tail
Descriptor Matching process, event clusters that reappear in a different
polarity are continually tracked, elongating the feature tracks. Thanks to our
clustering approach in spatio-temporal space, our method automatically solves
feature detection and feature tracking simultaneously. Also, eCDT can extract
feature tracks at any frequency with an adjustable time window, which does not
corrupt the high temporal resolution of the original event data. Our method
achieves 30% better feature tracking ages compared with the state-of-the-art
approach while also having a low error approximately equal to it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS2022 accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MONet: Multi-scale Overlap Network for Duplication Detection in
  Biomedical Images <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekraam Sabir, Soumyaroop Nandi, Wael AbdAlmageed, Prem Natarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation of biomedical images to misrepresent experimental results has
plagued the biomedical community for a while. Recent interest in the problem
led to the curation of a dataset and associated tasks to promote the
development of biomedical forensic methods. Of these, the largest manipulation
detection task focuses on the detection of duplicated regions between images.
Traditional computer-vision based forensic models trained on natural images are
not designed to overcome the challenges presented by biomedical images. We
propose a multi-scale overlap detection model to detect duplicated image
regions. Our model is structured to find duplication hierarchically, so as to
reduce the number of patch operations. It achieves state-of-the-art performance
overall and on multiple biomedical image categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICIP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MHR-Net: Multiple-Hypothesis Reconstruction of Non-Rigid Shapes from 2D
  Views <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitian Zeng, Xin Yu, Jiaxu Miao, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from
Motion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a
2D view, and it also selects the most likely reconstruction from the set. To
deal with the challenging unsupervised generation of non-rigid shapes, we
develop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net.
The non-rigid shape is first expressed as the sum of a coarse shape basis and a
flexible shape deformation, then multiple hypotheses are generated with
uncertainty modeling of the deformation part. MHR-Net is optimized with
reprojection loss on the basis and the best hypothesis. Furthermore, we design
a new Procrustean Residual Loss, which reduces the rigid rotations between
similar shapes and further improves the performance. Experiments show that
MHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL
and 300-VW datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Adaptive Transformations for Weakly Supervised Point Cloud
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghua Wu, Yicheng Wu, Guosheng Lin, Jianfei Cai, Chen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised point cloud segmentation, i.e. semantically segmenting a
point cloud with only a few labeled points in the whole 3D scene, is highly
desirable due to the heavy burden of collecting abundant dense annotations for
the model training. However, existing methods remain challenging to accurately
segment 3D point clouds since limited annotated data may lead to insufficient
guidance for label propagation to unlabeled data. Considering the
smoothness-based methods have achieved promising progress, in this paper, we
advocate applying the consistency constraint under various perturbations to
effectively regularize unlabeled 3D points. Specifically, we propose a novel
DAT (\textbf{D}ual \textbf{A}daptive \textbf{T}ransformations) model for weakly
supervised point cloud segmentation, where the dual adaptive transformations
are performed via an adversarial strategy at both point-level and region-level,
aiming at enforcing the local and structural smoothness constraints on 3D point
clouds. We evaluate our proposed DAT model with two popular backbones on the
large-scale S3DIS and ScanNet-V2 datasets. Extensive experiments demonstrate
that our model can effectively leverage the unlabeled 3D points and achieve
significant performance gains on both datasets, setting new state-of-the-art
performance for weakly supervised point cloud segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Future Captioning Model for Explaining Likely Collisions in
  Daily Tasks <span class="chip">ICIP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motonari Kambara, Komei Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domestic service robots that support daily tasks are a promising solution for
elderly or disabled people. It is crucial for domestic service robots to
explain the collision risk before they perform actions. In this paper, our aim
is to generate a caption about a future event. We propose the Relational Future
Captioning Model (RFCM), a crossmodal language generation model for the future
captioning task. The RFCM has the Relational Self-Attention Encoder to extract
the relationships between events more effectively than the conventional
self-attention in transformers. We conducted comparison experiments, and the
results show the RFCM outperforms a baseline method on two datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at ICIP2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incremental Task Learning with Incremental Rank Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rakib Hyder, Ken Shao, Boyu Hou, Panos Markopoulos, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental Task learning (ITL) is a category of continual learning that
seeks to train a single network for multiple tasks (one after another), where
training data for each task is only available during the training of that task.
Neural networks tend to forget older tasks when they are trained for the newer
tasks; this property is often known as catastrophic forgetting. To address this
issue, ITL methods use episodic memory, parameter regularization, masking and
pruning, or extensible network structures. In this paper, we propose a new
incremental task learning framework based on low-rank factorization. In
particular, we represent the network weights for each layer as a linear
combination of several rank-1 matrices. To update the network for a new task,
we learn a rank-1 (or low-rank) matrix and add that to the weights of every
layer. We also introduce an additional selector vector that assigns different
weights to the low-rank matrices learned for the previous tasks. We show that
our approach performs better than the current state-of-the-art methods in terms
of accuracy and forgetting. Our method also offers better memory efficiency
compared to episodic memory- and mask-based approaches. Our code will be
available at https://github.com/CSIPlab/task-increment-rank-update.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at
  https://github.com/CSIPlab/task-increment-rank-update.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Unaware Knowledge <span class="highlight-title">Distillation</span> for Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bytasandram Yaswanth Reddy, Shiv Ram Dubey, Rakesh Kumar Sanodiya, Ravi Ranjan Prasad Karn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing data-dependent hashing methods use large backbone networks with
millions of parameters and are computationally complex. Existing knowledge
distillation methods use logits and other features of the deep (teacher) model
and as knowledge for the compact (student) model, which requires the teacher's
network to be fine-tuned on the context in parallel with the student model on
the context. Training teacher on the target context requires more time and
computational resources. In this paper, we propose context unaware knowledge
distillation that uses the knowledge of the teacher model without fine-tuning
it on the target context. We also propose a new efficient student model
architecture for knowledge distillation. The proposed approach follows a
two-step process. The first step involves pre-training the student model with
the help of context unaware knowledge distillation from the teacher model. The
second step involves fine-tuning the student model on the context of image
retrieval. In order to show the efficacy of the proposed approach, we compare
the retrieval results, no. of parameters and no. of operations of the student
models with the teacher models under different retrieval frameworks, including
deep cauchy hashing (DCH) and central similarity quantization (CSQ). The
experimental results confirm that the proposed approach provides a promising
trade-off between the retrieval results and efficiency. The code used in this
paper is released publicly at \url{https://github.com/satoru2001/CUKDFIR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International Conference on Computer Vision and Machine
  Intelligence (CVMI), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Is MattEr: Temporal <span class="highlight-title">Self-supervision</span> for Video <span class="highlight-title">Transformer</span>s <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Jaehyung Kim, Dongyoon Han, Hwanjun Song, Jung-Woo Ha, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding temporal dynamics of video is an essential aspect of learning
better video representations. Recently, transformer-based architectural designs
have been extensively explored for video tasks due to their capability to
capture long-term dependency of input sequences. However, we found that these
Video Transformers are still biased to learn spatial dynamics rather than
temporal ones, and debiasing the spurious correlation is critical for their
performance. Based on the observations, we design simple yet effective
self-supervised tasks for video models to learn temporal dynamics better.
Specifically, for debiasing the spatial bias, our method learns the temporal
order of video frames as extra self-supervision and enforces the randomly
shuffled frames to have low-confidence outputs. Also, our method learns the
temporal flow direction of video tokens among consecutive frames for enhancing
the correlation toward temporal dynamics. Under various video action
recognition tasks, we demonstrate the effectiveness of our method and its
compatibility with state-of-the-art Video Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022. Code is available at
  https://github.com/alinlab/temporal-selfsupervision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moment Centralization based Gradient Descent Optimizers for
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanth Sadu, Shiv Ram Dubey, SR Sreeja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have shown very appealing performance
for many computer vision applications. The training of CNNs is generally
performed using stochastic gradient descent (SGD) based optimization
techniques. The adaptive momentum-based SGD optimizers are the recent trends.
However, the existing optimizers are not able to maintain a zero mean in the
first-order moment and struggle with optimization. In this paper, we propose a
moment centralization-based SGD optimizer for CNNs. Specifically, we impose the
zero mean constraints on the first-order moment explicitly. The proposed moment
centralization is generic in nature and can be integrated with any of the
existing adaptive momentum-based optimizers. The proposed idea is tested with
three state-of-the-art optimization techniques, including Adam, Radam, and
Adabelief on benchmark CIFAR10, CIFAR100, and TinyImageNet datasets for image
classification. The performance of the existing optimizers is generally
improved when integrated with the proposed moment centralization. Further, The
results of the proposed moment centralization are also better than the existing
gradient centralization. The analytical analysis using the toy example shows
that the proposed method leads to a shorter and smoother optimization
trajectory. The source code is made publicly available at
\url{https://github.com/sumanthsadhu/MC-optimizer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International Conference on Computer Vision and Machine
  Intelligence (CVMI), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> RCRN: Real-world Character Image Restoration Network via Skeleton
  Extraction <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daqian Shi, Xiaolei Diao, <span class="highlight-author">Hao Tan</span>g, Xiaomin Li, Hao Xing, Hao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing high-quality character image datasets is challenging because
real-world images are often affected by image degradation. There are
limitations when applying current image restoration methods to such real-world
character images, since (i) the categories of noise in character images are
different from those in general images; (ii) real-world character images
usually contain more complex image degradation, e.g., mixed noise at different
noise levels. To address these problems, we propose a real-world character
restoration network (RCRN) to effectively restore degraded character images,
where character skeleton information and scale-ensemble feature extraction are
utilized to obtain better restoration performance. The proposed method consists
of a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet
aims to preserve the structural consistency of the character and normalize
complex noise. Then, CiRNet reconstructs clean images from degraded character
images and their skeletons. Due to the lack of benchmarks for real-world
character image restoration, we constructed a dataset containing 1,606
character images with real-world degradation to evaluate the validity of the
proposed method. The experimental results demonstrate that RCRN outperforms
state-of-the-art methods quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CharFormer: A Glyph Fusion based Attentive Framework for High-precision
  Character Image Denoising <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daqian Shi, Xiaolei Diao, Lida Shi, <span class="highlight-author">Hao Tan</span>g, Yang Chi, Chuntao Li, Hao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Degraded images commonly exist in the general sources of character images,
leading to unsatisfactory character recognition results. Existing methods have
dedicated efforts to restoring degraded character images. However, the
denoising results obtained by these methods do not appear to improve character
recognition performance. This is mainly because current methods only focus on
pixel-level information and ignore critical features of a character, such as
its glyph, resulting in character-glyph damage during the denoising process. In
this paper, we introduce a novel generic framework based on glyph fusion and
attention mechanisms, i.e., CharFormer, for precisely recovering character
images without changing their inherent glyphs. Unlike existing frameworks,
CharFormer introduces a parallel target task for capturing additional
information and injecting it into the image denoising backbone, which will
maintain the consistency of character glyphs during character image denoising.
Moreover, we utilize attention-based networks for global-local feature
interaction, which will help to deal with blind denoising and enhance denoising
performance. We compare CharFormer with state-of-the-art methods on multiple
datasets. The experimental results show the superiority of CharFormer
quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Centric Unsupervised Image Captioning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihang Meng, David Yang, Xuefei Cao, Ashish Shah, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning is a longstanding problem in the field of computer vision
and natural language processing. To date, researchers have produced impressive
state-of-the-art performance in the age of deep learning. Most of these
state-of-the-art, however, requires large volume of annotated image-caption
pairs in order to train their models. When given an image dataset of interests,
practitioner needs to annotate the caption for each image in the training set
and this process needs to happen for each newly collected image dataset. In
this paper, we explore the task of unsupervised image captioning which utilizes
unpaired images and texts to train the model so that the texts can come from
different sources than the images. A main school of research on this topic that
has been shown to be effective is to construct pairs from the images and texts
in the training set according to their overlap of objects. Unlike in the
supervised setting, these constructed pairings are however not guaranteed to
have fully overlapping set of objects. Our work in this paper overcomes this by
harvesting objects corresponding to a given sentence from the training set,
even if they don't belong to the same image. When used as input to a
transformer, such mixture of objects enables larger if not full object
coverage, and when supervised by the corresponding sentence, produced results
that outperform current state of the art unsupervised methods by a significant
margin. Building upon this finding, we further show that (1) additional
information on relationship between objects and attributes of objects also
helps in boosting performance; and (2) our method also extends well to
non-English image captioning, which usually suffers from a scarcer level of
annotations. Our findings are supported by strong empirical results. Our code
is available at https://github.com/zihangm/obj-centric-unsup-caption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Primitive-based Shape Abstraction via Nonparametric Bayesian Inference <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.14714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.14714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wu, Weixiao Liu, Sipu Ruan, Gregory S. Chirikjian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape abstraction has drawn great interest over the years. Apart from
low-level representations such as meshes and voxels, researchers also seek to
semantically abstract complex objects with basic geometric primitives. Recent
deep learning methods rely heavily on datasets, with limited generality to
unseen categories. Furthermore, abstracting an object accurately yet with a
small number of primitives still remains a challenge. In this paper, we propose
a novel non-parametric Bayesian statistical method to infer an abstraction,
consisting of an unknown number of geometric primitives, from a point cloud. We
model the generation of points as observations sampled from an infinite mixture
of Gaussian Superquadric Taper Models (GSTM). Our approach formulates the
abstraction as a clustering problem, in which: 1) each point is assigned to a
cluster via the Chinese Restaurant Process (CRP); 2) a primitive representation
is optimized for each cluster, and 3) a merging post-process is incorporated to
provide a concise representation. We conduct extensive experiments on two
datasets. The results indicate that our method outperforms the state-of-the-art
in terms of accuracy and is generalizable to various types of objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantized GAN for Complex Music Generation from Dance Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Zhu, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal
framework that generates complex musical samples conditioned on dance videos.
Our proposed framework takes dance video frames and human body motions as
input, and learns to generate music samples that plausibly accompany the
corresponding input. Unlike most existing conditional music generation works
that generate specific types of mono-instrumental sounds using symbolic audio
representations (e.g., MIDI), and that usually rely on pre-defined musical
synthesizers, in this work we generate dance music in complex styles (e.g.,
pop, breaking, etc.) by employing a Vector Quantized (VQ) audio representation,
and leverage both its generality and high abstraction capacity of its symbolic
and continuous counterparts. By performing an extensive set of experiments on
multiple datasets, and following a comprehensive evaluation protocol, we assess
the generative qualities of our proposal against alternatives. The attained
quantitative results, which measure the music consistency, beats
correspondence, and music diversity, demonstrate the effectiveness of our
proposed method. Last but not least, we curate a challenging dance-music
dataset of in-the-wild TikTok videos, which we use to further demonstrate the
efficacy of our approach in real-world applications -- and which we hope to
serve as a starting point for relevant future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and code at https://github.com/L-YeZhu/D2M-GAN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Score-based Generative Models with Preconditioned Diffusion
  Sampling <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02196v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02196v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Ma, Li Zhang, Xiatian Zhu, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Code is available at https://github.com/fudan-zvg/PDS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Label Correction is a Good Booster When Learning with Extremely
  Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Xiangyu Peng, Shuo Yang, Jianfei Yang, Zheng Zhu, Xinchao Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning with noisy labels has aroused much research interest since data
annotations, especially for large-scale datasets, may be inevitably imperfect.
Recent approaches resort to a semi-supervised learning problem by dividing
training samples into clean and noisy sets. This paradigm, however, is prone to
significant degeneration under heavy label noise, as the number of clean
samples is too small for conventional methods to behave well. In this paper, we
introduce a novel framework, termed as LC-Booster, to explicitly tackle
learning under extreme noise. The core idea of LC-Booster is to incorporate
label correction into the sample selection, so that more purified samples,
through the reliable label correction, can be utilized for training, thereby
alleviating the confirmation bias. Experiments show that LC-Booster advances
state-of-the-art results on several noisy-label benchmarks, including CIFAR-10,
CIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\% noise
ratio, LC-Booster achieves 92.9\% and 48.4\% accuracy on CIFAR-10 and
CIFAR-100, surpassing state-of-the-art methods by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LC-Booster is an efficient method for LNL problems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoViT: Low Latency Graph-based Audio-Visual Voice Separation <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan F. Montesinos, Venkatesh S. Kadandale, Gloria Haro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an audio-visual approach for voice separation which
produces state-of-the-art results at a low latency in two scenarios: speech and
singing voice. The model is based on a two-stage network. Motion cues are
obtained with a lightweight graph convolutional network that processes face
landmarks. Then, both audio and motion features are fed to an audio-visual
transformer which produces a fairly good estimation of the isolated target
source. In a second stage, the predominant voice is enhanced with an audio-only
network. We present different ablation studies and comparison to
state-of-the-art methods. Finally, we explore the transferability of models
trained for speech separation in the task of singing voice separation. The
demos, code, and weights are available in https://ipcv.github.io/VoViT/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tan M. Dinh, Rang Nguyen, Binh-Son Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we conduct a study on the state-of-the-art methods for
text-to-image synthesis and propose a framework to evaluate these methods. We
consider syntheses where an image contains a single or multiple objects. Our
study outlines several issues in the current evaluation pipeline: (i) for image
quality assessment, a commonly used metric, e.g., Inception Score (IS), is
often either miscalibrated for the single-object case or misused for the
multi-object case; (ii) for text relevance and object accuracy assessment,
there is an overfitting phenomenon in the existing R-precision (RP) and
Semantic Object Accuracy (SOA) metrics, respectively; (iii) for multi-object
case, many vital factors for evaluation, e.g., object fidelity, positional
alignment, counting alignment, are largely dismissed; (iv) the ranking of the
methods based on current metrics is highly inconsistent with real images. To
overcome these issues, we propose a combined set of existing and new metrics to
systematically evaluate the methods. For existing metrics, we offer an improved
version of IS named IS* by using temperature scaling to calibrate the
confidence of the classifier used by IS; we also propose a solution to mitigate
the overfitting issues of RP and SOA. For new metrics, we develop counting
alignment, positional alignment, object-centric IS, and object-centric FID
metrics for evaluating the multi-object case. We show that benchmarking with
our bag of metrics results in a highly consistent ranking among existing
methods that is well-aligned with human evaluation. As a by-product, we create
AttnGAN++, a simple but strong baseline for the benchmark by stabilizing the
training of AttnGAN using spectral normalization. We also release our toolbox,
so-called TISE, for advocating fair and consistent evaluation of text-to-image
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022; TISE toolbox is available at
  https://github.com/VinAIResearch/tise-toolbox</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VL-LTR: Learning Class-wise <span class="highlight-title">Visual-Linguistic</span> Representation for
  Long-Tailed Visual Recognition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13579v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13579v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyao Tian, Wenhai Wang, Xizhou Zhu, Jifeng Dai, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based models encounter challenges when processing long-tailed
data in the real world. Existing solutions usually employ some balancing
strategies or transfer learning to deal with the class imbalance problem, based
on the image modality. In this work, we present a visual-linguistic long-tailed
recognition framework, termed VL-LTR, and conduct empirical studies on the
benefits of introducing text modality for long-tailed recognition (LTR).
Compared to existing approaches, the proposed VL-LTR has the following merits.
(1) Our method can not only learn visual representation from images but also
learn corresponding linguistic representation from noisy class-level text
descriptions collected from the Internet; (2) Our method can effectively use
the learned visual-linguistic representation to improve the visual recognition
performance, especially for classes with fewer image samples. We also conduct
extensive experiments and set the new state-of-the-art performance on
widely-used LTR benchmarks. Notably, our method achieves 77.2% overall accuracy
on ImageNet-LT, which significantly outperforms the previous best method by
over 17 points, and is close to the prevailing performance training on the full
ImageNet. Code is available at https://github.com/ChangyaoTian/VL-LTR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022; 14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REALY: Rethinking the Evaluation of 3D Face Reconstruction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenghao Chai, Haoxian Zhang, Jing Ren, Di Kang, Zhengzhuo Xu, Xuefei Zhe, Chun Yuan, Linchao Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of 3D face reconstruction results typically relies on a rigid
shape alignment between the estimated 3D model and the ground-truth scan. We
observe that aligning two shapes with different reference points can largely
affect the evaluation results. This poses difficulties for precisely diagnosing
and improving a 3D face reconstruction method. In this paper, we propose a
novel evaluation approach with a new benchmark REALY, consists of 100 globally
aligned face scans with accurate facial keypoints, high-quality region masks,
and topology-consistent meshes. Our approach performs region-wise shape
alignment and leads to more accurate, bidirectional correspondences during
computing the shape errors. The fine-grained, region-wise evaluation results
provide us detailed understandings about the performance of state-of-the-art 3D
face reconstruction methods. For example, our experiments on single-image based
reconstruction methods reveal that DECA performs the best on nose regions,
while GANFit performs better on cheek regions. Besides, a new and high-quality
3DMM basis, HIFI3D++, is further derived using the same procedure as we
construct REALY to align and retopologize several 3D face datasets. We will
release REALY, HIFI3D++, and our new evaluation pipeline at
https://realy3dface.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022, camera-ready version; Project page:
  https://realy3dface.com; Code: https://github.com/czh-98/REALY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIMO: Gaze-Informed Human Motion Prediction in Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu, Yebin Liu, C. Karen Liu, Leonidas J. Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with the eye gaze that serves as a
surrogate for inferring human intent. By employing inertial sensors for motion
capture, our data collection is not tied to specific scenes, which further
boosts the motion dynamics observed from our subjects. We perform an extensive
study of the benefits of leveraging the eye gaze for ego-centric human motion
prediction with various state-of-the-art architectures. Moreover, to realize
the full potential of the gaze, we propose a novel network architecture that
enables bidirectional communication between the gaze and motion branches. Our
network achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from eye gaze and the denoised gaze
feature modulated by the motion. Code and data can be found at
https://github.com/y-zheng18/GIMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODA: A Real-World Road Corner Case <span class="highlight-title">Dataset</span> for Object Detection in
  Autonomous Driving <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, Xiaodan Liang, Zhenguo Li, Hang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Perspective on Stabilizing GANs training: Direct Adversarial
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.09041v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.09041v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Li, Pengfei Xia, Rentuo Tao, Hongjing Niu, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) are the most popular image generation
models that have achieved remarkable progress on various computer vision tasks.
However, training instability is still one of the open problems for all
GAN-based algorithms. Quite a number of methods have been proposed to stabilize
the training of GANs, the focuses of which were respectively put on the loss
functions, regularization and normalization technologies, training algorithms,
and model architectures. Different from the above methods, in this paper, a new
perspective on stabilizing GANs training is presented. It is found that
sometimes the images produced by the generator act like adversarial examples of
the discriminator during the training process, which may be part of the reason
causing the unstable training of GANs. With this finding, we propose the Direct
Adversarial Training (DAT) method to stabilize the training process of GANs.
Furthermore, we prove that the DAT method is able to minimize the Lipschitz
constant of the discriminator adaptively. The advanced performance of DAT is
verified on multiple loss functions, network architectures, hyper-parameters,
and datasets. Specifically, DAT achieves significant improvements of 11.5% FID
on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10
unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom
unconditional generation based on SSGAN. Code will be available at
https://github.com/iceli1007/DAT-GAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Emerging Topics in Computational
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-world Semantic Segmentation via Contrasting and Clustering
  <span class="highlight-title">Vision-Language</span> Embedding <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quande Liu, Youpeng Wen, Jianhua Han, Chunjing Xu, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To bridge the gap between supervised semantic segmentation and real-world
applications that acquires one model to recognize arbitrary new concepts,
recent zero-shot segmentation attracts a lot of attention by exploring the
relationships between unseen and seen object categories, yet requiring large
amounts of densely-annotated data with diverse base classes. In this paper, we
propose a new open-world semantic segmentation pipeline that makes the first
attempt to learn to segment semantic objects of various open-world categories
without any efforts on dense annotations, by purely exploiting the
image-caption data that naturally exist on the Internet. Our method,
Vision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a
text encoder to generate visual and text embeddings for the image-caption data,
with two core components that endow its segmentation ability: First, the image
encoder is jointly trained with a vision-based contrasting and a cross-modal
contrasting, which encourage the visual embeddings to preserve both
fine-grained semantics and high-level category information that are crucial for
the segmentation task. Furthermore, an online clustering head is devised over
the image encoder, which allows to dynamically segment the visual embeddings
into distinct semantic groups such that they can be classified by comparing
with various text embeddings to complete our segmentation pipeline. Experiments
show that without using any data with dense annotations, our method can
directly segment objects of arbitrary categories, outperforming zero-shot
segmentation methods that require data labeling on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with
  100M FLOPs <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.03815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.03815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Zhang, Zhuo Chen, Zhao Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a Collaboration of Experts (CoE) framework to pool
together the expertise of multiple networks towards a common aim. Each expert
is an individual network with expertise on a unique portion of the dataset,
which enhances the collective capacity. Given a sample, an expert is selected
by the delegator, which simultaneously outputs a rough prediction to support
early termination. To fulfill this framework, we propose three modules to impel
each model to play its role, namely weight generation module (WGM), label
generation module (LGM) and variance calculation module (VCM). Our method
achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy
with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE
further achieves the accuracy of 80.0% with only 100M FLOPs for the first time.
More importantly, our method is hardware friendly and achieves a 3-6x speedup
compared with some existing conditional computation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Deep Classifiers Agree: Analyzing Correlations between Learning
  Order and Image Statistics <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.08997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.08997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iuliia Pliushch, Martin Mundt, Nicolas Lupp, Visvanathan Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although a plethora of architectural variants for deep classification has
been introduced over time, recent works have found empirical evidence towards
similarities in their training process. It has been hypothesized that neural
networks converge not only to similar representations, but also exhibit a
notion of empirical agreement on which data instances are learned first.
Following in the latter works$'$ footsteps, we define a metric to quantify the
relationship between such classification agreement over time, and posit that
the agreement phenomenon can be mapped to core statistics of the investigated
dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,
ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to
be independent of specific architectures, training hyper-parameters or labels,
albeit follows an ordering according to image statistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ECCV 2022. Version includes supplementary
  material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Learning for Image-based Detection of Molecular Alterations
  in Digital Pathology <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Teichmann, Andre Aichert, Hanibal Bohnenberger, Philipp Ströbel, Tobias Heimann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for classification of whole slide images (WSI) in digital
pathology predominantly utilize a two-stage learning pipeline. The first stage
identifies areas of interest (e.g. tumor tissue), while the second stage
processes cropped tiles from these areas in a supervised fashion. During
inference, a large number of tiles are combined into a unified prediction for
the entire slide. A major drawback of such approaches is the requirement for
task-specific auxiliary labels which are not acquired in clinical routine. We
propose a novel learning pipeline for WSI classification that is trainable
end-to-end and does not require any auxiliary annotations. We apply our
approach to predict molecular alterations for a number of different use-cases,
including detection of microsatellite instability in colorectal tumors and
prediction of specific mutations for colon, lung, and breast cancer cases from
The Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to
be competitive with state of the art two-stage pipelines. We believe our
approach can facilitate future research in digital pathology and contribute to
solve a large range of problems around the prediction of cancer phenotypes,
hopefully enabling personalized therapies for more patients in future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2022; 8.5 Pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style-Hallucinated Dual Consistency Learning for Domain Generalized
  Semantic Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the task of synthetic-to-real domain generalized
semantic segmentation, which aims to learn a model that is robust to unseen
real-world scenes using only synthetic data. The large domain shift between
synthetic and real-world data, including the limited source environmental
variations and the large distribution gap between synthetic and real-world
data, significantly hinders the model performance on unseen real-world scenes.
In this work, we propose the Style-HAllucinated Dual consistEncy learning
(SHADE) framework to handle such domain shift. Specifically, SHADE is
constructed based on two consistency constraints, Style Consistency (SC) and
Retrospection Consistency (RC). SC enriches the source situations and
encourages the model to learn consistent representation across
style-diversified samples. RC leverages real-world knowledge to prevent the
model from overfitting to synthetic data and thus largely keeps the
representation consistent between the synthetic and real-world models.
Furthermore, we present a novel style hallucination module (SHM) to generate
style-diversified samples that are essential to consistency learning. SHM
selects basis styles from the source distribution, enabling the model to
dynamically generate diverse and realistic samples during training. Experiments
show that our SHADE yields significant improvement and outperforms
state-of-the-art methods by 5.05% and 8.35% on the average mIoU of three
real-world datasets on single- and multi-source settings, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facial Depth and Normal Estimation using Single Dual-Pixel Camera <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjun Kang, Jaesung Choe, Hyowon Ha, Hae-Gon Jeon, Sunghoon Im, In So Kweon, KuK-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many mobile manufacturers recently have adopted Dual-Pixel (DP) sensors in
their flagship models for faster auto-focus and aesthetic image captures.
Despite their advantages, research on their usage for 3D facial understanding
has been limited due to the lack of datasets and algorithmic designs that
exploit parallax in DP images. This is because the baseline of sub-aperture
images is extremely narrow and parallax exists in the defocus blur region. In
this paper, we introduce a DP-oriented Depth/Normal network that reconstructs
the 3D facial geometry. For this purpose, we collect a DP facial data with more
than 135K images for 101 persons captured with our multi-camera structured
light systems. It contains the corresponding ground-truth 3D models including
depth map and surface normal in metric scale. Our dataset allows the proposed
matching network to be generalized for 3D facial depth/normal estimation. The
proposed network consists of two novel modules: Adaptive Sampling Module and
Adaptive Normal Module, which are specialized in handling the defocus blur in
DP images. Finally, the proposed method achieves state-of-the-art performances
over recent DP-based depth/normal estimation methods. We also demonstrate the
applicability of the estimated depth/normal to face spoofing and relighting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github page : https://github.com/MinJunKang/DualPixelFace To be
  appeared in ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust outlier detection by de-biasing VAE likelihoods <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.08760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.08760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Chauhan, Barath Mohan U, Pradeep Shenoy, Manish Gupta, Devarajan Sridharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep networks often make confident, yet, incorrect, predictions when tested
with outlier data that is far removed from their training distributions.
Likelihoods computed by deep generative models (DGMs) are a candidate metric
for outlier detection with unlabeled data. Yet, previous studies have shown
that DGM likelihoods are unreliable and can be easily biased by simple
transformations to input data. Here, we examine outlier detection with
variational autoencoders (VAEs), among the simplest of DGMs. We propose novel
analytical and algorithmic approaches to ameliorate key biases with VAE
likelihoods. Our bias corrections are sample-specific, computationally
inexpensive, and readily computed for various decoder visible distributions.
Next, we show that a well-known image pre-processing technique -- contrast
stretching -- extends the effectiveness of bias correction to further improve
outlier detection. Our approach achieves state-of-the-art accuracies with nine
grayscale and natural image datasets, and demonstrates significant advantages
-- both with speed and performance -- over four recent, competing approaches.
In summary, lightweight remedies suffice to achieve robust outlier detection
with VAEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. 20 pages and 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CF-ViT: A General Coarse-to-Fine Method for Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03821v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03821v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViT) have made many breakthroughs in computer vision
tasks. However, considerable redundancy arises in the spatial dimension of an
input image, leading to massive computational costs. Therefore, We propose a
coarse-to-fine vision transformer (CF-ViT) to relieve computational burden
while retaining performance in this paper. Our proposed CF-ViT is motivated by
two important observations in modern ViT models: (1) The coarse-grained patch
splitting can locate informative regions of an input image. (2) Most images can
be well recognized by a ViT model in a small-length token sequence. Therefore,
our CF-ViT implements network inference in a two-stage manner. At coarse
inference stage, an input image is split into a small-length patch sequence for
a computationally economical classification. If not well recognized, the
informative patches are identified and further re-split in a fine-grained
granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For
example, without any compromise on performance, CF-ViT reduces 53% FLOPs of
LV-ViT, and also achieves 2.01x throughput.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure PLP-SLAM: Efficient Sparse Mapping and Localization using
  Point, Line and Plane for Monocular, RGB-D and Stereo Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwen Shu, Jiaxuan Wang, Alain Pagani, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper demonstrates a visual SLAM system that utilizes point and line
cloud for robust camera localization, simultaneously, with an embedded
piece-wise planar reconstruction (PPR) module which in all provides a
structural map. To build a scale consistent map in parallel with tracking, such
as employing a single camera brings the challenge of reconstructing geometric
primitives with scale ambiguity, and further introduces the difficulty in graph
optimization of bundle adjustment (BA). We address these problems by proposing
several run-time optimizations on the reconstructed lines and planes. The
system is then extended with depth and stereo sensors based on the design of
the monocular framework. The results show that our proposed SLAM tightly
incorporates the semantic features to boost both frontend tracking as well as
backend optimization. We evaluate our system exhaustively on various datasets,
and open-source our code for the community
(https://github.com/PeterFWS/Structure-PLP-SLAM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The pre-print version, v2 add supplementary materials, code
  open-source: https://github.com/PeterFWS/Structure-PLP-SLAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class-agnostic Object Detection with <span class="highlight-title">Multi-modal</span> <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11430v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11430v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and novel objects. In this paper, we advocate
that existing methods lack a top-down supervision signal governed by
human-understandable semantics. For the first time in literature, we
demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned
image-text pairs can effectively bridge this gap. Our extensive experiments
across various domains and novel objects show the state-of-the-art performance
of MViTs to localize generic objects in images. Based on the observation that
existing MViTs do not include multi-scale feature processing and usually
require longer training schedules, we develop an efficient MViT architecture
using multi-scale deformable attention and late vision-language fusion. We show
the significance of MViT proposals in a diverse range of applications including
open-world object detection, salient and camouflage object detection,
supervised and self-supervised detection tasks. Further, MViTs can adaptively
generate proposals given a specific language query and thus offer enhanced
interactability. Code: \url{https://git.io/J1HPY}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CA-SSL: Class-Agnostic Semi-Supervised Learning for Detection and
  Segmentation <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.04966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.04966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Qi, Jason Kuen, Zhe Lin, Jiuxiang Gu, Fengyun Rao, Dian Li, Weidong Guo, Zhen Wen, Ming-Hsuan Yang, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve instance-level detection/segmentation performance, existing
self-supervised and semi-supervised methods extract either task-unrelated or
task-specific training signals from unlabeled data. We show that these two
approaches, at the two extreme ends of the task-specificity spectrum, are
suboptimal for the task performance. Utilizing too little task-specific
training signals causes underfitting to the ground-truth labels of downstream
tasks, while the opposite causes overfitting to the ground-truth labels. To
this end, we propose a novel Class-Agnostic Semi-Supervised Learning (CA-SSL)
framework to achieve a more favorable task-specificity balance in extracting
training signals from unlabeled data. CA-SSL has three training stages that act
on either ground-truth labels (labeled data) or pseudo labels (unlabeled data).
This decoupling strategy avoids the complicated scheme in traditional SSL
methods that balances the contributions from both data types. Especially, we
introduce a warmup training stage to achieve a more optimal balance in task
specificity by ignoring class information in the pseudo labels, while
preserving localization training signals. As a result, our warmup model can
better avoid underfitting/overfitting when fine-tuned on the ground-truth
labels in detection and segmentation tasks. Using 3.6M unlabeled data, we
achieve a significant performance gain of 4.7% over ImageNet-pretrained
baseline on FCOS object detection. In addition, our warmup model demonstrates
excellent transferability to other detection and segmentation frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared in ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast-MoCo: Boost Momentum-based <span class="highlight-title">Contrastive Learning</span> with Combinatorial
  Patches <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzheng Ci, Chen Lin, Lei Bai, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive-based self-supervised learning methods achieved great success in
recent years. However, self-supervision requires extremely long training epochs
(e.g., 800 epochs for MoCo v3) to achieve promising results, which is
unacceptable for the general academic community and hinders the development of
this topic. This work revisits the momentum-based contrastive learning
frameworks and identifies the inefficiency in which two augmented views
generate only one positive pair. We propose Fast-MoCo - a novel framework that
utilizes combinatorial patches to construct multiple positive pairs from two
augmented views, which provides abundant supervision signals that bring
significant acceleration with neglectable extra computational cost. Fast-MoCo
trained with 100 epochs achieves 73.5% linear evaluation accuracy, similar to
MoCo v3 (ResNet-50 backbone) trained with 800 epochs. Extra training (200
epochs) further improves the result to 75.1%, which is on par with
state-of-the-art methods. Experiments on several downstream tasks also confirm
the effectiveness of Fast-MoCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2022 European Conference on Computer
  Vision (ECCV 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Analysis of the Object Detection Pipeline on UAVs <span class="chip">WACV23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Amadeus Varga, Sebastian Koch, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An object detection pipeline comprises a camera that captures the scene and
an object detector that processes these images. The quality of the images
directly affects the performance of the object detector. Many works nowadays
focus either on improving the image quality or improving the object detection
models independently, but neglect the importance of joint optimization of the
two subsystems. The goal of this paper is to tune the detection throughput and
accuracy of existing object detectors in the remote sensing scenario by
focusing on optimizing the input images tailored to the object detector. To
achieve this, we empirically analyze the influence of two selected camera
calibration parameters (camera distortion correction and gamma correction) and
five image parameters (quantization, compression, resolution, color model,
additional channels) for these applications. For our experiments, we utilize
three UAV data sets from different domains and a mixture of large and small
state-of-the-art object detector models to provide an extensive evaluation of
the influence of the pipeline parameters. Finally, we realize an object
detection pipeline prototype on an embedded platform for an UAV and give a best
practice recommendation for building object detection pipelines based on our
findings. We show that not all parameters have an equal impact on detection
accuracy and data throughput, and that by using a suitable compromise between
parameters we are able to achieve higher detection accuracy for lightweight
object detection models, while keeping the same data throughput.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted WACV23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using
  Unsupervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paula Delgado-Santos, Ruben Tolosana, Richard Guest, Ruben Vera, Farzin Deravi, Aythami Morales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous studies in the literature have already shown the potential of
biometrics on mobile devices for authentication purposes. However, it has been
shown that, the learning processes associated to biometric systems might expose
sensitive personal information about the subjects. This study proposes
GaitPrivacyON, a novel mobile gait biometrics verification approach that
provides accurate authentication results while preserving the sensitive
information of the subject. It comprises two modules: i) a convolutional
Autoencoder that transforms attributes of the biometric raw data, such as the
gender or the activity being performed, into a new privacy-preserving
representation; and ii) a mobile gait verification system based on the
combination of Convolutional Neural Networks (CNNs) and Recurrent Neural
Networks (RNNs) with a Siamese architecture. The main advantage of
GaitPrivacyON is that the first module (convolutional Autoencoder) is trained
in an unsupervised way, without specifying the sensitive attributes of the
subject to protect. The experimental results achieved using two popular
databases (MotionSense and MobiAct) suggest the potential of GaitPrivacyON to
significantly improve the privacy of the subject while keeping user
authentication results higher than 99% Area Under the Curve (AUC). To the best
of our knowledge, this is the first mobile gait verification approach that
considers privacy-preserving methods trained in an unsupervised way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability of deep vision-based autonomous driving systems: <span class="highlight-title">Review</span>
  and challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.05307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.05307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey reviews explainability methods for vision-based self-driving
systems trained with behavior cloning. The concept of explainability has
several facets and the need for explainability is strong in driving, a
safety-critical application. Gathering contributions from several research
fields, namely computer vision, deep learning, autonomous driving, explainable
AI (X-AI), this survey tackles several points. First, it discusses definitions,
context, and motivation for gaining more interpretability and explainability
from self-driving systems, as well as the challenges that are specific to this
application. Second, methods providing explanations to a black-box self-driving
system in a post-hoc fashion are comprehensively organized and detailed. Third,
approaches from the literature that aim at building more interpretable
self-driving systems by design are presented and discussed in detail. Finally,
remaining open-challenges and potential future research directions are
identified and examined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersFormer: 3D Lane Detection via Perspective <span class="highlight-title">Transformer</span> and the
  OpenLane Benchmark <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for 3D lane detection have been recently proposed to address the
issue of inaccurate lane layouts in many autonomous driving scenarios
(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to
their simple designs of the spatial transformation between front view and
bird's eye view (BEV) and the lack of a realistic dataset. Towards these
issues, we present PersFormer: an end-to-end monocular 3D lane detector with a
novel Transformer-based spatial feature transformation module. Our model
generates BEV features by attending to related front-view local regions with
camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor
design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing
the feature consistency and sharing the benefits of multi-task learning.
Moreover, we release one of the first large-scale real-world 3D lane datasets:
OpenLane, with high-quality annotation and scenario diversity. OpenLane
contains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories,
along with scene tags and the closed-in-path object annotations to encourage
the development of lane detection and more industrial-related autonomous
driving methods. We show that PersFormer significantly outperforms competitive
baselines in the 3D lane detection task on our new OpenLane dataset as well as
Apollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art
algorithms in the 2D task on OpenLane. The project page is available at
https://github.com/OpenPerceptionX/PersFormer_3DLane and OpenLane dataset is
provided at https://github.com/OpenPerceptionX/OpenLane.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022 (Oral). Project page:
  https://github.com/OpenPerceptionX/PersFormer_3DLane | OpenLane dataset:
  https://github.com/OpenPerceptionX/OpenLane</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmenting white matter hyperintensities on isotropic three-dimensional
  Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison
  of Deep learning tools on a Norwegian national imaging database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Soria Roevang, Per Selnes, Bradley John MacIntosh, Inge Rasmus Groote, Lene Paalhaugen, Carole Sudre, Tormod Fladby, Atle Bjoernerud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated segmentation of white matter hyperintensities (WMHs) is an
essential step in neuroimaging analysis of Magnetic Resonance Imaging (MRI).
Fluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast that is
particularly useful to visualize and quantify WMHs, a hallmark of cerebral
small vessel disease and Alzheimer's disease (AD). Clinical MRI protocols
migrate to a three-dimensional (3D) FLAIR-weighted acquisition to enable high
spatial resolution in all three voxel dimensions. The current study details the
deployment of deep learning tools to enable automated WMH segmentation and
characterization from 3D FLAIR-weighted images acquired as part of a national
AD imaging initiative.
  Among 642 participants (283 male, mean age: (65.18 +/- 9.33) years) from the
DDI study, two in-house networks were trained and validated across five
national collection sites. Three models were tested on a held-out subset of the
internal data from the 642 participants and an external dataset with 29 cases
from an international collaborator. These test sets were evaluated
independently. Five established WMH performance metrics were used for
comparison against ground truth human-in-the-loop segmentation.
  Results of the three networks tested, the 3D nnU-Net had the best performance
with an average dice similarity coefficient score of 0.78 +/- 0.10, performing
better than both the in-house developed 2.5D model and the SOTA Deep Bayesian
network.
  With the increasing use of 3D FLAIR-weighted images in MRI protocols, our
results suggest that WMH segmentation models can be trained on 3D data and
yield WMH segmentation performance that is comparable to or better than
state-of-the-art without the need for including T1-weighted image series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, 7 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Face Recognition with Learnable Privacy Budgets in
  Frequency Domain <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhen Ji, Huan Wang, Yuge Huang, Jiaxiang Wu, Xingkun Xu, Shouhong Ding, ShengChuan Zhang, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code is available at
  https://github.com/Tencent/TFace/tree/master/recognition/tasks/dctdp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Study of the performance and scalability of federated learning for
  medical imaging with intermittent clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judith Sáinz-Pardo Díaz, Álvaro López García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy and area under the curve) and reduction of execution times
will be studied with respect to the classical case (the centralized approach).
Different clients will be simulated from the training data, selected in an
unbalanced manner, i.e., they do not all have the same number of data. The
results of considering three or ten clients are exposed and compared between
them and against the centralized case. Two approaches to follow will be
analyzed in the case of intermittent clients, as in a real scenario some
clients may leave the training, and some new ones may enter the training. The
evolution of the results for the test set in terms of accuracy, area under the
curve and execution time is shown as the number of clients into which the
original data is divided increases. Finally, improvements and future work in
the field are proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Feature Alignment Network for Unsupervised Video Object
  Segmentation <span class="chip">ECCV-2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gensheng Pei, Fumin Shen, Yazhou Yao, Guo-Sen Xie, Zhenmin Tang, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow is an easily conceived and precious cue for advancing
unsupervised video object segmentation (UVOS). Most of the previous methods
directly extract and fuse the motion and appearance features for segmenting
target objects in the UVOS setting. However, optical flow is intrinsically an
instantaneous velocity of all pixels among consecutive frames, thus making the
motion features not aligned well with the primary objects among the
corresponding frames. To solve the above challenge, we propose a concise,
practical, and efficient architecture for appearance and motion feature
alignment, dubbed hierarchical feature alignment network (HFAN). Specifically,
the key merits in HFAN are the sequential Feature AlignMent (FAM) module and
the Feature AdaptaTion (FAT) module, which are leveraged for processing the
appearance and motion features hierarchically. FAM is capable of aligning both
appearance and motion features with the primary object semantic
representations, respectively. Further, FAT is explicitly designed for the
adaptive fusion of appearance and motion features to achieve a desirable
trade-off between cross-modal features. Extensive experiments demonstrate the
effectiveness of the proposed HFAN, which reaches a new state-of-the-art
performance on DAVIS-16, achieving 88.7 $\mathcal{J}\&\mathcal{F}$ Mean, i.e.,
a relative improvement of 3.5% over the best published result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCPL: <span class="highlight-title">Contrastive</span> Coherence Preserving Loss for Versatile Style Transfer <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04808v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04808v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Wu, Zhen Zhu, Junping Du, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to devise a universally versatile style transfer method
capable of performing artistic, photo-realistic, and video style transfer
jointly, without seeing videos during training. Previous single-frame methods
assume a strong constraint on the whole image to maintain temporal consistency,
which could be violated in many cases. Instead, we make a mild and reasonable
assumption that global inconsistency is dominated by local inconsistencies and
devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local
patches. CCPL can preserve the coherence of the content source during style
transfer without degrading stylization. Moreover, it owns a neighbor-regulating
mechanism, resulting in a vast reduction of local distortions and considerable
visual quality improvement. Aside from its superior performance on versatile
style transfer, it can be easily extended to other tasks, such as
image-to-image translation. Besides, to better fuse content and style features,
we propose Simple Covariance Transformation (SCT) to effectively align
second-order statistics of the content feature with the style feature.
Experiments demonstrate the effectiveness of the resulting model for versatile
style transfer, when armed with CCPL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022 as an oral paper; code url:
  https://github.com/JarrentWu1031/CCPL Video demo:
  https://youtu.be/scZuJCXhL14</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FEAR: Fast, Efficient, Accurate and Robust Visual Tracker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.07957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.07957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasyl Borsuk, Roman Vei, Orest Kupyn, Tetiana Martyniuk, Igor Krashenyi, Jiři Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FEAR, a family of fast, efficient, accurate, and robust Siamese
visual trackers. We present a novel and efficient way to benefit from
dual-template representation for object model adaption, which incorporates
temporal information with only a single learnable parameter. We further improve
the tracker architecture with a pixel-wise fusion block. By plugging-in
sophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L
trackers surpass most Siamese trackers on several academic benchmarks in both
accuracy and efficiency. Employed with the lightweight backbone, the optimized
version FEAR-XS offers more than 10 times faster tracking than current Siamese
trackers while maintaining near state-of-the-art results. FEAR-XS tracker is
2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In
addition, we expand the definition of the model efficiency by introducing FEAR
benchmark that assesses energy consumption and execution speed. We show that
energy consumption is a limiting factor for trackers on mobile devices. Source
code, pretrained models, and evaluation protocol are available at
https://github.com/PinataFarms/FEARTracker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOTR: End-to-End Multiple-Object Tracking with <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.03247v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.03247v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, Yichen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal modeling of objects is a key challenge in multiple object tracking
(MOT). Existing methods track by associating detections through motion-based
and appearance-based similarity heuristics. The post-processing nature of
association prevents end-to-end exploitation of temporal variations in video
sequence. In this paper, we propose MOTR, which extends DETR and introduces
track query to model the tracked instances in the entire video. Track query is
transferred and updated frame-by-frame to perform iterative prediction over
time. We propose tracklet-aware label assignment to train track queries and
newborn object queries. We further propose temporal aggregation network and
collective average loss to enhance temporal relation modeling. Experimental
results on DanceTrack show that MOTR significantly outperforms state-of-the-art
method, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our
concurrent works, TrackFormer and TransTrack, on association performance. MOTR
can serve as a stronger baseline for future research on temporal modeling and
Transformer-based trackers. Code is available at
https://github.com/megvii-research/MOTR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. Code is available at
  https://github.com/megvii-research/MOTR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhou, Zheng Ge, Songtao Liu, Weixin Mao, Zeming Li, Haiyan Yu, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To date, the most powerful semi-supervised object detectors (SS-OD) are based
on pseudo-boxes, which need a sequence of post-processing with fine-tuned
hyper-parameters. In this work, we propose replacing the sparse pseudo-boxes
with the dense prediction as a united and straightforward form of pseudo-label.
Compared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any
post-processing method, thus retaining richer information. We also introduce a
region selection technique to highlight the key information while suppressing
the noise carried by dense labels. We name our proposed SS-OD algorithm that
leverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows
superior performance under various settings compared with the pseudo-box-based
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot
  Image Classification <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yi, Xiaoqian Shen, Yunhao Gou, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main question we address in this paper is how to scale up visual
recognition of unseen classes, also known as zero-shot learning, to tens of
thousands of categories as in the ImageNet-21K benchmark. At this scale,
especially with many fine-grained categories included in ImageNet-21K, it is
critical to learn quality visual semantic representations that are
discriminative enough to recognize unseen classes and distinguish them from
seen ones. We propose a \emph{H}ierarchical \emph{G}raphical knowledge
\emph{R}epresentation framework for the confidence-based classification method,
dubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp
class inheritance relations by utilizing hierarchical conceptual knowledge. Our
method significantly outperformed all existing techniques, boosting the
performance by 7\% compared to the runner-up approach on the ImageNet-21K
benchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We
also analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and
3-hops, demonstrating its generalization ability. Our benchmark and code are
available at https://kaiyi.me/p/hgrnet.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PETR: Position Embedding Transformation for Multi-View 3D Object
  Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05625v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05625v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfei Liu, Tiancai Wang, Xiangyu Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. Code is available at
  \url{https://github.com/megvii-research/PETR}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Learning the Right Attention Point for Feature Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.06257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.06257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqiang Lin, Pengdi Huang, Chi-Wing Fu, Kai Xu, Hao Zhang, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel attention-based mechanism to learn enhanced point features
for point cloud processing tasks, e.g., classification and segmentation. Unlike
prior works, which were trained to optimize the weights of a pre-selected set
of attention points, our approach learns to locate the best attention points to
maximize the performance of a specific task, e.g., point cloud classification.
Importantly, we advocate the use of single attention point to facilitate
semantic understanding in point feature learning. Specifically, we formulate a
new and simple convolution, which combines convolutional features from an input
point and its corresponding learned attention point, or LAP, for short. Our
attention mechanism can be easily incorporated into state-of-the-art point
cloud classification and segmentation networks. Extensive experiments on common
benchmarks such as ModelNet40, ShapeNetPart, and S3DIS all demonstrate that our
LAP-enabled networks consistently outperform the respective original networks,
as well as other competitive alternatives, which employ multiple attention
points, either pre-selected or learned under our LAP framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Object Detection with a <span class="highlight-title">Self-supervised</span> Lidar Scene Flow Backbone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekim Yurtsever, Emeç Erçelik, Mingyu Liu, Zhijie Yang, Hanzhen Zhang, Pınar Topçam, Maximilian Listl, Yılmaz Kaan Çaylı, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art lidar-based 3D object detection methods rely on supervised
learning and large labeled datasets. However, annotating lidar data is
resource-consuming, and depending only on supervised learning limits the
applicability of trained models. Self-supervised training strategies can
alleviate these issues by learning a general point cloud backbone model for
downstream 3D vision tasks. Against this backdrop, we show the relationship
between self-supervised multi-frame flow representations and single-frame 3D
detection hypotheses. Our main contribution leverages learned flow and motion
representations and combines a self-supervised backbone with a supervised 3D
detection head. First, a self-supervised scene flow estimation model is trained
with cycle consistency. Then, the point cloud encoder of this model is used as
the backbone of a single-frame 3D object detection head model. This second 3D
object detection model learns to utilize motion representations to distinguish
dynamic objects exhibiting different movement patterns. Experiments on KITTI
and nuScenes benchmarks show that the proposed self-supervised pre-training
increases 3D detection performance significantly.
https://github.com/emecercelik/ssl-3d-detection.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compound Prototype Matching for Few-shot Action Recognition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05515v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05515v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijin Yang, Yifei Huang, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot action recognition aims to recognize novel action classes using only
a small number of labeled training samples. In this work, we propose a novel
approach that first summarizes each video into compound prototypes consisting
of a group of global prototypes and a group of focused prototypes, and then
compares video similarity based on the prototypes. Each global prototype is
encouraged to summarize a specific aspect from the entire video, for example,
the start/evolution of the action. Since no clear annotation is provided for
the global prototypes, we use a group of focused prototypes to focus on certain
timestamps in the video. We compare video similarity by matching the compound
prototypes between the support and query videos. The global prototypes are
directly matched to compare videos from the same perspective, for example, to
compare whether two actions start similarly. For the focused prototypes, since
actions have various temporal variations in the videos, we apply bipartite
matching to allow the comparison of actions with different temporal positions
and shifts. Experiments demonstrate that our proposed method achieves
state-of-the-art results on multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMRotate: A Rotated Object Detection Benchmark using PyTorch <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhou, Xue Yang, Gefan Zhang, Jiabao Wang, Yanyi Liu, Liping Hou, Xue Jiang, Xingzhao Liu, Junchi Yan, Chengqi Lyu, Wenwei Zhang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 tables, MMRotate is accepted by ACM MM 2022 (OS Track).
  Yue Zhou and Xue Yang provided equal contribution. The code is publicly
  released at https://github.com/open-mmlab/mmrotate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point
  Clouds for Closing Domain Gap <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongwei Chen, Zihao Wang, Longkun Zou, Ke Chen, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic analyses of object point clouds are largely driven by releasing of
benchmarking datasets, including synthetic ones whose instances are sampled
from object CAD models. However, learning from synthetic data may not
generalize to practical scenarios, where point clouds are typically incomplete,
non-uniformly distributed, and noisy. Such a challenge of Simulation-to-Reality
(Sim2Real) domain gap could be mitigated via learning algorithms of domain
adaptation; however, we argue that generation of synthetic point clouds via
more physically realistic rendering is a powerful alternative, as systematic
non-uniform noise patterns can be captured. To this end, we propose an
integrated scheme consisting of physically realistic synthesis of object point
clouds via rendering stereo images via projection of speckle patterns onto CAD
models and a novel quasi-balanced self-training designed for more balanced data
distribution by sparsity-driven selection of pseudo labeled samples for long
tailed classes. Experiment results can verify the effectiveness of our method
as well as both of its modules for unsupervised domain adaptation on point
cloud classification, achieving the state-of-the-art performance. Source codes
and the SpeckleNet synthetic dataset are available at
https://github.com/Gorilla-Lab-SCUT/QS3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> LayoutLMv3: <span class="highlight-title">Pre-train</span>ing for Document AI with Unified Text and Image
  Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.08387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.08387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training techniques have achieved remarkable progress in
Document AI. Most multimodal pre-trained models use a masked language modeling
objective to learn bidirectional representations on the text modality, but they
differ in pre-training objectives for the image modality. This discrepancy adds
difficulty to multimodal representation learning. In this paper, we propose
\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with
unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a
word-patch alignment objective to learn cross-modal alignment by predicting
whether the corresponding image patch of a text word is masked. The simple
unified architecture and training objectives make LayoutLMv3 a general-purpose
pre-trained model for both text-centric and image-centric Document AI tasks.
Experimental results show that LayoutLMv3 achieves state-of-the-art performance
not only in text-centric tasks, including form understanding, receipt
understanding, and document visual question answering, but also in
image-centric tasks such as document image classification and document layout
analysis. The code and models are publicly available at
\url{https://aka.ms/layoutlmv3}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Content Preserving Image Translation with Texture Co-occurrence and
  Spatial Self-Similarity for Texture Debiasing and Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongkyun Kang, Dongkyu Won, Miguel Luna, Philip Chikontwe, Kyung Soo Hong, June Hong Ahn, Sang Hyun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained on datasets with texture bias usually perform poorly on
out-of-distribution samples since biased representations are embedded into the
model. Recently, various image translation and debiasing methods have attempted
to disentangle texture biased representations for downstream tasks, but
accurately discarding biased features without altering other relevant
information is still challenging. In this paper, we propose a novel framework
that leverages image translation to generate additional training images using
the content of a source image and the texture of a target image with a
different bias property to explicitly mitigate texture bias when training a
model on a target task. Our model ensures texture similarity between the target
and generated images via a texture co-occurrence loss while preserving content
details from source images with a spatial self-similarity loss. Both the
generated and original training images are combined to train improved
classification or segmentation models robust to inconsistent texture bias.
Evaluation on five classification- and two segmentation-datasets with known
texture biases demonstrates the utility of our method, and reports significant
improvements over recent state-of-the-art methods in all cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing, Reconstructing, and Simulating: the UrbanScene3D <span class="highlight-title">Dataset</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.04286v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.04286v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UrbanScene3D, a large-scale data platform for research of urban
scene perception and reconstruction. UrbanScene3D contains over 128k
high-resolution images covering 16 scenes including large-scale real urban
regions and synthetic cities with 136 km^2 area in total. The dataset also
contains high-precision LiDAR scans and hundreds of image sets with different
observation patterns, which provide a comprehensive benchmark to design and
evaluate aerial path planning and 3D reconstruction algorithms. In addition,
the dataset, which is built on Unreal Engine and Airsim simulator together with
the manually annotated unique instance label for each building in the dataset,
enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D
bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with
physical engine and lighting system not only produce variety of data but also
enable users to simulate cars or drones in the proposed urban environment for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera ready; Project page: https://vcc.tech/UrbanScene3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial
  Training <span class="chip">ECCV
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are vulnerable to adversarial attacks: adding well-crafted,
imperceptible perturbations to their input can modify their output. Adversarial
training is one of the most effective approaches in training robust models
against such attacks. However, it is much slower than vanilla training of
neural networks since it needs to construct adversarial examples for the entire
training data at every iteration, hampering its effectiveness. Recently, Fast
Adversarial Training (FAT) was proposed that can obtain robust models
efficiently. However, the reasons behind its success are not fully understood,
and more importantly, it can only train robust models for $\ell_\infty$-bounded
attacks as it uses FGSM during training. In this paper, by leveraging the
theory of coreset selection, we show how selecting a small subset of training
data provides a general, more principled approach toward reducing the time
complexity of robust training. Unlike existing methods, our approach can be
adapted to a wide variety of training objectives, including TRADES,
$\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental
results indicate that our approach speeds up adversarial training by 2-3 times
while experiencing a slight reduction in the clean and robust accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 17th European Conference on Computer Vision (ECCV
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show Me What I Like: Detecting User-Specific Video Highlights Using
  Content-Based Multi-Head Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uttaran Bhattacharya, Gang Wu, Stefano Petrangeli, Viswanathan Swaminathan, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to detect individualized highlights for users on given
target videos based on their preferred highlight clips marked on previous
videos they have watched. Our method explicitly leverages the contents of both
the preferred clips and the target videos using pre-trained features for the
objects and the human activities. We design a multi-head attention mechanism to
adaptively weigh the preferred clips based on their object- and
human-activity-based contents, and fuse them using these weights into a single
feature representation for each user. We compute similarities between these
per-user feature representations and the per-frame features computed from the
desired target videos to estimate the user-specific highlight clips from the
target videos. We test our method on a large-scale highlight detection dataset
containing the annotated highlights of individual users. Compared to current
baselines, we observe an absolute improvement of 2-4% in the mean average
precision of the detected highlights. We also perform extensive ablation
experiments on the number of preferred highlight clips associated with each
user as well as on the object- and human-activity-based feature representations
to validate that our method is indeed both content-based and user-specific.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patch-level Representation Learning for <span class="highlight-title">Self-supervised</span> Vision
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Hankook Lee, Jaehyung Kim, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-supervised learning (SSL) methods have shown impressive results
in learning visual representations from unlabeled images. This paper aims to
improve their performance further by utilizing the architectural advantages of
the underlying neural network, as the current state-of-the-art visual pretext
tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.
In particular, we focus on Vision Transformers (ViTs), which have gained much
attention recently as a better architectural choice, often outperforming
convolutional networks for various visual tasks. The unique characteristic of
ViT is that it takes a sequence of disjoint patches from an image and processes
patch-level representations internally. Inspired by this, we design a simple
yet effective visual pretext task, coined SelfPatch, for learning better
patch-level representations. To be specific, we enforce invariance against each
patch and its neighbors, i.e., each patch treats similar neighboring patches as
positive samples. Consequently, training ViTs with SelfPatch learns more
semantically meaningful relations among patches (without using human-annotated
labels), which can be beneficial, in particular, to downstream tasks of a dense
prediction type. Despite its simplicity, we demonstrate that it can
significantly improve the performance of existing SSL methods for various
visual tasks, including object detection and semantic segmentation.
Specifically, SelfPatch significantly improves the recent self-supervised ViT,
DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance
segmentation, and +2.9 mIoU on ADE20K semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022 (Oral). Code is available at
  https://github.com/alinlab/SelfPatch</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-to-Robot Imitation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Bahl, Abhinav Gupta, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at RSS 2022. Demos at https://human2robot.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theseus: A Library for Differentiable Nonlinear Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Pineda, Taosha Fan, Maurizio Monge, Shobha Venkataraman, Paloma Sodhi, Ricky Chen, Joseph Ortiz, Daniel DeTone, Austin Wang, Stuart Anderson, Jing Dong, Brandon Amos, Mustafa Mukadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Theseus, an efficient application-agnostic open source library for
differentiable nonlinear least squares (DNLS) optimization built on PyTorch,
providing a common framework for end-to-end structured learning in robotics and
vision. Existing DNLS implementations are application specific and do not
always incorporate many ingredients important for efficiency. Theseus is
application-agnostic, as we illustrate with several example applications that
are built using the same underlying differentiable components, such as
second-order optimizers, standard costs functions, and Lie groups. For
efficiency, Theseus incorporates support for sparse solvers, automatic
vectorization, batching, GPU acceleration, and gradient computation with
implicit differentiation and direct loss minimization. We do extensive
performance evaluation in a set of applications, demonstrating significant
efficiency gains and better scalability when these features are incorporated.
Project page: https://sites.google.com/view/theseus-ai
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Convolutional Neural Network Approach to Supernova Time-Series
  Classification <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helen Qu, Masao Sako, Anais Moller, Cyrille Doux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the brightest objects in the universe, supernovae (SNe) are powerful
explosions marking the end of a star's lifetime. Supernova (SN) type is defined
by spectroscopic emission lines, but obtaining spectroscopy is often
logistically unfeasible. Thus, the ability to identify SNe by type using
time-series image data alone is crucial, especially in light of the increasing
breadth and depth of upcoming telescopes. We present a convolutional neural
network method for fast supernova time-series classification, with observed
brightness data smoothed in both the wavelength and time directions with
Gaussian process regression. We apply this method to full duration and
truncated SN time-series, to simulate retrospective as well as real-time
classification performance. Retrospective classification is used to
differentiate cosmologically useful Type Ia SNe from other SN types, and this
method achieves >99% accuracy on this task. We are also able to differentiate
between 6 SN types with 60% accuracy given only two nights of data and 98%
accuracy retrospectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ICML 2022 Workshop on Machine Learning for
  Astrophysics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regret Minimization with Noisy Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdian, Jieming Mao, Kangning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a typical optimization problem, the task is to pick one of a number of
options with the lowest cost or the highest value. In practice, these
cost/value quantities often come through processes such as measurement or
machine learning, which are noisy, with quantifiable noise distributions. To
take these noise distributions into account, one approach is to assume a prior
for the values, use it to build a posterior, and then apply standard stochastic
optimization to pick a solution. However, in many practical applications, such
prior distributions may not be available. In this paper, we study such
scenarios using a regret minimization model.
  In our model, the task is to pick the highest one out of $n$ values. The
values are unknown and chosen by an adversary, but can be observed through
noisy channels, where additive noises are stochastically drawn from known
distributions. The goal is to minimize the regret of our selection, defined as
the expected difference between the highest and the selected value on the
worst-case choices of values. We show that the na\"ive algorithm of picking the
highest observed value has regret arbitrarily worse than the optimum, even when
$n = 2$ and the noises are unbiased in expectation. On the other hand, we
propose an algorithm which gives a constant-approximation to the optimal regret
for any $n$. Our algorithm is conceptually simple, computationally efficient,
and requires only minimal knowledge of the noise distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep equilibrium networks are sensitive to initialization statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atish Agarwala, Samuel S. Schoenholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep equilibrium networks (DEQs) are a promising way to construct models
which trade off memory for compute. However, theoretical understanding of these
models is still lacking compared to traditional networks, in part because of
the repeated application of a single set of weights. We show that DEQs are
sensitive to the higher order statistics of the matrix families from which they
are initialized. In particular, initializing with orthogonal or symmetric
matrices allows for greater stability in training. This gives us a practical
prescription for initializations which allow for training with a broader range
of initial weight scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unrolled algorithms for group synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Janco, Tamir Bendory
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The group synchronization problem involves estimating a collection of group
elements from noisy measurements of their pairwise ratios. This task is a key
component in many computational problems, including the molecular
reconstruction problem in single-particle cryo-electron microscopy (cryo-EM).
The standard methods to estimate the group elements are based on iteratively
applying linear and non-linear operators. Motivated by the structural
similarity to deep neural networks, we adopt the concept of algorithm
unrolling, where training data is used to optimize the algorithm. We design
unrolled algorithms for several group synchronization instances, including
synchronization over the group of 3-D rotations: the synchronization problem in
cryo-EM. We also apply a similar approach to the multi-reference alignment
problem. We show by numerical experiments that the unrolling strategy
outperforms existing synchronization algorithms in a wide variety of scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SphereFed: Hyperspherical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Dong, Sai Qian Zhang, Ang Li, H. T. Kung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning aims at training a global model from multiple
decentralized devices (i.e. clients) without exchanging their private local
data. A key challenge is the handling of non-i.i.d. (independent identically
distributed) data across multiple clients that may induce disparities of their
local features. We introduce the Hyperspherical Federated Learning (SphereFed)
framework to address the non-i.i.d. issue by constraining learned
representations of data points to be on a unit hypersphere shared by clients.
Specifically, all clients learn their local representations by minimizing the
loss with respect to a fixed classifier whose weights span the unit
hypersphere. After federated training in improving the global model, this
classifier is further calibrated with a closed-form solution by minimizing a
mean squared loss. We show that the calibration solution can be computed
efficiently and distributedly without direct access of local data. Extensive
experiments indicate that our SphereFed approach is able to improve the
accuracy of multiple existing federated learning algorithms by a considerable
margin (up to 6% on challenging datasets) with enhanced computation and
communication efficiency across datasets and model architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bounding generalization error with input compression: An empirical study
  with infinite-width networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Galloway, Anna Golubeva, Mahmoud Salem, Mihai Nica, Yani Ioannou, Graham W. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Generalization Error (GE) of Deep Neural Networks (DNNs) is an
important task that often relies on availability of held-out data. The ability
to better predict GE based on a single training set may yield overarching DNN
design principles to reduce a reliance on trial-and-error, along with other
performance assessment advantages. In search of a quantity relevant to GE, we
investigate the Mutual Information (MI) between the input and final layer
representations, using the infinite-width DNN limit to bound MI. An existing
input compression-based GE bound is used to link MI and GE. To the best of our
knowledge, this represents the first empirical study of this bound. In our
attempt to empirically falsify the theoretical bound, we find that it is often
tight for best-performing models. Furthermore, it detects randomization of
training labels in many cases, reflects test-time perturbation robustness, and
works well given only few training samples. These results are promising given
that input compression is broadly applicable where MI can be estimated with
confidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages main content, 26 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Generational Population-Based Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Wan, Cong Lu, Jack Parker-Holder, Philip J. Ball, Vu Nguyen, Binxin Ru, Michael A. Osborne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) offers the potential for training generally
capable agents that can interact autonomously in the real world. However, one
key limitation is the brittleness of RL algorithms to core hyperparameters and
network architecture choice. Furthermore, non-stationarities such as evolving
training data and increased agent complexity mean that different
hyperparameters and architectures may be optimal at different points of
training. This motivates AutoRL, a class of methods seeking to automate these
design choices. One prominent class of AutoRL methods is Population-Based
Training (PBT), which have led to impressive performance in several large scale
settings. In this paper, we introduce two new innovations in PBT-style methods.
First, we employ trust-region based Bayesian Optimization, enabling full
coverage of the high-dimensional mixed hyperparameter search space. Second, we
show that using a generational approach, we can also learn both architectures
and hyperparameters jointly on-the-fly in a single training run. Leveraging the
new highly parallelizable Brax physics engine, we show that these innovations
lead to large performance gains, significantly outperforming the tuned baseline
while learning entire configurations on the fly. Code is available at
https://github.com/xingchenwan/bgpbt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AutoML Conference 2022. 10 pages, 4 figure, 3 tables (28 pages, 10
  figures, 7 tables including references and appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Greedy Pursuit for Feature Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandipan Das, Alireza M. Javid, Prakash Borpatra Gohain, Yonina C. Eldar, Saikat Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a greedy algorithm to select $N$ important features among $P$
input features for a non-linear prediction problem. The features are selected
one by one sequentially, in an iterative loss minimization procedure. We use
neural networks as predictors in the algorithm to compute the loss and hence,
we refer to our method as neural greedy pursuit (NGP). NGP is efficient in
selecting $N$ features when $N \ll P$, and it provides a notion of feature
importance in a descending order following the sequential selection procedure.
We experimentally show that NGP provides better performance than several
feature selection methods such as DeepLIFT and Drop-one-out loss. In addition,
we experimentally show a phase transition behavior in which perfect selection
of all $N$ features without false positives is possible when the training data
size exceeds a threshold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Green, Quantized Federated Learning over Wireless Networks: An
  Energy-Efficient Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Walid Saad, Mohammad Mozaffari, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a green, quantized FL framework, which represents data with a
finite precision level in both local training and uplink transmission, is
proposed. Here, the finite precision level is captured through the use of
quantized neural networks (QNNs) that quantize weights and activations in
fixed-precision format. In the considered FL model, each device trains its QNN
and transmits a quantized training result to the base station. Energy models
for the local training and the transmission with quantization are rigorously
derived. To minimize the energy consumption and the number of communication
rounds simultaneously, a multi-objective optimization problem is formulated
with respect to the number of local iterations, the number of selected devices,
and the precision levels for both local training and transmission while
ensuring convergence under a target accuracy constraint. To solve this problem,
the convergence rate of the proposed FL system is analytically derived with
respect to the system control variables. Then, the Pareto boundary of the
problem is characterized to provide efficient solutions using the normal
boundary inspection method. Design insights on balancing the tradeoff between
the two objectives are drawn from using the Nash bargaining solution and
analyzing the derived convergence rate. Simulation results show that the
proposed FL framework can reduce energy consumption until convergence by up to
52% compared to a baseline FL algorithm that represents data with full
precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Centric Epidemic Forecasting: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rodríguez, Harshavardhan Kamarthi, Pulak Agarwal, Javen Ho, Mira Patel, Suchet Sapre, B. Aditya Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has brought forth the importance of epidemic
forecasting for decision makers in multiple domains, ranging from public health
to the economy as a whole. While forecasting epidemic progression is frequently
conceptualized as being analogous to weather forecasting, however it has some
key differences and remains a non-trivial task. The spread of diseases is
subject to multiple confounding factors spanning human behavior, pathogen
dynamics, weather and environmental conditions. Research interest has been
fueled by the increased availability of rich data sources capturing previously
unobservable facets and also due to initiatives from government public health
and funding agencies. This has resulted, in particular, in a spate of work on
'data-centered' solutions which have shown potential in enhancing our
forecasting capabilities by leveraging non-traditional data sources as well as
recent innovations in AI and machine learning. This survey delves into various
data-driven methodological and practical advancements and introduces a
conceptual framework to navigate through them. First, we enumerate the large
number of epidemiological datasets and novel data streams that are relevant to
epidemic forecasting, capturing various factors like symptomatic online
surveys, retail and commerce, mobility, genomics data and more. Next, we
discuss methods and modeling paradigms focusing on the recent data-driven
statistical and deep-learning based methods as well as on the novel class of
hybrid models that combine domain knowledge of mechanistic models with the
effectiveness and flexibility of statistical approaches. We also discuss
experiences and challenges that arise in real-world deployment of these
forecasting systems including decision-making informed by forecasts. Finally,
we highlight some challenges and open problems found across the forecasting
pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Transmitting Bits: Context, Semantics, and <span class="highlight-title">Task-Oriented</span>
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deniz Gunduz, Zhijin Qin, Inaki Estella Aguerri, Harpreet S. Dhillon, Zhaohui Yang, Aylin Yener, Kai Kit Wong, Chan-Byoung Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication systems to date primarily aim at reliably communicating bit
sequences. Such an approach provides efficient engineering designs that are
agnostic to the meanings of the messages or to the goal that the message
exchange aims to achieve. Next generation systems, however, can be potentially
enriched by folding message semantics and goals of communication into their
design. Further, these systems can be made cognizant of the context in which
communication exchange takes place, providing avenues for novel design
insights. This tutorial summarizes the efforts to date, starting from its early
adaptations, semantic-aware and task-oriented communications, covering the
foundations, algorithms and potential implementations. The focus is on
approaches that utilize information theory to provide the foundations, as well
as the significant role of learning in semantics and task-aware communications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Riemannian Stochastic Gradient Method for Nested Composition
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dewei Zhang, Sam Davanloo Tajbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work considers optimization of composition of functions in a nested form
over Riemannian manifolds where each function contains an expectation. This
type of problems is gaining popularity in applications such as policy
evaluation in reinforcement learning or model customization in meta-learning.
The standard Riemannian stochastic gradient methods for non-compositional
optimization cannot be directly applied as stochastic approximation of inner
functions create bias in the gradients of the outer functions. For two-level
composition optimization, we present a Riemannian Stochastic Composition
Gradient Descent (R-SCGD) method that finds an approximate stationary point,
with expected squared Riemannian gradient smaller than $\epsilon$, in
$O(\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer
function and stochastic function and gradient oracles of the inner function.
Furthermore, we generalize the R-SCGD algorithms for problems with multi-level
nested compositional structures, with the same complexity of $O(\epsilon^{-2})$
for the first-order stochastic oracle. Finally, the performance of the R-SCGD
method is numerically evaluated over a policy evaluation problem in
reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Dynamics Learning for Predictive Control with an Application to
  Aerial Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Z. Jiahao, Kong Yao Chee, M. Ani Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the task of improving the accuracy of dynamic
models for model predictive control (MPC) in an online setting. Even though
prediction models can be learned and applied to model-based controllers, these
models are often learned offline. In this offline setting, training data is
first collected and a prediction model is learned through an elaborated
training procedure. After the model is trained to a desired accuracy, it is
then deployed in a model predictive controller. However, since the model is
learned offline, it does not adapt to disturbances or model errors observed
during deployment. To improve the adaptiveness of the model and the controller,
we propose an online dynamics learning framework that continually improves the
accuracy of the dynamic model during deployment. We adopt knowledge-based
neural ordinary differential equations (KNODE) as the dynamic models, and use
techniques inspired by transfer learning to continually improve the model
accuracy. We demonstrate the efficacy of our framework with a quadrotor robot,
and verify the framework in both simulations and physical experiments. Results
show that the proposed approach is able to account for disturbances that are
possibly time-varying, while maintaining good trajectory tracking performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A coherence parameter characterizing generative compressed sensing with
  Fourier measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Berk, Simone Brugiapaglia, Babhru Joshi, Yaniv Plan, Matthew Scott, Özgür Yilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Bora et al. (2017), a mathematical framework was developed for compressed
sensing guarantees in the setting where the measurement matrix is Gaussian and
the signal structure is the range of a generative neural network (GNN). The
problem of compressed sensing with GNNs has since been extensively analyzed
when the measurement matrix and/or network weights follow a subgaussian
distribution. We move beyond the subgaussian assumption, to measurement
matrices that are derived by sampling uniformly at random rows of a unitary
matrix (including subsampled Fourier measurements as a special case).
Specifically, we prove the first known restricted isometry guarantee for
generative compressed sensing with subsampled isometries, and provide recovery
bounds with nearly order-optimal sample complexity, addressing an open problem
of Scarlett et al. (2022, p. 10). Recovery efficacy is characterized by the
coherence, a new parameter, which measures the interplay between the range of
the network and the measurement matrix. Our approach relies on subspace
counting arguments and ideas central to high-dimensional probability.
Furthermore, we propose a regularization strategy for training GNNs to have
favourable coherence with the measurement operator. We provide compelling
numerical simulations that support this regularized training strategy: our
strategy yields low coherence networks that require fewer measurements for
signal recovery. This, together with our theoretical results, supports
coherence as a natural quantity for characterizing generative compressed
sensing with subsampled isometries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty in <span class="highlight-title">Contrastive Learning</span>: On the Predictability of Downstream
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Ardeshir, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The superior performance of some of today's state-of-the-art deep learning
models is to some extent owed to extensive (self-)supervised contrastive
pretraining on large-scale datasets. In contrastive learning, the network is
presented with pairs of positive (similar) and negative (dissimilar) datapoints
and is trained to find an embedding vector for each datapoint, i.e., a
representation, which can be further fine-tuned for various downstream tasks.
In order to safely deploy these models in critical decision-making systems, it
is crucial to equip them with a measure of their uncertainty or reliability.
However, due to the pairwise nature of training a contrastive model, and the
lack of absolute labels on the output (an abstract embedding vector), adapting
conventional uncertainty estimation techniques to such models is non-trivial.
In this work, we study whether the uncertainty of such a representation can be
quantified for a single datapoint in a meaningful way. In other words, we
explore if the downstream performance on a given datapoint is predictable,
directly from its pre-trained embedding. We show that this goal can be achieved
by directly estimating the distribution of the training data in the embedding
space and accounting for the local consistency of the representations. Our
experiments show that this notion of uncertainty for an embedding vector often
strongly correlates with its downstream accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-parametric Analysis for Mixed Integer Linear Programming: An
  Application to Transmission Planning and Congestion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Liu, Rui Bo, Siyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing existing transmission lines is a useful tool to combat transmission
congestion and guarantee transmission security with increasing demand and
boosting the renewable energy source. This study concerns the selection of
lines whose capacity should be expanded and by how much from the perspective of
independent system operator (ISO) to minimize the system cost with the
consideration of transmission line constraints and electricity generation and
demand balance conditions, and incorporating ramp-up and startup ramp rates,
shutdown ramp rates, ramp-down rate limits and minimum up and minimum down
times. For that purpose, we develop the ISO unit commitment and economic
dispatch model and show it as a right-hand side uncertainty multiple parametric
analysis for the mixed integer linear programming (MILP) problem. We first
relax the binary variable to continuous variables and employ the Lagrange
method and Karush-Kuhn-Tucker conditions to obtain optimal solutions (optimal
decision variables and objective function) and critical regions associated with
active and inactive constraints. Further, we extend the traditional branch and
bound method for the large-scale MILP problem by determining the upper bound of
the problem at each node, then comparing the difference between the upper and
lower bounds and reaching the approximate optimal solution within the decision
makers' tolerated error range. In additional, the objective function's first
derivative on the parameters of each line is used to inform the selection of
lines to ease congestion and maximize social welfare. Finally, the amount of
capacity upgrade will be chosen by balancing the cost-reduction rate of the
objective function on parameters and the cost of the line upgrade. Our findings
are supported by numerical simulation and provide transmission line planners
with decision-making guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metadata Representations for Queryable ML Model Zoos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Li, Rihan Hai, Alessandro Bozzon, Asterios Katsifodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) practitioners and organizations are building model zoos
of pre-trained models, containing metadata describing properties of the ML
models and datasets that are useful for reporting, auditing, reproducibility,
and interpretability purposes. The metatada is currently not standardised; its
expressivity is limited; and there is no interoperable way to store and query
it. Consequently, model search, reuse, comparison, and composition are
hindered. In this paper, we advocate for standardized ML model meta-data
representation and management, proposing a toolkit supported to help
practitioners manage and query that metadata.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for
  COVID-19 Screening With Chest Radiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ma, Pengcheng Xi, Karim Habashy, Ashkan Ebadi, Stéphane Tremblay, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building AI models with trustworthiness is important especially in regulated
areas such as healthcare. In tackling COVID-19, previous work uses
convolutional neural networks as the backbone architecture, which has shown to
be prone to over-caution and overconfidence in making decisions, rendering them
less trustworthy -- a crucial flaw in the context of medical imaging. In this
study, we propose a feature learning approach using Vision Transformers, which
use an attention-based mechanism, and examine the representation learning
capability of Transformers as a new backbone architecture for medical imaging.
Through the task of classifying COVID-19 chest radiographs, we investigate into
whether generalization capabilities benefit solely from Vision Transformers'
architectural advances. Quantitative and qualitative evaluations are conducted
on the trustworthiness of the models, through the use of "trust score"
computation and a visual explainability technique. We conclude that the
attention-based feature learning approach is promising in building trustworthy
deep learning models for healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 39th International Conference on Machine Learning,
  Workshop on Healthcare AI and COVID-19</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A sharp uniform-in-time error estimate for Stochastic Gradient Langevin
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Yuliang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a sharp uniform-in-time error estimate for the Stochastic
Gradient Langevin Dynamics (SGLD), which is a popular sampling algorithm. Under
mild assumptions, we obtain a uniform-in-time $O(\eta^2)$ bound for the
KL-divergence between the SGLD iteration and the Langevin diffusion, where
$\eta$ is the step size (or learning rate). Our analysis is also valid for
varying step sizes. Based on this, we are able to obtain an $O(\eta)$ bound for
the distance between the SGLD iteration and the invariant distribution of the
Langevin diffusion, in terms of Wasserstein or total variation distances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Caltech Fish Counting <span class="highlight-title">Dataset</span>: A Benchmark for Multiple-Object
  Tracking and Counting <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Kay, Peter Kulits, Suzanne Stathatos, Siqi Deng, Erik Young, Sara Beery, Grant Van Horn, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for
detecting, tracking, and counting fish in sonar videos. We identify sonar
videos as a rich source of data for advancing low signal-to-noise computer
vision applications and tackling domain generalization in multiple-object
tracking (MOT) and counting. In comparison to existing MOT and counting
datasets, which are largely restricted to videos of people and vehicles in
cities, CFC is sourced from a natural-world domain where targets are not easily
resolvable and appearance features cannot be easily leveraged for target
re-identification. With over half a million annotations in over 1,500 videos
sourced from seven different sonar cameras, CFC allows researchers to train MOT
and counting algorithms and evaluate generalization performance at unseen test
locations. We perform extensive baseline experiments and identify key
challenges and opportunities for advancing the state of the art in
generalization in MOT and counting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. 33 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstract Demonstrations and Adaptive Exploration for Efficient and
  Stable Multi-step Sparse Reward Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Yang, Ze Ji, Jing Wu, Yu-kun Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Deep Reinforcement Learning (DRL) has been popular in many
disciplines including robotics, state-of-the-art DRL algorithms still struggle
to learn long-horizon, multi-step and sparse reward tasks, such as stacking
several blocks given only a task-completion reward signal. To improve learning
efficiency for such tasks, this paper proposes a DRL exploration technique,
termed A^2, which integrates two components inspired by human experiences:
Abstract demonstrations and Adaptive exploration. A^2 starts by decomposing a
complex task into subtasks, and then provides the correct orders of subtasks to
learn. During training, the agent explores the environment adaptively, acting
more deterministically for well-mastered subtasks and more stochastically for
ill-learnt subtasks. Ablation and comparative experiments are conducted on
several grid-world tasks and three robotic manipulation tasks. We demonstrate
that A^2 can aid popular DRL algorithms (DQN, DDPG, and SAC) to learn more
efficiently and stably in these environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 27th IEEE International Conference on Automation and
  Computing (ICAC2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assaying Out-Of-Distribution Generalization in Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, Bernhard Schölkopf, Francesco Locatello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since out-of-distribution generalization is a generally ill-posed problem,
various proxy targets (e.g., calibration, adversarial robustness, algorithmic
corruptions, invariance across shifts) were studied across different research
programs resulting in different recommendations. While sharing the same
aspirational goal, these approaches have never been tested under the same
experimental conditions on real data. In this paper, we take a unified view of
previous work, highlighting message discrepancies that we address empirically,
and providing recommendations on how to measure the robustness of a model and
how to improve it. To this end, we collect 172 publicly available dataset pairs
for training and out-of-distribution evaluation of accuracy, calibration error,
adversarial attacks, environment invariance, and synthetic corruptions. We
fine-tune over 31k networks, from nine different architectures in the many- and
few-shot setting. Our findings confirm that in- and out-of-distribution
accuracies tend to increase jointly, but show that their relation is largely
dataset-dependent, and in general more nuanced and more complex than posited by
previous, smaller scale studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Algorithms for <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mary Phuong, Marcus Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 15 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Predictive Clustering Trees for (Hierarchical)
  Multi-label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jurica Levatić, Michelangelo Ceci, Dragi Kocev, Sašo Džeroski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a common approach to learning predictive
models using not only labeled examples, but also unlabeled examples. While SSL
for the simple tasks of classification and regression has received a lot of
attention from the research community, this is not properly investigated for
complex prediction tasks with structurally dependent variables. This is the
case of multi-label classification and hierarchical multi-label classification
tasks, which may require additional information, possibly coming from the
underlying distribution in the descriptive space provided by unlabeled
examples, to better face the challenging task of predicting simultaneously
multiple class labels.
  In this paper, we investigate this aspect and propose a (hierarchical)
multi-label classification method based on semi-supervised learning of
predictive clustering trees. We also extend the method towards ensemble
learning and propose a method based on the random forest approach. Extensive
experimental evaluation conducted on 23 datasets shows significant advantages
of the proposed method and its extension with respect to their supervised
counterparts. Moreover, the method preserves interpretability and reduces the
time complexity of classical tree-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Over-the-Air Federated Edge Learning with Hierarchical Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozan Aygün, Mohammad Kazemi, Deniz Gündüz, Tolga M. Duman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine federated learning (FL) with over-the-air (OTA) aggregation, where
mobile users (MUs) aim to reach a consensus on a global model with the help of
a parameter server (PS) that aggregates the local gradients. In OTA FL, MUs
train their models using local data at every training round and transmit their
gradients simultaneously using the same frequency band in an uncoded fashion.
Based on the received signal of the superposed gradients, the PS performs a
global model update. While the OTA FL has a significantly decreased
communication cost, it is susceptible to adverse channel effects and noise.
Employing multiple antennas at the receiver side can reduce these effects, yet
the path-loss is still a limiting factor for users located far away from the
PS. To ameliorate this issue, in this paper, we propose a wireless-based
hierarchical FL scheme that uses intermediate servers (ISs) to form clusters at
the areas where the MUs are more densely located. Our scheme utilizes OTA
cluster aggregations for the communication of the MUs with their corresponding
IS, and OTA global aggregations from the ISs to the PS. We present a
convergence analysis for the proposed algorithm, and show through numerical
evaluations of the derived analytical expressions and experimental results that
utilizing ISs results in a faster convergence and a better performance than the
OTA FL alone while using less transmit power. We also validate the results on
the performance using different number of cluster iterations with different
datasets and data distributions. We conclude that the best choice of cluster
aggregations depends on the data distribution among the MUs and the clusters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity of <span class="highlight-title">Pre-train</span>ed and Fine-tuned Representations <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Goerttler, Klaus Obermayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In transfer learning, only the last part of the networks - the so-called head
- is often fine-tuned. Representation similarity analysis shows that the most
significant change still occurs in the head even if all weights are updatable.
However, recent results from few-shot learning have shown that representation
change in the early layers, which are mostly convolutional, is beneficial,
especially in the case of cross-domain adaption. In our paper, we find out
whether that also holds true for transfer learning. In addition, we analyze the
change of representation in transfer learning, both during pre-training and
fine-tuning, and find out that pre-trained structure is unlearned if not
usable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop of Updatable Machine Learning at ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Kirch, Rafael Pagés, Sergio Arnaldo, Sergio Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VoloGAN, an adversarial domain adaptation network that translates
synthetic RGB-D images of a high-quality 3D model of a person, into RGB-D
images that could be generated with a consumer depth sensor. This system is
especially useful to generate high amount training data for single-view 3D
reconstruction algorithms replicating the real-world capture conditions, being
able to imitate the style of different sensor types, for the same high-end 3D
model database. The network uses a CycleGAN framework with a U-Net architecture
for the generator and a discriminator inspired by SIV-GAN. We use different
optimizers and learning rate schedules to train the generator and the
discriminator. We further construct a loss function that considers image
channels individually and, among other metrics, evaluates the structural
similarity. We demonstrate that CycleGANs can be used to apply adversarial
domain adaptation of synthetic 3D data to train a volumetric video generator
model having only few training samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Learning for the Resource-Constrained Classification Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danit Shifman Abukasis, Izack Cohen, Xiaochen Xian, Kejun Huang, Gonen Singer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource-constrained classification tasks are common in real-world
applications such as allocating tests for disease diagnosis, hiring decisions
when filling a limited number of positions, and defect detection in
manufacturing settings under a limited inspection budget. Typical
classification algorithms treat the learning process and the resource
constraints as two separate and sequential tasks. Here we design an adaptive
learning approach that considers resource constraints and learning jointly by
iteratively fine-tuning misclassification costs. Via a structured experimental
study using a publicly available data set, we evaluate a decision tree
classifier that utilizes the proposed approach. The adaptive learning approach
performs significantly better than alternative approaches, especially for
difficult classification problems in which the performance of common approaches
may be unsatisfactory. We envision the adaptive learning approach as an
important addition to the repertoire of techniques for handling
resource-constrained classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view hierarchical Variational AutoEncoders with Factor Analysis
  latent space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Guerrero-López, Carlos Sevilla-Salcedo, Vanessa Gómez-Verdejo, Pablo M. Olmos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world databases are complex, they usually present redundancy and shared
correlations between heterogeneous and multiple representations of the same
data. Thus, exploiting and disentangling shared information between views is
critical. For this purpose, recent studies often fuse all views into a shared
nonlinear complex latent space but they lose the interpretability. To overcome
this limitation, here we propose a novel method to combine multiple Variational
AutoEncoders (VAE) architectures with a Factor Analysis latent space (FA-VAE).
Concretely, we use a VAE to learn a private representation of each
heterogeneous view in a continuous latent space. Then, we model the shared
latent space by projecting every private variable to a low-dimensional latent
space using a linear projection matrix. Thus, we create an interpretable
hierarchical dependency between private and shared information. This way, the
novel model is able to simultaneously: (i) learn from multiple heterogeneous
views, (ii) obtain an interpretable hierarchical shared space, and, (iii)
perform transfer learning between generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages main work, 2 pages supplementary, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCARA: Scalable Graph Neural Networks with Feature-Oriented Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyi Liao, Dingheng Mo, Siqiang Luo, Xiang Li, Pengcheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in data processing have stimulated the demand for learning
graphs of very large scales. Graph Neural Networks (GNNs), being an emerging
and powerful approach in solving graph learning tasks, are known to be
difficult to scale up. Most scalable models apply node-based techniques in
simplifying the expensive graph message-passing propagation procedure of GNN.
However, we find such acceleration insufficient when applied to million- or
even billion-scale graphs. In this work, we propose SCARA, a scalable GNN with
feature-oriented optimization for graph computation. SCARA efficiently computes
graph embedding from node features, and further selects and reuses feature
computation results to reduce overhead. Theoretical analysis indicates that our
model achieves sub-linear time complexity with a guaranteed precision in
propagation process as well as GNN training and inference. We conduct extensive
experiments on various datasets to evaluate the efficacy and efficiency of
SCARA. Performance comparison with baselines shows that SCARA can reach up to
100x graph propagation acceleration than current state-of-the-art methods with
fast convergence and comparable accuracy. Most notably, it is efficient to
process precomputation on the largest available billion-scale GNN dataset
Papers100M (111M nodes, 1.6B edges) in 100 seconds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedX: Unsupervised Federated Learning with Cross Knowledge <span class="highlight-title">Distillation</span> <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xing Xie, Meeyoung Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents FedX, an unsupervised federated learning framework. Our
model learns unbiased representation from decentralized and heterogeneous local
data. It employs a two-sided knowledge distillation with contrastive learning
as a core component, allowing the federated system to function without
requiring clients to share any data features. Furthermore, its adaptable
architecture can be used as an add-on module for existing unsupervised
algorithms in federated settings. Experiments show that our model improves
performance significantly (1.58--5.52pp) on five unsupervised algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and will be published at ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the development of a Bayesian optimisation framework for complex
  unknown systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Diessner, Yu Guan, Kevin J. Wilson, Richard D. Whalley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimisation provides an effective method to optimise expensive
black box functions. It has recently been applied to problems in fluid
dynamics. This paper studies and compares common Bayesian optimisation
algorithms empirically on a range of synthetic test functions. It investigates
the choice of acquisition function and number of training samples, exact
calculation of acquisition functions and Monte Carlo based approaches and both
single-point and multi-point optimisation. The test functions considered cover
a wide selection of challenges and therefore serve as an ideal test bed to
understand the performance of Bayesian optimisation and to identify general
situations where Bayesian optimisation performs well and poorly. This knowledge
can be utilised in applications, including those in fluid dynamics, where
objective functions are unknown. The results of this investigation show that
the choices to be made are less relevant for relatively simple functions, while
optimistic acquisition functions such as Upper Confidence Bound should be
preferred for more complex objective functions. Furthermore, results from the
Monte Carlo approach are comparable to results from analytical acquisition
functions. In instances where the objective function allows parallel
evaluations, the multi-point approach offers a quicker alternative, yet it may
potentially require more objective function evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Neural Networks by Modelling Semi-Active Shock Absorber 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Zink, Martin Schiele, Valentin Ivanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A permanently increasing number of on-board automotive control systems
requires new approaches to their digital mapping that improves functionality in
terms of adaptability and robustness as well as enables their easier on-line
software update. As it can be concluded from many recent studies, various
methods applying neural networks (NN) can be good candidates for relevant
digital twin (DT) tools in automotive control system design, for example, for
controller parameterization and condition monitoring. However, the NN-based DT
has strong requirements to an adequate amount of data to be used in training
and design. In this regard, the paper presents an approach, which demonstrates
how the regression tasks can be efficiently handled by the modeling of a
semi-active shock absorber within the DT framework. The approach is based on
the adaptation of time series augmentation techniques to the stationary data
that increases the variance of the latter. Such a solution gives a background
to elaborate further data engineering methods for the data preparation of
sophisticated databases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Treatment Effect with Trained Kernels of the
  Nadaraya-Watson Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei V. Konstantinov, Stanislav R. Kirpichenko, Lev V. Utkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new method for estimating the conditional average treatment effect is
proposed in the paper. It is called TNW-CATE (the Trainable Nadaraya-Watson
regression for CATE) and based on the assumption that the number of controls is
rather large whereas the number of treatments is small. TNW-CATE uses the
Nadaraya-Watson regression for predicting outcomes of patients from the control
and treatment groups. The main idea behind TNW-CATE is to train kernels of the
Nadaraya-Watson regression by using a weight sharing neural network of a
specific form. The network is trained on controls, and it replaces standard
kernels with a set of neural subnetworks with shared parameters such that every
subnetwork implements the trainable kernel, but the whole network implements
the Nadaraya-Watson estimator. The network memorizes how the feature vectors
are located in the feature space. The proposed approach is similar to the
transfer learning when domains of source and target data are similar, but tasks
are different. Various numerical simulation experiments illustrate TNW-CATE and
compare it with the well-known T-learner, S-learner and X-learner for several
types of the control and treatment outcome functions. The code of proposed
algorithms implementing TNW-CATE is available in
https://github.com/Stasychbr/TNW-CATE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active-Learning-as-a-Service: An Efficient MLOps System for Data-Centric
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizheng Huang, Huaizheng Zhang, Yuanming Li, Chiew Tong Lau, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of today's AI applications requires not only model training
(Model-centric) but also data engineering (Data-centric). In data-centric AI,
active learning (AL) plays a vital role, but current AL tools can not perform
AL tasks efficiently. To this end, this paper presents an efficient MLOps
system for AL, named ALaaS (Active-Learning-as-a-Service). Specifically, ALaaS
adopts a server-client architecture to support an AL pipeline and implements
stage-level parallelism for high efficiency. Meanwhile, caching and batching
techniques are employed to further accelerate the AL process. In addition to
efficiency, ALaaS ensures accessibility with the help of the design philosophy
of configuration-as-a-service. It also abstracts an AL process to several
components and provides rich APIs for advanced users to extend the system to
new scenarios. Extensive experiments show that ALaaS outperforms all other
baselines in terms of latency and throughput. Further ablation studies
demonstrate the effectiveness of our design as well as ALaaS's ease to use. Our
code is available at \url{https://github.com/MLSysOps/alaas}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identity Testing for High-Dimensional Distributions via Entropy
  Tensorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Blanca, Zongchen Chen, Daniel Štefankovič, Eric Vigoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present improved algorithms and matching statistical and computational
lower bounds for the problem of identity testing $n$-dimensional distributions.
In the identity testing problem, we are given as input an explicit distribution
$\mu$, an $\varepsilon>0$, and access to a sampling oracle for a hidden
distribution $\pi$. The goal is to distinguish whether the two distributions
$\mu$ and $\pi$ are identical or are at least $\varepsilon$-far apart. When
there is only access to full samples from the hidden distribution $\pi$, it is
known that exponentially many samples may be needed, and hence previous works
have studied identity testing with additional access to various conditional
sampling oracles. We consider here a significantly weaker conditional sampling
oracle, called the Coordinate Oracle, and provide a fairly complete
computational and statistical characterization of the identity testing problem
in this new model.
  We prove that if an analytic property known as approximate tensorization of
entropy holds for the visible distribution $\mu$, then there is an efficient
identity testing algorithm for any hidden $\pi$ that uses
$\tilde{O}(n/\varepsilon)$ queries to the Coordinate Oracle. Approximate
tensorization of entropy is a classical tool for proving optimal mixing time
bounds of Markov chains for high-dimensional distributions, and recently has
been established for many families of distributions via spectral independence.
We complement our algorithmic result for identity testing with a matching
$\Omega(n/\varepsilon)$ statistical lower bound for the number of queries under
the Coordinate Oracle. We also prove a computational phase transition: for
sparse antiferromagnetic Ising models over $\{+1,-1\}^n$, in the regime where
approximate tensorization of entropy fails, there is no efficient identity
testing algorithm unless RP=NP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Bagging Methods for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranab Islam, Shaan Khosla, Arthur Lok, Mudit Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models leverage increasingly large numbers of parameters to
achieve performance on natural language understanding tasks. Ensembling these
models in specific configurations for downstream tasks show even further
performance improvements. In this paper, we perform an analysis of bagging
language models and compare single language models to bagged ensembles that are
roughly equivalent in terms of final model size. We explore an array of model
bagging configurations for natural language understanding tasks with final
ensemble sizes ranging from 300M parameters to 1.5B parameters and determine
that our ensembling methods are at best roughly equivalent to single LM
baselines. We note other positive effects of bagging and pruning in specific
scenarios according to findings in our experiments such as variance reduction
and minor performance improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lazy Estimation of Variable Importance for Large Neural Networks <span class="chip">ICML'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Gao, Abby Stevens, Rebecca Willet, Garvesh Raskutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As opaque predictive models increasingly impact many areas of modern life,
interest in quantifying the importance of a given input variable for making a
specific prediction has grown. Recently, there has been a proliferation of
model-agnostic methods to measure variable importance (VI) that analyze the
difference in predictive power between a full model trained on all variables
and a reduced model that excludes the variable(s) of interest. A bottleneck
common to these methods is the estimation of the reduced model for each
variable (or subset of variables), which is an expensive process that often
does not come with theoretical guarantees. In this work, we propose a fast and
flexible method for approximating the reduced model with important inferential
guarantees. We replace the need for fully retraining a wide neural network by a
linearization initialized at the full model parameters. By adding a ridge-like
penalty to make the problem convex, we prove that when the ridge penalty
parameter is sufficiently large, our method estimates the variable importance
measure with an error rate of $O(\frac{1}{\sqrt{n}})$ where $n$ is the number
of training samples. We also show that our estimator is asymptotically normal,
enabling us to provide confidence bounds for the VI estimates. We demonstrate
through simulations that our method is fast and accurate under several
data-generating regimes, and we demonstrate its real-world applicability on a
seasonal climate forecasting example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MoEC: Mixture of Expert Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Xie, Shaohan Huang, Tianyu Chen, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely Mixture of Experts (MoE) has received great interest due to its
promising scaling capability with affordable computational overhead. MoE
converts dense layers into sparse experts, and utilizes a gated routing network
to make experts conditionally activated. However, as the number of experts
grows, MoE with outrageous parameters suffers from overfitting and sparse data
allocation. Such problems are especially severe on tasks with limited data,
thus hindering the progress for MoE models to improve performance by scaling
up. In this work, we propose Mixture of Expert Clusters - a general approach to
enable expert layers to learn more diverse and appropriate knowledge by
imposing variance-based constraints on the routing stage. We further propose a
cluster-level expert dropout strategy specifically designed for the expert
cluster structure. Our experiments reveal that MoEC could improve performance
on machine translation and natural language understanding tasks, and raise the
performance upper bound for scaling up experts under limited data. We also
verify that MoEC plays a positive role in mitigating overfitting and sparse
data allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actor-Critic based Improper Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadi Zaki, Avinash Mohan, Aditya Gopalan, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an improper reinforcement learning setting where a learner is
given $M$ base controllers for an unknown Markov decision process, and wishes
to combine them optimally to produce a potentially new controller that can
outperform each of the base ones. This can be useful in tuning across
controllers, learnt possibly in mismatched or simulated environments, to obtain
a good controller for a given target environment with relatively few trials.
  Towards this, we propose two algorithms: (1) a Policy Gradient-based
approach; and (2) an algorithm that can switch between a simple Actor-Critic
(AC) based scheme and a Natural Actor-Critic (NAC) scheme depending on the
available information. Both algorithms operate over a class of improper
mixtures of the given controllers. For the first case, we derive convergence
rate guarantees assuming access to a gradient oracle. For the AC-based approach
we provide convergence rate guarantees to a stationary point in the basic AC
case and to a global optimum in the NAC case. Numerical results on (i) the
standard control theoretic benchmark of stabilizing an cartpole; and (ii) a
constrained queueing task show that our improper policy optimization algorithm
can stabilize the system even when the base policies at its disposal are
unstable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2102.08201</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection
  and Forensics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we proposed XG-BoT, an explainable deep graph neural network
model for botnet node detection. The proposed model is mainly composed of a
botnet detector and an explainer for automatic forensics. The XG-BoT detector
can effectively detect malicious botnet nodes under large-scale networks.
Specifically, it utilizes a grouped reversible residual connection with a graph
isomorphism network to learn expressive node representations from the botnet
communication graphs. The explainer in XG-BoT can perform automatic network
forensics by highlighting suspicious network flows and related botnet nodes. We
evaluated XG-BoT on real-world, large-scale botnet network graphs. Overall,
XG-BoT is able to outperform the state-of-the-art in terms of evaluation
metrics. In addition, we show that the XG-BoT explainer can generate useful
explanations based on GNNExplainer for automatic network forensics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Vertical Logistic Regression Privacy-Preserving? A Comprehensive
  Privacy Analysis and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzheng Hu, Tianle Cai, Jinyong Shan, Shange Tang, Chaochao Cai, Ethan Song, Bo Li, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider vertical logistic regression (VLR) trained with mini-batch
gradient descent -- a setting which has attracted growing interest among
industries and proven to be useful in a wide range of applications including
finance and medical research. We provide a comprehensive and rigorous privacy
analysis of VLR in a class of open-source Federated Learning frameworks, where
the protocols might differ between one another, yet a procedure of obtaining
local gradients is implicitly shared. We first consider the honest-but-curious
threat model, in which the detailed implementation of protocol is neglected and
only the shared procedure is assumed, which we abstract as an oracle. We find
that even under this general setting, single-dimension feature and label can
still be recovered from the other party under suitable constraints of batch
size, thus demonstrating the potential vulnerability of all frameworks
following the same philosophy. Then we look into a popular instantiation of the
protocol based on Homomorphic Encryption (HE). We propose an active attack that
significantly weaken the constraints on batch size in the previous analysis via
generating and compressing auxiliary ciphertext. To address the privacy leakage
within the HE-based protocol, we develop a simple-yet-effective countermeasure
based on Differential Privacy (DP), and provide both utility and privacy
guarantees for the updated algorithm. Finally, we empirically verify the
effectiveness of our attack and defense on benchmark datasets. Altogether, our
findings suggest that all vertical federated learning frameworks that solely
depend on HE might contain severe privacy risks, and DP, which has already
demonstrated its power in horizontal federated learning, can also play a
crucial role in the vertical setting, especially when coupled with HE or secure
multi-party computation (MPC) techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can You Fool AI by Doing a 180? $\unicode{x2013}$ A Case Study on
  Authorship Analysis of Texts by Arata Osada 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jagna Nieuwazny, Karol Nowakowski, Michal Ptaszynski, Fumito Masui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is our attempt at answering a twofold question covering the areas
of ethics and authorship analysis. Firstly, since the methods used for
performing authorship analysis imply that an author can be recognized by the
content he or she creates, we were interested in finding out whether it would
be possible for an author identification system to correctly attribute works to
authors if in the course of years they have undergone a major psychological
transition. Secondly, and from the point of view of the evolution of an
author's ethical values, we checked what it would mean if the authorship
attribution system encounters difficulties in detecting single authorship. We
set out to answer those questions through performing a binary authorship
analysis task using a text classifier based on a pre-trained transformer model
and a baseline method relying on conventional similarity metrics. For the test
set, we chose works of Arata Osada, a Japanese educator and specialist in the
history of education, with half of them being books written before the World
War II and another half in the 1950s, in between which he underwent a
transformation in terms of political opinions. As a result, we were able to
confirm that in the case of texts authored by Arata Osada in a time span of
more than 10 years, while the classification accuracy drops by a large margin
and is substantially lower than for texts by other non-fiction writers,
confidence scores of the predictions remain at a similar level as in the case
of a shorter time span, indicating that the classifier was in many instances
tricked into deciding that texts written over a time span of multiple years
were actually written by two different people, which in turn leads us to
believe that such a change can affect authorship analysis, and that historical
events have great impact on a person's ethical outlook as expressed in their
writings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Goal-Conditioned Reinforcement Learning with Variational
  Causal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Ding, Haohong Lin, Bo Li, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal component to attaining generalizable solutions in human
intelligence, reasoning provides great potential for reinforcement learning
(RL) agents' generalization towards varied goals by summarizing part-to-whole
arguments and discovering cause-and-effect relations. However, how to discover
and represent causalities remains a huge gap that hinders the development of
causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal
Graph (CG), a structure built upon the relation between objects and events. We
novelly formulate the GCRL problem into variational likelihood maximization
with CG as latent variables. To optimize the derived objective, we propose a
framework with theoretical performance guarantees that alternates between two
steps: using interventional data to estimate the posterior of CG; using CG to
learn generalizable models and interpretable policies. Due to the lack of
public benchmarks that verify generalization capability under reasoning, we
design nine tasks and then empirically show the effectiveness of the proposed
method against five baselines on these tasks. Further theoretical analysis
shows that our performance improvement is attributed to the virtuous cycle of
causal discovery, transition modeling, and policy training, which aligns with
the experimental evidence in extensive ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incremental Task Learning with Incremental Rank Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rakib Hyder, Ken Shao, Boyu Hou, Panos Markopoulos, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental Task learning (ITL) is a category of continual learning that
seeks to train a single network for multiple tasks (one after another), where
training data for each task is only available during the training of that task.
Neural networks tend to forget older tasks when they are trained for the newer
tasks; this property is often known as catastrophic forgetting. To address this
issue, ITL methods use episodic memory, parameter regularization, masking and
pruning, or extensible network structures. In this paper, we propose a new
incremental task learning framework based on low-rank factorization. In
particular, we represent the network weights for each layer as a linear
combination of several rank-1 matrices. To update the network for a new task,
we learn a rank-1 (or low-rank) matrix and add that to the weights of every
layer. We also introduce an additional selector vector that assigns different
weights to the low-rank matrices learned for the previous tasks. We show that
our approach performs better than the current state-of-the-art methods in terms
of accuracy and forgetting. Our method also offers better memory efficiency
compared to episodic memory- and mask-based approaches. Our code will be
available at https://github.com/CSIPlab/task-increment-rank-update.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at
  https://github.com/CSIPlab/task-increment-rank-update.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Action Translator for Meta Reinforcement Learning on
  Sparse-Reward Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Guo, Qiucheng Wu, Honglak Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of
training tasks simultaneously and quickly adapting to new tasks. It requires
massive amounts of data drawn from training tasks to infer the common structure
shared among tasks. Without heavy reward engineering, the sparse rewards in
long-horizon tasks exacerbate the problem of sample efficiency in meta-RL.
Another challenge in meta-RL is the discrepancy of difficulty level among
tasks, which might cause one easy task dominating learning of the shared policy
and thus preclude policy adaptation to new tasks. This work introduces a novel
objective function to learn an action translator among training tasks. We
theoretically verify that the value of the transferred policy with the action
translator can be close to the value of the source policy and our objective
function (approximately) upper bounds the value difference. We propose to
combine the action translator with context-based meta-RL algorithms for better
data collection and more efficient exploration during meta-training. Our
approach empirically improves the sample efficiency and performance of meta-RL
algorithms on sparse-reward tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Is MattEr: Temporal <span class="highlight-title">Self-supervision</span> for Video <span class="highlight-title">Transformer</span>s <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Jaehyung Kim, Dongyoon Han, Hwanjun Song, Jung-Woo Ha, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding temporal dynamics of video is an essential aspect of learning
better video representations. Recently, transformer-based architectural designs
have been extensively explored for video tasks due to their capability to
capture long-term dependency of input sequences. However, we found that these
Video Transformers are still biased to learn spatial dynamics rather than
temporal ones, and debiasing the spurious correlation is critical for their
performance. Based on the observations, we design simple yet effective
self-supervised tasks for video models to learn temporal dynamics better.
Specifically, for debiasing the spatial bias, our method learns the temporal
order of video frames as extra self-supervision and enforces the randomly
shuffled frames to have low-confidence outputs. Also, our method learns the
temporal flow direction of video tokens among consecutive frames for enhancing
the correlation toward temporal dynamics. Under various video action
recognition tasks, we demonstrate the effectiveness of our method and its
compatibility with state-of-the-art Video Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022. Code is available at
  https://github.com/alinlab/temporal-selfsupervision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A-SFS: Semi-supervised Feature Selection based on Multi-task
  <span class="highlight-title">Self-supervision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifeng Qiu, Wanxin Zeng, Dahua Liao, Ning Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature selection is an important process in machine learning. It builds an
interpretable and robust model by selecting the features that contribute the
most to the prediction target. However, most mature feature selection
algorithms, including supervised and semi-supervised, fail to fully exploit the
complex potential structure between features. We believe that these structures
are very important for the feature selection process, especially when labels
are lacking and data is noisy.
  To this end, we innovatively introduce a deep learning-based self-supervised
mechanism into feature selection problems, namely batch-Attention-based
Self-supervision Feature Selection(A-SFS). Firstly, a multi-task
self-supervised autoencoder is designed to uncover the hidden structure among
features with the support of two pretext tasks. Guided by the integrated
information from the multi-self-supervised learning model, a batch-attention
mechanism is designed to generate feature weights according to batch-based
feature selection patterns to alleviate the impacts introduced by a handful of
noisy data. This method is compared to 14 major strong benchmarks, including
LightGBM and XGBoost. Experimental results show that A-SFS achieves the highest
accuracy in most datasets. Furthermore, this design significantly reduces the
reliance on labels, with only 1/10 labeled data needed to achieve the same
performance as those state of art baselines. Results show that A-SFS is also
most robust to the noisy and missing data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, accepted by knowledge-based systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Science and Machine Learning in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Benelli, Thomas Y. Chen, Javier Duarte, Matthew Feickert, Matthew Graham, Lindsey Gray, Dan Hackett, Phil Harris, Shih-Chieh Hsu, Gregor Kasieczka, Elham E. Khoda, Matthias Komm, Mia Liu, Mark S. Neubauer, Scarlet Norberg, Alexx Perloff, Marcel Rieger, Claire Savard, Kazuhiro Terao, Savannah Thais, Avik Roy, Jean-Roch Vlimant, Grigorios Chachamis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing role of data science (DS) and machine learning (ML) in
high-energy physics (HEP) is well established and pertinent given the complex
detectors, large data, sets and sophisticated analyses at the heart of HEP
research. Moreover, exploiting symmetries inherent in physics data have
inspired physics-informed ML as a vibrant sub-field of computer science
research. HEP researchers benefit greatly from materials widely available
materials for use in education, training and workforce development. They are
also contributing to these materials and providing software to DS/ML-related
fields. Increasingly, physics departments are offering courses at the
intersection of DS, ML and physics, often using curricula developed by HEP
researchers and involving open software and data used in HEP. In this white
paper, we explore synergies between HEP research and DS/ML education, discuss
opportunities and challenges at this intersection, and propose community
activities that will be mutually beneficial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contribution to Snowmass 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balanced <span class="highlight-title">Contrastive Learning</span> for Long-Tailed Visual Recognition <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Jianggang,  Zhu,  Zheng,  Wang,  Jingjing,  Chen, Yi-Ping Phoebe,  Chen,  Yu-Gang,  Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data typically follow a long-tailed distribution, where a few
majority categories occupy most of the data while most minority categories
contain a limited number of samples. Classification models minimizing
cross-entropy struggle to represent and classify the tail classes. Although the
problem of learning unbiased classifiers has been well studied, methods for
representing imbalanced data are under-explored. In this paper, we focus on
representation learning for imbalanced data. Recently, supervised contrastive
learning has shown promising performance on balanced data recently. However,
through our theoretical analysis, we find that for long-tailed data, it fails
to form a regular simplex which is an ideal geometric configuration for
representation learning. To correct the optimization behavior of SCL and
further improve the performance of long-tailed visual recognition, we propose a
novel loss for balanced contrastive learning (BCL). Compared with SCL, we have
two improvements in BCL: class-averaging, which balances the gradient
contribution of negative classes; class-complement, which allows all classes to
appear in every mini-batch. The proposed balanced contrastive learning (BCL)
method satisfies the condition of forming a regular simplex and assists the
optimization of cross-entropy. Equipped with BCL, the proposed two-branch
framework can obtain a stronger feature representation and achieve competitive
performance on long-tailed benchmark datasets such as CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist2018. Our code is available at
\href{https://github.com/FlamieZhu/BCL}{this URL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Forget to Buy Milk: Contextually Aware Grocery Reminder Household
  Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ayub, Chrystopher L. Nehaniv, Kerstin Dautenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive robots operating in household environments would require items to
be available in the house to perform assistive tasks. However, when these items
run out, the assistive robot must remind its user to buy the missing items. In
this paper, we present a computational architecture that can allow a robot to
learn personalized contextual knowledge of a household through interactions
with its user. The architecture can then use the learned knowledge to make
predictions about missing items from the household over a long period of time.
The architecture integrates state-of-the-art perceptual learning algorithms,
cognitive models of memory encoding and learning, a reasoning module for
predicting missing items from the household, and a graphical user interface
(GUI) to interact with the user. The architecture is integrated with the Fetch
mobile manipulator robot and validated in a large indoor environment with
multiple contexts and objects. Our experimental results show that the robot can
adapt to an environment by learning contextual knowledge through interactions
with its user. The robot can also use the learned knowledge to correctly
predict missing items over multiple weeks and it is robust against sensory and
perceptual errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICDL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepBNN: towards a precise Binary Neural Network with Enhanced Feature
  Map via Repeating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xulong Shi, Zhi Qi, Jiaxuan Cai, Keqi Fu, Yaru Zhao, Zan Li, Xuanyu Liu, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary neural network (BNN) is an extreme quantization version of
convolutional neural networks (CNNs) with all features and weights mapped to
just 1-bit. Although BNN saves a lot of memory and computation demand to make
CNN applicable on edge or mobile devices, BNN suffers the drop of network
performance due to the reduced representation capability after binarization. In
this paper, we propose a new replaceable and easy-to-use convolution module
RepConv, which enhances feature maps through replicating input or output along
channel dimension by $\beta$ times without extra cost on the number of
parameters and convolutional computation. We also define a set of RepTran rules
to use RepConv throughout BNN modules like binary convolution, fully connected
layer and batch normalization. Experiments demonstrate that after the RepTran
transformation, a set of highly cited BNNs have achieved universally better
performance than the original BNN versions. For example, the Top-1 accuracy of
Rep-ReCU-ResNet-20, i.e., a RepBconv enhanced ReCU-ResNet-20, reaches 88.97% on
CIFAR-10, which is 1.47% higher than that of the original network. And
Rep-AdamBNN-ReActNet-A achieves 71.342% Top-1 accuracy on ImageNet, a fresh
state-of-the-art result of BNNs. Code and models are available
at:https://github.com/imfinethanks/Rep_AdamBNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has absolutely nothing to do with repvgg, rep means
  repeating</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decorrelative Network Architecture for Robust Electrocardiogram
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Wiedeman, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has made great progresses in medical data analysis,
but the lack of robustness and interpretability has kept these methods from
being widely deployed. In particular, data-driven models are vulnerable to
adversarial attacks, which are small, targeted perturbations that dramatically
degrade model performance. As a recent example, while deep learning has shown
impressive performance in electrocardiogram (ECG) classification, Han et al.
crafted realistic perturbations that fooled the network 74% of the time [2020].
Current adversarial defense paradigms are computationally intensive and
impractical for many high dimensional problems. Previous research indicates
that a network vulnerability is related to the features learned during
training. We propose a novel approach based on ensemble decorrelation and
Fourier partitioning for training parallel network arms into a decorrelated
architecture to learn complementary features, significantly reducing the chance
of a perturbation fooling all arms of the deep learning model. We test our
approach in ECG classification, demonstrating a much-improved 77.2% chance of
at least one correct network arm on the strongest adversarial attack tested, in
contrast to a 21.7% chance from a comparable ensemble. Our approach does not
require expensive optimization with adversarial samples, and thus can be scaled
to large problems. These methods can easily be applied to other tasks for
improved network robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAP: Differentially Private Graph Neural Networks with Aggregation
  Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Sajadmanesh, Ali Shahin Shamsabadi, Aurélien Bellet, Daniel Gatica-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of learning Graph Neural Networks (GNNs)
with Differential Privacy (DP). We propose a novel differentially private GNN
based on Aggregation Perturbation (GAP), which adds stochastic noise to the
GNN's aggregation function to statistically obfuscate the presence of a single
edge (edge-level privacy) or a single node and all its adjacent edges
(node-level privacy). Tailored to the specifics of private learning, GAP's new
architecture is composed of three separate modules: (i) the encoder module,
where we learn private node embeddings without relying on the edge information;
(ii) the aggregation module, where we compute noisy aggregated node embeddings
based on the graph structure; and (iii) the classification module, where we
train a neural network on the private aggregations for node classification
without further querying the graph edges. GAP's major advantage over previous
approaches is that it can benefit from multi-hop neighborhood aggregations, and
guarantees both edge-level and node-level DP not only for training, but also at
inference with no additional costs beyond the training's privacy budget. We
analyze GAP's formal privacy guarantees using R\'enyi DP and conduct empirical
experiments over three real-world graph datasets. We demonstrate that GAP
offers significantly better accuracy-privacy trade-offs than state-of-the-art
DP-GNN approaches and naive MLP-based baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PACTran: PAC-Bayesian Metrics for Estimating the Transferability of
  <span class="highlight-title">Pretrain</span>ed Models to Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Ding, Xi Chen, Tomer Levinboim, Beer Changpinyo, Radu Soricut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing abundance of pretrained models in recent years, the
problem of selecting the best pretrained checkpoint for a particular downstream
classification task has been gaining increased attention. Although several
methods have recently been proposed to tackle the selection problem (e.g. LEEP,
H-score), these methods resort to applying heuristics that are not well
motivated by learning theory. In this paper we present PACTran, a theoretically
grounded family of metrics for pretrained model selection and transferability
measurement. We first show how to derive PACTran metrics from the optimal
PAC-Bayesian bound under the transfer learning setting. We then empirically
evaluate three metric instantiations of PACTran on a number of vision tasks
(VTAB) as well as a language-and-vision (OKVQA) task. An analysis of the
results shows PACTran is a more consistent and effective transferability
measure compared to existing selection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision 2022 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Ground Metric Learning using Wasserstein Singular Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.06278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.06278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geert-Jan Huizing, Laura Cantini, Gabriel Peyré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defining meaningful distances between samples in a dataset is a fundamental
problem in machine learning. Optimal Transport (OT) lifts a distance between
features (the "ground metric") to a geometrically meaningful distance between
samples. However, there is usually no straightforward choice of ground metric.
Supervised ground metric learning approaches exist but require labeled data. In
absence of labels, only ad-hoc ground metrics remain. Unsupervised ground
metric learning is thus a fundamental problem to enable data-driven
applications of OT. In this paper, we propose for the first time a canonical
answer by simultaneously computing an OT distance between samples and between
features of a dataset. These distance matrices emerge naturally as positive
singular vectors of the function mapping ground metrics to OT distances. We
provide criteria to ensure the existence and uniqueness of these singular
vectors. We then introduce scalable computational methods to approximate them
in high-dimensional settings, using stochastic approximation and entropic
regularization. Finally, we showcase Wasserstein Singular Vectors on a
single-cell RNA-sequencing dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Classification of $G$-invariant Shallow Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devanshu Agrawal, James Ostrowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When trying to fit a deep neural network (DNN) to a $G$-invariant target
function with respect to a group $G$, it only makes sense to constrain the DNN
to be $G$-invariant as well. However, there can be many different ways to do
this, thus raising the problem of "$G$-invariant neural architecture design":
What is the optimal $G$-invariant architecture for a given problem? Before we
can consider the optimization problem itself, we must understand the search
space, the architectures in it, and how they relate to one another. In this
paper, we take a first step towards this goal; we prove a theorem that gives a
classification of all $G$-invariant single-hidden-layer or "shallow" neural
network ($G$-SNN) architectures with ReLU activation for any finite orthogonal
group $G$. The proof is based on a correspondence of every $G$-SNN to a signed
permutation representation of $G$ acting on the hidden neurons. The
classification is equivalently given in terms of the first cohomology classes
of $G$, thus admitting a topological interpretation. Based on a code
implementation, we enumerate the $G$-SNN architectures for some example groups
$G$ and visualize their structure. We draw the network morphisms between the
enumerated architectures that can be leveraged during neural architecture
search (NAS). Finally, we prove that architectures corresponding to
inequivalent cohomology classes in a given cohomology ring coincide in function
space only when their weight matrices are zero, and we discuss the implications
of this in the context of NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures; corrected proof of Lemma 20, fixed typos
  including in the statement of Thm. 5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoViT: Low Latency Graph-based Audio-Visual Voice Separation <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan F. Montesinos, Venkatesh S. Kadandale, Gloria Haro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an audio-visual approach for voice separation which
produces state-of-the-art results at a low latency in two scenarios: speech and
singing voice. The model is based on a two-stage network. Motion cues are
obtained with a lightweight graph convolutional network that processes face
landmarks. Then, both audio and motion features are fed to an audio-visual
transformer which produces a fairly good estimation of the isolated target
source. In a second stage, the predominant voice is enhanced with an audio-only
network. We present different ablation studies and comparison to
state-of-the-art methods. Finally, we explore the transferability of models
trained for speech separation in the task of singing voice separation. The
demos, code, and weights are available in https://ipcv.github.io/VoViT/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating State of the Art, Forecasting Ensembles- and Meta-learning
  Strategies for Model Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pieter Cawood, Terence van Zyl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Single-loop Alternating Gradient Projection Algorithm for
  Nonconvex-Concave and Convex-Nonconcave Minimax Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.02032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.02032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Xu, Huiling Zhang, Yang Xu, Guanghui Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much recent research effort has been directed to the development of efficient
algorithms for solving minimax problems with theoretical convergence guarantees
due to the relevance of these problems to a few emergent applications. In this
paper, we propose a unified single-loop alternating gradient projection (AGP)
algorithm for solving smooth nonconvex-(strongly) concave and (strongly)
convex-nonconcave minimax problems. AGP employs simple gradient projection
steps for updating the primal and dual variables alternatively at each
iteration. We show that it can find an $\varepsilon$-stationary point of the
objective function in $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp.
$\mathcal{O}\left( \varepsilon ^{-4} \right)$) iterations under
nonconvex-strongly concave (resp. nonconvex-concave) setting. Moreover, its
gradient complexity to obtain an $\varepsilon$-stationary point of the
objective function is bounded by $\mathcal{O}\left( \varepsilon ^{-2} \right)$
(resp., $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under the strongly
convex-nonconcave (resp., convex-nonconcave) setting. To the best of our
knowledge, this is the first time that a simple and unified single-loop
algorithm is developed for solving both nonconvex-(strongly) concave and
(strongly) convex-nonconcave minimax problems. Moreover, the complexity results
for solving the latter (strongly) convex-nonconcave minimax problems have never
been obtained before in the literature. Numerical results show the efficiency
of the proposed AGP algorithm. Furthermore, we extend the AGP algorithm by
presenting a block alternating proximal gradient (BAPG) algorithm for solving
more general multi-block nonsmooth nonconvex-(strongly) concave and (strongly)
convex-nonconcave minimax problems. We can similarly establish the gradient
complexity of the proposed algorithm under these four different settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Myopic Multifidelity Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Fiore, Laura Mainini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a popular framework for the optimization of black
box functions. Multifidelity methods allows to accelerate Bayesian optimization
by exploiting low-fidelity representations of expensive objective functions.
Popular multifidelity Bayesian strategies rely on sampling policies that
account for the immediate reward obtained evaluating the objective function at
a specific input, precluding greater informative gains that might be obtained
looking ahead more steps. This paper proposes a non-myopic multifidelity
Bayesian framework to grasp the long-term reward from future steps of the
optimization. Our computational strategy comes with a two-step lookahead
multifidelity acquisition function that maximizes the cumulative reward
obtained measuring the improvement in the solution over two steps ahead. We
demonstrate that the proposed algorithm outperforms a standard multifidelity
Bayesian framework on popular benchmark optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODA: A Real-World Road Corner Case <span class="highlight-title">Dataset</span> for Object Detection in
  Autonomous Driving <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, Xiaodan Liang, Zhenguo Li, Hang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative <span class="highlight-title">Survey</span> of Deep Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueying Zhan, Qingzhong Wang, Kuan-hao Huang, Haoyi Xiong, Dejing Dou, Antoni B. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning (DL) is data-hungry and usually relies on extensive
labeled data to deliver good performance, Active Learning (AL) reduces labeling
costs by selecting a small proportion of samples from unlabeled data for
labeling and training. Therefore, Deep Active Learning (DAL) has risen as a
feasible solution for maximizing model performance under a limited labeling
cost/budget in recent years. Although abundant methods of DAL have been
developed and various literature reviews conducted, the performance evaluation
of DAL methods under fair comparison settings is not yet available. Our work
intends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by
re-implementing 19 highly-cited DAL methods. We survey and categorize
DAL-related works and construct comparative experiments across frequently used
datasets and DAL algorithms. Additionally, we explore some factors (e.g., batch
size, number of epochs in the training process) that influence the efficacy of
DAL, which provides better references for researchers to design their DAL
experiments or carry out DAL-related applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLGOPerf: An ML Guided Inliner to Optimize Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir H. Ashouri, Mostafa Elhoushi, Yuzhe Hua, Xiang Wang, Muhammad Asif Manzoor, Bryan Chan, Yaoqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the past 25 years, we have witnessed an extensive application of Machine
Learning to the Compiler space; the selection and the phase-ordering problem.
However, limited works have been upstreamed into the state-of-the-art
compilers, i.e., LLVM, to seamlessly integrate the former into the optimization
pipeline of a compiler to be readily deployed by the user. MLGO was among the
first of such projects and it only strives to reduce the code size of a binary
with an ML-based Inliner using Reinforcement Learning.
  This paper presents MLGOPerf; the first end-to-end framework capable of
optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model
to generate rewards used for training a retargeted Reinforcement learning
agent, previously used as the primary model by MLGO. It does so by predicting
the post-inlining speedup of a function under analysis and it enables a fast
training framework for the primary model which otherwise wouldn't be practical.
The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with
respect to LLVM's optimization at O3 when trained for performance on SPEC
CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach
provides up to 26% increased opportunities to autotune code regions for our
benchmarks which can be translated into an additional 3.7% speedup value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 2: Added the missing Table 6. The short version of this work
  is accepted at ACM/IEEE CASES 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Implicit Bias of Gradient Descent on Separable Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1710.10345v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1710.10345v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine gradient descent on unregularized logistic regression problems,
with homogeneous linear predictors on linearly separable datasets. We show the
predictor converges to the direction of the max-margin (hard margin SVM)
solution. The result also generalizes to other monotone decreasing loss
functions with an infimum at infinity, to multi-class problems, and to training
a weight layer in a deep network in a certain restricted setting. Furthermore,
we show this convergence is very slow, and only logarithmic in the convergence
of the loss itself. This can help explain the benefit of continuing to optimize
the logistic or cross-entropy loss even after the training error is zero and
the training loss is extremely small, and, as we show, even if the validation
loss increases. Our methodology can also aid in understanding implicit
regularization n more complex models and with other optimization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a few minor issues in v4: typo in assumption 2, Latex issue in
  Lemma 1, and added a few words to proof sketch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Perspective on Stabilizing GANs training: Direct Adversarial
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.09041v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.09041v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Li, Pengfei Xia, Rentuo Tao, Hongjing Niu, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) are the most popular image generation
models that have achieved remarkable progress on various computer vision tasks.
However, training instability is still one of the open problems for all
GAN-based algorithms. Quite a number of methods have been proposed to stabilize
the training of GANs, the focuses of which were respectively put on the loss
functions, regularization and normalization technologies, training algorithms,
and model architectures. Different from the above methods, in this paper, a new
perspective on stabilizing GANs training is presented. It is found that
sometimes the images produced by the generator act like adversarial examples of
the discriminator during the training process, which may be part of the reason
causing the unstable training of GANs. With this finding, we propose the Direct
Adversarial Training (DAT) method to stabilize the training process of GANs.
Furthermore, we prove that the DAT method is able to minimize the Lipschitz
constant of the discriminator adaptively. The advanced performance of DAT is
verified on multiple loss functions, network architectures, hyper-parameters,
and datasets. Specifically, DAT achieves significant improvements of 11.5% FID
on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10
unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom
unconditional generation based on SSGAN. Code will be available at
https://github.com/iceli1007/DAT-GAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Emerging Topics in Computational
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with
  100M FLOPs <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.03815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.03815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Zhang, Zhuo Chen, Zhao Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a Collaboration of Experts (CoE) framework to pool
together the expertise of multiple networks towards a common aim. Each expert
is an individual network with expertise on a unique portion of the dataset,
which enhances the collective capacity. Given a sample, an expert is selected
by the delegator, which simultaneously outputs a rough prediction to support
early termination. To fulfill this framework, we propose three modules to impel
each model to play its role, namely weight generation module (WGM), label
generation module (LGM) and variance calculation module (VCM). Our method
achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy
with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE
further achieves the accuracy of 80.0% with only 100M FLOPs for the first time.
More importantly, our method is hardware friendly and achieves a 3-6x speedup
compared with some existing conditional computation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Deep Classifiers Agree: Analyzing Correlations between Learning
  Order and Image Statistics <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.08997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.08997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iuliia Pliushch, Martin Mundt, Nicolas Lupp, Visvanathan Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although a plethora of architectural variants for deep classification has
been introduced over time, recent works have found empirical evidence towards
similarities in their training process. It has been hypothesized that neural
networks converge not only to similar representations, but also exhibit a
notion of empirical agreement on which data instances are learned first.
Following in the latter works$'$ footsteps, we define a metric to quantify the
relationship between such classification agreement over time, and posit that
the agreement phenomenon can be mapped to core statistics of the investigated
dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,
ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to
be independent of specific architectures, training hyper-parameters or labels,
albeit follows an ordering according to image statistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ECCV 2022. Version includes supplementary
  material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEN : Cooperatively Evolving Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sobhan Babu, Ravindra Guravannavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GANs contain two competing modules: the generator module is trained to
generate new examples, and the discriminator module is trained to discriminate
real examples from generated examples. Training procedure of GAN is modeled as
a finitely repeated simultaneous game. Each module tries to increase it's
performance at every repetition of the base game (at every batch of training
data) in a non-cooperative manner. We observed that each module can perform
better, if training is modeled as an infinitely repeated simultaneous game. At
every repetition of the base game (at every batch of training data) the
stronger module (whose performance is increased or remains the same compared to
the previous batch of training data) cooperate with weaker module (whose
performance is decreased compared to the previous batch of training data) and
only weaker module is allowed to increase it's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Not Sleep on Linear Models: Simple and Interpretable Techniques
  Outperform Deep Learning for Sleep Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeroen Van Der Donckt, Jonas Van Der Donckt, Emiel Deprost, Nicolas Vandenbussche, Michael Rademaker, Gilles Vandewiele, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, research in automatic sleep scoring has mainly
focused on developing increasingly complex deep learning architectures.
However, recently these approaches achieved only marginal improvements, often
at the expense of requiring more data and more expensive training procedures.
Despite all these efforts and their satisfactory performance, automatic sleep
staging solutions are not widely adopted in a clinical context yet. We argue
that most deep learning solutions for sleep scoring are limited in their
real-world applicability as they are hard to train, deploy, and reproduce.
Moreover, these solutions lack interpretability and transparency, which are
often key to increase adoption rates. In this work, we revisit the problem of
sleep stage classification using classical machine learning. Results show that
state-of-the-art performance can be achieved with a conventional machine
learning pipeline consisting of preprocessing, feature extraction, and a simple
machine learning model. In particular, we analyze the performance of a linear
model and a non-linear (gradient boosting) model. Our approach surpasses
state-of-the-art (that uses the same data) on two public datasets: Sleep-EDF
SC-20 (MF1 0.810) and Sleep-EDF ST (MF1 0.795), while achieving competitive
results on Sleep-EDF SC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that,
for the sleep stage scoring task, the expressiveness of an engineered feature
vector is on par with the internally learned representations of deep learning
models. This observation opens the door to clinical adoption, as a
representative feature vector allows to leverage both the interpretability and
successful track record of traditional machine learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Submitted to Biomedical
  Signal Processing and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds via Convex Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04985v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04985v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gábor Lugosi, Gergely Neu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the celebrated works of Russo and Zou (2016,2019) and Xu and Raginsky
(2017), it has been well known that the generalization error of supervised
learning algorithms can be bounded in terms of the mutual information between
their input and the output, given that the loss of any fixed hypothesis has a
subgaussian tail. In this work, we generalize this result beyond the standard
choice of Shannon's mutual information to measure the dependence between the
input and the output. Our main result shows that it is indeed possible to
replace the mutual information by any strongly convex function of the joint
input-output distribution, with the subgaussianity condition on the losses
replaced by a bound on an appropriately chosen norm capturing the geometry of
the dependence measure. This allows us to derive a range of generalization
bounds that are either entirely new or strengthen previously known ones.
Examples include bounds stated in terms of $p$-norm divergences and the
Wasserstein-2 distance, which are respectively applicable for heavy-tailed loss
distributions and highly smooth loss functions. Our analysis is entirely based
on elementary tools from convex analysis by tracking the growth of a potential
function associated with the dependence measure and the loss function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust outlier detection by de-biasing VAE likelihoods <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.08760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.08760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Chauhan, Barath Mohan U, Pradeep Shenoy, Manish Gupta, Devarajan Sridharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep networks often make confident, yet, incorrect, predictions when tested
with outlier data that is far removed from their training distributions.
Likelihoods computed by deep generative models (DGMs) are a candidate metric
for outlier detection with unlabeled data. Yet, previous studies have shown
that DGM likelihoods are unreliable and can be easily biased by simple
transformations to input data. Here, we examine outlier detection with
variational autoencoders (VAEs), among the simplest of DGMs. We propose novel
analytical and algorithmic approaches to ameliorate key biases with VAE
likelihoods. Our bias corrections are sample-specific, computationally
inexpensive, and readily computed for various decoder visible distributions.
Next, we show that a well-known image pre-processing technique -- contrast
stretching -- extends the effectiveness of bias correction to further improve
outlier detection. Our approach achieves state-of-the-art accuracies with nine
grayscale and natural image datasets, and demonstrates significant advantages
-- both with speed and performance -- over four recent, competing approaches.
In summary, lightweight remedies suffice to achieve robust outlier detection
with VAEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. 20 pages and 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Models Are Equal: Predicting Model Transferability in a
  Self-challenging Fisher Space <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Shao, Xun Zhao, Yixiao Ge, Zhaoyang Zhang, Lei Yang, Xiaogang Wang, Ying Shan, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses an important problem of ranking the pre-trained deep
neural networks and screening the most transferable ones for downstream tasks.
It is challenging because the ground-truth model ranking for each task can only
be generated by fine-tuning the pre-trained models on the target dataset, which
is brute-force and computationally expensive. Recent advanced methods proposed
several lightweight transferability metrics to predict the fine-tuning results.
However, these approaches only capture static representations but neglect the
fine-tuning dynamics. To this end, this paper proposes a new transferability
metric, called \textbf{S}elf-challenging \textbf{F}isher \textbf{D}iscriminant
\textbf{A}nalysis (\textbf{SFDA}), which has many appealing benefits that
existing works do not have. First, SFDA can embed the static features into a
Fisher space and refine them for better separability between classes. Second,
SFDA uses a self-challenging mechanism to encourage different pre-trained
models to differentiate on hard examples. Third, SFDA can easily select
multiple pre-trained models for the model ensemble. Extensive experiments on
$33$ pre-trained models of $11$ downstream tasks show that SFDA is efficient,
effective, and robust when measuring the transferability of pre-trained models.
For instance, compared with the state-of-the-art method NLEEP, SFDA
demonstrates an average of $59.1$\% gain while bringing $22.5$x speedup in
wall-clock time. The code will be available at
\url{https://github.com/TencentARC/SFDA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera ready. 24 pages, 11 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utterance Weighted Multi-Dilation Temporal Convolutional Networks for
  Monaural Speech Dereverberation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Ravenscroft, Stefan Goetze, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech dereverberation is an important stage in many speech technology
applications. Recent work in this area has been dominated by deep neural
network models. Temporal convolutional networks (TCNs) are deep learning models
that have been proposed for sequence modelling in the task of dereverberating
speech. In this work a weighted multi-dilation depthwise-separable convolution
is proposed to replace standard depthwise-separable convolutions in TCN models.
This proposed convolution enables the TCN to dynamically focus on more or less
local information in its receptive field at each convolutional block in the
network. It is shown that this weighted multi-dilation temporal convolutional
network (WD-TCN) consistently outperforms the TCN across various model
configurations and using the WD-TCN model is a more parameter efficient method
to improve the performance of the model than increasing the number of
convolutional blocks. The best performance improvement over the baseline TCN is
0.55 dB scale-invariant signal-to-distortion ratio (SISDR) and the best
performing WD-TCN model attains 12.26 dB SISDR on the WHAMR dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IWAENC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning inducing points and uncertainty on molecular data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Tsitsvero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty control and scalability to large datasets are the two main issues
for the deployment of Gaussian process models into the autonomous material and
chemical space exploration pipelines. One way to address both of these issues
is by introducing the latent inducing variables and choosing the right
approximation for the marginal log-likelihood objective. Here, we show that
variational learning of the inducing points in the high-dimensional molecular
descriptor space significantly improves both the prediction quality and
uncertainty estimates on test configurations from a sample molecular dynamics
dataset. Additionally, we show that inducing points can learn to represent the
configurations of the molecules of different types that were not present within
the initialization set of inducing points. Among several evaluated approximate
marginal log-likelihood objectives, we show that the predictive log-likelihood
provides both the predictive quality comparable to the exact Gaussian process
model and excellent uncertainty control. Finally, we comment on whether a
machine learning model makes predictions by interpolating the molecular
configurations in high-dimensional descriptor space. We show that despite our
intuition, and even for densely sampled molecular dynamics datasets, most of
the predictions are done in the extrapolation regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Automated Feature Monitoring for Data Streams <span class="chip">KDD22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Conde, Ricardo Moreira, João Torres, Pedro Cardoso, Hugo R. C. Ferreira, Marco O. P. Sampaio, João Tiago Ascensão, Pedro Bizarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring the behavior of automated real-time stream processing systems has
become one of the most relevant problems in real world applications. Such
systems have grown in complexity relying heavily on high dimensional input
data, and data hungry Machine Learning (ML) algorithms. We propose a flexible
system, Feature Monitoring (FM), that detects data drifts in such data sets,
with a small and constant memory footprint and a small computational cost in
streaming applications. The method is based on a multi-variate statistical test
and is data driven by design (full reference distributions are estimated from
the data). It monitors all features that are used by the system, while
providing an interpretable features ranking whenever an alarm occurs (to aid in
root cause analysis). The computational and memory lightness of the system
results from the use of Exponential Moving Histograms. In our experimental
study, we analyze the system's behavior with its parameters and, more
importantly, show examples where it detects problems that are not directly
related to a single feature. This illustrates how FM eliminates the need to add
custom signals to detect specific types of problems and that monitoring the
available space of features is often enough.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures. AutoML, KDD22, August 14-17, 2022, Washington,
  DC, US</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Algorithms and Lower Bounds for Linear Regression with Norm
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlin Chen, Ronald de Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lasso and Ridge are important minimization problems in machine learning and
statistics. They are versions of linear regression with squared loss where the
vector $\theta\in\mathbb{R}^d$ of coefficients is constrained in either
$\ell_1$-norm (for Lasso) or in $\ell_2$-norm (for Ridge). We study the
complexity of quantum algorithms for finding $\varepsilon$-minimizers for these
minimization problems. We show that for Lasso we can get a quadratic quantum
speedup in terms of $d$ by speeding up the cost-per-iteration of the
Frank-Wolfe algorithm, while for Ridge the best quantum algorithms are linear
in $d$, as are the best classical algorithms. As a byproduct of our quantum
lower bound for Lasso, we also prove the first classical lower bound for Lasso
that is tight up to polylog-factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Main changes are the addition of a tight classical lower bound
  for Lasso, and small improvements in the existing text. 38 pages LaTeX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe reinforcement learning for multi-energy management systems with
  known constraint functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03830v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03830v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Glenn Ceusters, Luis Ramirez Camargo, Rüdiger Franke, Ann Nowé, Maarten Messagie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) is a promising optimal control technique for
multi-energy management systems. It does not require a model a priori -
reducing the upfront and ongoing project-specific engineering effort and is
capable of learning better representations of the underlying system dynamics.
However, vanilla RL does not provide constraint satisfaction guarantees -
resulting in various potentially unsafe interactions within its safety-critical
environment. In this paper, we present two novel safe RL methods, namely
SafeFallback and GiveSafe, where the safety constraint formulation is decoupled
from the RL formulation and which provides hard-constraint satisfaction
guarantees both during training a (near) optimal policy (which involves
exploratory and exploitative, i.e. greedy, steps) as well as during deployment
of any policy (e.g. random agents or offline trained RL agents). In a simulated
multi-energy systems case study we have shown that both methods start with a
significantly higher utility (i.e. useful policy) compared to a vanilla RL
benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed
SafeFallback method even can outperform the vanilla RL benchmark (102,9% to
100%). We conclude that both methods are viably safety constraint handling
techniques applicable beyond RL, as demonstrated with random policies while
still providing hard-constraint guarantees. Finally, we propose directions for
future work to i.a. improve the constraint functions itself as more data
becomes available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability of deep vision-based autonomous driving systems: <span class="highlight-title">Review</span>
  and challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.05307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.05307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey reviews explainability methods for vision-based self-driving
systems trained with behavior cloning. The concept of explainability has
several facets and the need for explainability is strong in driving, a
safety-critical application. Gathering contributions from several research
fields, namely computer vision, deep learning, autonomous driving, explainable
AI (X-AI), this survey tackles several points. First, it discusses definitions,
context, and motivation for gaining more interpretability and explainability
from self-driving systems, as well as the challenges that are specific to this
application. Second, methods providing explanations to a black-box self-driving
system in a post-hoc fashion are comprehensively organized and detailed. Third,
approaches from the literature that aim at building more interpretable
self-driving systems by design are presented and discussed in detail. Finally,
remaining open-challenges and potential future research directions are
identified and examined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FactGraph: Evaluating Factuality in Summarization with Semantic Graph
  Representations <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, <span class="highlight-author">Mohit Bansal</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent improvements in abstractive summarization, most current
approaches generate summaries that are not factually consistent with the source
document, severely restricting their trust and usage in real-world
applications. Recent works have shown promising improvements in factuality
error identification using text or dependency arc entailments; however, they do
not consider the entire semantic graph simultaneously. To this end, we propose
FactGraph, a method that decomposes the document and the summary into
structured meaning representations (MR), which are more suitable for factuality
evaluation. MRs describe core semantic concepts and their relations,
aggregating the main content in both document and summary in a canonical form,
and reducing data sparsity. FactGraph encodes such graphs using a graph encoder
augmented with structure-aware adapters to capture interactions among the
concepts based on the graph connectivity, along with text representations using
an adapter-based text encoder. Experiments on different benchmarks for
evaluating factuality show that FactGraph outperforms previous approaches by up
to 15%. Furthermore, FactGraph improves performance on identifying content
verifiability errors and better captures subsentence-level factual
inconsistencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2022 (15 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GATE: Gated Additive Tree Ensemble for Tabular Classification and
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manu Joseph, Harsh Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel high-performance, parameter and computationally efficient
deep learning architecture for tabular data, Gated Additive Tree
Ensemble(GATE). GATE uses a gating mechanism, inspired from GRU, as a feature
representation learning unit with an in-built feature selection mechanism. We
combine it with an ensemble of differentiable, non-linear decision trees,
re-weighted with simple self-attention to predict our desired output. We
demonstrate that GATE is a competitive alternative to SOTA approaches like
GBDTs, NODE, FT Transformers, etc. by experiments on several public datasets
(both classification and regression). The code will be uploaded as soon as the
paper comes out of review.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Study of the performance and scalability of federated learning for
  medical imaging with intermittent clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judith Sáinz-Pardo Díaz, Álvaro López García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy and area under the curve) and reduction of execution times
will be studied with respect to the classical case (the centralized approach).
Different clients will be simulated from the training data, selected in an
unbalanced manner, i.e., they do not all have the same number of data. The
results of considering three or ten clients are exposed and compared between
them and against the centralized case. Two approaches to follow will be
analyzed in the case of intermittent clients, as in a real scenario some
clients may leave the training, and some new ones may enter the training. The
evolution of the results for the test set in terms of accuracy, area under the
curve and execution time is shown as the number of clients into which the
original data is divided increases. Finally, improvements and future work in
the field are proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sufficient Statistic Memory AMP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Liu, Shunqi Huang, Brian M. Kurkoski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate message passing (AMP) type algorithms have been widely used in
the signal reconstruction of certain large random linear systems. A key feature
of the AMP-type algorithms is that their dynamics can be correctly described by
state evolution. However, the state evolution does not necessarily be
convergent. To solve the convergence problem of the state evolution of AMP-type
algorithms in principle, this paper proposes a memory AMP (MAMP) under a
sufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We
show that the covariance matrices of SS-MAMP are L-banded and convergent. Given
an arbitrary MAMP, we can construct an SS-MAMP by damping, which not only
ensures the convergence of the state evolution, but also preserves the
orthogonality, i.e., its dynamics can be correctly described by state
evolution. As a byproduct, we prove that the Bayes-optimal orthogonal/vector
AMP (BO-OAMP/VAMP) is an SS-MAMP. As a result, we reveal two interesting
properties of BO-OAMP/VAMP for large systems: 1) the covariance matrices are
L-banded and are convergent, and 2) damping and memory are not needed (i.e., do
not bring performance improvement). As an example, we construct a sufficient
statistic Bayes-optimal MAMP (SS-BO-MAMP) whose state evolution converges to
the minimum (i.e., Bayes-optimal) mean square error (MSE) predicted by replica
methods. In addition, the MSE of SS-BO-MAMP is not worse than the original
BO-MAMP. Finally, simulations are provided to verify the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Double-column, 16 pages, submitted to IEEE Transactions on
  Information Theory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Minimum Description Length Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Moskovitz, Ta-Chu Kao, Maneesh Sahani, Matthew M. Botvinick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel framework for multitask reinforcement learning based on
the minimum description length (MDL) principle. In this approach, which we term
MDL-control (MDL-C), the agent learns the common structure among the tasks with
which it is faced and then distills it into a simpler representation which
facilitates faster convergence and generalization to new tasks. In doing so,
MDL-C naturally balances adaptation to each task with epistemic uncertainty
about the task distribution. We motivate MDL-C via formal connections between
the MDL principle and Bayesian inference, derive theoretical performance
guarantees, and demonstrate MDL-C's empirical effectiveness on both discrete
and high-dimensional continuous control tasks. %Empirically, this framework is
used to modify existing policy optimization approaches and improves their
multitask performance in both discrete and high-dimensional continuous control
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible learning of quantum states with generative query neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhu, Ya-Dong Wu, Ge Bai, Dong-Sheng Wang, Yuexuan Wang, Giulio Chiribella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are a powerful tool for the characterization of quantum
states.
  Existing networks are typically trained with experimental data gathered from
the specific quantum state that needs to be characterized.
  But is it possible to train a neural network offline and to make predictions
about quantum states other than the ones used for the training?
  Here we introduce a model of network that can be trained with classically
simulated data from a fiducial set of states and measurements, and can later be
used to characterize quantum states that share structural similarities with the
states in the fiducial set. With little guidance of quantum physics, the
network builds its own data-driven representation of quantum states, and then
uses it to predict the outcome statistics of quantum measurements that have not
been performed yet.
  The state representation produced by the network can also be used for tasks
beyond the prediction of outcome statistics, including clustering of quantum
states and identification of different phases of matter.
  Our network model provides a flexible approach that can be applied to online
learning scenarios, where predictions must be generated as soon as experimental
data become available, and to blind learning scenarios where the learner has
only access to an encrypted description of the quantum hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Major improvements in the presentation; new numerical experiments
  that illustrate the applicability of our neural network to various types of
  states</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Recommender System for Recommending Smartphones to Prospective
  Customers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.12876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.12876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik K. Biswas, Songlin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems are a subclass of machine learning systems that employ
sophisticated information filtering strategies to reduce the search time and
suggest the most relevant items to any particular user. Hybrid recommender
systems combine multiple recommendation strategies in different ways to benefit
from their complementary advantages. Some hybrid recommender systems have
combined collaborative filtering and content-based approaches to build systems
that are more robust. In this paper, we propose a hybrid recommender system,
which combines Alternating Least Squares (ALS) based collaborative filtering
with deep learning to enhance recommendation performance as well as overcome
the limitations associated with the collaborative filtering approach,
especially concerning its cold start problem. In essence, we use the outputs
from ALS (collaborative filtering) to influence the recommendations from a Deep
Neural Network (DNN), which combines characteristic, contextual, structural and
sequential information, in a big data processing framework. We have conducted
several experiments in testing the efficacy of the proposed hybrid architecture
in recommending smartphones to prospective customers and compared its
performance with other open-source recommenders. The results have shown that
the proposed system has outperformed several existing hybrid recommender
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expert Systems With Applications, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Minimization for Personalized Federated Semi-Supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhang Shi, Siguang Chen, Haijun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since federated learning (FL) has been introduced as a decentralized learning
technique with privacy preservation, statistical heterogeneity of distributed
data stays the main obstacle to achieve robust performance and stable
convergence in FL applications. Model personalization methods have been studied
to overcome this problem. However, existing approaches are mainly under the
prerequisite of fully labeled data, which is unrealistic in practice due to the
requirement of expertise. The primary issue caused by partial-labeled condition
is that, clients with deficient labeled data can suffer from unfair performance
gain because they lack adequate insights of local distribution to customize the
global model. To tackle this problem, 1) we propose a novel personalized
semi-supervised learning paradigm which allows partial-labeled or unlabeled
clients to seek labeling assistance from data-related clients (helper agents),
thus to enhance their perception of local data; 2) based on this paradigm, we
design an uncertainty-based data-relation metric to ensure that selected
helpers can provide trustworthy pseudo labels instead of misleading the local
training; 3) to mitigate the network overload introduced by helper searching,
we further develop a helper selection protocol to achieve efficient
communication with negligible performance sacrifice. Experiments show that our
proposed method can obtain superior performance and more stable convergence
than other related works with partial labeled data, especially in highly
heterogeneous setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Bandits with Knapsacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.11881v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.11881v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole Immorlica, Karthik Abinav Sankararaman, Robert Schapire, Aleksandrs Slivkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider Bandits with Knapsacks (henceforth, BwK), a general model for
multi-armed bandits under supply/budget constraints. In particular, a bandit
algorithm needs to solve a well-known knapsack problem: find an optimal packing
of items into a limited-size knapsack. The BwK problem is a common
generalization of numerous motivating examples, which range from dynamic
pricing to repeated auctions to dynamic ad allocation to network routing and
scheduling. While the prior work on BwK focused on the stochastic version, we
pioneer the other extreme in which the outcomes can be chosen adversarially.
This is a considerably harder problem, compared to both the stochastic version
and the "classic" adversarial bandits, in that regret minimization is no longer
feasible. Instead, the objective is to minimize the competitive ratio: the
ratio of the benchmark reward to the algorithm's reward.
  We design an algorithm with competitive ratio O(log T) relative to the best
fixed distribution over actions, where T is the time horizon; we also prove a
matching lower bound. The key conceptual contribution is a new perspective on
the stochastic version of the problem. We suggest a new algorithm for the
stochastic version, which builds on the framework of regret minimization in
repeated games and admits a substantially simpler analysis compared to prior
work. We then analyze this algorithm for the adversarial version and use it as
a subroutine to solve the latter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extended abstract appeared in FOCS 2019. The definitive version
  was published in JACM '22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial
  Training <span class="chip">ECCV
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are vulnerable to adversarial attacks: adding well-crafted,
imperceptible perturbations to their input can modify their output. Adversarial
training is one of the most effective approaches in training robust models
against such attacks. However, it is much slower than vanilla training of
neural networks since it needs to construct adversarial examples for the entire
training data at every iteration, hampering its effectiveness. Recently, Fast
Adversarial Training (FAT) was proposed that can obtain robust models
efficiently. However, the reasons behind its success are not fully understood,
and more importantly, it can only train robust models for $\ell_\infty$-bounded
attacks as it uses FGSM during training. In this paper, by leveraging the
theory of coreset selection, we show how selecting a small subset of training
data provides a general, more principled approach toward reducing the time
complexity of robust training. Unlike existing methods, our approach can be
adapted to a wide variety of training objectives, including TRADES,
$\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental
results indicate that our approach speeds up adversarial training by 2-3 times
while experiencing a slight reduction in the clean and robust accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 17th European Conference on Computer Vision (ECCV
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patch-level Representation Learning for <span class="highlight-title">Self-supervised</span> Vision
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Hankook Lee, Jaehyung Kim, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-supervised learning (SSL) methods have shown impressive results
in learning visual representations from unlabeled images. This paper aims to
improve their performance further by utilizing the architectural advantages of
the underlying neural network, as the current state-of-the-art visual pretext
tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.
In particular, we focus on Vision Transformers (ViTs), which have gained much
attention recently as a better architectural choice, often outperforming
convolutional networks for various visual tasks. The unique characteristic of
ViT is that it takes a sequence of disjoint patches from an image and processes
patch-level representations internally. Inspired by this, we design a simple
yet effective visual pretext task, coined SelfPatch, for learning better
patch-level representations. To be specific, we enforce invariance against each
patch and its neighbors, i.e., each patch treats similar neighboring patches as
positive samples. Consequently, training ViTs with SelfPatch learns more
semantically meaningful relations among patches (without using human-annotated
labels), which can be beneficial, in particular, to downstream tasks of a dense
prediction type. Despite its simplicity, we demonstrate that it can
significantly improve the performance of existing SSL methods for various
visual tasks, including object detection and semantic segmentation.
Specifically, SelfPatch significantly improves the recent self-supervised ViT,
DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance
segmentation, and +2.9 mIoU on ADE20K semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022 (Oral). Code is available at
  https://github.com/alinlab/SelfPatch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing the CO2 emission reduction of ridesplitting and its
  determinants based on real-world data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiang Li, Yuanyuan Li, Ziyuan Pu, Long Cheng, Lei Wang, Linchuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ridesplitting, which is a form of pooled ridesourcing service, has great
potential to alleviate the negative impacts of ridesourcing on the environment.
However, most existing studies only explored its theoretical environmental
benefits based on optimization models and simulations. By contrast, this study
aims to reveal the real-world emission reduction of ridesplitting and its
determinants based on the observed data of ridesourcing in Chengdu, China.
Integrating the trip data with the COPERT model, this study calculates the CO2
emissions of shared rides (ridesplitting) and their substituted single rides
(regular ridesourcing) to estimate the CO2 emission reduction of each
ridesplitting trip. The results show that not all ridesplitting trips reduce
emissions from ridesourcing in the real world. The CO2 emission reduction rate
of ridesplitting varies from trip to trip, averaging at 43.15g/km. Then,
interpretable machine learning models, gradient boosting machines, are applied
to explore the relationship between the CO2 emission reduction rate of
ridesplitting and its determinants. Based on the SHapley Additive exPlanations
(SHAP) method, the overlap rate and detour rate of shared rides are identified
to be the most important factors that determine the CO2 emission reduction rate
of ridesplitting. Increasing the overlap rate, the number of shared rides,
average speed, and ride distance ratio while decreasing the detour rate, actual
trip distance, and ride distance gap can increase the CO2 emission reduction
rate of ridesplitting. In addition, nonlinear effects and interactions of the
determinants are examined through the partial dependence plots. To sum up, this
study provides a scientific method for the government and ridesourcing
companies to better assess and optimize the environmental benefits of
ridesplitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Prospective Approach for Human-to-Human Interaction Recognition from
  Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural
  Network with GUI Application Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08146v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08146v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Mohi Uddin Khan, Abdullah Bin Shams, Md. Mohsin Sarker Raihan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 5G wireless technology and socioeconomic transformation
have brought a paradigm shift in sensor applications. Wi-Fi signal demonstrates
a strong correlation between its temporal variation and body movements, which
can be leveraged to recognize human activity. In this article, we demonstrate
the cognitive ability of device free mutual human-to-human interaction
recognition method based on the time scale Wi-Fi channel state information. The
mutual activities examined are steady-state, approaching, departing,
handshaking, high-five, hugging, kicking (left-leg), kicking (right-leg),
pointing (left-hand), pointing (right-hand), punching(left-hand), punching
(right-hand), and pushing. We explore and propose a Self-Attention furnished
Bidirectional Gated Recurrent Neural Network model to classify 13
human-to-human mutual interaction types from the time-series data. Our proposed
model can recognize a two subject pair mutual interaction with a maximum
benchmark accuracy of 94%. This has been expanded for ten subject pairs, which
secured a benchmark accuracy of 88% with improved classification around the
interaction-transition region. Also, an executable graphical user interface
(GUI) is developed, using the PyQt5 python module, to subsequently display the
overall mutual human-interaction recognition procedure in real-time. Finally,
we conclude with a brief discourse regarding the possible solutions to the
handicaps that resulted in curtailments observed during the study. Such, Wi-Fi
channel perturbation pattern analysis is believed to be an efficient,
economical and privacy-friendly approach to be potentially utilized in mutual
human-interaction recognition for indoor activity monitoring, surveillance
system, smart health monitoring systems and independent assisted living.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 Pages. This is the Pre-print version article submitted for
  Peer-Review to a prestigious journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A label-efficient two-sample test <span class="chip">UAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08861v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08861v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Li, Gautam Dasarathy, Karthikeyan Natesan Ramamurthy, Visar Berisha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-sample tests evaluate whether two samples are realizations of the same
distribution (the null hypothesis) or two different distributions (the
alternative hypothesis). We consider a new setting for this problem where
sample features are easily measured whereas sample labels are unknown and
costly to obtain. Accordingly, we devise a three-stage framework in service of
performing an effective two-sample test with only a small number of sample
label queries: first, a classifier is trained with samples uniformly labeled to
model the posterior probabilities of the labels; second, a novel query scheme
dubbed \emph{bimodal query} is used to query labels of samples from both
classes, and last, the classical Friedman-Rafsky (FR) two-sample test is
performed on the queried samples. Theoretical analysis and extensive
experiments performed on several datasets demonstrate that the proposed test
controls the Type I error and has decreased Type II error relative to uniform
querying and certainty-based querying. Source code for our algorithms and
experimental results is available at
\url{https://github.com/wayne0908/Label-Efficient-Two-Sample}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 38th conference on Uncertainty in Artificial
  Intelligence (UAI2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNRank: Learning Global Rankings from Pairwise Comparisons via Directed
  Graph Neural Networks <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00211v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00211v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan He, Quan Gan, David Wipf, Gesine Reinert, Junchi Yan, Mihai Cucuringu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering global rankings from pairwise comparisons has wide applications
from time synchronization to sports team ranking. Pairwise comparisons
corresponding to matches in a competition can be construed as edges in a
directed graph (digraph), whose nodes represent e.g. competitors with an
unknown rank. In this paper, we introduce neural networks into the ranking
recovery problem by proposing the so-called GNNRank, a trainable GNN-based
framework with digraph embedding. Moreover, new objectives are devised to
encode ranking upsets/violations. The framework involves a ranking score
estimation approach, and adds an inductive bias by unfolding the Fiedler vector
computation of the graph constructed from a learnable similarity matrix.
Experimental results on extensive data sets show that our methods attain
competitive and often superior performance against baselines, as well as
showing promising transfer ability. Codes and preprocessed data are at:
\url{https://github.com/SherylHYX/GNNRank}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022 spotlight; 32 pages (9 pages for main text)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Balancing for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While machine learning models rapidly advance the state-of-the-art on various
real-world tasks, out-of-domain (OOD) generalization remains a challenging
problem given the vulnerability of these models to spurious correlations. We
propose a causally-motivated balanced mini-batch sampling strategy to transform
the observed train distribution to a balanced distribution that is free of
spurious correlations. We argue that the Bayes optimal classifier trained on
such balanced distribution is minimax optimal across a diverse enough
environment space. We also provide an identifiability guarantee of the latent
variable model of the proposed underlying data generation process with
invariant causal mechanisms, by utilizing enough number of train environments.
Experiments are conducted on three domain generalization datasets,
demonstrating empirically that our balanced mini-batch sampling strategy
improves the performance of four different established domain generalization
model baselines compared to the random mini-batch sampling strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning generates custom-made logistic regression models for
  explaining how breast cancer subtypes are classified 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2001.06988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2001.06988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuma Shibahara, Chisa Wada, Yasuho Yamashita, Kazuhiro Fujita, Masamichi Sato, Junichi Kuwata, Atsushi Okamoto, Yoshimasa Ono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating the intrinsic subtypes of breast cancer is crucial for
deciding the best treatment strategy. Deep learning can predict the subtypes
from genetic information more accurately than conventional statistical methods,
but to date, deep learning has not been directly utilized to examine which
genes are associated with which subtypes. To clarify the mechanisms embedded in
the intrinsic subtypes, we developed an explainable deep learning model called
a point-wise linear (PWL) model that generates a custom-made logistic
regression for each patient. Logistic regression, which is familiar to both
physicians and medical informatics researchers, allows us to analyze the
importance of the feature variables, and the PWL model harnesses these
practical abilities of logistic regression. In this study, we show that
analyzing breast cancer subtypes is clinically beneficial for patients and one
of the best ways to validate the capability of the PWL model. First, we trained
the PWL model with RNA-seq data to predict PAM50 intrinsic subtypes and applied
it to the 41/50 genes of PAM50 through the subtype prediction task. Second, we
developed a deep enrichment analysis method to reveal the relationships between
the PAM50 subtypes and the copy numbers of breast cancer. Our findings showed
that the PWL model utilized genes relevant to the cell cycle-related pathways.
These preliminary successes in breast cancer subtype analysis demonstrate the
potential of our analysis strategy to clarify the mechanisms underlying breast
cancer and improve overall clinical outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Liu, Xueqi Ma, Yibing Zhan, Liang Ding, Dapeng Tao, Bo Du, Wenbin Hu, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) tend to suffer from high computation costs due
to the exponentially increasing scale of graph data and the number of model
parameters, which restricts their utility in practical applications. To this
end, some recent works focus on sparsifying GNNs with the lottery ticket
hypothesis (LTH) to reduce inference costs while maintaining performance
levels. However, the LTH-based methods suffer from two major drawbacks: 1) they
require exhaustive and iterative training of dense models, resulting in an
extremely large training computation cost, and 2) they only trim graph
structures and model parameters but ignore the node feature dimension, where
significant redundancy exists. To overcome the above limitations, we propose a
comprehensive graph gradual pruning framework termed CGP. This is achieved by
designing a during-training graph pruning paradigm to dynamically prune GNNs
within one training process. Unlike LTH-based methods, the proposed CGP
approach requires no re-training, which significantly reduces the computation
costs. Furthermore, we design a co-sparsifying strategy to comprehensively trim
all three core elements of GNNs: graph structures, node features, and model
parameters. Meanwhile, aiming at refining the pruning operation, we introduce a
regrowth process into our CGP framework, in order to re-establish the pruned
but important connections. The proposed CGP is evaluated by using a node
classification task across 6 GNN architectures, including shallow models (GCN
and GAT), shallow-but-deep-propagation models (SGC and APPNP), and deep models
(GCNII and ResGCN), on a total of 14 real-world graph datasets, including
large-scale graph datasets from the challenging Open Graph Benchmark.
Experiments reveal that our proposed strategy greatly improves both training
and inference efficiency while matching or even exceeding the accuracy of
existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 27 figures, submitting to IEEE TNNLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learnable Mixed-precision and Dimension Reduction Co-design for
  Low-storage Activation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Shan Tai, Cheng-Yang Chang, Chieh-Fang Teng,  AnYeu,  Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep convolutional neural networks (CNNs) have achieved many
eye-catching results. However, deploying CNNs on resource-constrained edge
devices is constrained by limited memory bandwidth for transmitting large
intermediated data during inference, i.e., activation. Existing research
utilizes mixed-precision and dimension reduction to reduce computational
complexity but pays less attention to its application for activation
compression. To further exploit the redundancy in activation, we propose a
learnable mixed-precision and dimension reduction co-design system, which
separates channels into groups and allocates specific compression policies
according to their importance. In addition, the proposed dynamic searching
technique enlarges search space and finds out the optimal bit-width allocation
automatically. Our experimental results show that the proposed methods improve
3.54%/1.27% in accuracy and save 0.18/2.02 bits per value over existing
mixed-precision methods on ResNet18 and MobileNetv2, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Validation in Recommender Systems: Framework for Multi-layer
  Performance Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wissam Al Jurdi, Jacques Bou Abdo, Jacques Demerjian, Abdallah Makhoul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpreting the performance results of models that attempt to realize user
behavior in platforms that employ recommenders is a big challenge that
researchers and practitioners continue to face. Although current evaluation
tools possess the capacity to provide solid general overview of a system's
performance, they still lack consistency and effectiveness in their use as
evident in most recent studies on the topic. Current traditional assessment
techniques tend to fail to detect variations that could occur on smaller
subsets of the data and lack the ability to explain how such variations affect
the overall performance. In this article, we focus on the concept of data
clustering for evaluation in recommenders and apply a neighborhood assessment
method for the datasets of recommender system applications. This new method,
named neighborhood-based evaluation, aids in better understanding critical
performance variations in more compact subsets of the system to help spot
weaknesses where such variations generally go unnoticed with conventional
metrics and are typically averaged out. This new modular evaluation layer
complements the existing assessment mechanisms and provides the possibility of
several applications to the recommender ecosystem such as model evolution
tests, fraud/attack detection and a possibility for hosting a hybrid model
setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HICF: Hyperbolic Informative Collaborative Filtering <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menglin Yang, Zhihao Li, Min Zhou, Jiahong Liu, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the prevalence of the power-law distribution in user-item
networks, hyperbolic space has attracted considerable attention and achieved
impressive performance in the recommender system recently. The advantage of
hyperbolic recommendation lies in that its exponentially increasing capacity is
well-suited to describe the power-law distributed user-item network whereas the
Euclidean equivalent is deficient. Nonetheless, it remains unclear which kinds
of items can be effectively recommended by the hyperbolic model and which
cannot. To address the above concerns, we take the most basic recommendation
technique, collaborative filtering, as a medium, to investigate the behaviors
of hyperbolic and Euclidean recommendation models. The results reveal that (1)
tail items get more emphasis in hyperbolic space than that in Euclidean space,
but there is still ample room for improvement; (2) head items receive modest
attention in hyperbolic space, which could be considerably improved; (3) and
nonetheless, the hyperbolic models show more competitive performance than
Euclidean models. Driven by the above observations, we design a novel learning
method, named hyperbolic informative collaborative filtering (HICF), aiming to
compensate for the recommendation effectiveness of the head item while at the
same time improving the performance of the tail item. The main idea is to adapt
the hyperbolic margin ranking learning, making its pull and push procedure
geometric-aware, and providing informative guidance for the learning of both
head and tail items. Extensive experiments back up the analytic findings and
also show the effectiveness of the proposed method. The work is valuable for
personalized recommendations since it reveals that the hyperbolic space
facilitates modeling the tail item, which often represents user-customized
preferences or new products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining (KDD '22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Recommender System for Recommending Smartphones to Prospective
  Customers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.12876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.12876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik K. Biswas, Songlin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems are a subclass of machine learning systems that employ
sophisticated information filtering strategies to reduce the search time and
suggest the most relevant items to any particular user. Hybrid recommender
systems combine multiple recommendation strategies in different ways to benefit
from their complementary advantages. Some hybrid recommender systems have
combined collaborative filtering and content-based approaches to build systems
that are more robust. In this paper, we propose a hybrid recommender system,
which combines Alternating Least Squares (ALS) based collaborative filtering
with deep learning to enhance recommendation performance as well as overcome
the limitations associated with the collaborative filtering approach,
especially concerning its cold start problem. In essence, we use the outputs
from ALS (collaborative filtering) to influence the recommendations from a Deep
Neural Network (DNN), which combines characteristic, contextual, structural and
sequential information, in a big data processing framework. We have conducted
several experiments in testing the efficacy of the proposed hybrid architecture
in recommending smartphones to prospective customers and compared its
performance with other open-source recommenders. The results have shown that
the proposed system has outperformed several existing hybrid recommender
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expert Systems With Applications, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Rui<span class="highlight-author">yang Liu</span>, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as "semantic sensitivity". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Multi-interest News Sequence for News Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongyao Wang, Wenpeng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A session-based news recommender system recommends the next news to a user by
modeling the potential interests embedded in a sequence of news read/clicked by
her/him in a session. Generally, a user's interests are diverse, namely there
are multiple interests corresponding to different types of news, e.g., news of
distinct topics, within a session. %Modeling such multiple interests is
critical for precise news recommendation. However, most of existing methods
typically overlook such important characteristic and thus fail to distinguish
and model the potential multiple interests of a user, impeding accurate
recommendation of the next piece of news. Therefore, this paper proposes
multi-interest news sequence (MINS) model for news recommendation. In MINS, a
news encoder based on self-attention is devised on learn an informative
embedding for each piece of news, and then a novel parallel interest network is
devised to extract the potential multiple interests embedded in the news
sequence in preparation for the subsequent next-news recommendations. The
experimental results on a real-world dataset demonstrate that our model can
achieve better performance than the state-of-the-art compared models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A self-contained and self-explanatory DNA storage system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Li, Jiashu Wu, Junbiao Dai, Qingshan Jiang, Qiang Qu, Xiaoluo Huang, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research on DNA storage usually focuses on the improvement of storage
density by developing effective encoding and decoding schemes while lacking the
consideration on the uncertainty in ultra-long-term data storage and retention.
Consequently, the current DNA storage systems are often not self-contained,
implying that they have to resort to external tools for the restoration of the
stored DNA data. This may result in high risks in data loss since the required
tools might not be available due to the high uncertainty in far future. To
address this issue, we propose in this paper a self-contained DNA storage
system that can bring self-explanatory to its stored data without relying on
any external tool. To this end, we design a specific DNA file format whereby a
separate storage scheme is developed to reduce the data redundancy while an
effective indexing is designed for random read operations to the stored data
file. We verified through experimental data that the proposed self-contained
and self-explanatory method can not only get rid of the reliance on external
tools for data restoration but also minimise the data redundancy brought about
when the amount of data to be stored reaches a certain scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-target Filter and Detector for Unknown-number Speaker Diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16007v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16007v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chin-Yi Cheng, Hung-Shin Lee, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A strong representation of a target speaker can aid in extracting important
information regarding the speaker and detecting the corresponding temporal
regions in a multi-speaker conversation. In this study, we propose a neural
architecture that simultaneously extracts speaker representations that are
consistent with the speaker diarization objective and detects the presence of
each speaker frame by frame, regardless of the number of speakers in the
conversation. A speaker representation (known as a z-vector) extractor and
frame-speaker contextualizer, which is realized by a residual network and
processing data in both the temporal and speaker dimensions, are integrated
into a unified framework. Testing on the CALLHOME corpus reveals that our model
outperforms most methods presented to date. An evaluation in a more challenging
case of concurrent speakers ranging from two to seven demonstrates that our
model also achieves relative diarization error rate reductions of 26.35% and
6.4% over two typical baselines, namely the traditional x-vector clustering
model and attention-based model, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Signal Processing Letters</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-18T00:00:00Z">2022-07-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Data Augmentation for Robust Visual Question Answering <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Chen, Yuhang Zheng, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data Augmentation (DA) -- generating extra training samples beyond original
training set -- has been widely-used in today's unbiased VQA models to mitigate
the language biases. Current mainstream DA strategies are synthetic-based
methods, which synthesize new samples by either editing some visual
regions/words, or re-generating them from scratch. However, these synthetic
samples are always unnatural and error-prone. To avoid this issue, a recent DA
work composes new augmented samples by randomly pairing pristine images and
other human-written questions. Unfortunately, to guarantee augmented samples
have reasonable ground-truth answers, they manually design a set of heuristic
rules for several question types, which extremely limits its generalization
abilities. To this end, we propose a new Knowledge Distillation based Data
Augmentation for VQA, dubbed KDDAug. Specifically, we first relax the
requirements of reasonable image-question pairs, which can be easily applied to
any question types. Then, we design a knowledge distillation (KD) based answer
assignment to generate pseudo answers for all composed image-question pairs,
which are robust to both in-domain and out-of-distribution settings. Since
KDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into
any VQA architectures. Extensive ablation studies on multiple backbones and
benchmarks have demonstrated the effectiveness and generalization abilities of
KDDAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022; Codes: https://github.com/ItemZheng/KDDAug</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOAL: Towards Benchmarking Few-Shot Sports Game Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaan Wang, Tingyi Zhang, Haoxiang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sports game summarization aims to generate sports news based on real-time
commentaries. The task has attracted wide research attention but is still
under-explored probably due to the lack of corresponding English datasets.
Therefore, in this paper, we release GOAL, the first English sports game
summarization dataset. Specifically, there are 103 commentary-news pairs in
GOAL, where the average lengths of commentaries and news are 2724.9 and 476.3
words, respectively. Moreover, to support the research in the semi-supervised
setting, GOAL additionally provides 2,160 unlabeled commentary documents. Based
on our GOAL, we build and evaluate several baselines, including extractive and
abstractive baselines. The experimental results show the challenges of this
task still remain. We hope our work could promote the research of sports game
summarization. The dataset has been released at
https://github.com/krystalan/goal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MAD for Robust Reinforcement Learning in Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Donato, Lei Yu, Wang Ling, <span class="highlight-author">Chris Dyer</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new distributed policy gradient algorithm and show that it
outperforms existing reward-aware training procedures such as REINFORCE,
minimum risk training (MRT) and proximal policy optimization (PPO) in terms of
training stability and generalization performance when optimizing machine
translation models. Our algorithm, which we call MAD (on account of using the
mean absolute deviation in the importance weighting calculation), has
distributed data generators sampling multiple candidates per source sentence on
worker nodes, while a central learner updates the policy. MAD depends crucially
on two variance reduction strategies: (1) a conditional reward normalization
method that ensures each source sentence has both positive and negative reward
translation examples and (2) a new robust importance weighting scheme that acts
as a conditional entropy regularizer. Experiments on a variety of translation
tasks show that policies learned using the MAD algorithm perform very well when
using both greedy decoding and beam search, and that the learned policies are
sensitive to the specific reward used during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link
  Prediction and Entity Typing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Luo, Haihong E, Ling Tan, Xueyuan Lin, Gengxian Zhou, Jundi Li, Tianyu Yao, Kaiyang Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary attribute
value descriptions, which is considered to be more comprehensive and specific
than a triple-based fact. However, the existing hyper-relational KG embedding
methods in a single view are limited in application due to weakening the
hierarchical structure representing the affiliation between entities. To break
this limitation, we propose a dual-view hyper-relational KG (DH-KG) structure
which contains a hyper-relational instance view for entities and a
hyper-relational ontology view for concepts abstracted hierarchically from
entities to jointly model hyper-relational and hierarchical information. In
this paper, we first define link prediction and entity typing tasks on DH-KG
and construct two DH-KG datasets, JW44K-6K extracted from Wikidata and HTDM
based on medical data. Furthermore, We propose a DH-KG embedding model DHGE,
based on GRAN encoder, HGNN, and joint learning. Experimental results show that
DHGE outperforms baseline models on DH-KG. We also provide an example of the
application of this technology in the field of hypertension medication. Our
model and datasets are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlexU-AIC at Arabic Hate Speech 2022: Contrast to Classify 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Shapiro, Ayman Khalafallah, Marwan Torki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online presence on social media platforms such as Facebook and Twitter has
become a daily habit for internet users. Despite the vast amount of services
the platforms offer for their users, users suffer from cyber-bullying, which
further leads to mental abuse and may escalate to cause physical harm to
individuals or targeted groups. In this paper, we present our submission to the
Arabic Hate Speech 2022 Shared Task Workshop (OSACT5 2022) using the associated
Arabic Twitter dataset. The shared task consists of 3 sub-tasks, sub-task A
focuses on detecting whether the tweet is offensive or not. Then, For offensive
Tweets, sub-task B focuses on detecting whether the tweet is hate speech or
not. Finally, For hate speech Tweets, sub-task C focuses on detecting the
fine-grained type of hate speech among six different classes. Transformer
models proved their efficiency in classification tasks, but with the problem of
over-fitting when fine-tuned on a small or an imbalanced dataset. We overcome
this limitation by investigating multiple training paradigms such as
Contrastive learning and Multi-task learning along with Classification
fine-tuning and an ensemble of our top 5 performers. Our proposed solution
achieved 0.841, 0.817, and 0.476 macro F1-average in sub-tasks A, B, and C
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying COVID-19 vaccine narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Li, Carolina Scarton, Xingyi Song, Kalina Bontcheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  COVID-19 vaccine hesitancy is widespread, despite governments' information
campaigns and WHO efforts. One of the reasons behind this is vaccine
disinformation which widely spreads in social media. In particular, recent
surveys have established that vaccine disinformation is impacting negatively
citizen trust in COVID-19 vaccination. At the same time, fact-checkers are
struggling with detecting and tracking of vaccine disinformation, due to the
large scale of social media. To assist fact-checkers in monitoring vaccine
narratives online, this paper studies a new vaccine narrative classification
task, which categorises COVID-19 vaccine claims into one of seven categories.
Following a data augmentation approach, we first construct a novel dataset for
this new classification task, focusing on the minority classes. We also make
use of fact-checker annotated data. The paper also presents a neural vaccine
narrative classifier that achieves an accuracy of 84% under cross-validation.
The classifier is publicly available for researchers and journalists.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automated Classification of Attackers' TTPs by combining NLP
  with ML Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemens Sauerwein, Alexander Pfohl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasingly sophisticated and growing number of threat actors along with
the sheer speed at which cyber attacks unfold, make timely identification of
attacks imperative to an organisations' security. Consequently, persons
responsible for security employ a large variety of information sources
concerning emerging attacks, attackers' course of actions or indicators of
compromise. However, a vast amount of the needed security information is
available in unstructured textual form, which complicates the automated and
timely extraction of attackers' Tactics, Techniques and Procedures (TTPs). In
order to address this problem we systematically evaluate and compare different
Natural Language Processing (NLP) and machine learning techniques used for
security information extraction in research. Based on our investigations we
propose a data processing pipeline that automatically classifies unstructured
text according to attackers' tactics and techniques derived from a knowledge
base of adversary tactics, techniques and procedures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STT: Soft Template Tuning for Few-Shot Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Yu, Wei Wang, Chunyuan Li, Ruiyi Zhang, Zhanpeng Jin, Changyou Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning has been an extremely effective tool to adapt a pre-trained
model to downstream tasks. However, standard prompt-based methods mainly
consider the case of sufficient data of downstream tasks. It is still unclear
whether the advantage can be transferred to the few-shot regime, where only
limited data are available for each downstream task. Although some works have
demonstrated the potential of prompt-tuning under the few-shot setting, the
main stream methods via searching discrete prompts or tuning soft prompts with
limited data are still very challenging. Through extensive empirical studies,
we find that there is still a gap between prompt tuning and fully fine-tuning
for few-shot learning. To bridge the gap, we propose a new prompt-tuning
framework, called Soft Template Tuning (STT). STT combines manual and auto
prompts, and treats downstream classification tasks as a masked language
modeling task. Comprehensive evaluation on different settings suggests STT can
close the gap between fine-tuning and prompt-based methods without introducing
additional parameters. Significantly, it can even outperform the time- and
resource-consuming fine-tuning method on sentiment classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Brains Can't Detect Fake News: A Neuro-Cognitive Study of Textual
  Disinformation Susceptibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagri Arisoy, Anuradha Mandal, Nitesh Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of digital disinformation (aka "fake news") is arguably one of the
most significant threats on the Internet which can cause individual and
societal harm of large scales. The susceptibility to fake news attacks hinges
on whether Internet users perceive a fake news article/snippet to be legitimate
after reading it. In this paper, we attempt to garner an in-depth understanding
of users' susceptibility to text-centric fake news attacks via a
neuro-cognitive methodology. We investigate the neural underpinnings relevant
to fake/real news through EEG. We run an experiment with human users to pursue
a thorough investigation of users' perception and cognitive processing of
fake/real news. We analyze the neural activity associated with the fake/real
news detection task for different categories of news articles. Our results show
there may be no statistically significant or automatically inferable
differences in the way the human brain processes the fake vs. real news, while
marked differences are observed when people are subject to (real/fake) news vs.
resting state and even between some different categories of fake news. This
neuro-cognitive finding may help to justify users' susceptibility to fake news
attacks, as also confirmed from the behavioral analysis. In other words, the
fake news articles may seem almost indistinguishable from the real news
articles in both behavioral and neural domains. Our work serves to dissect the
fundamental neural phenomena underlying fake news attacks and explains users'
susceptibility to these attacks through the limits of human biology. We believe
this could be a notable insight for the researchers and practitioners
suggesting the human detection of fake news might be ineffective, which may
also have an adverse impact on the design of automated detection approaches
that crucially rely upon human labeling of text articles for building training
models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 tables, 2 figures, published in PST2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Large-Vocabulary Neural Language Models by Private Federated
  Learning for Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingbin Xu, Congzheng Song, Ye Tian, Neha Agrawal, Filip Granqvist, Rogier van Dalen, Xiao Zhang, Arturo Argueta, Shiyi Han, Yaqiao Deng, Leo Liu, Anmol Walia, Alex Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a technique to train models using data distributed
across devices. Differential Privacy (DP) provides a formal privacy guarantee
for sensitive data. Our goal is to train a large neural network language model
(NNLM) on compute-constrained devices while preserving privacy using FL and DP.
However, the DP-noise introduced to the model increases as the model size
grows, which often prevents convergence. We propose Partial Embedding Updates
(PEU), a novel technique to decrease noise by decreasing payload size.
Furthermore, we adopt Low Rank Adaptation (LoRA) and Noise Contrastive
Estimation (NCE) to reduce the memory demands of large models on
compute-constrained devices. This combination of techniques makes it possible
to train large-vocabulary language models while preserving accuracy and
privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selection Bias Induced Spurious Correlations in Large Language Models <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily McMilin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we show how large language models (LLMs) can learn statistical
dependencies between otherwise unconditionally independent variables due to
dataset selection bias. To demonstrate the effect, we developed a masked gender
task that can be applied to BERT-family models to reveal spurious correlations
between predicted gender pronouns and a variety of seemingly gender-neutral
variables like date and location, on pre-trained (unmodified) BERT and RoBERTa
large models. Finally, we provide an online demo, inviting readers to
experiment further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, Published at the ICML 2022 Workshop on Spurious
  Correlations, Invariance, and Stability</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRCLens: an MRC <span class="highlight-title">Dataset</span> Bias Detection Toolkit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhong, Haohan Wang, Eric P. Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent neural models have shown remarkable empirical results in Machine
Reading Comprehension, but evidence suggests sometimes the models take
advantage of dataset biases to predict and fail to generalize on out-of-sample
data. While many other approaches have been proposed to address this issue from
the computation perspective such as new architectures or training procedures,
we believe a method that allows researchers to discover biases, and adjust the
data or the models in an earlier stage will be beneficial. Thus, we introduce
MRCLens, a toolkit that detects whether biases exist before users train the
full model. For the convenience of introducing the toolkit, we also provide a
categorization of common biases in MRC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>dataperf workshop at IMCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Sequence Models for Text Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saheed Salahudeen Abdullahi, Sun Yiming, Shamsuddeen Hassan Muhammad, Abdulrasheed Mustapha, Ahmad Muhammad Aminu, Abdulkadir Abdullahi, Musa Bello, Saminu Mohammad Aliyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of data generated on the Internet in the current
information age is a driving force for the digital economy. Extraction of
information is the major value in an accumulated big data. Big data dependency
on statistical analysis and hand-engineered rules machine learning algorithms
are overwhelmed with vast complexities inherent in human languages. Natural
Language Processing (NLP) is equipping machines to understand these human
diverse and complicated languages. Text Classification is an NLP task which
automatically identifies patterns based on predefined or undefined labeled
sets. Common text classification application includes information retrieval,
modeling news topic, theme extraction, sentiment analysis, and spam detection.
In texts, some sequences of words depend on the previous or next word sequences
to make full meaning; this is a challenging dependency task that requires the
machine to be able to store some previous important information to impact
future meaning. Sequence models such as RNN, GRU, and LSTM is a breakthrough
for tasks with long-range dependencies. As such, we applied these models to
Binary and Multi-class classification. Results generated were excellent with
most of the models performing within the range of 80% and 94%. However, this
result is not exhaustive as we believe there is room for improvement if
machines are to compete with humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using attention methods to predict judicial outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vithor Gomes Ferreira Bertalan, Evandro Eduardo Seron Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal Judgment Prediction is one of the most acclaimed fields for the
combined area of NLP, AI, and Law. By legal prediction we mean an intelligent
systems capable to predict specific judicial characteristics, such as judicial
outcome, a judicial class, predict an specific case. In this research, we have
used AI classifiers to predict judicial outcomes in the Brazilian legal system.
For this purpose, we developed a text crawler to extract data from the official
Brazilian electronic legal systems. These texts formed a dataset of
second-degree murder and active corruption cases. We applied different
classifiers, such as Support Vector Machines and Neural Networks, to predict
judicial outcomes by analyzing textual features from the dataset. Our research
showed that Regression Trees, Gated Recurring Units and Hierarchical Attention
Networks presented higher metrics for different subsets. As a final goal, we
explored the weights of one of the algorithms, the Hierarchical Attention
Networks, to find a sample of the most important words used to absolve or
convict defendants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net
  for the Single-Corpus and Cross-Corpus Speech Emotion Recognition <span class="chip">IJCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin-Cheng Wen, Jia-Xin Ye, Yan Luo, Yong Xu, Xuan-Ze Wang, Chang-Li Wu, Kun-Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Emotion Recognition (SER) has become a growing focus of research in
human-computer interaction. An essential challenge in SER is to extract common
attributes from different speakers or languages, especially when a specific
source corpus has to be trained to recognize the unknown data coming from
another speech corpus. To address this challenge, a Capsule Network (CapsNet)
and Transfer Learning based Mixed Task Net (CTLMTNet) are proposed to deal with
both the singlecorpus and cross-corpus SER tasks simultaneously in this paper.
For the single-corpus task, the combination of Convolution-Pooling and
Attention CapsNet module CPAC) is designed by embedding the self-attention
mechanism to the CapsNet, guiding the module to focus on the important features
that can be fed into different capsules. The extracted high-level features by
CPAC provide sufficient discriminative ability. Furthermore, to handle the
cross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module
(CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can
learn the domain-invariant emotion representations through extracting the
strong emotion commonness. Experiments including ablation studies and
visualizations on both singleand cross-corpus tasks using four well-known SER
datasets in different languages are conducted for performance evaluation and
comparison. The results indicate that in both tasks the CTL-MTNet showed better
performance in all cases compared to a number of state-of-the-art methods. The
source code and the supplementary materials are available at:
https://github.com/MLDMXM2017/CTLMTNet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>this paper has been accepted by IJCAI 2022. Please cite it by:
  Xin-Cheng Wen#, JiaXin Ye#, Yan Luo, Yong Xu, Xuan-Ze WANG, Chang-Li Wu,
  Kun-Hong Liu*, CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed
  Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition,
  IJCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Amortized Noisy Channel Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Yuanzhe Pang, He He, <span class="highlight-author">Kyunghyun Cho</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noisy channel models have been especially effective in neural machine
translation (NMT). However, recent approaches like "beam search and rerank"
(BSR) incur significant computation overhead during inference, making
real-world application infeasible. We aim to study if it is possible to build
an amortized noisy channel NMT model such that when we do greedy decoding
during inference, the translation accuracy matches that of BSR in terms of
reward (based on the source-to-target log probability and the target-to-source
log probability) and quality (based on BLEU and BLEURT). We attempt three
approaches to train the new model: knowledge distillation, one-step-deviation
imitation learning, and Q learning. The first approach obtains the noisy
channel signal from a pseudo-corpus, and the latter two approaches aim to
optimize toward a noisy-channel MT reward directly. For all three approaches,
the generated translations fail to achieve rewards comparable to BSR, but the
translation quality approximated by BLEU and BLEURT is similar to the quality
of BSR-produced translations. Additionally, all three approaches speed up
inference by 1-2 orders of magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INLG 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Induction enabling Recommending and Trend Analysis: A
  Corporate Research Community Use Case <span class="chip">ISWC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandana Mihindukulasooriya, Mike Sava, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Irene Yachbes, Aditya Gidh, Jillian Duckwitz, Kovit Nisar, Michael Santos, Alfio Gliozzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A research division plays an important role of driving innovation in an
organization. Drawing insights, following trends, keeping abreast of new
research, and formulating strategies are increasingly becoming more challenging
for both researchers and executives as the amount of information grows in both
velocity and volume. In this paper we present a use case of how a corporate
research community, IBM Research, utilizes Semantic Web technologies to induce
a unified Knowledge Graph from both structured and textual data obtained by
integrating various applications used by the community related to research
projects, academic papers, datasets, achievements and recognition. In order to
make the Knowledge Graph more accessible to application developers, we
identified a set of common patterns for exploiting the induced knowledge and
exposed them as APIs. Those patterns were born out of user research which
identified the most valuable use cases or user pain points to be alleviated. We
outline two distinct scenarios: recommendation and analytics for business use.
We will discuss these scenarios in detail and provide an empirical evaluation
on entity recommendation specifically. The methodology used and the lessons
learned from this work can be applied to other organizations facing similar
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISWC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Value of Gazetteer in Chinese Named Entity Recognition <span class="chip">NLPCC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianglong Chen, Xiangji Zeng, Jiangang Zhu, Yin Zhang, Bojia Lin, Yang Yang, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gazetteer is widely used in Chinese named entity recognition (NER) to enhance
span boundary detection and type classification. However, to further understand
the generalizability and effectiveness of gazetteers, the NLP community still
lacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,
we first re-examine the effectiveness several common practices of the
gazetteer-enhanced NER models and carry out a series of detailed analysis to
evaluate the relationship between the model performance and the gazetteer
characteristics, which can guide us to build a more suitable gazetteer. The
findings of this paper are as follows: (1) the gazetteer improves most of the
situations that the traditional NER model datasets are difficult to learn. (2)
the performance of model greatly benefits from the high-quality pre-trained
lexeme embeddings. (3) a good gazetteer should cover more entities that can be
matched in both the training set and testing set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NLPCC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIA 2022 Shared Task Submission: Leveraging Entity Representations,
  Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhucheng Tu, Sarguna Janani Padmanabhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe our two-stage system for the Multilingual Information Access
(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The
first stage consists of multilingual passage retrieval with a hybrid dense and
sparse retrieval strategy. The second stage consists of a reader which outputs
the answer from the top passages returned by the first stage. We show the
efficacy of using a multilingual language model with entity representations in
pretraining, sparse retrieval signals to help dense retrieval, and
Fusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA
and 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we
obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of
31.61. We improve over the official baseline by over 4 F1 points on both the
development and test sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>System description for the Multilingual Information Access 2022
  Shared Task</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CPT: A <span class="highlight-title">Pre-Train</span>ed Unbalanced <span class="highlight-title">Transformer</span> for Both Chinese Language
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05729v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05729v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Hang Yan, Fei Yang, Li Zhe, Hujun Bao, <span class="highlight-author">Xipeng Qiu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we take the advantage of previous pre-trained models (PTMs)
and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different
from previous Chinese PTMs, CPT is designed to utilize the shared knowledge
between natural language understanding (NLU) and natural language generation
(NLG) to boost the performance. CPT consists of three parts: a shared encoder,
an understanding decoder, and a generation decoder. Two specific decoders with
a shared encoder are pre-trained with masked language modeling (MLM) and
denoising auto-encoding (DAE) tasks, respectively. With the partially shared
architecture and multi-task pre-training, CPT can (1) learn specific knowledge
of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that
fully exploits the potential of the model. Moreover, the unbalanced Transformer
saves the computational and storage cost, which makes CPT competitive and
greatly accelerates the inference of text generation. Experimental results on a
wide range of Chinese NLU and NLG tasks show the effectiveness of CPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/fastnlp/CPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying public values and spatial conflicts in urban planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rico H. Herzog, Juliana E. Gonçalves, Geertje Slingerland, Reinout Kleinhans, Holger Prang, Frances Brazier, Trivik Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the diverse and often competing values of citizens, and resolving
the consequent public value conflicts, are of significant importance for
inclusive and integrated urban development. Scholars have highlighted that
relational, value-laden urban space gives rise to many diverse conflicts that
vary both spatially and temporally. Although notions of public value conflicts
have been conceived in theory, there are very few empirical studies that
identify such values and their conflicts in urban space. Building on public
value theory and using a case-study mixed-methods approach, this paper proposes
a new approach to empirically investigate public value conflicts in urban
space. Using unstructured participatory data of 4,528 citizen contributions
from a Public Participation Geographic Information Systems in Hamburg, Germany,
natural language processing and spatial clustering techniques are used to
identify areas of potential value conflicts. Four expert workshops assess and
interpret these quantitative findings. Integrating both quantitative and
qualitative results, 19 general public values and a total of 9 archetypical
conflicts are identified. On the basis of these results, this paper proposes a
new conceptual tool of Public Value Spheres that extends the theoretical notion
of public-value conflicts and helps to further account for the value-laden
nature of urban space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion Intensity and its Control for Emotional Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, Berrak Sisman, Rajib Rana, Björn W. Schuller, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Affective Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Large Language Models Can Be Strong Differentially Private Learners <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05679v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05679v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechen Li, Florian Tramèr, <span class="highlight-author">Percy Liang</span>, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages; ICLR 2022 camera ready with additional writing
  clarification and no \vspace!</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Pixel Restoration as a Pretext Task for Transferable
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashmat Shadab Malik, Shahina K Kunhimon, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transferable adversarial attacks optimize adversaries from a pretrained
surrogate model and known label space to fool the unknown black-box models.
Therefore, these attacks are restricted by the availability of an effective
surrogate model. In this work, we relax this assumption and propose Adversarial
Pixel Restoration as a self-supervised alternative to train an effective
surrogate model from scratch under the condition of no labels and few data
samples. Our training approach is based on a min-max objective which reduces
overfitting via an adversarial objective and thus optimizes for a more
generalizable surrogate model. Our proposed attack is complimentary to our
adversarial pixel restoration and is independent of any task specific objective
as it can be launched in a self-supervised manner. We successfully demonstrate
the adversarial transferability of our approach to Vision Transformers as well
as Convolutional Neural Networks for the tasks of classification, object
detection, and video segmentation. Our codes & pre-trained surrogate models are
available at: https://github.com/HashmatShadab/APR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DeFlowSLAM: <span class="highlight-title">Self-Supervised</span> Scene Motion Decomposition for Dynamic Dense
  SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicai Ye, Xingyuan Yu, Xinyue Lan, Yuhang Ming, Jinyu Li, Hujun Bao, Zhao<span class="highlight-author">peng Cui</span>, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel dual-flow representation of scene motion that decomposes
the optical flow into a static flow field caused by the camera motion and
another dynamic flow field caused by the objects' movements in the scene. Based
on this representation, we present a dynamic SLAM, dubbed DeFlowSLAM, that
exploits both static and dynamic pixels in the images to solve the camera
poses, rather than simply using static background pixels as other dynamic SLAM
systems do. We propose a dynamic update module to train our DeFlowSLAM in a
self-supervised manner, where a dense bundle adjustment layer takes in
estimated static flow fields and the weights controlled by the dynamic mask and
outputs the residual of the optimized static flow fields, camera poses, and
inverse depths. The static and dynamic flow fields are estimated by warping the
current image to the neighboring images, and the optical flow can be obtained
by summing the two fields. Extensive experiments demonstrate that DeFlowSLAM
generalizes well to both static and dynamic scenes as it exhibits comparable
performance to the state-of-the-art DROID-SLAM in static and less dynamic
scenes while significantly outperforming DROID-SLAM in highly dynamic
environments. Code and data are available on the project webpage: \urlstyle{tt}
\textcolor{url_color}{\url{https://zju3dv.github.io/deflowslam/}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://zju3dv.github.io/deflowslam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Aware Observer Network for Out-of-Distribution Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Besnier, Andrei Bursuc, David Picard, Alexandre Briot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on Observer Network has shown promising results on
Out-Of-Distribution (OOD) detection for semantic segmentation. These methods
have difficulty in precisely locating the point of interest in the image, i.e,
the anomaly. This limitation is due to the difficulty of fine-grained
prediction at the pixel level. To address this issue, we provide instance
knowledge to the observer. We extend the approach of ObsNet by harnessing an
instance-wise mask prediction. We use an additional, class agnostic, object
detector to filter and aggregate observer predictions. Finally, we predict an
unique anomaly score for each instance in the image. We show that our proposed
method accurately disentangle in-distribution objects from Out-Of-Distribution
objects on three datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Data Augmentation for Robust Visual Question Answering <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Chen, Yuhang Zheng, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data Augmentation (DA) -- generating extra training samples beyond original
training set -- has been widely-used in today's unbiased VQA models to mitigate
the language biases. Current mainstream DA strategies are synthetic-based
methods, which synthesize new samples by either editing some visual
regions/words, or re-generating them from scratch. However, these synthetic
samples are always unnatural and error-prone. To avoid this issue, a recent DA
work composes new augmented samples by randomly pairing pristine images and
other human-written questions. Unfortunately, to guarantee augmented samples
have reasonable ground-truth answers, they manually design a set of heuristic
rules for several question types, which extremely limits its generalization
abilities. To this end, we propose a new Knowledge Distillation based Data
Augmentation for VQA, dubbed KDDAug. Specifically, we first relax the
requirements of reasonable image-question pairs, which can be easily applied to
any question types. Then, we design a knowledge distillation (KD) based answer
assignment to generate pseudo answers for all composed image-question pairs,
which are robust to both in-domain and out-of-distribution settings. Since
KDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into
any VQA architectures. Extensive ablation studies on multiple backbones and
benchmarks have demonstrated the effectiveness and generalization abilities of
KDDAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022; Codes: https://github.com/ItemZheng/KDDAug</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Diverse and Faithful One-shot Adaption of Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabo Zhang, Mingshuai Yao, Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-shot generative domain adaption aims to transfer a pre-trained generator
on one domain to a new domain using one reference image only. However, it
remains very challenging for the adapted generator (i) to generate diverse
images inherited from the pre-trained generator while (ii) faithfully acquiring
the domain-specific attributes and styles of the reference image. In this
paper, we present a novel one-shot generative domain adaption method, i.e.,
DiFa, for diverse generation and faithful adaptation. For global-level
adaptation, we leverage the difference between the CLIP embedding of reference
image and the mean embedding of source images to constrain the target
generator. For local-level adaptation, we introduce an attentive style loss
which aligns each intermediate token of adapted image with its corresponding
token of the reference image. To facilitate diverse generation, selective
cross-domain consistency is introduced to select and retain the domain-sharing
attributes in the editing latent $\mathcal{W}+$ space to inherit the diversity
of pre-trained generator. Extensive experiments show that our method
outperforms the state-of-the-arts both quantitatively and qualitatively,
especially for the cases of large domain gaps. Moreover, our DiFa can easily be
extended to zero-shot generative domain adaption with appealing results. Code
is available at https://github.com/1170300521/DiFa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/1170300521/DiFa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Temporal Lift Pooling for Continuous Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianyu Hu, Liqing Gao, Ze<span class="highlight-author">kang Liu</span>, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pooling methods are necessities for modern neural networks for increasing
receptive fields and lowering down computational costs. However, commonly used
hand-crafted pooling approaches, e.g., max pooling and average pooling, may not
well preserve discriminative features. While many researchers have elaborately
designed various pooling variants in spatial domain to handle these limitations
with much progress, the temporal aspect is rarely visited where directly
applying hand-crafted methods or these specialized spatial variants may not be
optimal. In this paper, we derive temporal lift pooling (TLP) from the Lifting
Scheme in signal processing to intelligently downsample features of different
temporal hierarchies. The Lifting Scheme factorizes input signals into various
sub-bands with different frequency, which can be viewed as different temporal
movement patterns. Our TLP is a three-stage procedure, which performs signal
decomposition, component weighting and information fusion to generate a refined
downsized feature map. We select a typical temporal task with long sequences,
i.e. continuous sign language recognition (CSLR), as our testbed to verify the
effectiveness of TLP. Experiments on two large-scale datasets show TLP
outperforms hand-crafted methods and specialized spatial variants by a large
margin (1.5%) with similar computational overhead. As a robust feature
extractor, TLP exhibits great generalizability upon multiple backbones on
various datasets and achieves new state-of-the-art results on two large-scale
CSLR datasets. Visualizations further demonstrate the mechanism of TLP in
correcting gloss borders. Code is released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Novelty Detection via Relational Reasoning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Cappio Borlino, Silvia Bucci, Tatiana Tommasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic novelty detection aims at discovering unknown categories in the test
data. This task is particularly relevant in safety-critical applications, such
as autonomous driving or healthcare, where it is crucial to recognize unknown
objects at deployment time and issue a warning to the user accordingly. Despite
the impressive advancements of deep learning research, existing models still
need a finetuning stage on the known categories in order to recognize the
unknown ones. This could be prohibitive when privacy rules limit data access,
or in case of strict memory and computational constraints (e.g. edge
computing). We claim that a tailored representation learning strategy may be
the right solution for effective and efficient semantic novelty detection.
Besides extensively testing state-of-the-art approaches for this task, we
propose a novel representation learning paradigm based on relational reasoning.
It focuses on learning how to measure semantic similarity rather than
recognizing known categories. Our experiments show that this knowledge is
directly transferable to a wide range of scenarios, and it can be exploited as
a plug-and-play module to convert closed-set recognition models into reliable
open-set ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label2Label: A Language Modeling Framework for Multi-Attribute Learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanhua Li, Zhexuan Cao, Jianjiang Feng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects are usually associated with multiple attributes, and these attributes
often exhibit high correlations. Modeling complex relationships between
attributes poses a great challenge for multi-attribute learning. This paper
proposes a simple yet generic framework named Label2Label to exploit the
complex attribute correlations. Label2Label is the first attempt for
multi-attribute prediction from the perspective of language modeling.
Specifically, it treats each attribute label as a "word" describing the sample.
As each sample is annotated with multiple attribute labels, these "words" will
naturally form an unordered but meaningful "sentence", which depicts the
semantic information of the corresponding sample. Inspired by the remarkable
success of pre-training language models in NLP, Label2Label introduces an
image-conditioned masked language model, which randomly masks some of the
"word" tokens from the label "sentence" and aims to recover them based on the
masked "sentence" and the context conveyed by image features. Our intuition is
that the instance-wise attribute relations are well grasped if the neural net
can infer the missing attributes based on the context and the remaining
attribute hints. Label2Label is conceptually simple and empirically powerful.
Without incorporating task-specific prior knowledge and highly specialized
network designs, our approach achieves state-of-the-art results on three
different multi-attribute learning tasks, compared to highly customized
domain-specific methods. Code is available at
https://github.com/Li-Wanhua/Label2Label.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Video Super Resolution with Patch-Based Temporal Redundancy
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Huang, Hang Dong, Jinshan Pan, Chao Zhu, Yu Guo, Ding Liu, Lean Fu, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of existing video super-resolution (VSR) algorithms stems mainly
exploiting the temporal information from the neighboring frames. However, none
of these methods have discussed the influence of the temporal redundancy in the
patches with stationary objects and background and usually use all the
information in the adjacent frames without any discrimination. In this paper,
we observe that the temporal redundancy will bring adverse effect to the
information propagation,which limits the performance of the most existing VSR
methods. Motivated by this observation, we aim to improve existing VSR
algorithms by handling the temporal redundancy patches in an optimized manner.
We develop two simple yet effective plug and play methods to improve the
performance of existing local and non-local propagation-based VSR algorithms on
widely-used public videos. For more comprehensive evaluating the robustness and
performance of existing VSR algorithms, we also collect a new dataset which
contains a variety of public videos as testing set. Extensive evaluations show
that the proposed methods can significantly improve the performance of existing
VSR methods on the collected videos from wild scenarios while maintain their
performance on existing commonly used datasets. The code is available at
https://github.com/HYHsimon/Boosted-VSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action-based <span class="highlight-title">Contrastive Learning</span> for Trajectory Prediction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marah Halawa, Olaf Hellwich, Pia Bideau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction is an essential task for successful human robot
interaction, such as in autonomous driving. In this work, we address the
problem of predicting future pedestrian trajectories in a first person view
setting with a moving camera. To that end, we propose a novel action-based
contrastive learning loss, that utilizes pedestrian action information to
improve the learned trajectory embeddings. The fundamental idea behind this new
loss is that trajectories of pedestrians performing the same action should be
closer to each other in the feature space than the trajectories of pedestrians
with significantly different actions. In other words, we argue that behavioral
information about pedestrian action influences their future trajectory.
Furthermore, we introduce a novel sampling strategy for trajectories that is
able to effectively increase negative and positive contrastive samples.
Additional synthetic trajectory samples are generated using a trained
Conditional Variational Autoencoder (CVAE), which is at the core of several
models developed for trajectory prediction. Results show that our proposed
contrastive framework employs contextual information about pedestrian behavior,
i.e. action, effectively, and it learns a better trajectory representation.
Thus, integrating the proposed contrastive framework within a trajectory
prediction model improves its results and outperforms state-of-the-art methods
on three trajectory prediction benchmarks [31, 32, 26].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will appear in the proceedings of The European Conference
  on Computer Vision (ECCV 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards High-Fidelity Single-view Holistic Reconstruction of Indoor
  Scenes <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Liu, Yujian Zheng, Guanying Chen, Shuguang Cui, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new framework to reconstruct holistic 3D indoor scenes including
both room background and indoor objects from single-view images. Existing
methods can only produce 3D shapes of indoor objects with limited geometry
quality because of the heavy occlusion of indoor scenes. To solve this, we
propose an instance-aligned implicit function (InstPIFu) for detailed object
reconstruction. Combining with instance-aligned attention module, our method is
empowered to decouple mixed local features toward the occluded instances.
Additionally, unlike previous methods that simply represents the room
background as a 3D bounding box, depth map or a set of planes, we recover the
fine geometry of the background via implicit representation. Extensive
experiments on the e SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets
demonstrate that our method outperforms existing approaches in both background
and foreground object reconstruction. Our code and model will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, project page: https://github.com/UncleMEDM/InstPIFu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Action Affinity and Continuity for Semi-supervised Temporal
  Action Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guodong Ding, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised learning approach to the temporal action
segmentation task. The goal of the task is to temporally detect and segment
actions in long, untrimmed procedural videos, where only a small set of videos
are densely labelled, and a large collection of videos are unlabelled. To this
end, we propose two novel loss functions for the unlabelled data: an action
affinity loss and an action continuity loss. The action affinity loss guides
the unlabelled samples learning by imposing the action priors induced from the
labelled set. Action continuity loss enforces the temporal continuity of
actions, which also provides frame-wise classification supervision. In
addition, we propose an Adaptive Boundary Smoothing (ABS) approach to build
coarser action boundaries for more robust and reliable learning. The proposed
loss functions and ABS were evaluated on three benchmarks. Results show that
they significantly improved action segmentation performance with a low amount
(5% and 10%) of labelled data and achieved comparable results to full
supervision with 50% labelled data. Furthermore, ABS succeeded in boosting
performance when integrated into fully-supervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpolation, extrapolation, and local generalization in common neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Bonnasse-Gahot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a long history of works showing that neural networks have hard
time extrapolating beyond the training set. A recent study by Balestriero et
al. (2021) challenges this view: defining interpolation as the state of
belonging to the convex hull of the training set, they show that the test set,
either in input or neural space, cannot lie for the most part in this convex
hull, due to the high dimensionality of the data, invoking the well known curse
of dimensionality. Neural networks are then assumed to necessarily work in
extrapolative mode. We here study the neural activities of the last hidden
layer of typical neural networks. Using an autoencoder to uncover the intrinsic
space underlying the neural activities, we show that this space is actually
low-dimensional, and that the better the model, the lower the dimensionality of
this intrinsic space. In this space, most samples of the test set actually lie
in the convex hull of the training set: under the convex hull definition, the
models thus happen to work in interpolation regime. Moreover, we show that
belonging to the convex hull does not seem to be the relevant criteria.
Different measures of proximity to the training set are actually better related
to performance accuracy. Thus, typical neural networks do seem to operate in
interpolation regime. Good generalization performances are linked to the
ability of a neural network to operate well in such a regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Partition Implicit with Surface Codes for 3D Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Chen, Yu-Shen Liu, Zhihong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep implicit functions have shown remarkable shape modeling ability in
various 3D computer vision tasks. One drawback is that it is hard for them to
represent a 3D shape as multiple parts. Current solutions learn various
primitives and blend the primitives directly in the spatial space, which still
struggle to approximate the 3D shape accurately. To resolve this problem, we
introduce a novel implicit representation to represent a single 3D shape as a
set of parts in the latent space, towards both highly accurate and plausibly
interpretable shape modeling. Our insight here is that both the part learning
and the part blending can be conducted much easier in the latent space than in
the spatial space. We name our method Latent Partition Implicit (LPI), because
of its ability of casting the global shape modeling into multiple local part
modeling, which partitions the global shape unity. LPI represents a shape as
Signed Distance Functions (SDFs) using surface codes. Each surface code is a
latent code representing a part whose center is on the surface, which enables
us to flexibly employ intrinsic attributes of shapes or additional surface
properties. Eventually, LPI can reconstruct both the shape and the parts on the
shape, both of which are plausible meshes. LPI is a multi-level representation,
which can partition a shape into different numbers of parts after training. LPI
can be learned without ground truth signed distances, point normals or any
supervision for part partition. LPI outperforms the latest methods under the
widely used benchmarks in terms of reconstruction accuracy and modeling
interpretability. Our code, data and models are available at
https://github.com/chenchao15/LPI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages,14figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FakeCLR: Exploring <span class="highlight-title">Contrastive Learning</span> for Solving Latent Discontinuity
  in Data-Efficient GANs <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Li, Chaoyue Wang, Heliang Zheng, Jing Zhang, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-Efficient GANs (DE-GANs), which aim to learn generative models with a
limited amount of training data, encounter several challenges for generating
high-quality samples. Since data augmentation strategies have largely
alleviated the training instability, how to further improve the generative
performance of DE-GANs becomes a hotspot. Recently, contrastive learning has
shown the great potential of increasing the synthesis quality of DE-GANs, yet
related principles are not well explored. In this paper, we revisit and compare
different contrastive learning strategies in DE-GANs, and identify (i) the
current bottleneck of generative performance is the discontinuity of latent
space; (ii) compared to other contrastive learning strategies,
Instance-perturbation works towards latent space continuity, which brings the
major improvement to DE-GANs. Based on these observations, we propose FakeCLR,
which only applies contrastive learning on perturbed fake samples, and devises
three related training techniques: Noise-related Latent Augmentation,
Diversity-aware Queue, and Forgetting Factor of Queue. Our experimental results
manifest the new state of the arts on both few-shot generation and limited-data
generation. On multiple datasets, FakeCLR acquires more than 15% FID
improvement compared to existing DE-GANs. Code is available at
https://github.com/iceli1007/FakeCLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Unifying Event Detection and Captioning as Sequence Generation via
  <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Qi Zhang</span>, Yuqing Song, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense video captioning aims to generate corresponding text descriptions for a
series of events in the untrimmed video, which can be divided into two
sub-tasks, event detection and event captioning. Unlike previous works that
tackle the two sub-tasks separately, recent works have focused on enhancing the
inter-task association between the two sub-tasks. However, designing inter-task
interactions for event detection and captioning is not trivial due to the large
differences in their task specific solutions. Besides, previous event detection
methods normally ignore temporal dependencies between events, leading to event
redundancy or inconsistency problems. To tackle above the two defects, in this
paper, we define event detection as a sequence generation task and propose a
unified pre-training and fine-tuning framework to naturally enhance the
inter-task association between event detection and captioning. Since the model
predicts each event with previous events as context, the inter-dependency
between events is fully exploited and thus our model can detect more diverse
and consistent events in the video. Experiments on the ActivityNet dataset show
that our model outperforms the state-of-the-art methods, and can be further
boosted when pre-trained on extra large-scale video-text data. Code is
available at \url{https://github.com/QiQAng/UEDVC}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CACTUSS: Common Anatomical CT-US Space for US examinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yordanka Velikova, Walter Simson, Mehrdad Salehi, Mohammad Farid Azampour, Philipp Paprottka, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abdominal aortic aneurysm (AAA) is a vascular disease in which a section of
the aorta enlarges, weakening its walls and potentially rupturing the vessel.
Abdominal ultrasound has been utilized for diagnostics, but due to its limited
image quality and operator dependency, CT scans are usually required for
monitoring and treatment planning. Recently, abdominal CT datasets have been
successfully utilized to train deep neural networks for automatic aorta
segmentation. Knowledge gathered from this solved task could therefore be
leveraged to improve US segmentation for AAA diagnosis and monitoring. To this
end, we propose CACTUSS: a common anatomical CT-US space, which acts as a
virtual bridge between CT and US modalities to enable automatic AAA screening
sonography. CACTUSS makes use of publicly available labelled data to learn to
segment based on an intermediary representation that inherits properties from
both US and CT. We train a segmentation network in this new representation and
employ an additional image-to-image translation network which enables our model
to perform on real B-mode images. Quantitative comparisons against fully
supervised methods demonstrate the capabilities of CACTUSS in terms of Dice
Score and diagnostic metrics, showing that our method also meets the clinical
requirements for AAA scanning and diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExAgt: Expert-guided Augmentation for Representation Learning of Traffic
  Scenarios <span class="chip">SC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshman Balasubramanian, Jonas Wurst, Robin Egolf, Michael Botsch, Wolfgang Utschick, Ke Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning in recent years has been addressed with
self-supervised learning methods. The input data is augmented into two
distorted views and an encoder learns the representations that are invariant to
distortions -- cross-view prediction. Augmentation is one of the key components
in cross-view self-supervised learning frameworks to learn visual
representations. This paper presents ExAgt, a novel method to include expert
knowledge for augmenting traffic scenarios, to improve the learnt
representations without any human annotation. The expert-guided augmentations
are generated in an automated fashion based on the infrastructure, the
interactions between the EGO and the traffic participants and an ideal sensor
model. The ExAgt method is applied in two state-of-the-art cross-view
prediction methods and the representations learnt are tested in downstream
tasks like classification and clustering. Results show that the ExAgt method
improves representation learning compared to using only standard augmentations
and it provides a better representation space stability. The code is available
at \url{https://github.com/lab176344/ExAgt}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper in ITSC 2022, Macau, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-incremental Novel Class Discovery <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhankar Roy, Mingxuan Liu, Zhun Zhong, Nicu Sebe, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the new task of class-incremental Novel Class Discovery
(class-iNCD), which refers to the problem of discovering novel categories in an
unlabelled data set by leveraging a pre-trained model that has been trained on
a labelled data set containing disjoint yet related categories. Apart from
discovering novel classes, we also aim at preserving the ability of the model
to recognize previously seen base categories. Inspired by rehearsal-based
incremental learning methods, in this paper we propose a novel approach for
class-iNCD which prevents forgetting of past information about the base classes
by jointly exploiting base class feature prototypes and feature-level knowledge
distillation. We also propose a self-training clustering strategy that
simultaneously clusters novel categories and trains a joint classifier for both
the base and novel classes. This makes our method able to operate in a
class-incremental setting. Our experiments, conducted on three common
benchmarks, demonstrate that our method significantly outperforms
state-of-the-art approaches. Code is available at
https://github.com/OatmealLiu/class-iNCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Aware Reference Synthesis for Multi-View Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ri Cheng, Yuqi Sun, Bo Yan, Weimin Tan, Chenxi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multi-view multimedia applications struggle between high-resolution
(HR) visual experience and storage or bandwidth constraints. Therefore, this
paper proposes a Multi-View Image Super-Resolution (MVISR) task. It aims to
increase the resolution of multi-view images captured from the same scene. One
solution is to apply image or video super-resolution (SR) methods to
reconstruct HR results from the low-resolution (LR) input view. However, these
methods cannot handle large-angle transformations between views and leverage
information in all multi-view images. To address these problems, we propose the
MVSRnet, which uses geometry information to extract sharp details from all LR
multi-view to support the SR of the LR input view. Specifically, the proposed
Geometry-Aware Reference Synthesis module in MVSRnet uses geometry information
and all multi-view LR images to synthesize pixel-aligned HR reference images.
Then, the proposed Dynamic High-Frequency Search network fully exploits the
high-frequency textural details in reference images for SR. Extensive
experiments on several benchmarks show that our method significantly improves
over the state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, ACM MULTIMEDIA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetrized Robust Procrustes: Constant-Factor Approximation and Exact
  Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Amir, Shahar Kovalsky, Nadav Dym
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical $\textit{Procrustes}$ problem is to find a rigid motion
(orthogonal transformation and translation) that best aligns two given
point-sets in the least-squares sense. The $\textit{Robust Procrustes}$ problem
is an important variant, in which a power-1 objective is used instead of least
squares to improve robustness to outliers. While the optimal solution of the
least-squares problem can be easily computed in closed form, dating back to
Sch\"onemann (1966), no such solution is known for the power-1 problem. In this
paper we propose a novel convex relaxation for the Robust Procrustes problem.
Our relaxation enjoys several theoretical and practical advantages:
Theoretically, we prove that our method provides a $\sqrt{2}$-factor
approximation to the Robust Procrustes problem, and that, under appropriate
assumptions, it exactly recovers the true rigid motion from point
correspondences contaminated by outliers. In practice, we find in numerical
experiments on both synthetic and real robust Procrustes problems, that our
method performs similarly to the standard Iteratively Reweighted Least Squares
(IRLS). However the convexity of our algorithm allows incorporating additional
convex penalties, which are not readily amenable to IRLS. This turns out to be
a substantial advantage, leading to improved results in high-dimensional
problems, including non-rigid shape alignment and semi-supervised interlingual
word translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Brain-Inspired Decoder for Natural Visual Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyi Li, Shengjie Zheng, Yufan Liao, Rongqi Hong, Weiliang Chen, Chenggnag He, Xiaojian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding images from brain activity has been a challenge. Owing to the
development of deep learning, there are available tools to solve this problem.
The decoded image, which aims to map neural spike trains to low-level visual
features and high-level semantic information space. Recently, there are a few
studies of decoding from spike trains, however, these studies pay less
attention to the foundations of neuroscience and there are few studies that
merged receptive field into visual image reconstruction. In this paper, we
propose a deep learning neural network architecture with biological properties
to reconstruct visual image from spike trains. As far as we know, we
implemented a method that integrated receptive field property matrix into loss
function at the first time. Our model is an end-to-end decoder from neural
spike trains to images. We not only merged Gabor filter into auto-encoder which
used to generate images but also proposed a loss function with receptive field
properties. We evaluated our decoder on two datasets which contain macaque
primary visual cortex neural spikes and salamander retina ganglion cells (RGCs)
spikes. Our results show that our method can effectively combine receptive
field features to reconstruct images, providing a new approach to visual
reconstruction based on neural information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study of the performance and scalablity of federated learning for
  medical imaging with intermittent clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judith Sáinz-Pardo Díaz, Álvaro López García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy and area under the curve) and reduction of execution times
will be studied with respect to the classical case (the centralized approach).
Different clients will be simulated from the training data, selected in an
unbalanced manner, i.e., they do not all have the same number of data. The
results of considering three or ten clients are exposed and compared between
them and against the centralized case. Two approaches to follow will be
analyzed in the case of intermittent clients, as in a real scenario some
clients may leave the training, and some new ones may enter the training. The
evolution of the results for the test set in terms of accuracy, area under the
curve and execution time is shown as the number of clients into which the
original data is divided increases. Finally, improvements and future work in
the field are proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-manifold Attention for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Konstantinidis, Ilias Papastratis, Kosmas Dimitropoulos, Petros Daras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer are very popular nowadays due to their state-of-the-art
performance in several computer vision tasks, such as image classification and
action recognition. Although the performance of Vision Transformers have been
greatly improved by employing Convolutional Neural Networks, hierarchical
structures and compact forms, there is limited research on ways to utilize
additional data representations to refine the attention map derived from the
multi-head attention of a Transformer network. This work proposes a novel
attention mechanism, called multi-manifold attention, that can substitute any
standard attention mechanism in a Transformer-based network. The proposed
attention models the input space in three distinct manifolds, namely Euclidean,
Symmetric Positive Definite and Grassmann, with different statistical and
geometrical properties, guiding the network to take into consideration a rich
set of information that describe the appearance, color and texture of an image,
for the computation of a highly descriptive attention map. In this way, a
Vision Transformer with the proposed attention is guided to become more
attentive towards discriminative features, leading to improved classification
results, as shown by the experimental results on several well-known image
classification datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latency-Aware Collaborative Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixing Lei, Shunli Ren, Yue Hu, Wenjun Zhang, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception has recently shown great potential to improve
perception capabilities over single-agent perception. Existing collaborative
perception methods usually consider an ideal communication environment.
However, in practice, the communication system inevitably suffers from latency
issues, causing potential performance degradation and high risks in
safety-critical applications, such as autonomous driving. To mitigate the
effect caused by the inevitable communication latency, from a machine learning
perspective, we present the first latency-aware collaborative perception
system, which actively adopts asynchronous perceptual features from multiple
agents to the same timestamp, promoting the robustness and effectiveness of
collaboration. To achieve such a feature-level synchronization, we propose a
novel latency compensation module, calledSyncNet, which leverages
feature-attention symbiotic estimation and time modulation techniques.
Experimental results show that our method outperforms the state-of-the-art
collaborative perception method by 15.6% on the latest collaborative perception
dataset V2X-SIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, Accepted by European conference on computer
  vision, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Cross-Query-and-Support Attention Weighted Mask Aggregation for
  Few-Shot Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Shi, Dong Wei, Yu Zhang, Donghuan Lu, Munan Ning, Jiashun Chen, Kai Ma, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research into Few-shot Semantic Segmentation (FSS) has attracted great
attention, with the goal to segment target objects in a query image given only
a few annotated support images of the target class. A key to this challenging
task is to fully utilize the information in the support images by exploiting
fine-grained correlations between the query and support images. However, most
existing approaches either compressed the support information into a few
class-wise prototypes, or used partial support information (e.g., only
foreground) at the pixel level, causing non-negligible information loss. In
this paper, we propose Dense pixel-wise Cross-query-and-support Attention
weighted Mask Aggregation (DCAMA), where both foreground and background support
information are fully exploited via multi-level pixel-wise correlations between
paired query and support features. Implemented with the scaled dot-product
attention in the Transformer architecture, DCAMA treats every query pixel as a
token, computes its similarities with all support pixels, and predicts its
segmentation label as an additive aggregation of all the support pixels' labels
-- weighted by the similarities. Based on the unique formulation of DCAMA, we
further propose efficient and effective one-pass inference for n-shot
segmentation, where pixels of all support images are collected for the mask
aggregation at once. Experiments show that our DCAMA significantly advances the
state of the art on standard FSS benchmarks of PASCAL-5i, COCO-20i, and
FSS-1000, e.g., with 3.1%, 9.7%, and 3.6% absolute improvements in 1-shot mIoU
over previous best records. Ablative studies also verify the design DCAMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Fine-grained Image Classification via Multi-Frequency
  Neighborhood and Double-cross Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hegui Zhu, Zhan Gao, Jiayi Wang, Yange Zhou, Chengqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional fine-grained image classification typically relies on large-scale
training samples with annotated ground-truth. However, some sub-categories may
have few available samples in real-world applications. In this paper, we
propose a novel few-shot fine-grained image classification network (FicNet)
using multi-frequency Neighborhood (MFN) and double-cross modulation (DCM).
Module MFN is adopted to capture the information in spatial domain and
frequency domain. Then, the self-similarity and multi-frequency components are
extracted to produce multi-frequency structural representation. DCM employs
bi-crisscross component and double 3D cross-attention components to modulate
the embedding process by considering global context information and subtle
relationship between categories, respectively. The comprehensive experiments on
three fine-grained benchmark datasets for two few-shot tasks verify that FicNet
has excellent performance compared to the state-of-the-art methods. Especially,
the experiments on two datasets, "Caltech-UCSD Birds" and "Stanford Cars", can
obtain classification accuracy 93.17\% and 95.36\%, respectively. They are even
higher than that the general fine-grained image classification methods can
achieve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniFormer: Unified Multi-view Fusion <span class="highlight-title">Transformer</span> for Spatial-Temporal
  Representation in Bird's-Eye-View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's eye view (BEV) representation is a new perception formulation for
autonomous driving, which is based on spatial fusion. Further, temporal fusion
is also introduced in BEV representation and gains great success. In this work,
we propose a new method that unifies both spatial and temporal fusion and
merges them into a unified mathematical formulation. The unified fusion could
not only provide a new perspective on BEV fusion but also brings new
capabilities. With the proposed unified spatial-temporal fusion, our method
could support long-range fusion, which is hard to achieve in conventional BEV
methods. Moreover, the BEV fusion in our work is temporal-adaptive, and the
weights of temporal fusion are learnable. In contrast, conventional methods
mainly use fixed and equal weights for temporal fusion. Besides, the proposed
unified fusion could avoid information lost in conventional BEV fusion methods
and make full use of features. Extensive experiments and ablation studies on
the NuScenes dataset show the effectiveness of the proposed method and our
method gains the state-of-the-art performance in the map segmentation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, <span class="highlight-author">Deng Cai</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D detection has drawn much attention from the community due to its
low cost and setup simplicity. It takes an RGB image as input and predicts 3D
boxes in the 3D space. The most challenging sub-task lies in the instance depth
estimation. Previous works usually use a direct estimation method. However, in
this paper we point out that the instance depth on the RGB image is
non-intuitive. It is coupled by visual depth clues and instance attribute
clues, making it hard to be directly learned in the network. Therefore, we
propose to reformulate the instance depth to the combination of the instance
visual surface depth (visual depth) and the instance attribute depth (attribute
depth). The visual depth is related to objects' appearances and positions on
the image. By contrast, the attribute depth relies on objects' inherent
attributes, which are invariant to the object affine transformation on the
image. Correspondingly, we decouple the 3D location uncertainty into visual
depth uncertainty and attribute depth uncertainty. By combining different types
of depths and associated uncertainties, we can obtain the final instance depth.
Furthermore, data augmentation in monocular 3D detection is usually limited due
to the physical nature, hindering the boost of performance. Based on the
proposed instance depth disentanglement strategy, we can alleviate this
problem. Evaluated on KITTI, our method achieves new state-of-the-art results,
and extensive ablation studies validate the effectiveness of each component in
our method. The codes are released at https://github.com/SPengLiang/DID-M3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Angular Gap: Reducing the Uncertainty of Image Difficulty through Model
  Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohua Peng, Mobarakol Islam, Mei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curriculum learning needs example difficulty to proceed from easy to hard.
However, the credibility of image difficulty is rarely investigated, which can
seriously affect the effectiveness of curricula. In this work, we propose
Angular Gap, a measure of difficulty based on the difference in angular
distance between feature embeddings and class-weight embeddings built by
hyperspherical learning. To ascertain difficulty estimation, we introduce
class-wise model calibration, as a post-training technique, to the learnt
hyperbolic space. This bridges the gap between probabilistic model calibration
and angular distance estimation of hyperspherical learning. We show the
superiority of our calibrated Angular Gap over recent difficulty metrics on
CIFAR10-H and ImageNetV2. We further propose Angular Gap based curriculum
learning for unsupervised domain adaptation that can translate from learning
easy samples to mining hard samples. We combine this curriculum with a
state-of-the-art self-training method, Cycle Self Training (CST). The proposed
Curricular CST learns robust representations and outperforms recent baselines
on Office31 and VisDA 2017.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiFormer: Hierarchical Multi-scale Representations Using <span class="highlight-title">Transformer</span>s
  for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moein Heidari, Amirhossein Kazerouni, Milad Soltany, Reza Azad, Ehsan Khodapanah Aghdam, Julien Cohen-Adad, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have been the consensus for medical
image segmentation tasks. However, they suffer from the limitation in modeling
long-range dependencies and spatial correlations due to the nature of
convolution operation. Although transformers were first developed to address
this issue, they fail to capture low-level features. In contrast, it is
demonstrated that both local and global features are crucial for dense
prediction, such as segmenting in challenging contexts. In this paper, we
propose HiFormer, a novel method that efficiently bridges a CNN and a
transformer for medical image segmentation. Specifically, we design two
multi-scale feature representations using the seminal Swin Transformer module
and a CNN-based encoder. To secure a fine fusion of global and local features
obtained from the two aforementioned representations, we propose a Double-Level
Fusion (DLF) module in the skip connection of the encoder-decoder structure.
Extensive experiments on various medical image segmentation datasets
demonstrate the effectiveness of HiFormer over other CNN-based,
transformer-based, and hybrid methods in terms of computational complexity, and
quantitative and qualitative results. Our code is publicly available at:
https://github.com/amirhossein-kz/HiFormer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Alignment in Video Super-Resolution <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuwei Shi, Jinjin Gu, Liangbin Xie, Xintao Wang, Yujiu Yang, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of adjacent frames is considered an essential operation in
video super-resolution (VSR). Advanced VSR models, including the latest VSR
Transformers, are generally equipped with well-designed alignment modules.
However, the progress of the self-attention mechanism may violate this common
sense. In this paper, we rethink the role of alignment in VSR Transformers and
make several counter-intuitive observations. Our experiments show that: (i) VSR
Transformers can directly utilize multi-frame information from unaligned
videos, and (ii) existing alignment methods are sometimes harmful to VSR
Transformers. These observations indicate that we can further improve the
performance of VSR Transformers simply by removing the alignment module and
adopting a larger attention window. Nevertheless, such designs will
dramatically increase the computational burden, and cannot deal with large
motions. Therefore, we propose a new and efficient alignment method called
patch alignment, which aligns image patches instead of pixels. VSR Transformers
equipped with patch alignment could demonstrate state-of-the-art performance on
multiple benchmarks. Our work provides valuable insights on how multi-frame
information is used in VSR and how to select alignment methods for different
networks/datasets. Codes and models will be released at
https://github.com/XPixelGroup/RethinkVSRAlignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Distributed Image Compression with Cross-Attention Feature
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Mital, Ezgi Ozyilkan, Ali Garjani, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel deep neural network (DNN) architecture for compressing an
image when a correlated image is available as side information only at the
decoder side, a special case of the well-known and heavily studied distributed
source coding (DSC) problem. In particular, we consider a pair of stereo
images, which have overlapping fields of view, captured by a synchronized and
calibrated pair of cameras; and therefore, are highly correlated. We assume
that one image of the pair is to be compressed and transmitted, while the other
image is available only at the decoder. In the proposed architecture, the
encoder maps the input image to a latent space using a DNN, quantizes the
latent representation, and compresses it losslessly using entropy coding. The
proposed decoder extracts useful information common between the images solely
from the available side information, as well as a latent representation of the
side information. Then, the latent representations of the two images, one
received from the encoder, the other extracted locally, along with the locally
generated common information, are fed to the respective decoders of the two
images. We employ a cross-attention module (CAM) to align the feature maps
obtained in the intermediate layers of the respective decoders of the two
images, thus allowing better utilization of the side information. We train and
demonstrate the effectiveness of the proposed algorithm on various realistic
setups, such as KITTI and Cityscape datasets of stereo image pairs. Our results
show that the proposed architecture is capable of exploiting the decoder-only
side information in a more efficient manner as it outperforms previous works.
We also show that the proposed method is able to provide significant gains even
in the case of uncalibrated and unsynchronized camera array use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Feature Alignment Network for Unsupervised Video Object
  Segmentation <span class="chip">ECCV-2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gensheng Pei, Yazhou Yao, Guo-Sen Xie, Fumin Shen, Zhenmin Tang, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow is an easily conceived and precious cue for advancing
unsupervised video object segmentation (UVOS). Most of the previous methods
directly extract and fuse the motion and appearance features for segmenting
target objects in the UVOS setting. However, optical flow is intrinsically an
instantaneous velocity of all pixels among consecutive frames, thus making the
motion features not aligned well with the primary objects among the
corresponding frames. To solve the above challenge, we propose a concise,
practical, and efficient architecture for appearance and motion feature
alignment, dubbed hierarchical feature alignment network (HFAN). Specifically,
the key merits in HFAN are the sequential Feature AlignMent (FAM) module and
the Feature AdaptaTion (FAT) module, which are leveraged for processing the
appearance and motion features hierarchically. FAM is capable of aligning both
appearance and motion features with the primary object semantic
representations, respectively. Further, FAT is explicitly designed for the
adaptive fusion of appearance and motion features to achieve a desirable
trade-off between cross-modal features. Extensive experiments demonstrate the
effectiveness of the proposed HFAN, which reaches a new state-of-the-art
performance on DAVIS-16, achieving 88.7 $\mathcal{J}\&\mathcal{F}$ Mean, i.e.,
a relative improvement of 3.5% over the best published result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmenting white matter hyperintensities on isotropic three-dimensional
  Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison
  of Deep learning tools on a Norwegian national imaging database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Soria Roevang, Per Selnes, Bradley John MacIntosh, Inge Rasmus Groote, Lene Paalhaugen, Carole Sudre, Tormod Fladby, Atle Bjoernerud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introduction Automated segmentation of white matter hyperintensities (WMHs)
is an essential step in neuroimaging analysis of Magnetic Resonance Imaging
(MRI). Fluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast
that is particularly useful to visualize and quantify WMHs, a hallmark of
cerebral small vessel disease and Alzheimer's disease (AD). Clinical MRI
protocols migrate to a three-dimensional (3D) FLAIR-weighted acquisition to
enable high spatial resolution in all three voxel dimensions. The current study
details the deployment of deep learning tools to enable automated WMH
segmentation and characterization from 3D FLAIR-weighted images acquired as
part of a national AD imaging initiative.
  Materials and methods Among 642 participants (283 male, mean age: (65.18 +/-
9.33) years) from the DDI study, two in-house networks were trained and
validated across five national collection sites. Three models were tested on a
held-out subset of the internal data from the 642 participants and an external
dataset with 29 cases from an international collaborator. These test sets were
evaluated independently. Five established WMH performance metrics were used for
comparison against ground truth human-in-the-loop segmentation.
  Results Of the three networks tested, the 3D nnU-Net had the best performance
with an average dice similarity coefficient score of 0.78 +/- 0.10, performing
better than both the in-house developed 2.5D model and the SOTA Deep Bayesian
network.
  Conclusion With the increasing use of 3D FLAIR-weighted images in MRI
protocols, our results suggest that WMH segmentation models can be trained on
3D data and yield WMH segmentation performance that is comparable to or better
than state-of-the-art without the need for including T1-weighted image series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, 7 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-dimension Geospatial feature learning for urban region function
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjia Xu, Jiuniu Wang, Yirong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban region function recognition plays a vital character in monitoring and
managing the limited urban areas. Since urban functions are complex and full of
social-economic properties, simply using remote sensing~(RS) images equipped
with physical and optical information cannot completely solve the
classification task. On the other hand, with the development of mobile
communication and the internet, the acquisition of geospatial big data~(GBD)
becomes possible. In this paper, we propose a Multi-dimension Feature Learning
Model~(MDFL) using high-dimensional GBD data in conjunction with RS images for
urban region function recognition. When extracting multi-dimension features,
our model considers the user-related information modeled by their activity, as
well as the region-based information abstracted from the region graph.
Furthermore, we propose a decision fusion network that integrates the decisions
from several neural networks and machine learning classifiers, and the final
decision is made considering both the visual cue from the RS images and the
social information from the GBD data. Through quantitative evaluation, we
demonstrate that our model achieves overall accuracy at 92.75, outperforming
the state-of-the-art by 10 percent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-world Semantic Segmentation via Contrasting and Clustering
  <span class="highlight-title">Vision-Language</span> Embedding <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quande Liu, Youpeng Wen, Jianhua Han, Chunjing Xu, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To bridge the gap between supervised semantic segmentation and real-world
applications that acquires one model to recognize arbitrary new concepts,
recent zero-shot segmentation attracts a lot of attention by exploring the
relationships between unseen and seen object categories, yet requiring large
amounts of densely-annotated data with diverse base classes. In this paper, we
propose a new open-world semantic segmentation pipeline that makes the first
attempt to learn to segment semantic objects of various open-world categories
without any efforts on dense annotations, by purely exploiting the
image-caption data that naturally exist on the Internet. Our method,
Vision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a
text encoder to generate visual and text embeddings for the image-caption data,
with two core components that endow its segmentation ability: First, the image
encoder is jointly trained with a vision-based contrasting and a cross-modal
contrasting, which encourage the visual embeddings to preserve both
fine-grained semantics and high-level category information that are crucial for
the segmentation task. Furthermore, an online clustering head is devised over
the image encoder, which allows to dynamically segment the visual embeddings
into distinct semantic groups such that they can be classified by comparing
with various text embeddings to complete our segmentation pipeline. Experiments
show that without using any data with dense annotations, our method can
directly segment objects of arbitrary categories, outperforming zero-shot
segmentation methods that require data labeling on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic universal taxonomies for multi-domain semantic segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petra Bevandić, Siniša Šegvić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training semantic segmentation models on multiple datasets has sparked a lot
of recent interest in the computer vision community. This interest has been
motivated by expensive annotations and a desire to achieve proficiency across
multiple visual domains. However, established datasets have mutually
incompatible labels which disrupt principled inference in the wild. We address
this issue by automatic construction of universal taxonomies through iterative
dataset integration. Our method detects subset-superset relationships between
dataset-specific labels, and supports learning of sub-class logits by treating
super-classes as partial labels. We present experiments on collections of
standard datasets and demonstrate competitive generalization performance with
respect to previous work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting PatchMatch Multi-View Stereo for Urban 3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a complete pipeline for image-based 3D reconstruction of urban
scenarios is proposed, based on PatchMatch Multi-View Stereo (MVS). Input
images are firstly fed into an off-the-shelf visual SLAM system to extract
camera poses and sparse keypoints, which are used to initialize PatchMatch
optimization. Then, pixelwise depths and normals are iteratively computed in a
multi-scale framework with a novel depth-normal consistency loss term and a
global refinement algorithm to balance the inherently local nature of
PatchMatch. Finally, a large-scale point cloud is generated by back-projecting
multi-view consistent estimates in 3D. The proposed approach is carefully
evaluated against both classical MVS algorithms and monocular depth networks on
the KITTI dataset, showing state of the art performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Poster presentation at IEEE Intelligent Vehicles Symposium (IV 2022,
  https://iv2022.com/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient View Clustering and Selection for City-Scale 3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image datasets have been steadily growing in size, harming the feasibility
and efficiency of large-scale 3D reconstruction methods. In this paper, a novel
approach for scaling Multi-View Stereo (MVS) algorithms up to arbitrarily large
collections of images is proposed. Specifically, the problem of reconstructing
the 3D model of an entire city is targeted, starting from a set of videos
acquired by a moving vehicle equipped with several high-resolution cameras.
Initially, the presented method exploits an approximately uniform distribution
of poses and geometry and builds a set of overlapping clusters. Then, an
Integer Linear Programming (ILP) problem is formulated for each cluster to
select an optimal subset of views that guarantees both visibility and
matchability. Finally, local point clouds for each cluster are separately
computed and merged. Since clustering is independent from pairwise visibility
information, the proposed algorithm runs faster than existing literature and
allows for a massive parallelization. Extensive testing on urban data are
discussed to show the effectiveness and the scalability of this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral presentation at ICIAP 2021 (https://www.iciap2021.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Assignment for Geometry Aware Local Feature Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dihe Huang, Ying Chen, Shang Xu, Yong Liu, Wenlong Wu, Yikang Ding, Chengjie Wang, Fan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local image feature matching, aiming to identify and correspond similar
regions from image pairs, is an essential concept in computer vision. Most
existing image matching approaches follow a one-to-one assignment principle and
employ mutual nearest neighbor to guarantee unique correspondence between local
features across images. However, images from different conditions may hold
large-scale variations or viewpoint diversification so that one-to-one
assignment may cause ambiguous or missing representations in dense matching. In
this paper, we introduce AdaMatcher, a novel detector-free local feature
matching method, which first correlates dense features by a lightweight feature
interaction module and estimates co-visible area of the paired images, then
performs a patch-level many-to-one assignment to predict match proposals, and
finally refines them based on a one-to-one refinement module. Extensive
experiments show that AdaMatcher outperforms solid baselines and achieves
state-of-the-art results on many downstream tasks. Additionally, the
many-to-one assignment and one-to-one refinement module can be used as a
refinement network for other matching methods, such as SuperGlue, to boost
their performance further. Code will be available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time End-to-End Video Text Spotter with <span class="highlight-title">Contrastive</span> Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wejia Wu, Zhuang Li, Jiahong Li, Chunhua Shen, Hong Zhou, Size Li, Zhongyuan Wang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video text spotting(VTS) is the task that requires simultaneously detecting,
tracking and recognizing text in the video. Existing video text spotting
methods typically develop sophisticated pipelines and multiple models, which is
not friend for real-time applications. Here we propose a real-time end-to-end
video text spotter with Contrastive Representation learning (CoText). Our
contributions are three-fold: 1) CoText simultaneously address the three tasks
(e.g., text detection, tracking, recognition) in a real-time end-to-end
trainable framework. 2) With contrastive learning, CoText models long-range
dependencies and learning temporal information across multiple frames. 3) A
simple, lightweight architecture is designed for effective and accurate
performance, including GPU-parallel detection post-processing, CTC-based
recognition head with Masked RoI. Extensive experiments show the superiority of
our method. Especially, CoText achieves an video text spotting IDF1 of 72.0% at
41.0 FPS on ICDAR2015video, with 10.5% and 32.0 FPS improvement the previous
best method. The code can be found at github.com/weijiawu/CoText.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-head Cascaded Swin <span class="highlight-title">Transformer</span>s with Attention to k-space Sampling
  Pattern for Accelerated MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mevan Ekanayake, Kamlesh Pawar, Mehrtash Harandi, Gary Egan, Zhaolin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global correlations are widely seen in human anatomical structures due to
similarity across tissues and bones. These correlations are reflected in
magnetic resonance imaging (MRI) scans as a result of close-range proton
density and T1/T2 parameter. Furthermore, to achieve accelerated MRI, k-space
data are undersampled which causes global aliasing artifacts. Convolutional
neural network (CNN) models are widely utilized for accelerated MRI
reconstruction, but those models are limited in capturing global correlations
due to the intrinsic locality of the convolution operation. The
self-attention-based transformer models are capable of capturing global
correlations among image features, however, the current contributions of
transformer models for MRI reconstruction are minute. The existing
contributions mostly provide CNN-transformer hybrid solutions and rarely
leverage the physics of MRI. In this paper, we propose a physics-based
stand-alone (convolution free) transformer model titled, the Multi-head
Cascaded Swin Transformers (McSTRA) for accelerated MRI reconstruction. McSTRA
combines several interconnected MRI physics-related concepts with the
transformer networks: it exploits global MR features via the shifted window
self-attention mechanism; it extracts MR features belonging to different
spectral components separately using a multi-head setup; it iterates between
intermediate de-aliasing and k-space correction via a cascaded network with
data consistency in k-space and intermediate loss computations; furthermore, we
propose a novel positional embedding generation mechanism to guide
self-attention utilizing the point spread function corresponding to the
undersampling mask. Our model significantly outperforms state-of-the-art MRI
reconstruction methods both visually and quantitatively while depicting
improved resolution and removal of aliasing artifacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TokenMix: Rethinking Image Mixing for Data Augmentation in Vision
  <span class="highlight-title">Transformer</span>s <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Boxiao Liu, Hang Zhou, Hongsheng Li, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CutMix is a popular augmentation technique commonly used for training modern
convolutional and transformer vision networks. It was originally designed to
encourage Convolution Neural Networks (CNNs) to focus more on an image's global
context instead of local information, which greatly improves the performance of
CNNs. However, we found it to have limited benefits for transformer-based
architectures that naturally have a global receptive field. In this paper, we
propose a novel data augmentation technique TokenMix to improve the performance
of vision transformers. TokenMix mixes two images at token level via
partitioning the mixing region into multiple separated parts. Besides, we show
that the mixed learning target in CutMix, a linear combination of a pair of the
ground truth labels, might be inaccurate and sometimes counter-intuitive. To
obtain a more suitable target, we propose to assign the target score according
to the content-based neural activation maps of the two images from a
pre-trained teacher model, which does not need to have high performance. With
plenty of experiments on various vision transformer architectures, we show that
our proposed TokenMix helps vision transformers focus on the foreground area to
infer the classes and enhances their robustness to occlusion, with consistent
performance gains. Notably, we improve DeiT-T/S/B with +1% ImageNet top-1
accuracy. Besides, TokenMix enjoys longer training, which achieves 81.2% top-1
accuracy on ImageNet with DeiT-S trained for 400 epochs. Code is available at
https://github.com/Sense-X/TokenMix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code: https://github.com/Sense-X/TokenMix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPIB: An MPI-Based Bokeh Rendering Framework for Realistic Partial
  Occlusion Effects <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juewen Peng, Jianming Zhang, Xianrui Luo, Hao Lu, Ke Xian, Zhiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial occlusion effects are a phenomenon that blurry objects near a camera
are semi-transparent, resulting in partial appearance of occluded background.
However, it is challenging for existing bokeh rendering methods to simulate
realistic partial occlusion effects due to the missing information of the
occluded area in an all-in-focus image. Inspired by the learnable 3D scene
representation, Multiplane Image (MPI), we attempt to address the partial
occlusion by introducing a novel MPI-based high-resolution bokeh rendering
framework, termed MPIB. To this end, we first present an analysis on how to
apply the MPI representation to bokeh rendering. Based on this analysis, we
propose an MPI representation module combined with a background inpainting
module to implement high-resolution scene representation. This representation
can then be reused to render various bokeh effects according to the controlling
parameters. To train and test our model, we also design a ray-tracing-based
bokeh generator for data generation. Extensive experiments on synthesized and
real-world images validate the effectiveness and flexibility of this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022; Project: https://juewenpeng.github.io/MPIB/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEAM: Greedy Learning for Large-Scale Accelerated MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batu Ozturkler, Arda Sahiner, Tolga Ergen, Arjun D Desai, Christopher M Sandino, Shreyas Vasanawala, John M Pauly, Morteza Mardani, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unrolled neural networks have recently achieved state-of-the-art accelerated
MRI reconstruction. These networks unroll iterative optimization algorithms by
alternating between physics-based consistency and neural-network based
regularization. However, they require several iterations of a large neural
network to handle high-dimensional imaging tasks such as 3D MRI. This limits
traditional training algorithms based on backpropagation due to prohibitively
large memory and compute requirements for calculating gradients and storing
intermediate activations. To address this challenge, we propose Greedy LEarning
for Accelerated MRI (GLEAM) reconstruction, an efficient training strategy for
high-dimensional imaging settings. GLEAM splits the end-to-end network into
decoupled network modules. Each module is optimized in a greedy manner with
decoupled gradient updates, reducing the memory footprint during training. We
show that the decoupled gradient updates can be performed in parallel on
multiple graphical processing units (GPUs) to further reduce training time. We
present experiments with 2D and 3D datasets including multi-coil knee, brain,
and dynamic cardiac cine MRI. We observe that: i) GLEAM generalizes as well as
state-of-the-art memory-efficient baselines such as gradient checkpointing and
invertible networks with the same memory footprint, but with 1.3x faster
training; ii) for the same memory footprint, GLEAM yields 1.1dB PSNR gain in 2D
and 1.8 dB in 3D over end-to-end baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semantic-aware Attention and Visual Shielding Network for
  Cloth-changing Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan Gao, Hongwei Wei, Weili Guan, Jie Nie, Meng Wang, Shenyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing person reidentification (ReID) is a newly emerging research
topic that aims to retrieve pedestrians whose clothes are changed. Since the
human appearance with different clothes exhibits large variations, it is very
difficult for existing approaches to extract discriminative and robust feature
representations. Current works mainly focus on body shape or contour sketches,
but the human semantic information and the potential consistency of pedestrian
features before and after changing clothes are not fully explored or are
ignored. To solve these issues, in this work, a novel semantic-aware attention
and visual shielding network for cloth-changing person ReID (abbreviated as
SAVS) is proposed where the key idea is to shield clues related to the
appearance of clothes and only focus on visual semantic information that is not
sensitive to view/posture changes. Specifically, a visual semantic encoder is
first employed to locate the human body and clothing regions based on human
semantic segmentation information. Then, a human semantic attention module
(HSA) is proposed to highlight the human semantic information and reweight the
visual feature map. In addition, a visual clothes shielding module (VCS) is
also designed to extract a more robust feature representation for the
cloth-changing task by covering the clothing regions and focusing the model on
the visual semantic information unrelated to the clothes. Most importantly,
these two modules are jointly explored in an end-to-end unified framework.
Extensive experiments demonstrate that the proposed method can significantly
outperform state-of-the-art methods, and more robust features can be extracted
for cloth-changing persons. Compared with FSAM (published in CVPR 2021), this
method can achieve improvements of 32.7% (16.5%) and 14.9% (-) on the LTCC and
PRCC datasets in terms of mAP (rank-1), respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2108.04527</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entity-enhanced Adaptive Reconstruction Network for Weakly Supervised
  Referring Expression Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Zechao Li, Qi Tian, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised Referring Expression Grounding (REG) aims to ground a
particular target in an image described by a language expression while lacking
the correspondence between target and expression. Two main problems exist in
weakly supervised REG. First, the lack of region-level annotations introduces
ambiguities between proposals and queries. Second, most previous weakly
supervised REG methods ignore the discriminative location and context of the
referent, causing difficulties in distinguishing the target from other
same-category objects. To address the above challenges, we design an
entity-enhanced adaptive reconstruction network (EARN). Specifically, EARN
includes three modules: entity enhancement, adaptive grounding, and
collaborative reconstruction. In entity enhancement, we calculate semantic
similarity as supervision to select the candidate proposals. Adaptive grounding
calculates the ranking score of candidate proposals upon subject, location and
context with hierarchical attention. Collaborative reconstruction measures the
ranking result from three perspectives: adaptive reconstruction, language
reconstruction and attribute classification. The adaptive mechanism helps to
alleviate the variance of different referring expressions. Experiments on five
datasets show EARN outperforms existing state-of-the-art methods. Qualitative
results demonstrate that the proposed EARN can better handle the situation
where multiple objects of a particular category are situated together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, accepted by TPAMI. arXiv admin note: text
  overlap with arXiv:1908.10568</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Representations of Physiological Signals for Fake Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalin Stefanov, Bhawna Paliwal, Abhinav Dhall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic fake videos are a potential tool for spreading harmful
misinformation given our increasing online presence and information intake.
This paper presents a multimodal learning-based method for detection of real
and fake videos. The method combines information from three modalities - audio,
video, and physiology. We investigate two strategies for combining the video
and physiology modalities, either by augmenting the video with information from
the physiology or by novelly learning the fusion of those two modalities with a
proposed Graph Convolutional Network architecture. Both strategies for
combining the two modalities rely on a novel method for generation of visual
representations of physiological signals. The detection of real and fake videos
is then based on the dissimilarity between the audio and modified video
modalities. The proposed method is evaluated on two benchmark datasets and the
results show significant increase in detection performance compared to previous
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin
  Memory Model <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Kei Cheng, Alexander G. Schwing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present XMem, a video object segmentation architecture for long videos
with unified feature memory stores inspired by the Atkinson-Shiffrin memory
model. Prior work on video object segmentation typically only uses one type of
feature memory. For videos longer than a minute, a single feature memory model
tightly links memory consumption and accuracy. In contrast, following the
Atkinson-Shiffrin model, we develop an architecture that incorporates multiple
independent yet deeply-connected feature memory stores: a rapidly updated
sensory memory, a high-resolution working memory, and a compact thus sustained
long-term memory. Crucially, we develop a memory potentiation algorithm that
routinely consolidates actively used working memory elements into the long-term
memory, which avoids memory explosion and minimizes performance decay for
long-term prediction. Combined with a new memory reading mechanism, XMem
greatly exceeds state-of-the-art performance on long-video datasets while being
on par with state-of-the-art methods (that do not work on long videos) on
short-video datasets. Code is available at https://hkchengrex.github.io/XMem
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Project page:
  https://hkchengrex.github.io/XMem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Bypasses Are Better Vision <span class="highlight-title">Transformer</span> Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibo Jie, Zhi-Hong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretrain-then-finetune paradigm has been widely adopted in computer
vision. But as the size of Vision Transformer (ViT) grows exponentially, the
full finetuning becomes prohibitive in view of the heavier storage overhead.
Motivated by parameter-efficient transfer learning (PETL) on language
transformers, recent studies attempt to insert lightweight adaptation modules
(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune
these modules while the pretrained weights are frozen. However, these modules
were originally proposed to finetune language models. Although ported well to
ViT, their design lacks prior knowledge for visual tasks. In this paper, we
propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation
modules, introducing only a small amount (less than 0.5% of model parameters)
of trainable parameters to adapt the large ViT. Different from other PETL
methods, Convpass benefits from the hard-coded inductive bias of convolutional
layers and thus is more suitable for visual tasks, especially in the low-data
regime. Experimental results on VTAB-1k benchmark and few-shot learning
datasets demonstrate that Convpass outperforms current language-oriented
adaptation modules, demonstrating the necessity to tailor vision-oriented
adaptation modules for vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Det: Towards Open-vocabulary Detection using Uncurated Images <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to establish a scalable pipeline for expanding an
object detector towards novel/unseen categories, using zero manual annotations.
To achieve that, we make the following four contributions: (i) in pursuit of
generalisation, we propose a two-stage open-vocabulary object detector, where
the class-agnostic object proposals are classified with a text encoder from
pre-trained visual-language model; (ii) To pair the visual latent space (of RPN
box proposals) with that of the pre-trained text encoder, we propose the idea
of regional prompt learning to align the textual embedding space with regional
visual object features; (iii) To scale up the learning procedure towards
detecting a wider spectrum of objects, we exploit the available online resource
via a novel self-training framework, which allows to train the proposed
detector on a large corpus of noisy uncurated web images. Lastly, (iv) to
evaluate our proposed detector, termed as PromptDet, we conduct extensive
experiments on the challenging LVIS and MS-COCO dataset. PromptDet shows
superior performance over existing approaches with fewer additional training
images and zero manual annotations whatsoever. Project page with code:
https://fcjian.github.io/promptdet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind Image Decomposition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.11364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.11364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Han, Weihao Li, Pengfei Fang, Chunyi Sun, Jie Hong, Mohammad Ali Armin, Lars Petersson, Hongdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and study a novel task named Blind Image Decomposition (BID),
which requires separating a superimposed image into constituent underlying
images in a blind setting, that is, both the source components involved in
mixing as well as the mixing mechanism are unknown. For example, rain may
consist of multiple components, such as rain streaks, raindrops, snow, and
haze. Rainy images can be treated as an arbitrary combination of these
components, some of them or all of them. How to decompose superimposed images,
like rainy images, into distinct source components is a crucial step toward
real-world vision systems. To facilitate research on this new task, we
construct multiple benchmark datasets, including mixed image decomposition
across multiple domains, real-scenario deraining, and joint
shadow/reflection/watermark removal. Moreover, we propose a simple yet general
Blind Image Decomposition Network (BIDeN) to serve as a strong baseline for
future work. Experimental results demonstrate the tenability of our benchmarks
and the effectiveness of BIDeN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project page:
  https://junlinhan.github.io/projects/BID.html. Code:
  https://github.com/JunlinHan/BID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PyMAF-X, a regression-based approach to recovering a full-body
parametric model from a single image. This task is very challenging since minor
parametric deviation may lead to noticeable misalignment between the estimated
mesh and the input image. Moreover, when integrating part-specific estimations
to the full-body model, existing solutions tend to either degrade the alignment
or produce unnatural wrist poses. To address these issues, we propose a
Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for
well-aligned human mesh recovery and extend it as PyMAF-X for the recovery of
expressive full-body models. The core idea of PyMAF is to leverage a feature
pyramid and rectify the predicted parameters explicitly based on the mesh-image
alignment status. Specifically, given the currently predicted parameters,
mesh-aligned evidence will be extracted from finer-resolution features
accordingly and fed back for parameter rectification. To enhance the alignment
perception, an auxiliary dense supervision is employed to provide mesh-image
correspondence guidance while spatial alignment attention is introduced to
enable the awareness of the global contexts for our network. When extending
PyMAF for full-body mesh recovery, an adaptive integration strategy is proposed
in PyMAF-X to produce natural wrist poses while maintaining the well-aligned
performance of the part-specific estimations. The efficacy of our approach is
validated on several benchmark datasets for body-only and full-body mesh
recovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment
and achieve new state-of-the-art results. The project page with code and video
results can be found at https://www.liuyebin.com/pymaf-x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An eXpressive extension of PyMAF [arXiv:2103.16507], Supporting
  SMPL-X, Project page: https://www.liuyebin.com/pymaf-x</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class-agnostic Object Detection with <span class="highlight-title">Multi-modal</span> <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and novel objects. In this paper, we advocate
that existing methods lack a top-down supervision signal governed by
human-understandable semantics. For the first time in literature, we
demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned
image-text pairs can effectively bridge this gap. Our extensive experiments
across various domains and novel objects show the state-of-the-art performance
of MViTs to localize generic objects in images. Based on the observation that
existing MViTs do not include multi-scale feature processing and usually
require longer training schedules, we develop an efficient MViT architecture
using multi-scale deformable attention and late vision-language fusion. We show
the significance of MViT proposals in a diverse range of applications including
open-world object detection, salient and camouflage object detection,
supervised and self-supervised detection tasks. Further, MViTs can adaptively
generate proposals given a specific language query and thus offer enhanced
interactability. Code: \url{https://git.io/J1HPY}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Vision <span class="highlight-title">Transformer</span>s Robust to Patch Perturbations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jindong Gu, Volker Tresp, Yao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Vision Transformer (ViT) have demonstrated its impressive
performance in image classification, which makes it a promising alternative to
Convolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image
as a sequence of image patches. The patch-based input image representation
makes the following question interesting: How does ViT perform when individual
input image patches are perturbed with natural corruptions or adversarial
perturbations, compared to CNNs? In this work, we study the robustness of ViT
to patch-wise perturbations. Surprisingly, we find that ViTs are more robust to
naturally corrupted patches than CNNs, whereas they are more vulnerable to
adversarial patches. Furthermore, we discover that the attention mechanism
greatly affects the robustness of vision transformers. Specifically, the
attention module can help improve the robustness of ViT by effectively ignoring
natural corrupted patches. However, when ViTs are attacked by an adversary, the
attention mechanism can be easily fooled to focus more on the adversarially
perturbed patches and cause a mistake. Based on our analysis, we propose a
simple temperature-scaling based method to improve the robustness of ViT
against adversarial patches. Extensive qualitative and quantitative experiments
are performed to support our findings, understanding, and improvement of ViT
robustness to patch-wise perturbations across a set of transformer-based
architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedding <span class="highlight-title">contrastive</span> unsupervised features to cluster in- and
  out-of-distribution noise in corrupted image <span class="highlight-title">dataset</span>s <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Albert, Eric Arazo, Noel E. O'Connor, Kevin McGuinness
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using search engines for web image retrieval is a tempting alternative to
manual curation when creating an image dataset, but their main drawback remains
the proportion of incorrect (noisy) samples retrieved. These noisy samples have
been evidenced by previous works to be a mixture of in-distribution (ID)
samples, assigned to the incorrect category but presenting similar visual
semantics to other classes in the dataset, and out-of-distribution (OOD)
images, which share no semantic correlation with any category from the dataset.
The latter are, in practice, the dominant type of noisy images retrieved. To
tackle this noise duality, we propose a two stage algorithm starting with a
detection step where we use unsupervised contrastive feature learning to
represent images in a feature space. We find that the alignment and uniformity
principles of contrastive learning allow OOD samples to be linearly separated
from ID samples on the unit hypersphere. We then spectrally embed the
unsupervised representations using a fixed neighborhood size and apply an
outlier sensitive clustering at the class level to detect the clean and OOD
clusters as well as ID noisy outliers. We finally train a noise robust neural
network that corrects ID noise to the correct category and utilizes OOD samples
in a guided contrastive objective, clustering them to improve low-level
features. Our algorithm improves the state-of-the-art results on synthetic
noise image datasets as well as real-world web-crawled data. Our work is fully
reproducible github.com/PaulAlbert31/SNCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose L. Gómez, Gabriel Villalonga, Antonio M. López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image segmentation is addressed by training deep models. Since
supervised training draws to a curse of human-based image labeling, using
synthetic images with automatically generated ground truth together with
unlabeled real-world images is a promising alternative. This implies to address
an unsupervised domain adaptation (UDA) problem. In this paper, we proposed a
new co-training process for synth-to-real UDA of semantic segmentation models.
First, we design a self-training procedure which provides two initial models.
Then, we keep training these models in a collaborative manner for obtaining the
final model. The overall process treats the deep models as black boxes and
drives their collaboration at the level of pseudo-labeled target images, i.e.,
neither modifying loss functions is required, nor explicit feature alignment.
We test our proposal on standard synthetic and real-world datasets. Our
co-training shows improvements of 15-20 percentage points of mIoU over
baselines, so establishing new state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compound Prototype Matching for Few-shot Action Recognition <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijin Yang, Yifei Huang, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot action recognition aims to recognize novel action classes using only
a small number of labeled training samples. In this work, we propose a novel
approach that first summarizes each video into compound prototypes consisting
of a group of global prototypes and a group of focused prototypes, and then
compares video similarity based on the prototypes. Each global prototype is
encouraged to summarize a specific aspect from the entire video, for example,
the start/evolution of the action. Since no clear annotation is provided for
the global prototypes, we use a group of focused prototypes to focus on certain
timestamps in the video. We compare video similarity by matching the compound
prototypes between the support and query videos. The global prototypes are
directly matched to compare videos from the same perspective, for example, to
compare whether two actions start similarly. For the focused prototypes, since
actions have various temporal variations in the videos, we apply bipartite
matching to allow the comparison of actions with different temporal positions
and shifts. Experiments demonstrate that our proposed method achieves
state-of-the-art results on multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Grand Unification of Object Tracking <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Instances as 1D Kernels <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao, Weicai Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a 3D instance representation, termed instance kernels, where
instances are represented by one-dimensional vectors that encode the semantic,
positional, and shape information of 3D instances. We show that instance
kernels enable easy mask inference by simply scanning kernels over the entire
scenes, avoiding the heavy reliance on proposals or heuristic clustering
algorithms in standard 3D instance segmentation pipelines. The idea of instance
kernel is inspired by recent success of dynamic convolutions in 2D/3D instance
segmentation. However, we find it non-trivial to represent 3D instances due to
the disordered and unstructured nature of point cloud data, e.g., poor instance
localization can significantly degrade instance representation. To remedy this,
we construct a novel 3D instance encoding paradigm. First, potential instance
centroids are localized as candidates. Then, a candidate merging scheme is
devised to simultaneously aggregate duplicated candidates and collect context
around the merged centroids to form the instance kernels. Once instance kernels
are available, instance masks can be reconstructed via dynamic convolutions
whose weights are conditioned on instance kernels. The whole pipeline is
instantiated with a dynamic kernel network (DKNet). Results show that DKNet
outperforms the state of the arts on both ScanNetV2 and S3DIS datasets with
better instance localization. Code is available:
https://github.com/W1zheng/DKNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in ECCV, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinlin Hu, Pascal Fua, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent 6D object pose estimation methods, including unsupervised ones,
require many real training images. Unfortunately, for some applications, such
as those in space or deep under water, acquiring real images, even unannotated,
is virtually impossible. In this paper, we propose a method that can be trained
solely on synthetic images, or optionally using a few additional real ones.
Given a rough pose estimate obtained from a first network, it uses a second
network to predict a dense 2D correspondence field between the image rendered
using the rough pose and the real image and infers the required pose
correction. This approach is much less sensitive to the domain shift between
synthetic and real images than state-of-the-art methods. It performs on par
with methods that require annotated real images for training when not using
any, and outperforms them considerably when using as few as twenty real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge
  <span class="highlight-title">Distillation</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.05892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.05892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Oh, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As segmentation labels are scarce, extensive researches have been conducted
to train segmentation networks with domain adaptation, semi-supervised or
self-supervised learning techniques to utilize abundant unlabeled dataset.
However, these approaches appear different from each other, so it is not clear
how these approaches can be combined for better performance. Inspired by recent
multi-domain image translation approaches, here we propose a novel segmentation
framework using adaptive instance normalization (AdaIN), so that a single
generator is trained to perform both domain adaptation and semi-supervised
segmentation tasks via knowledge distillation by simply changing task-specific
AdaIN codes. Specifically, our framework is designed to deal with difficult
situations in chest X-ray radiograph (CXR) segmentation, where labels are only
available for normal data, but trained model should be applied to both normal
and abnormal data. The proposed network demonstrates great generalizability
under domain shift and achieves the state-of-the-art performance for abnormal
CXR segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multimodal</span> <span class="highlight-title">Transformer</span> with Variable-length Memory for
  <span class="highlight-title">Vision-and-Language</span> Navigation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.05759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.05759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Lin, Yi Jiang, Jianfei Cai, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) is a task that an agent is required to
follow a language instruction to navigate to the goal position, which relies on
the ongoing interactions with the environment during moving. Recent
Transformer-based VLN methods have made great progress benefiting from the
direct connections between visual observations and the language instruction via
the multimodal cross-attention mechanism. However, these methods usually
represent temporal context as a fixed-length vector by using an LSTM decoder or
using manually designed hidden states to build a recurrent Transformer.
Considering a single fixed-length vector is often insufficient to capture
long-term temporal context, in this paper, we introduce Multimodal Transformer
with Variable-length Memory (MTVM) for visually-grounded natural language
navigation by modelling the temporal context explicitly. Specifically, MTVM
enables the agent to keep track of the navigation trajectory by directly
storing previous activations in a memory bank. To further boost the
performance, we propose a memory-aware consistency loss to help learn a better
joint representation of temporal context with random masked instructions. We
evaluate MTVM on popular R2R and CVDN datasets, and our model improves Success
Rate on R2R unseen validation and test set by 2% each, and reduce Goal Process
by 1.6m on CVDN test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction-Guided <span class="highlight-title">Distillation</span> for Dense Object Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Mateusz Ochal, Amos Storkey, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world object detection models should be cheap and accurate. Knowledge
distillation (KD) can boost the accuracy of a small, cheap detection model by
leveraging useful information from a larger teacher model. However, a key
challenge is identifying the most informative features produced by the teacher
for distillation. In this work, we show that only a very small fraction of
features within a ground-truth bounding box are responsible for a teacher's
high detection performance. Based on this, we propose Prediction-Guided
Distillation (PGD), which focuses distillation on these key predictive regions
of the teacher and yields considerable gains in performance over many existing
KD baselines. In addition, we propose an adaptive weighting scheme over the key
regions to smooth out their influence and achieve even better performance. Our
proposed approach outperforms current state-of-the-art KD baselines on a
variety of advanced one-stage detection architectures. Specifically, on the
COCO dataset, our method achieves between +3.1% and +4.6% AP improvement using
ResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On
the CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,
also using these backbones. Our code is available at
https://github.com/ChenhongyiYang/PGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SearchMorph:Multi-scale Correlation Iterative Network for Deformable
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Fan, Shuxin Zhuang, Zhemin Zhuang, Ye Yuan, Shunmin Qiu, Alex Noel Joseph Raj, Yibiao Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration can obtain dynamic information about images,
which is of great significance in medical image analysis. The unsupervised deep
learning registration method can quickly achieve high registration accuracy
without labels. However, these methods generally suffer from uncorrelated
features, poor ability to register large deformations and details, and
unnatural deformation fields. To address the issues above, we propose an
unsupervised multi-scale correlation iterative registration network
(SearchMorph). In the proposed network, we introduce a correlation layer to
strengthen the relevance between features and construct a correlation pyramid
to provide multi-scale relevance information for the network. We also design a
deformation field iterator, which improves the ability of the model to register
details and large deformations through the search module and GRU while ensuring
that the deformation field is realistic. We use single-temporal brain MR images
and multi-temporal echocardiographic sequences to evaluate the model's ability
to register large deformations and details. The experimental results
demonstrate that the method in this paper achieves the highest registration
accuracy and the lowest folding point ratio using a short elapsed time to
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CANF-VC: Conditional Augmented Normalizing Flows for Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Han Ho, Chih-Peng Chang, Peng-Yu Chen, Alessandro Gnutti, Wen-Hsiao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end learning-based video compression system,
termed CANF-VC, based on conditional augmented normalizing flows (CANF). Most
learned video compression systems adopt the same hybrid-based coding
architecture as the traditional codecs. Recent research on conditional coding
has shown the sub-optimality of the hybrid-based coding and opens up
opportunities for deep generative models to take a key role in creating new
coding frameworks. CANF-VC represents a new attempt that leverages the
conditional ANF to learn a video generative model for conditional inter-frame
coding. We choose ANF because it is a special type of generative model, which
includes variational autoencoder as a special case and is able to achieve
better expressiveness. CANF-VC also extends the idea of conditional coding to
motion coding, forming a purely conditional coding framework. Extensive
experimental results on commonly used datasets confirm the superiority of
CANF-VC to the state-of-the-art methods. The source code of CANF-VC is
available at https://github.com/NYCU-MAPL/CANF-VC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Visual Tracking with Exemplar <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09686v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09686v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Blatter, Menelaos Kanakis, Martin Danelljan, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design of more complex and powerful neural network models has
significantly advanced the state-of-the-art in visual object tracking. These
advances can be attributed to deeper networks, or the introduction of new
building blocks, such as transformers. However, in the pursuit of increased
tracking performance, runtime is often hindered. Furthermore, efficient
tracking architectures have received surprisingly little attention. In this
paper, we introduce the Exemplar Transformer, a transformer module utilizing a
single instance level attention layer for realtime visual object tracking.
E.T.Track, our visual tracker that incorporates Exemplar Transformer modules,
runs at 47 FPS on a CPU. This is up to 8x faster than other transformer-based
models. When compared to lightweight trackers that can operate in realtime on
standard CPUs, E.T.Track consistently outperforms all other methods on the
LaSOT, OTB-100, NFS, TrackingNet, and VOT-ST2020 datasets. The code will be
made publicly available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScoreNet: Learning Non-Uniform Attention and Augmentation for
  <span class="highlight-title">Transformer</span>-Based Histopathological Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Stegmüller, Behzad Bozorgtabar, Antoine Spahr, Jean-Philippe Thiran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in digital pathology is hindered by high-resolution images and the
prohibitive cost of exhaustive localized annotations. The commonly used
paradigm to categorize pathology images is patch-based processing, which often
incorporates multiple instance learning (MIL) to aggregate local patch-level
representations yielding image-level prediction. Nonetheless, diagnostically
relevant regions may only take a small fraction of the whole tissue, and
current MIL-based approaches often process images uniformly, discarding the
inter-patches interactions. To alleviate these issues, we propose ScoreNet, a
new efficient transformer that exploits a differentiable recommendation stage
to extract discriminative image regions and dedicate computational resources
accordingly. The proposed transformer leverages the local and global attention
of a few dynamically recommended high-resolution regions at an efficient
computational cost. We further introduce a novel mixing data-augmentation,
namely ScoreMix, by leveraging the image's semantic distribution to guide the
data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly
simple and mitigates the pitfalls of previous augmentations, which assume a
uniform semantic distribution and risk mislabeling the samples. Thorough
experiments and ablation studies on three breast cancer histology datasets of
Haematoxylin & Eosin (H&E) have validated the superiority of our approach over
prior arts, including transformer-based models on tumour regions-of-interest
(TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation
demonstrates better generalization capabilities and achieves new
state-of-the-art (SOTA) results with only 50% of the data compared to other
mixing augmentation variants. Finally, ScoreNet yields high efficacy and
outperforms SOTA efficient transformers, namely TransPath and SwinTransformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View
  Cameras with Negative Plane <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Wang, Kailun Yang, Hao Shi, Peng Li, Fei Gao, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-inertial-odometry has attracted extensive attention in the field of
autonomous driving and robotics. The size of Field of View (FoV) plays an
important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a
large FoV enables to perceive a wide range of surrounding scene elements and
features. However, when the field of the camera reaches the negative half
plane, one cannot simply use [u,v,1]^T to represent the image feature points
anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for
cameras with extremely large FoV. We leverage a three-dimensional vector with
unit length to represent feature points, and design a series of algorithms to
overcome this challenge. To address the scarcity of panoramic visual odometry
datasets with ground-truth location and pose, we present the PALVIO dataset,
collected with a Panoramic Annular Lens (PAL) system with an entire FoV of
360{\deg}x(40{\deg}-120{\deg}) and an IMU sensor. With a comprehensive variety
of experiments, the proposed LF-VIO is verified on both the established PALVIO
benchmark and a public fisheye camera dataset with a FoV of
360{\deg}x(0{\deg}-93.5{\deg}). LF-VIO outperforms state-of-the-art
visual-inertial-odometry methods. Our dataset and code are made publicly
available at https://github.com/flysoaryun/LF-VIO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2022. Dataset and code are publicly available at
  https://github.com/flysoaryun/LF-VIO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Authentication Attacks on Projection-based Cancelable Biometric Schemes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15163v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15163v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Durbet, Pascal Lafourcade, Denis Migdal, Kevin Thiry-Atighehchi, Paul-Marie Grollemund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancelable biometric schemes aim at generating secure biometric templates by
combining user specific tokens, such as password, stored secret or salt, along
with biometric data. This type of transformation is constructed as a
composition of a biometric transformation with a feature extraction algorithm.
The security requirements of cancelable biometric schemes concern the
irreversibility, unlinkability and revocability of templates, without losing in
accuracy of comparison. While several schemes were recently attacked regarding
these requirements, full reversibility of such a composition in order to
produce colliding biometric characteristics, and specifically presentation
attacks, were never demonstrated to the best of our knowledge. In this paper,
we formalize these attacks for a traditional cancelable scheme with the help of
integer linear programming (ILP) and quadratically constrained quadratic
programming (QCQP). Solving these optimization problems allows an adversary to
slightly alter its fingerprint image in order to impersonate any individual.
Moreover, in an even more severe scenario, it is possible to simultaneously
impersonate several individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:1910.01389 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChildCI Framework: Analysis of Motor and Cognitive Development in
  Children-Computer Interaction for Age Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos Ruiz-Garcia, Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Jaime Herreros-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a comprehensive analysis of the different tests
proposed in the recent ChildCI framework, proving its potential for generating
a better understanding of children's neuromotor and cognitive development along
time, as well as their possible application in other research areas such as
e-Health and e-Learning. In particular, we propose a set of over 100 global
features related to motor and cognitive aspects of the children interaction
with mobile devices, some of them collected and adapted from the literature.
Furthermore, we analyse the robustness and discriminative power of the proposed
feature set including experimental results for the task of children age group
detection based on their motor and cognitive behaviors. Two different scenarios
are considered in this study: i) single-test scenario, and ii) multiple-test
scenario. Results over 93% accuracy are achieved using the publicly available
ChildCIdb_v1 database (over 400 children from 18 months to 8 years old),
proving the high correlation of children's age with the way they interact with
mobile devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Joint stereo 3D object detection and implicit surface reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichao Li, Kwang-<span class="highlight-author">Ting Chen</span>g
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first learning-based framework for category-level 3D object
detection and implicit shape estimation based on a pair of stereo RGB images in
the wild. Previous stereo 3D object detection approaches cannot describe the
complete shape details of the detected objects and often fails for the small
objects. In contrast, we propose a new progressive approach that can (1)
perform precise localization as well as provide a complete and
resolution-agnostic shape description for the detected objects and (2) produce
significantly more accurate orientation predictions for the tiny instances.
This approach features a new instance-level network that explicitly models the
unseen surface hallucination problem using point-based representations and uses
a new geometric representation for orientation refinement. Extensive
experiments show that our approach achieves state-of-the-art performance using
various metrics on the KITTI benchmark. Code and pre-trained models will be
available at this https URL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling Recognition from Detection: Single Shot Self-Reliant Scene
  Text Spotter <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Wu, Pengyuan Lyu, Guangming Lu, Chengquan Zhang, Kun Yao, Wenjie Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typical text spotters follow the two-stage spotting strategy: detect the
precise boundary for a text instance first and then perform text recognition
within the located text region. While such strategy has achieved substantial
progress, there are two underlying limitations. 1) The performance of text
recognition depends heavily on the precision of text detection, resulting in
the potential error propagation from detection to recognition. 2) The RoI
cropping which bridges the detection and recognition brings noise from
background and leads to information loss when pooling or interpolating from
feature maps. In this work we propose the single shot Self-Reliant Scene Text
Spotter (SRSTS), which circumvents these limitations by decoupling recognition
from detection. Specifically, we conduct text detection and recognition in
parallel and bridge them by the shared positive anchor point. Consequently, our
method is able to recognize the text instances correctly even though the
precise text boundaries are challenging to detect. Additionally, our method
reduces the annotation cost for text detection substantially. Extensive
experiments on regular-shaped benchmark and arbitrary-shaped benchmark
demonstrate that our SRSTS compares favorably to previous state-of-the-art
spotters in terms of both accuracy and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared in the Proceedings of the ACM International Conference
  on Multimedia (ACM MM), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Marine Bubble Flow Quantification Using Wide-Baseline Stereo
  Photogrammetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.07414v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.07414v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengkun She, Tim Weiß, Yifan Song, Peter Urban, Jens Greinert, Kevin Köser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable quantification of natural and anthropogenic gas release (e.g.\
CO$_2$, methane) from the seafloor into the water column, and potentially to
the atmosphere, is a challenging task. While ship-based echo sounders such as
single beam and multibeam systems allow detection of free gas, bubbles, in the
water even from a great distance, exact quantification utilizing the
hydroacoustic data requires additional parameters such as rise speed and bubble
size distribution. Optical methods are complementary in the sense that they can
provide high temporal and spatial resolution of single bubbles or bubble
streams from close distance. In this contribution we introduce a complete
instrument and evaluation method for optical bubble stream characterization
targeted at flows of up to 100ml/min and bubbles with a few millimeters radius.
The dedicated instrument employs a high-speed deep sea capable stereo camera
system that can record terabytes of bubble imagery when deployed at a seep site
for later automated analysis. Bubble characteristics can be obtained for short
sequences, then relocating the instrument to other locations, or in autonomous
mode of definable intervals up to several days, in order to capture bubble flow
variations due to e.g. tide dependent pressure changes or reservoir depletion.
Beside reporting the steps to make bubble characterization robust and
autonomous, we carefully evaluate the reachable accuracy to be in the range of
1-2\% of the bubble radius and propose a novel auto-calibration procedure that,
due to the lack of point correspondences, uses only the silhouettes of bubbles.
The system has been operated successfully in 1000m water depth at the Cascadia
margin offshore Oregon to assess methane fluxes from various seep locations.
Besides sample results we also report failure cases and lessons learnt during
deployment and method development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KTNet: Knowledge Transfer for Unpaired 3D Shape Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Cao, Wenxiao Zhang, Xin Wen, Zhen Dong, Yu-shen Liu, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unpaired 3D object completion aims to predict a complete 3D shape from an
incomplete input without knowing the correspondence between the complete and
incomplete shapes. In this paper, we propose the novel KTNet to solve this task
from the new perspective of knowledge transfer. KTNet elaborates a
teacher-assistant-student network to establish multiple knowledge transfer
processes. Specifically, the teacher network takes complete shape as input and
learns the knowledge of complete shape. The student network takes the
incomplete one as input and restores the corresponding complete shape. And the
assistant modules not only help to transfer the knowledge of complete shape
from the teacher to the student, but also judge the learning effect of the
student network. As a result, KTNet makes use of a more comprehensive
understanding to establish the geometric correspondence between complete and
incomplete shapes in a perspective of knowledge transfer, which enables more
detailed geometric inference for generating high-quality complete shapes. We
conduct comprehensive experiments on several datasets, and the results show
that our method outperforms previous methods of unpaired point cloud completion
by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Scene Understanding via Disentangled Instance Mesh Reconstruction <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, Gang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene reconstruction from point cloud is an essential and
challenging task for 3D scene understanding. This task requires not only to
recognize each instance in the scene, but also to recover their geometries
based on the partial observed point cloud. Existing methods usually attempt to
directly predict occupancy values of the complete object based on incomplete
point cloud proposals from a detection-based backbone. However, this framework
always fails to reconstruct high fidelity mesh due to the obstruction of
various detected false positive object proposals and the ambiguity of
incomplete point observations for learning occupancy values of complete
objects. To circumvent the hurdle, we propose a Disentangled Instance Mesh
Reconstruction (DIMR) framework for effective point scene understanding. A
segmentation-based backbone is applied to reduce false positive object
proposals, which further benefits our exploration on the relationship between
recognition and reconstruction. Based on the accurate proposals, we leverage a
mesh-aware latent code space to disentangle the processes of shape completion
and mesh generation, relieving the ambiguity caused by the incomplete point
observations. Furthermore, with access to the CAD model pool at test time, our
model can also be used to improve the reconstruction quality by performing mesh
retrieval without extra training. We thoroughly evaluate the reconstructed mesh
quality with multiple metrics, and demonstrate the superiority of our method on
the challenging ScanNet dataset. Code is available at
\url{https://github.com/ashawkey/dimr}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive</span> Vicinal Space for Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Na, Dongyoon Han, Hyung Jin Chang, Wonjun Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent unsupervised domain adaptation methods have utilized vicinal space
between the source and target domains. However, the equilibrium collapse of
labels, a problem where the source labels are dominant over the target labels
in the predictions of vicinal instances, has never been addressed. In this
paper, we propose an instance-wise minimax strategy that minimizes the entropy
of high uncertainty instances in the vicinal space to tackle the stated
problem. We divide the vicinal space into two subspaces through the solution of
the minimax problem: contrastive space and consensus space. In the contrastive
space, inter-domain discrepancy is mitigated by constraining instances to have
contrastive views and labels, and the consensus space reduces the confusion
between intra-domain categories. The effectiveness of our method is
demonstrated on public benchmarks, including Office-31, Office-Home, and
VisDA-C, achieving state-of-the-art performances. We further show that our
method outperforms the current state-of-the-art methods on PACS, which
indicates that our instance-wise approach works well for multi-source domain
adaptation as well. Code is available at https://github.com/NaJaeMin92/CoVi.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Invariant Visual Representations for Compositional Zero-Shot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00415v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00415v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Zhang, Kongming Liang, Ruoyi Du, Xian Sun, Zhanyu Ma, Jun Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions
using knowledge learned from seen attribute-object compositions in the training
set. Previous works mainly project an image and a composition into a common
embedding space to measure their compatibility score. However, both attributes
and objects share the visual representations learned above, leading the model
to exploit spurious correlations and bias towards seen pairs. Instead, we
reconsider CZSL as an out-of-distribution generalization problem. If an object
is treated as a domain, we can learn object-invariant features to recognize the
attributes attached to any object reliably. Similarly, attribute-invariant
features can also be learned when recognizing the objects with attributes as
domains. Specifically, we propose an invariant feature learning framework to
align different domains at the representation and gradient levels to capture
the intrinsic characteristics associated with the tasks. Experiments on two
CZSL benchmarks demonstrate that the proposed method significantly outperforms
the previous state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverted Pyramid Multi-task <span class="highlight-title">Transformer</span> for Dense Scene Understanding <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrong Ye, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task dense scene understanding is a thriving research domain that
requires simultaneous perception and reasoning on a series of correlated tasks
with pixel-wise prediction. Most existing works encounter a severe limitation
of modeling in the locality due to heavy utilization of convolution operations,
while learning interactions and inference in a global spatial-position and
multi-task context is critical for this problem. In this paper, we propose a
novel end-to-end Inverted Pyramid multi-task Transformer (InvPT) to perform
simultaneous modeling of spatial positions and multiple tasks in a unified
framework. To the best of our knowledge, this is the first work that explores
designing a transformer structure for multi-task dense prediction for scene
understanding. Besides, it is widely demonstrated that a higher spatial
resolution is remarkably beneficial for dense predictions, while it is very
challenging for existing transformers to go deeper with higher resolutions due
to huge complexity to large spatial size. InvPT presents an efficient
UP-Transformer block to learn multi-task feature interaction at gradually
increased resolutions, which also incorporates effective self-attention message
passing and multi-scale feature aggregation to produce task-specific prediction
at a high resolution. Our method achieves superior multi-task performance on
NYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms
previous state-of-the-arts. The code is available at
https://github.com/prismformore/InvPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV 2022 Conference. Code is available at
  https://github.com/prismformore/InvPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Decades of Colorization and Decolorization for Images and Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiguang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorization is a computer-aided process, which aims to give color to a gray
image or video. It can be used to enhance black-and-white images, including
black-and-white photos, old-fashioned films, and scientific imaging results. On
the contrary, decolorization is to convert a color image or video into a
grayscale one. A grayscale image or video refers to an image or video with only
brightness information without color information. It is the basis of some
downstream image processing applications such as pattern recognition, image
segmentation, and image enhancement. Different from image decolorization, video
decolorization should not only consider the image contrast preservation in each
video frame, but also respect the temporal and spatial consistency between
video frames. Researchers were devoted to develop decolorization methods by
balancing spatial-temporal consistency and algorithm efficiency. With the
prevalance of the digital cameras and mobile phones, image and video
colorization and decolorization have been paid more and more attention by
researchers. This paper gives an overview of the progress of image and video
colorization and decolorization methods in the last two decades.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic-Sparse Colorization Network for Deep Exemplar-based
  Colorization <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Bai, Chao Dong, Zenghao Chai, Andong Wang, Zhengzhuo Xu, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-based colorization approaches rely on reference image to provide
plausible colors for target gray-scale image. The key and difficulty of
exemplar-based colorization is to establish an accurate correspondence between
these two images. Previous approaches have attempted to construct such a
correspondence but are faced with two obstacles. First, using luminance
channels for the calculation of correspondence is inaccurate. Second, the dense
correspondence they built introduces wrong matching results and increases the
computation burden. To address these two problems, we propose Semantic-Sparse
Colorization Network (SSCN) to transfer both the global image style and
detailed semantic-related colors to the gray-scale image in a coarse-to-fine
manner. Our network can perfectly balance the global and local colors while
alleviating the ambiguous matching problem. Experiments show that our method
outperforms existing methods in both quantitative and qualitative evaluation
and achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022; 14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Graph <span class="highlight-title">Transformer</span> for Video Question Answering <span class="chip">ECCV'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbin Xiao, Pan Zhou, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a Video Graph Transformer (VGT) model for Video Quetion
Answering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic
graph transformer module which encodes video by explicitly capturing the visual
objects, their relations, and dynamics for complex spatio-temporal reasoning;
and 2) it exploits disentangled video and text Transformers for relevance
comparison between the video and text to perform QA, instead of entangled
cross-modal Transformer for answer classification. Vision-text communication is
done by additional cross-modal interaction modules. With more reasonable video
encoding and QA solution, we show that VGT can achieve much better performances
on VideoQA tasks that challenge dynamic relation reasoning than prior arts in
the pretraining-free scenario. Its performances even surpass those models that
are pretrained with millions of external data. We further show that VGT can
also benefit a lot from self-supervised cross-modal pretraining, yet with
orders of magnitude smaller data. These results clearly demonstrate the
effectiveness and superiority of VGT, and reveal its potential for more
data-efficient pretraining. With comprehensive analyses and some heuristic
observations, we hope that VGT can promote VQA research beyond coarse
recognition/description towards fine-grained relation reasoning in realistic
videos. Our code is available at https://github.com/sail-sg/VGT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">Overview</span> of Color Transfer and Style Transfer for Images and Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiguang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image or video appearance features (e.g., color, texture, tone, illumination,
and so on) reflect one's visual perception and direct impression of an image or
video. Given a source image (video) and a target image (video), the image
(video) color transfer technique aims to process the color of the source image
or video (note that the source image or video is also referred to the reference
image or video in some literature) to make it look like that of the target
image or video, i.e., transferring the appearance of the target image or video
to that of the source image or video, which can thereby change one's perception
of the source image or video. As an extension of color transfer, style transfer
refers to rendering the content of a target image or video in the style of an
artist with either a style sample or a set of images through a style transfer
model. As an emerging field, the study of style transfer has attracted the
attention of a large number of researchers. After decades of development, it
has become a highly interdisciplinary research with a variety of artistic
expression styles can be achieved. This paper provides an overview of color
transfer and style transfer methods over the past years.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Convolutional Neural Networks in the Frequency Domain <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06718v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06718v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyue Pan, Yixin Chen, Xin Niu, Wenbo Zhou, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Face Recognition with Learnable Privacy Budgets in
  Frequency Domain <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhen Ji, Huan Wang, Yuge Huang, Jiaxiang Wu, Xingkun Xu, Shouhong Ding, ShengChuan Zhang, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code is available at
  https://github.com/Tencent/TFace/tree/master/recognition/tasks/dctdp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Trajectory Prediction via Distribution Discrimination <span class="chip">ICCV 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.14204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.14204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Chen, Junlong Li, Nuoxing Zhou, Liangliang Ren, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction is confronted with the dilemma to capture the
multi-modal nature of future dynamics with both diversity and accuracy. In this
paper, we present a distribution discrimination (DisDis) method to predict
personalized motion patterns by distinguishing the potential distributions.
Motivated by that the motion pattern of each person is personalized due to
his/her habit, our DisDis learns the latent distribution to represent different
motion patterns and optimize it by the contrastive discrimination. This
distribution discrimination encourages latent distributions to be more
discriminative. Our method can be integrated with existing multi-modal
stochastic predictive models as a plug-and-play module to learn the more
discriminative latent distribution. To evaluate the latent distribution, we
further propose a new metric, probability cumulative minimum distance (PCMD)
curve, which cumulatively calculates the minimum distance on the sorted
probabilities. Experimental results on the ETH and UCY datasets show the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2021. Code: https://github.com/CHENGY12/DisDis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynaST: Dynamic Sparse <span class="highlight-title">Transformer</span> for Exemplar-Guided Image Generation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhua Liu, Jingwen Ye, Sucheng Ren, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One key challenge of exemplar-guided image generation lies in establishing
fine-grained correspondences between input and guided images. Prior approaches,
despite the promising results, have relied on either estimating dense attention
to compute per-point matching, which is limited to only coarse scales due to
the quadratic memory cost, or fixing the number of correspondences to achieve
linear complexity, which lacks flexibility. In this paper, we propose a dynamic
sparse attention based Transformer model, termed Dynamic Sparse Transformer
(DynaST), to achieve fine-level matching with favorable efficiency. The heart
of our approach is a novel dynamic-attention unit, dedicated to covering the
variation on the optimal number of tokens one position should focus on.
Specifically, DynaST leverages the multi-layer nature of Transformer structure,
and performs the dynamic attention scheme in a cascaded manner to refine
matching results and synthesize visually-pleasing outputs. In addition, we
introduce a unified training objective for DynaST, making it a versatile
reference-based image translation framework for both supervised and
unsupervised scenarios. Extensive experiments on three applications,
pose-guided person image generation, edge-based face synthesis, and undistorted
image style transfer, demonstrate that DynaST achieves superior performance in
local details, outperforming the state of the art while reducing the
computational cost significantly. Our code is available at
https://github.com/Huage001/DynaST
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsung-Han Wu, Yi-Syuan Liou, Shao-Ji Yuan, Hsin-Ying Lee, Tung-I Chen, Kuan-Chih Huang, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present D2ADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate labeling efficiency, we design a
dynamic scheduling policy to adjust the labeling budgets between domain
exploration and model uncertainty over time. Extensive experiments show that
our method outperforms existing active learning and domain adaptation baselines
on two benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than
5% target domain annotations, our method reaches comparable results with that
of full supervision. Our code is publicly available at
https://github.com/tsunghan-wu/D2ADA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. The code is available at
  https://github.com/tsunghan-wu/D2ADA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy
  for Source-free Domain Adaptation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Guang Chen, Jing Zhang, Zhijun Li, Wei He, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model
to the unlabeled target domain without accessing the well-labeled source data,
which is a much more practical setting due to the data privacy, security, and
transmission issues. To make up for the absence of source data, most existing
methods introduced feature prototype based pseudo-labeling strategies to
realize self-training model adaptation. However, feature prototypes are
obtained by instance-level predictions based feature clustering, which is
category-biased and tends to result in noisy labels since the visual domain
gaps between source and target are usually different between categories. In
addition, we found that a monocentric feature prototype may be ineffective to
represent each category and introduce negative transfer, especially for those
hard-transfer data. To address these issues, we propose a general
class-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task.
Specifically, for each target category, we first introduce a global inter-class
balanced sampling strategy to aggregate potential representative target
samples. Then, we design an intra-class multicentric clustering strategy to
achieve more robust and representative prototypes generation. In contrast to
existing strategies that update the pseudo label at a fixed training period, we
further introduce a dynamic pseudo labeling strategy to incorporate network
update information during model adaptation. Extensive experiments show that the
proposed model-agnostic BMD strategy significantly improves representative SFDA
methods to yield new state-of-the-art results. The code is available at
https://github.com/ispc-lab/BMD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version of ECCV 2022. Code is available at
  https://github.com/ispc-lab/BMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Perturbation-Constrained Adversarial Attack for Evaluating the
  Robustness of Optical Flow <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jenny Schmalfuss, Philipp Scholze, Andrés Bruhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent optical flow methods are almost exclusively judged in terms of
accuracy, while their robustness is often neglected. Although adversarial
attacks offer a useful tool to perform such an analysis, current attacks on
optical flow methods focus on real-world attacking scenarios rather than a
worst case robustness assessment. Hence, in this work, we propose a novel
adversarial attack - the Perturbation-Constrained Flow Attack (PCFA) - that
emphasizes destructivity over applicability as a real-world attack. PCFA is a
global attack that optimizes adversarial perturbations to shift the predicted
flow towards a specified target flow, while keeping the L2 norm of the
perturbation below a chosen bound. Our experiments demonstrate PCFA's
applicability in white- and black-box settings, and show it finds stronger
adversarial samples than previous attacks. Based on these strong samples, we
provide the first joint ranking of optical flow methods considering both
prediction quality and adversarial robustness, which reveals state-of-the-art
methods to be particularly vulnerable. Code is available at
https://github.com/cv-stuttgart/PCFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the European Conference on Computer Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Adaptive Negative Envision for Few-Shot Open-Set Recognition <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.13073v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.13073v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Huang, Jiawei Ma, Guangxing Han, Shih-Fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of few-shot open-set recognition (FSOR), which learns a
recognition system capable of both fast adaptation to new classes with limited
labeled examples and rejection of unknown negative samples. Traditional
large-scale open-set methods have been shown ineffective for FSOR problem due
to data limitation. Current FSOR methods typically calibrate few-shot
closed-set classifiers to be sensitive to negative samples so that they can be
rejected via thresholding. However, threshold tuning is a challenging process
as different FSOR tasks may require different rejection powers. In this paper,
we instead propose task-adaptive negative class envision for FSOR to integrate
threshold tuning into the learning process. Specifically, we augment the
few-shot closed-set classifier with additional negative prototypes generated
from few-shot examples. By incorporating few-shot class correlations in the
negative generation process, we are able to learn dynamic rejection boundaries
for FSOR tasks. Besides, we extend our method to generalized few-shot open-set
recognition (GFSOR), which requires classification on both many-shot and
few-shot classes as well as rejection of negative samples. Extensive
experiments on public benchmarks validate our methods on both problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCPL: <span class="highlight-title">Contrastive</span> Coherence Preserving Loss for Versatile Style Transfer <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Wu, Zhen Zhu, Junping Du, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to devise a universally versatile style transfer method
capable of performing artistic, photo-realistic, and video style transfer
jointly, without seeing videos during training. Previous single-frame methods
assume a strong constraint on the whole image to maintain temporal consistency,
which could be violated in many cases. Instead, we make a mild and reasonable
assumption that global inconsistency is dominated by local inconsistencies and
devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local
patches. CCPL can preserve the coherence of the content source during style
transfer without degrading stylization. Moreover, it owns a neighbor-regulating
mechanism, resulting in a vast reduction of local distortions and considerable
visual quality improvement. Aside from its superior performance on versatile
style transfer, it can be easily extended to other tasks, such as
image-to-image translation. Besides, to better fuse content and style features,
we propose Simple Covariance Transformation (SCT) to effectively align
second-order statistics of the content feature with the style feature.
Experiments demonstrate the effectiveness of the resulting model for versatile
style transfer, when armed with CCPL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022 as an oral paper; code url:
  https://github.com/JarrentWu1031/CCPL; Video demo:
  https://youtu.be/scZuJCXhL14</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEXTER: An end-to-end system to extract table contents from electronic
  medical health documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandhinee PR, Harinath Krishnamoorthy, Koushik Srivatsan, Anil Goyal, Sudarsun Santhiappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose DEXTER, an end to end system to extract information
from tables present in medical health documents, such as electronic health
records (EHR) and explanation of benefits (EOB). DEXTER consists of four
sub-system stages: i) table detection ii) table type classification iii) cell
detection; and iv) cell content extraction. We propose a two-stage transfer
learning-based approach using CDeC-Net architecture along with Non-Maximal
suppression for table detection. We design a conventional computer vision-based
approach for table type classification and cell detection using parameterized
kernels based on image size for detecting rows and columns. Finally, we extract
the text from the detected cells using pre-existing OCR engine Tessaract. To
evaluate our system, we manually annotated a sample of the real-world medical
dataset (referred to as Meddata) consisting of wide variations of documents (in
terms of appearance) covering different table structures, such as bordered,
partially bordered, borderless, or coloured tables. We experimentally show that
DEXTER outperforms the commercially available Amazon Textract and Microsoft
Azure Form Recognizer systems on the annotated real-world medical dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NIMBLE: A Non-rigid Hand Model with Bones and Muscles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04533v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04533v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang, Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, Jingyi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging Metaverse applications demand reliable, accurate, and photorealistic
reproductions of human hands to perform sophisticated operations as if in the
physical world. While real human hand represents one of the most intricate
coordination between bones, muscle, tendon, and skin, state-of-the-art
techniques unanimously focus on modeling only the skeleton of the hand. In this
paper, we present NIMBLE, a novel parametric hand model that includes the
missing key components, bringing 3D hand model to a new level of realism. We
first annotate muscles, bones and skins on the recent Magnetic Resonance
Imaging hand (MRI-Hand) dataset and then register a volumetric template hand
onto individual poses and subjects within the dataset. NIMBLE consists of 20
bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin
mesh. Via iterative shape registration and parameter learning, it further
produces shape blend shapes, pose blend shapes, and a joint regressor. We
demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.
By enforcing the inner bones and muscles to match anatomic and kinematic rules,
NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the
appearance of skin, we further construct a photometric HandStage to acquire
high-quality textures and normal maps to model wrinkles and palm print.
Finally, NIMBLE also benefits learning-based hand pose and shape estimation by
either synthesizing rich data or acting directly as a differentiable layer in
the inference network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive</span> <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-train</span>ing with Limited Resources <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu, Osamu Yoshie, Yubo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have
revealed the potential of aligning multi-modal representations with contrastive
learning. However, these works require a tremendous amount of data and
computational resources (e.g., billion-level web data and hundreds of GPUs),
which prevent researchers with limited resources from reproduction and further
exploration. To this end, we propose a stack of novel methods, which
significantly cut down the heavy resource dependency and allow us to conduct
dual-encoder multi-modal representation alignment with limited resources.
Besides, we provide a reproducible baseline of competitive results, namely
ZeroVL, with only 14M publicly accessible academic datasets and 8 V100 GPUs.
Additionally, we collect 100M web data for pre-training, and achieve comparable
or superior results than state-of-the-art methods, further proving the
effectiveness of our methods on large-scale data. We hope that this work will
provide useful data points and experience for future research in contrastive
vision-language pre-training. Code is available at
https://github.com/zerovl/ZeroVL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelfReformer: Self-Refined Network with <span class="highlight-title">Transformer</span> for Salient Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11283v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11283v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ke Yun, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global and local contexts significantly contribute to the integrity of
predictions in Salient Object Detection (SOD). Unfortunately, existing methods
still struggle to generate complete predictions with fine details. There are
two major problems in conventional approaches: first, for global context,
high-level CNN-based encoder features cannot effectively catch long-range
dependencies, resulting in incomplete predictions. Second, downsampling the
ground truth to fit the size of predictions will introduce inaccuracy as the
ground truth details are lost during interpolation or pooling. Thus, in this
work, we developed a Transformer-based network and framed a supervised task for
a branch to learn the global context information explicitly. Besides, we adopt
Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the
size of ground truth instead of the reverse. Thus details in the ground truth
are untouched. In addition, we developed a two-stage Context Refinement Module
(CRM) to fuse global context and automatically locate and refine the local
details in the predictions. The proposed network can guide and correct itself
based on the global and local context generated, thus is named, Self-Refined
Transformer (SelfReformer). Extensive experiments and evaluation results on
five benchmark datasets demonstrate the outstanding performance of the network,
and we achieved the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AFDetV2: Rethinking the Necessity of the Second Stage for Object
  Detection from Point Clouds <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao, Li Huang, Kun Li, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been two streams in the 3D detection from point clouds:
single-stage methods and two-stage methods. While the former is more
computationally efficient, the latter usually provides better detection
accuracy. By carefully examining the two-stage approaches, we have found that
if appropriately designed, the first stage can produce accurate box regression.
In this scenario, the second stage mainly rescores the boxes such that the
boxes with better localization get selected. From this observation, we have
devised a single-stage anchor-free network that can fulfill these requirements.
This network, named AFDetV2, extends the previous work by incorporating a
self-calibrated convolution block in the backbone, a keypoint auxiliary
supervision, and an IoU prediction branch in the multi-task head. As a result,
the detection accuracy is drastically boosted in the single-stage. To evaluate
our approach, we have conducted extensive experiments on the Waymo Open Dataset
and the nuScenes Dataset. We have observed that our AFDetV2 achieves the
state-of-the-art results on these two datasets, superior to all the prior arts,
including both the single-stage and the two-stage 3D detectors. AFDetV2 won the
1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge
2021. In addition, a variant of our model AFDetV2-Base was entitled the "Most
Efficient Model" by the Challenge Sponsor, showing a superior computational
efficiency. To demonstrate the generality of this single-stage method, we have
also applied it to the first stage of the two-stage networks. Without
exception, the results show that with the strengthened backbone and the
rescoring approach, the second stage refinement is no longer needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022; 1st Place Solution for the Real-time 3D Detection and the
  Most Efficient Model of the Waymo Open Dataset Challenges 2021
  (http://cvpr2021.wad.vision/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GroupViT: Semantic Segmentation Emerges from Text Supervision <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.11094v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.11094v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grouping and recognition are important components of visual scene
understanding, e.g., for object detection and semantic segmentation. With
end-to-end deep learning systems, grouping of image regions usually happens
implicitly via top-down supervision from pixel-level recognition labels.
Instead, in this paper, we propose to bring back the grouping mechanism into
deep networks, which allows semantic segments to emerge automatically with only
text supervision. We propose a hierarchical Grouping Vision Transformer
(GroupViT), which goes beyond the regular grid structure representation and
learns to group image regions into progressively larger arbitrary-shaped
segments. We train GroupViT jointly with a text encoder on a large-scale
image-text dataset via contrastive losses. With only text supervision and
without any pixel-level annotations, GroupViT learns to group together semantic
regions and successfully transfers to the task of semantic segmentation in a
zero-shot manner, i.e., without any further fine-tuning. It achieves a
zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on
PASCAL Context datasets, and performs competitively to state-of-the-art
transfer-learning methods requiring greater levels of supervision. We
open-source our code at https://github.com/NVlabs/GroupViT .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. Project page and code: https://jerryxu.net/GroupViT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Contextual Relationships for Cervical Abnormal Cell Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiong Liang, Shuo Feng, Qing Liu, Hulin Kuang, Jianfeng Liu, Liyan Liao, Yun Du, Jianxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cervical abnormal cell detection is a challenging task as the morphological
discrepancies between abnormal and normal cells are usually subtle. To
determine whether a cervical cell is normal or abnormal, cytopathologists
always take surrounding cells as references to identify its abnormality. To
mimic these behaviors, we propose to explore contextual relationships to boost
the performance of cervical abnormal cell detection. Specifically, both
contextual relationships between cells and cell-to-global images are exploited
to enhance features of each region of interest (RoI) proposals. Accordingly,
two modules, dubbed as RoI-relationship attention module (RRAM) and global RoI
attention module (GRAM), are developed and their combination strategies are
also investigated. We establish a strong baseline by using Double-Head Faster
R-CNN with feature pyramid network (FPN) and integrate our RRAM and GRAM into
it to validate the effectiveness of the proposed modules. Experiments conducted
on a large cervical cell detection dataset reveal that the introduction of RRAM
and GRAM both achieves better average precision (AP) than the baseline methods.
Moreover, when cascading RRAM and GRAM, our method outperforms the
state-of-the-art (SOTA) methods. Furthermore, we also show the proposed feature
enhancing scheme can facilitate both image-level and smear-level
classification. The code and trained models are publicly available at
https://github.com/CVIU-CSU/CR4CACD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 tables, and 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hidden Progress in Deep Learning: SGD Learns Parities Near the
  Computational Limit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is mounting empirical evidence of emergent phenomena in the
capabilities of deep learning methods as we scale up datasets, model sizes, and
training times. While there are some accounts of how these resources modulate
statistical capacity, far less is known about their effect on the computational
problem of model training. This work conducts such an exploration through the
lens of learning $k$-sparse parities of $n$ bits, a canonical family of
problems which pose theoretical computational barriers. In this setting, we
find that neural networks exhibit surprising phase transitions when scaling up
dataset size and running time. In particular, we demonstrate empirically that
with standard training, a variety of architectures learn sparse parities with
$n^{O(k)}$ examples, with loss (and error) curves abruptly dropping after
$n^{O(k)}$ iterations. These positive results nearly match known SQ lower
bounds, even without an explicit sparsity-promoting prior. We elucidate the
mechanisms of these phenomena with a theoretical analysis: we find that the
phase transition in performance is not due to SGD "stumbling in the dark" until
it finds the hidden set of features (a natural algorithm which also runs in
$n^{O(k)}$ time); instead, we show that SGD gradually amplifies a Fourier gap
in the population gradient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplifying Clustering with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Maria Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective functions used in spectral clustering are usually composed of
two terms: i) a term that minimizes the local quadratic variation of the
cluster assignments on the graph and; ii) a term that balances the clustering
partition and helps avoiding degenerate solutions. This paper shows that a
graph neural network, equipped with suitable message passing layers, can
generate good cluster assignments by optimizing only a balancing term. Results
on attributed graph datasets show the effectiveness of the proposed approach in
terms of clustering performance and computation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Package for Fast ABC-Boost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Li, Weijie Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents the open-source package which implements the series of
our boosting works in the past years. In particular, the package includes
mainly three lines of techniques, among which the following two are already the
standard implementations in popular boosted tree platforms:
  (i) The histogram-based (feature-binning) approach makes the tree
implementation convenient and efficient. In Li et al (2007), a simple
fixed-length adaptive binning algorithm was developed. In this report, we
demonstrate that such a simple algorithm is still surprisingly effective
compared to more sophisticated variants in popular tree platforms.
  (ii) The explicit gain formula in Li (20010) for tree splitting based on
second-order derivatives of the loss function typically improves, often
considerably, over the first-order methods. Although the gain formula in Li
(2010) was derived for logistic regression loss, it is a generic formula for
loss functions with second-derivatives. For example, the open-source package
also includes $L_p$ regression for $p\geq 1$.
  The main contribution of this package is the ABC-Boost (adaptive base class
boosting) for multi-class classification. The initial work in Li (2008) derived
a new set of derivatives of the classical multi-class logistic regression by
specifying a "base class". The accuracy can be substantially improved if the
base class is chosen properly. The major technical challenge is to design a
search strategy to select the base class. The prior published works implemented
an exhaustive search procedure to find the base class which is computationally
too expensive. Recently, a new report (Li and Zhao, 20022) presents a unified
framework of "Fast ABC-Boost" which allows users to efficiently choose the
proper search space for the base class.
  The package provides interfaces for linux, windows, mac, matlab, R, python.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rank-based Decomposable Losses in Machine Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Hu, Xin Wang, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have revealed an essential paradigm in designing loss functions
that differentiate individual losses vs. aggregate losses. The individual loss
measures the quality of the model on a sample, while the aggregate loss
combines individual losses/scores over each training sample. Both have a common
procedure that aggregates a set of individual values to a single numerical
value. The ranking order reflects the most fundamental relation among
individual values in designing losses. In addition, decomposability, in which a
loss can be decomposed into an ensemble of individual terms, becomes a
significant property of organizing losses/scores. This survey provides a
systematic and comprehensive review of rank-based decomposable losses in
machine learning. Specifically, we provide a new taxonomy of loss functions
that follows the perspectives of aggregate loss and individual loss. We
identify the aggregator to form such losses, which are examples of set
functions. We organize the rank-based decomposable losses into eight
categories. Following these categories, we review the literature on rank-based
aggregate losses and rank-based individual losses. We describe general formulas
for these losses and connect them with existing research topics. We also
suggest future research directions spanning unexplored, remaining, and emerging
issues in rank-based decomposable losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word Play for Playing Othello (Reverses) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samantha E. Miller Noever, David Noever
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models like OpenAI's Generative Pre-Trained Transformers (GPT-2/3)
capture the long-term correlations needed to generate text in a variety of
domains (such as language translators) and recently in gameplay (chess, Go, and
checkers). The present research applies both the larger (GPT-3) and smaller
(GPT-2) language models to explore the complex strategies for the game of
Othello (or Reverses). Given the game rules for rapid reversals of fortune, the
language model not only represents a candidate predictor of the next move based
on previous game moves but also avoids sparse rewards in gameplay. The language
model automatically captures or emulates championship-level strategies. The
fine-tuned GPT-2 model generates Othello games ranging from 13-71% completion,
while the larger GPT-3 model reaches 41% of a complete game. Like previous work
with chess and Go, these language models offer a novel way to generate
plausible game archives, particularly for comparing opening moves across a
larger sample than humanly possible to explore. A primary contribution of these
models magnifies (by two-fold) the previous record for player archives (120,000
human games over 45 years from 1977-2022), thus supplying the research
community with more diverse and original strategies for sampling with other
reinforcement learning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amplitude Scintillation Forecasting Using Bagged Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdollah Masoud Darya, Aisha Abdulla Al-Owais, Muhammad Mubasshir Shaikh, Ilias Fernini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electron density irregularities present within the ionosphere induce
significant fluctuations in global navigation satellite system (GNSS) signals.
Fluctuations in signal power are referred to as amplitude scintillation and can
be monitored through the S4 index. Forecasting the severity of amplitude
scintillation based on historical S4 index data is beneficial when real-time
data is unavailable. In this work, we study the possibility of using historical
data from a single GPS scintillation monitoring receiver to train a machine
learning (ML) model to forecast the severity of amplitude scintillation, either
weak, moderate, or severe, with respect to temporal and spatial parameters. Six
different ML models were evaluated and the bagged trees model was the most
accurate among them, achieving a forecasting accuracy of $81\%$ using a
balanced dataset, and $97\%$ using an imbalanced dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was presented at IGARSS 2022, Kuala Lumpur, Malaysia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information-Theoretic Analysis of Bayesian Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaury Gouverneur, Borja Rodríguez-Gálvez, Tobias J. Oechtering, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on the framework introduced by Xu and Raginksy [1] for supervised
learning problems, we study the best achievable performance for model-based
Bayesian reinforcement learning problems. With this purpose, we define minimum
Bayesian regret (MBR) as the difference between the maximum expected cumulative
reward obtainable either by learning from the collected data or by knowing the
environment and its dynamics. We specialize this definition to reinforcement
learning problems modeled as Markov decision processes (MDPs) whose kernel
parameters are unknown to the agent and whose uncertainty is expressed by a
prior distribution. One method for deriving upper bounds on the MBR is
presented and specific bounds based on the relative entropy and the Wasserstein
distance are given. We then focus on two particular cases of MDPs, the
multi-armed bandit problem (MAB) and the online optimization with partial
feedback problem. For the latter problem, we show that our bounds can recover
from below the current information-theoretic bounds by Russo and Van Roy [2].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages: 6 of the main text, 1 of references, and 3 of appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On stabilizing reinforcement learning without Lyapunov functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Osinenko, Grigory Yaremenko, Georgiy Malaniya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning remains one of the major directions of the
contemporary development of control engineering and machine learning. Nice
intuition, flexible settings, ease of application are among the many perks of
this methodology. From the standpoint of machine learning, the main strength of
a reinforcement learning agent is its ability to ``capture" (learn) the optimal
behavior in the given environment. Typically, the agent is built on neural
networks and it is their approximation abilities that give rise to the above
belief. From the standpoint of control engineering, however, reinforcement
learning has serious deficiencies. The most significant one is the lack of
stability guarantee of the agent-environment closed loop. A great deal of
research was and is being made towards stabilizing reinforcement learning.
Speaking of stability, the celebrated Lyapunov theory is the de facto tool. It
is thus no wonder that so many techniques of stabilizing reinforcement learning
rely on the Lyapunov theory in one way or another. In control theory, there is
an intricate connection between a stabilizing controller and a Lyapunov
function. Employing such a pair seems thus quite attractive to design
stabilizing reinforcement learning. However, computation of a Lyapunov function
is generally a cumbersome process. In this note, we show how to construct a
stabilizing reinforcement learning agent that does not employ such a function
at all. We only assume that a Lyapunov function exists, which is a natural
thing to do if the given system (read: environment) is stabilizable, but we do
not need to compute one.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning differentiable solvers for systems with hard constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geoffrey Négiar, Michael W. Mahoney, Aditi S. Krishnapriyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a practical method to enforce linear partial differential
equation (PDE) constraints for functions defined by neural networks (NNs), up
to a desired tolerance. By combining methods in differentiable physics and
applications of the implicit function theorem to NN models, we develop a
differentiable PDE-constrained NN layer. During training, our model learns a
family of functions, each of which defines a mapping from PDE parameters to PDE
solutions. At inference time, the model finds an optimal linear combination of
the functions in the learned family by solving a PDE-constrained optimization
problem. Our method provides continuous solutions over the domain of interest
that exactly satisfy desired physical constraints. Our results show that
incorporating hard constraints directly into the NN architecture achieves much
lower test error, compared to training on an unconstrained objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Back to the Manifold: Recovering from Out-of-Distribution States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfredo Reichlin, Giovanni Luca Marchetti, Hang Yin, Ali Ghadirzadeh, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from previously collected datasets of expert data offers the promise
of acquiring robotic policies without unsafe and costly online explorations.
However, a major challenge is a distributional shift between the states in the
training dataset and the ones visited by the learned policy at the test time.
While prior works mainly studied the distribution shift caused by the policy
during the offline training, the problem of recovering from out-of-distribution
states at the deployment time is not very well studied yet. We alleviate the
distributional shift at the deployment time by introducing a recovery policy
that brings the agent back to the training manifold whenever it steps out of
the in-distribution states, e.g., due to an external perturbation. The recovery
policy relies on an approximation of the training data density and a learned
equivariant mapping that maps visual observations into a latent space in which
translations correspond to the robot actions. We demonstrate the effectiveness
of the proposed method through several manipulation experiments on a real
robotic platform. Our results show that the recovery policy enables the agent
to complete tasks while the behavioral cloning alone fails because of the
distributional shift problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pGMM Kernel Regression and Comparisons with Boosted Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Li, Weijie Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we demonstrate the advantage of the pGMM (``powered generalized
min-max'') kernel in the context of (ridge) regression. In recent prior
studies, the pGMM kernel has been extensively evaluated for classification
tasks, for logistic regression, support vector machines, as well as deep neural
networks. In this paper, we provide an experimental study on ridge regression,
to compare the pGMM kernel regression with the ordinary ridge linear regression
as well as the RBF kernel ridge regression. Perhaps surprisingly, even without
a tuning parameter (i.e., $p=1$ for the power parameter of the pGMM kernel),
the pGMM kernel already performs well. Furthermore, by tuning the parameter
$p$, this (deceptively simple) pGMM kernel even performs quite comparably to
boosted trees.
  Boosting and boosted trees are very popular in machine learning practice. For
regression tasks, typically, practitioners use $L_2$ boost, i.e., for
minimizing the $L_2$ loss. Sometimes for the purpose of robustness, the $L_1$
boost might be a choice. In this study, we implement $L_p$ boost for $p\geq 1$
and include it in the package of ``Fast ABC-Boost''. Perhaps also surprisingly,
the best performance (in terms of $L_2$ regression loss) is often attained at
$p>2$, in some cases at $p\gg 2$. This phenomenon has already been demonstrated
by Li et al (UAI 2010) in the context of k-nearest neighbor classification
using $L_p$ distances. In summary, the implementation of $L_p$ boost provides
practitioners the additional flexibility of tuning boosting algorithms for
potentially achieving better accuracy in regression applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhanced Graph Representation for Machine Learning Based Automatic
  Intersection Management <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Klimke, Jasper Gerigk, Benjamin Völz, Michael Buchholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The improvement of traffic efficiency at urban intersections receives strong
research interest in the field of automated intersection management. So far,
mostly non-learning algorithms like reservation or optimization-based ones were
proposed to solve the underlying multi-agent planning problem. At the same
time, automated driving functions for a single ego vehicle are increasingly
implemented using machine learning methods. In this work, we build upon a
previously presented graph-based scene representation and graph neural network
to approach the problem using reinforcement learning. The scene representation
is improved in key aspects by using edge features in addition to the existing
node features for the vehicles. This leads to an increased representation
quality that is leveraged by an updated network architecture. The paper
provides an in-depth evaluation of the proposed method against baselines that
are commonly used in automatic intersection management. Compared to a
traditional signalized intersection and an enhanced first-in-first-out scheme,
a significant reduction of induced delay is observed at varying traffic
densities. Finally, the generalization capability of the graph-based
representation is evaluated by testing the policy on intersection layouts not
seen during training. The model generalizes virtually without restrictions to
smaller intersection layouts and within certain limits to larger ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, to be published in IEEE 25th International
  Conference on Intelligent Transportation Systems (ITSC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boolean Decision Rules for Reinforcement Learning Policy Summarisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James McCarthy, Rahul Nair, Elizabeth Daly, Radu Marinescu, Ivana Dusparic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability of Reinforcement Learning (RL) policies remains a challenging
research problem, particularly when considering RL in a safety context.
Understanding the decisions and intentions of an RL policy offer avenues to
incorporate safety into the policy by limiting undesirable actions. We propose
the use of a Boolean Decision Rules model to create a post-hoc rule-based
summary of an agent's policy. We evaluate our proposed approach using a DQN
agent trained on an implementation of a lava gridworld and show that, given a
hand-crafted feature representation of this gridworld, simple generalised rules
can be created, giving a post-hoc explainable summary of the agent's policy. We
discuss possible avenues to introduce safety into a RL agent's policy by using
rules generated by this rule-based model as constraints imposed on the agent's
policy, as well as discuss how creating simple rule summaries of an agent's
policy may help in the debugging process of RL agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upper Limb Movement Recognition utilising EEG and EMG Signals for
  Rehabilitative Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Zihao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Upper limb movement classification, which maps input signals to the target
activities, is one of the crucial areas in the control of rehabilitative
robotics. Classifiers are trained for the rehabilitative system to comprehend
the desires of the patient whose upper limbs do not function properly.
Electromyography (EMG) signals and Electroencephalography (EEG) signals are
used widely for upper limb movement classification. By analysing the
classification results of the real-time EEG and EMG signals, the system can
understand the intention of the user and predict the events that one would like
to carry out. Accordingly, it will provide external help to the user to assist
one to perform the activities. However, not all users process effective EEG and
EMG signals due to the noisy environment. The noise in the real-time data
collection process contaminates the effectiveness of the data. Moreover, not
all patients process strong EMG signals due to muscle damage and neuromuscular
disorder. To address these issues, we would like to propose a novel
decision-level multisensor fusion technique. In short, the system will
integrate EEG signals with EMG signals, retrieve effective information from
both sources to understand and predict the desire of the user, and thus provide
assistance. By testing out the proposed technique on a publicly available
WAY-EEG-GAL dataset, which contains EEG and EMG signals that were recorded
simultaneously, we manage to conclude the feasibility and effectiveness of the
novel system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 16 figures, 2 tables, Undergraduate Research Project in
  Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpolation, extrapolation, and local generalization in common neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Bonnasse-Gahot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a long history of works showing that neural networks have hard
time extrapolating beyond the training set. A recent study by Balestriero et
al. (2021) challenges this view: defining interpolation as the state of
belonging to the convex hull of the training set, they show that the test set,
either in input or neural space, cannot lie for the most part in this convex
hull, due to the high dimensionality of the data, invoking the well known curse
of dimensionality. Neural networks are then assumed to necessarily work in
extrapolative mode. We here study the neural activities of the last hidden
layer of typical neural networks. Using an autoencoder to uncover the intrinsic
space underlying the neural activities, we show that this space is actually
low-dimensional, and that the better the model, the lower the dimensionality of
this intrinsic space. In this space, most samples of the test set actually lie
in the convex hull of the training set: under the convex hull definition, the
models thus happen to work in interpolation regime. Moreover, we show that
belonging to the convex hull does not seem to be the relevant criteria.
Different measures of proximity to the training set are actually better related
to performance accuracy. Thus, typical neural networks do seem to operate in
interpolation regime. Good generalization performances are linked to the
ability of a neural network to operate well in such a regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Exploration for Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Lindner, Andreas Krause, Giorgia Ramponi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a
reward function from expert demonstrations. Many IRL algorithms require a known
transition model and sometimes even a known expert policy, or they at least
require access to a generative model. However, these assumptions are too strong
for many real-world applications, where the environment can be accessed only
through sequential interaction. We propose a novel IRL algorithm: Active
exploration for Inverse Reinforcement Learning (AceIRL), which actively
explores an unknown environment and expert policy to quickly learn the expert's
reward function and identify a good policy. AceIRL uses previous observations
to construct confidence intervals that capture plausible reward functions and
find exploration policies that focus on the most informative regions of the
environment. AceIRL is the first approach to active IRL with sample-complexity
bounds that does not require a generative model of the environment. AceIRL
matches the sample complexity of active IRL with a generative model in the
worst case. Additionally, we establish a problem-dependent bound that relates
the sample complexity of AceIRL to the suboptimality gap of a given IRL
problem. We empirically evaluate AceIRL in simulations and find that it
significantly outperforms more naive exploration strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Automated Feature Monitoring for Data Streams <span class="chip">KDD22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Conde, Ricardo Moreira, João Torres, Pedro Cardoso, Hugo Ferreira, Marco O. P. Sampaio, João Tiago Ascensão, Pedro Bizarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring the behavior of automated real-time stream processing systems has
become one of the most relevant problems in real world applications. Such
systems have grown in complexity relying heavily on high dimensional input
data, and data hungry Machine Learning (ML) algorithms. We propose a flexible
system, Feature Monitoring (FM), that detects data drifts in such data sets,
with a small and constant memory footprint and a small computational cost in
streaming applications. The method is based on a multi-variate statistical test
and is data driven by design (full reference distributions are estimated from
the data). It monitors all features that are used by the system, while
providing an interpretable features ranking whenever an alarm occurs (to aid in
root cause analysis). The computational and memory lightness of the system
results from the use of Exponential Moving Histograms. In our experimental
study, we analyze the system's behavior with its parameters and, more
importantly, show examples where it detects problems that are not directly
related to a single feature. This illustrates how FM eliminates the need to add
custom signals to detect specific types of problems and that monitoring the
available space of features is often enough.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures. AutoML, KDD22, August 14-17, 2022, Washington,
  DC, US</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FakeCLR: Exploring <span class="highlight-title">Contrastive Learning</span> for Solving Latent Discontinuity
  in Data-Efficient GANs <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Li, Chaoyue Wang, Heliang Zheng, Jing Zhang, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-Efficient GANs (DE-GANs), which aim to learn generative models with a
limited amount of training data, encounter several challenges for generating
high-quality samples. Since data augmentation strategies have largely
alleviated the training instability, how to further improve the generative
performance of DE-GANs becomes a hotspot. Recently, contrastive learning has
shown the great potential of increasing the synthesis quality of DE-GANs, yet
related principles are not well explored. In this paper, we revisit and compare
different contrastive learning strategies in DE-GANs, and identify (i) the
current bottleneck of generative performance is the discontinuity of latent
space; (ii) compared to other contrastive learning strategies,
Instance-perturbation works towards latent space continuity, which brings the
major improvement to DE-GANs. Based on these observations, we propose FakeCLR,
which only applies contrastive learning on perturbed fake samples, and devises
three related training techniques: Noise-related Latent Augmentation,
Diversity-aware Queue, and Forgetting Factor of Queue. Our experimental results
manifest the new state of the arts on both few-shot generation and limited-data
generation. On multiple datasets, FakeCLR acquires more than 15% FID
improvement compared to existing DE-GANs. Code is available at
https://github.com/iceli1007/FakeCLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Graph Gradual Pruning for Sparse Training in Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Liu, Xueqi Ma, Yinbing Zhan, Liang Ding, Dapeng Tao, Bo Du, Wenbin Hu, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) tend to suffer from high computation costs due
to the exponentially increasing scale of graph data and the number of model
parameters, which restricts their utility in practical applications. To this
end, some recent works focus on sparsifying GNNs with the lottery ticket
hypothesis (LTH) to reduce inference costs while maintaining performance
levels. However, the LTH-based methods suffer from two major drawbacks: 1) they
require exhaustive and iterative training of dense models, resulting in an
extremely large training computation cost, and 2) they only trim graph
structures and model parameters but ignore the node feature dimension, where
significant redundancy exists. To overcome the above limitations, we propose a
comprehensive graph gradual pruning framework termed CGP. This is achieved by
designing a during-training graph pruning paradigm to dynamically prune GNNs
within one training process. Unlike LTH-based methods, the proposed CGP
approach requires no re-training, which significantly reduces the computation
costs. Furthermore, we design a co-sparsifying strategy to comprehensively trim
all three core elements of GNNs: graph structures, node features, and model
parameters. Meanwhile, aiming at refining the pruning operation, we introduce a
regrowth process into our CGP framework, in order to re-establish the pruned
but important connections. The proposed CGP is evaluated by using a node
classification task across 6 GNN architectures, including shallow models (GCN
and GAT), shallow-but-deep-propagation models (SGC and APPNP), and deep models
(GCNII and ResGCN), on a total of 14 real-world graph datasets, including
large-scale graph datasets from the challenging Open Graph Benchmark.
Experiments reveal that our proposed strategy greatly improves both training
and inference efficiency while matching or even exceeding the accuracy of
existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 27 figures, submitting to IEEE TNNLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstraction between Structural Causal Models: A <span class="highlight-title">Review</span> of Definitions
  and Properties <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Massimo Zennaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural causal models (SCMs) are a widespread formalism to deal with
causal systems. A recent direction of research has considered the problem of
relating formally SCMs at different levels of abstraction, by defining maps
between SCMs and imposing a requirement of interventional consistency. This
paper offers a review of the solutions proposed so far, focusing on the formal
properties of a map between SCMs, and highlighting the different layers
(structural, distributional) at which these properties may be enforced. This
allows us to distinguish families of abstractions that may or may not be
permitted by choosing to guarantee certain properties instead of others. Such
an understanding not only allows to distinguish among proposal for causal
abstraction with more awareness, but it also allows to tailor the definition of
abstraction with respect to the forms of abstraction relevant to specific
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 pages appendix, 12 figures Submitted to Causal
  Representation Learning workshop at the 38th Conference on Uncertainty in
  Artificial Intelligence (UAI CRL 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FunQG: Molecular Representation Learning Via Quotient Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Hajiabolhassan, Zahra Taheri, Ali Hojatnia, Yavar Taheri Yeganeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning expressive molecular representations is crucial to facilitate the
accurate prediction of molecular properties. Despite the significant
advancement of graph neural networks (GNNs) in molecular representation
learning, they generally face limitations such as neighbors-explosion,
under-reaching, over-smoothing, and over-squashing. Also, GNNs usually have
high computational complexity because of the large-scale number of parameters.
Typically, such limitations emerge or increase when facing relatively
large-size graphs or using a deeper GNN model architecture. An idea to overcome
these problems is to simplify a molecular graph into a small, rich, and
informative one, which is more efficient and less challenging to train GNNs. To
this end, we propose a novel molecular graph coarsening framework named FunQG
utilizing Functional groups, as influential building blocks of a molecule to
determine its properties, based on a graph-theoretic concept called Quotient
Graph. By experiments, we show that the resulting informative graphs are much
smaller than the molecular graphs and thus are good candidates for training
GNNs. We apply the FunQG on popular molecular property prediction benchmarks
and then compare the performance of a GNN architecture on the obtained datasets
with several state-of-the-art baselines on the original datasets. By
experiments, this method significantly outperforms previous baselines on
various datasets, besides its dramatic reduction in the number of parameters
and low computational complexity. Therefore, the FunQG can be used as a simple,
cost-effective, and robust method for solving the molecular representation
learning problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetrized Robust Procrustes: Constant-Factor Approximation and Exact
  Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Amir, Shahar Kovalsky, Nadav Dym
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical $\textit{Procrustes}$ problem is to find a rigid motion
(orthogonal transformation and translation) that best aligns two given
point-sets in the least-squares sense. The $\textit{Robust Procrustes}$ problem
is an important variant, in which a power-1 objective is used instead of least
squares to improve robustness to outliers. While the optimal solution of the
least-squares problem can be easily computed in closed form, dating back to
Sch\"onemann (1966), no such solution is known for the power-1 problem. In this
paper we propose a novel convex relaxation for the Robust Procrustes problem.
Our relaxation enjoys several theoretical and practical advantages:
Theoretically, we prove that our method provides a $\sqrt{2}$-factor
approximation to the Robust Procrustes problem, and that, under appropriate
assumptions, it exactly recovers the true rigid motion from point
correspondences contaminated by outliers. In practice, we find in numerical
experiments on both synthetic and real robust Procrustes problems, that our
method performs similarly to the standard Iteratively Reweighted Least Squares
(IRLS). However the convexity of our algorithm allows incorporating additional
convex penalties, which are not readily amenable to IRLS. This turns out to be
a substantial advantage, leading to improved results in high-dimensional
problems, including non-rigid shape alignment and semi-supervised interlingual
word translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study of the performance and scalablity of federated learning for
  medical imaging with intermittent clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judith Sáinz-Pardo Díaz, Álvaro López García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy and area under the curve) and reduction of execution times
will be studied with respect to the classical case (the centralized approach).
Different clients will be simulated from the training data, selected in an
unbalanced manner, i.e., they do not all have the same number of data. The
results of considering three or ten clients are exposed and compared between
them and against the centralized case. Two approaches to follow will be
analyzed in the case of intermittent clients, as in a real scenario some
clients may leave the training, and some new ones may enter the training. The
evolution of the results for the test set in terms of accuracy, area under the
curve and execution time is shown as the number of clients into which the
original data is divided increases. Finally, improvements and future work in
the field are proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiFeSt: Manifold-based Feature Selection for Small Data Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Cohen, Tal Shnitzer, Yuval Kluger, Ronen Talmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new method for few-sample supervised feature
selection (FS). Our method first learns the manifold of the feature space of
each class using kernels capturing multi-feature associations. Then, based on
Riemannian geometry, a composite kernel is computed, extracting the differences
between the learned feature associations. Finally, a FS score based on spectral
analysis is proposed. Considering multi-feature associations makes our method
multivariate by design. This in turn allows for the extraction of the hidden
manifold underlying the features and avoids overfitting, facilitating
few-sample FS. We showcase the efficacy of our method on illustrative examples
and several benchmarks, where our method demonstrates higher accuracy in
selecting the informative features compared to competing methods. In addition,
we show that our FS leads to improved classification and better generalization
when applied to test data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DHGE: Dual-view Hyper-Relational Knowledge Graph Embedding for Link
  Prediction and Entity Typing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Luo, Haihong E, Ling Tan, Xueyuan Lin, Gengxian Zhou, Jundi Li, Tianyu Yao, Kaiyang Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary attribute
value descriptions, which is considered to be more comprehensive and specific
than a triple-based fact. However, the existing hyper-relational KG embedding
methods in a single view are limited in application due to weakening the
hierarchical structure representing the affiliation between entities. To break
this limitation, we propose a dual-view hyper-relational KG (DH-KG) structure
which contains a hyper-relational instance view for entities and a
hyper-relational ontology view for concepts abstracted hierarchically from
entities to jointly model hyper-relational and hierarchical information. In
this paper, we first define link prediction and entity typing tasks on DH-KG
and construct two DH-KG datasets, JW44K-6K extracted from Wikidata and HTDM
based on medical data. Furthermore, We propose a DH-KG embedding model DHGE,
based on GRAN encoder, HGNN, and joint learning. Experimental results show that
DHGE outperforms baseline models on DH-KG. We also provide an example of the
application of this technology in the field of hypertension medication. Our
model and datasets are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GATE: Gated Additive Tree Ensemble for Tabular Classification and
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manu Joseph, Harsh Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel high-performance, parameter and computationally efficient
deep learning architecture for tabular data, Gated Additive Tree
Ensemble(GATE). GATE uses a gating mechanism, inspired from GRU, as a feature
representation learning unit with an in-built feature selection mechanism. We
combine it with an ensemble of differentiable, non-linear decision trees,
re-weighted with simple self-attention to predict our desired output. We
demonstrate that GATE is a competitive alternative to SOTA approaches like
GBDTs, NODE, FT Transformers, etc. by experiments on several public datasets
(both classification and regression). The code will be uploaded as soon as the
paper comes out of review.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware-agnostic Computation for Large-scale Knowledge Graph Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caglar Demir, Axel-Cyrille Ngonga Ngomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph embedding research has mainly focused on learning continuous
representations of knowledge graphs towards the link prediction problem.
Recently developed frameworks can be effectively applied in research related
applications. Yet, these frameworks do not fulfill many requirements of
real-world applications. As the size of the knowledge graph grows, moving
computation from a commodity computer to a cluster of computers in these
frameworks becomes more challenging. Finding suitable hyperparameter settings
w.r.t. time and computational budgets are left to practitioners. In addition,
the continual learning aspect in knowledge graph embedding frameworks is often
ignored, although continual learning plays an important role in many real-world
(deep) learning-driven applications. Arguably, these limitations explain the
lack of publicly available knowledge graph embedding models for large knowledge
graphs. We developed a framework based on the frameworks DASK, Pytorch
Lightning and Hugging Face to compute embeddings for large-scale knowledge
graphs in a hardware-agnostic manner, which is able to address real-world
challenges pertaining to the scale of real application. We provide an
open-source version of our framework along with a hub of pre-trained models
having more than 11.4 B parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in Software Impacts journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-block-Single-probe Variance Reduced Estimator for Coupled
  Compositional Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Gang Li, Yibo Wang, Lijun Zhang, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variance reduction techniques such as SPIDER/SARAH/STORM have been
extensively studied to improve the convergence rates of stochastic non-convex
optimization, which usually maintain and update a sequence of estimators for a
single function across iterations. {\it What if we need to track multiple
functional mappings across iterations but only with access to stochastic
samples of $\mathcal{O}(1)$ functional mappings at each iteration?} There is an
important application in solving an emerging family of coupled compositional
optimization problems in the form of $\sum_{i=1}^m f_i(g_i(\mathbf{w}))$, where
$g_i$ is accessible through a stochastic oracle. The key issue is to track and
estimate a sequence of $\mathbf g(\mathbf{w})=(g_1(\mathbf{w}), \ldots,
g_m(\mathbf{w}))$ across iterations, where $\mathbf g(\mathbf{w})$ has $m$
blocks and it is only allowed to probe $\mathcal{O}(1)$ blocks to attain their
stochastic values and Jacobians. To improve the complexity for solving these
problems, we propose a novel stochastic method named Multi-block-Single-probe
Variance Reduced (MSVR) estimator to track the sequence of $\mathbf
g(\mathbf{w})$. It is inspired by STORM but introduces a customized error
correction term to alleviate the noise not only in stochastic samples for the
selected blocks but also in those blocks that are not sampled. With the help of
the MSVR estimator, we develop several algorithms for solving the
aforementioned compositional problems with improved complexities across a
spectrum of settings with non-convex/convex/strongly convex objectives. Our
results improve upon prior ones in several aspects, including the order of
sample complexities and dependence on the strong convexity parameter. Empirical
studies on multi-task deep AUC maximization demonstrate the better performance
of using the new estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Deep Belief Network based Auto encoder using novel Extended
  Garson Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satyam Kumar, Vadlamani Ravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most difficult task in machine learning is to interpret trained shallow
neural networks. Deep neural networks (DNNs) provide impressive results on a
larger number of tasks, but it is generally still unclear how decisions are
made by such a trained deep neural network. Providing feature importance is the
most important and popular interpretation technique used in shallow and deep
neural networks. In this paper, we develop an algorithm extending the idea of
Garson Algorithm to explain Deep Belief Network based Auto-encoder (DBNA). It
is used to determine the contribution of each input feature in the DBN. It can
be used for any kind of neural network with many hidden layers. The
effectiveness of this method is tested on both classification and regression
datasets taken from literature. Important features identified by this method
are compared against those obtained by Wald chi square (\c{hi}2). For 2 out of
4 classification datasets and 2 out of 5 regression datasets, our proposed
methodology resulted in the identification of better-quality features leading
to statistically more significant results vis-\`a-vis Wald \c{hi}2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 28 figures, 21 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Poisoning Attacks with Anomaly Detection in Federated
  Learning for Healthcare Applications: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Raza, Shujun Li, Kim-Phuc Tran, Ludovic Koehl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Federated Learning (FL) is steadily increasing, especially
in privacy-aware applications, such as healthcare. However, its applications
have been limited by security concerns due to various adversarial attacks, such
as poisoning attacks (model and data poisoning). Such attacks attempt to poison
the local models and data to manipulate the global models in order to obtain
undue benefits and malicious use. Traditional methods of data auditing to
mitigate poisoning attacks find their limited applications in FL because the
edge devices never share their raw data directly due to privacy concerns, and
are globally distributed with no insight into their training data. Thereafter,
it is challenging to develop appropriate strategies to address such attacks and
minimize their impact on the global model in federated learning. In order to
address such challenges in FL, we proposed a novel framework to detect
poisoning attacks using deep neural networks and support vector machines, in
the form of anomaly without acquiring any direct access or information about
the underlying training data of local edge devices. We illustrate and evaluate
the proposed framework using different state of art poisoning attacks for two
different healthcare applications: Electrocardiograph classification and human
activity recognition. Our experimental analysis shows that the proposed method
can efficiently detect poisoning attacks and can remove the identified poisoned
updated from the global aggregation. Thereafter can increase the performance of
the federated global.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We will updated this article soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ wPINNs: Weak Physics informed neural networks for approximating entropy
  solutions of hyperbolic conservation laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim De Ryck, Siddhartha Mishra, Roberto Molinaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics informed neural networks (PINNs) require regularity of solutions of
the underlying PDE to guarantee accurate approximation. Consequently, they may
fail at approximating discontinuous solutions of PDEs such as nonlinear
hyperbolic equations. To ameliorate this, we propose a novel variant of PINNs,
termed as weak PINNs (wPINNs) for accurate approximation of entropy solutions
of scalar conservation laws. wPINNs are based on approximating the solution of
a min-max optimization problem for a residual, defined in terms of Kruzkhov
entropies, to determine parameters for the neural networks approximating the
entropy solution as well as test functions. We prove rigorous bounds on the
error incurred by wPINNs and illustrate their performance through numerical
experiments to demonstrate that wPINNs can approximate entropy solutions
accurately.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Order Conditional Mutual Information Maximization for dealing with
  High-Order Dependencies in Feature Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Souza, Cristiano Premebida, Rui Araújo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel feature selection method based on the conditional
mutual information (CMI). The proposed High Order Conditional Mutual
Information Maximization (HOCMIM) incorporates high order dependencies into the
feature selection procedure and has a straightforward interpretation due to its
bottom-up derivation. The HOCMIM is derived from the CMI's chain expansion and
expressed as a maximization optimization problem. The maximization problem is
solved using a greedy search procedure, which speeds up the entire feature
selection process. The experiments are run on a set of benchmark datasets (20
in total). The HOCMIM is compared with eighteen state-of-the-art feature
selection algorithms, from the results of two supervised learning classifiers
(Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best
results in terms of accuracy and shows to be faster than high order feature
selection counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Meta-Reinforcement Learning Algorithm for Causal Discovery <span class="chip">UAI 22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Sauter, Erman Acar, Vincent François-Lavet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery is a major task with the utmost importance for machine
learning since causal structures can enable models to go beyond pure
correlation-based inference and significantly boost their performance. However,
finding causal structures from data poses a significant challenge both in
computational effort and accuracy, let alone its impossibility without
interventions in general. In this paper, we develop a meta-reinforcement
learning algorithm that performs causal discovery by learning to perform
interventions such that it can construct an explicit causal graph. Apart from
being useful for possible downstream applications, the estimated causal graph
also provides an explanation for the data-generating process. In this article,
we show that our algorithm estimates a good graph compared to the SOTA
approaches, even in environments whose underlying causal structure is
previously unseen. Further, we make an ablation study that shows how learning
interventions contribute to the overall performance of our approach. We
conclude that interventions indeed help boost the performance, efficiently
yielding an accurate estimate of the causal structure of a possibly unseen
environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted submission to CRL@UAI 22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Simulation-Based Inference in Cosmology with Bayesian Neural
  Networks <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Lemos, Miles Cranmer, Muntazir Abidi, ChangHoon Hahn, Michael Eickenberg, Elena Massara, David Yallup, Shirley Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference (SBI) is rapidly establishing itself as a standard
machine learning technique for analyzing data in cosmological surveys. Despite
continual improvements to the quality of density estimation by learned models,
applications of such techniques to real data are entirely reliant on the
generalization power of neural networks far outside the training distribution,
which is mostly unconstrained. Due to the imperfections in scientist-created
simulations, and the large computational expense of generating all possible
parameter combinations, SBI methods in cosmology are vulnerable to such
generalization issues. Here, we discuss the effects of both issues, and show
how using a Bayesian neural network framework for training SBI can mitigate
biases, and result in more reliable inference outside the training set. We
introduce cosmoSWAG, the first application of Stochastic Weight Averaging to
cosmology, and apply it to SBI trained for inference on the cosmic microwave
background.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted at the ML4Astro Machine Learning for
  Astrophysics Workshop at the Thirty-ninth International Conference on Machine
  Learning (ICML 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Convergence of Optimistic Gradient Ascent in Network Zero-Sum
  Extensive Form Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Piliouras, Lillian Ratliff, Ryann Sim, Stratis Skoulakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of learning in games has thus far focused primarily on normal form
games. In contrast, our understanding of learning in extensive form games
(EFGs) and particularly in EFGs with many agents lags far behind, despite them
being closer in nature to many real world applications. We consider the natural
class of Network Zero-Sum Extensive Form Games, which combines the global
zero-sum property of agent payoffs, the efficient representation of graphical
games as well the expressive power of EFGs. We examine the convergence
properties of Optimistic Gradient Ascent (OGA) in these games. We prove that
the time-average behavior of such online learning dynamics exhibits $O(1/T)$
rate convergence to the set of Nash Equilibria. Moreover, we show that the
day-to-day behavior also converges to Nash with rate $O(c^{-t})$ for some
game-dependent constant $c>0$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in SAGT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outlier Explanation via Sum-Product Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outlier explanation is the task of identifying a set of features that
distinguish a sample from normal data, which is important for downstream
(human) decision-making. Existing methods are based on beam search in the space
of feature subsets. They quickly becomes computationally expensive, as they
require to run an outlier detection algorithm from scratch for each feature
subset. To alleviate this problem, we propose a novel outlier explanation
algorithm based on Sum-Product Networks (SPNs), a class of probabilistic
circuits. Our approach leverages the tractability of marginal inference in SPNs
to compute outlier scores in feature subsets. By using SPNs, it becomes
feasible to perform backwards elimination instead of the usual forward beam
search, which is less susceptible to missing relevant features in an
explanation, especially when the number of features is large. We empirically
show that our approach achieves state-of-the-art results for outlier
explanation, outperforming recent search-based as well as deep learning-based
explanation methods
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-head Cascaded Swin <span class="highlight-title">Transformer</span>s with Attention to k-space Sampling
  Pattern for Accelerated MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mevan Ekanayake, Kamlesh Pawar, Mehrtash Harandi, Gary Egan, Zhaolin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global correlations are widely seen in human anatomical structures due to
similarity across tissues and bones. These correlations are reflected in
magnetic resonance imaging (MRI) scans as a result of close-range proton
density and T1/T2 parameter. Furthermore, to achieve accelerated MRI, k-space
data are undersampled which causes global aliasing artifacts. Convolutional
neural network (CNN) models are widely utilized for accelerated MRI
reconstruction, but those models are limited in capturing global correlations
due to the intrinsic locality of the convolution operation. The
self-attention-based transformer models are capable of capturing global
correlations among image features, however, the current contributions of
transformer models for MRI reconstruction are minute. The existing
contributions mostly provide CNN-transformer hybrid solutions and rarely
leverage the physics of MRI. In this paper, we propose a physics-based
stand-alone (convolution free) transformer model titled, the Multi-head
Cascaded Swin Transformers (McSTRA) for accelerated MRI reconstruction. McSTRA
combines several interconnected MRI physics-related concepts with the
transformer networks: it exploits global MR features via the shifted window
self-attention mechanism; it extracts MR features belonging to different
spectral components separately using a multi-head setup; it iterates between
intermediate de-aliasing and k-space correction via a cascaded network with
data consistency in k-space and intermediate loss computations; furthermore, we
propose a novel positional embedding generation mechanism to guide
self-attention utilizing the point spread function corresponding to the
undersampling mask. Our model significantly outperforms state-of-the-art MRI
reconstruction methods both visually and quantitatively while depicting
improved resolution and removal of aliasing artifacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kullback-Leibler and Renyi divergences in reproducing kernel Hil<span class="highlight-title">bert</span>
  space and Gaussian process settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Ha Quang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present formulations for regularized Kullback-Leibler and
R\'enyi divergences via the Alpha Log-Determinant (Log-Det) divergences between
positive Hilbert-Schmidt operators on Hilbert spaces in two different settings,
namely (i) covariance operators and Gaussian measures defined on reproducing
kernel Hilbert spaces (RKHS); and (ii) Gaussian processes with squared
integrable sample paths. For characteristic kernels, the first setting leads to
divergences between arbitrary Borel probability measures on a complete,
separable metric space. We show that the Alpha Log-Det divergences are
continuous in the Hilbert-Schmidt norm, which enables us to apply laws of large
numbers for Hilbert space-valued random variables. As a consequence of this, we
show that, in both settings, the infinite-dimensional divergences can be
consistently and efficiently estimated from their finite-dimensional versions,
using finite-dimensional Gram matrices/Gaussian measures and finite sample
data, with {\it dimension-independent} sample complexities in all cases. RKHS
methodology plays a central role in the theoretical analysis in both settings.
The mathematical formulation is illustrated by numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>74 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Macro Placement <span class="chip">ICML2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyong Oh, Roberto Bondesan, Dana Kianfar, Rehan Ahmed, Rishubh Khurana, Payal Agarwal, Romain Lepert, Mysore Sriram, Max Welling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Macro placement is the problem of placing memory blocks on a chip canvas. It
can be formulated as a combinatorial optimization problem over sequence pairs,
a representation which describes the relative positions of macros. Solving this
problem is particularly challenging since the objective function is expensive
to evaluate. In this paper, we develop a novel approach to macro placement
using Bayesian optimization (BO) over sequence pairs. BO is a machine learning
technique that uses a probabilistic surrogate model and an acquisition function
that balances exploration and exploitation to efficiently optimize a black-box
objective function. BO is more sample-efficient than reinforcement learning and
therefore can be used with more realistic objectives. Additionally, the ability
to learn from data and adapt the algorithm to the objective function makes BO
an appealing alternative to other black-box optimization methods such as
simulated annealing, which relies on problem-dependent heuristics and
parameter-tuning. We benchmark our algorithm on the fixed-outline macro
placement problem with the half-perimeter wire length objective and demonstrate
competitive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2022 Workshop on Adaptive Experimental Design and Active Learning
  in the Real World</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for Non-IID Data via Client Variance Reduction and
  Adaptive Server Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiep Nguyen, Lam Phan, Harikrishna Warrier, Yogesh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is an emerging technique used to collaboratively
train a global machine learning model while keeping the data localized on the
user devices. The main obstacle to FL's practical implementation is the
Non-Independent and Identical (Non-IID) data distribution across users, which
slows convergence and degrades performance. To tackle this fundamental issue,
we propose a method (ComFed) that enhances the whole training process on both
the client and server sides. The key idea of ComFed is to simultaneously
utilize client-variance reduction techniques to facilitate server aggregation
and global adaptive update techniques to accelerate learning. Our experiments
on the Cifar-10 classification task show that ComFed can improve
state-of-the-art algorithms dedicated to Non-IID data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLGOPerf: An ML Guided Inliner to Optimize Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir H. Ashouri, Mostafa Elhoushi, Yuzhe Hua, Xiang Wang, Muhammad Asif Manzoor, Bryan Chan, Yaoqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the past 25 years, we have witnessed an extensive application of Machine
Learning to the Compiler space; the selection and the phase-ordering problem.
However, limited works have been upstreamed into the state-of-the-art
compilers, i.e., LLVM, to seamlessly integrate the former into the optimization
pipeline of a compiler to be readily deployed by the user. MLGO was among the
first of such projects and it only strives to reduce the code size of a binary
with an ML-based Inliner using Reinforcement Learning.
  This paper presents MLGOPerf; the first end-to-end framework capable of
optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model
to generate rewards used for training a retargeted Reinforcement learning
agent, previously used as the primary model by MLGO. It does so by predicting
the post-inlining speedup of a function under analysis and it enables a fast
training framework for the primary model which otherwise wouldn't be practical.
The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with
respect to LLVM's optimization at O3 when trained for performance on SPEC
CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach
provides up to 26% increased opportunities to autotune code regions for our
benchmarks which can be translated into an additional 3.7% speedup value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 1: The short version of this work is accepted at ACM/IEEE
  CASES 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Manifold Learning with Graph Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelong Li, Ziheng Jiao, Hongyuan Zhang, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Admittedly, Graph Convolution Network (GCN) has achieved excellent results on
graph datasets such as social networks, citation networks, etc. However,
softmax used as the decision layer in these frameworks is generally optimized
with thousands of iterations via gradient descent. Furthermore, due to ignoring
the inner distribution of the graph nodes, the decision layer might lead to an
unsatisfactory performance in semi-supervised learning with less label support.
To address the referred issues, we propose a novel graph deep model with a
non-gradient decision layer for graph mining. Firstly, manifold learning is
unified with label local-structure preservation to capture the topological
information of the nodes. Moreover, owing to the non-gradient property,
closed-form solutions is achieved to be employed as the decision layer for GCN.
Particularly, a joint optimization method is designed for this graph model,
which extremely accelerates the convergence of the model. Finally, extensive
experiments show that the proposed model has achieved state-of-the-art
performance compared to the current models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Adversarial <span class="highlight-title">Contrastive Learning</span> via Asymmetric InfoNCE <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiying Yu, Jieming Lou, Xianyuan Zhan, Qizhang Li, Wangmeng Zuo, <span class="highlight-author">Yang Liu</span>, <span class="highlight-author">Jingjing Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning (CL) has recently been applied to adversarial learning
tasks. Such practice considers adversarial samples as additional positive views
of an instance, and by maximizing their agreements with each other, yields
better adversarial robustness. However, this mechanism can be potentially
flawed, since adversarial perturbations may cause instance-level identity
confusion, which can impede CL performance by pulling together different
instances with separate identities. To address this issue, we propose to treat
adversarial samples unequally when contrasted, with an asymmetric InfoNCE
objective ($A-InfoNCE$) that allows discriminating considerations of
adversarial samples. Specifically, adversaries are viewed as inferior positives
that induce weaker learning signals, or as hard negatives exhibiting higher
contrast to other negative samples. In the asymmetric fashion, the adverse
impacts of conflicting objectives between CL and adversarial learning can be
effectively mitigated. Experiments show that our approach consistently
outperforms existing Adversarial CL methods across different finetuning schemes
without additional computational cost. The proposed A-InfoNCE is also a generic
form that can be readily extended to other CL methods. Code is available at
https://github.com/yqy2001/A-InfoNCE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. Code is available at
  https://github.com/yqy2001/A-InfoNCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Protecting Global Properties of <span class="highlight-title">Dataset</span>s with Distribution Privacy
  Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle Chen, Olga Ohrimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alongside the rapid development of data collection and analysis techniques in
recent years, there is increasingly an emphasis on the need to address
information leakage associated with such usage of data. To this end, much work
in the privacy literature is devoted to the protection of individual users and
contributors of data. However, some situations instead require a different
notion of data confidentiality involving global properties aggregated over the
records of a dataset. Such notions of information protection are particularly
applicable for business and organization data, where global properties may
reflect trade secrets, or demographic data, which can be harmful if mishandled.
Recent work on property inference attacks furthermore shows how data analysis
algorithms can be susceptible to leaking these global properties of data,
highlighting the importance of developing mechanisms that can protect such
information.
  In this work, we demonstrate how a distribution privacy framework can be
applied to formalize the problem of protecting global properties of datasets.
Given this framework, we investigate several mechanisms and their tradeoffs for
providing this notion of data confidentiality. We analyze the theoretical
protection guarantees offered by these mechanisms under various data
assumptions, then implement and empirically evaluate these mechanisms for
several data analysis tasks. The results of our experiments show that our
mechanisms can indeed reduce the effectiveness of practical property inference
attacks while providing utility substantially greater than a crude group
differential privacy baseline. Our work thus provides groundwork for
theoretically supported mechanisms for protecting global properties of
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Neural Speech Coding <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Jiang, Xiulian Peng, Huaying Xue, Yuan Zhang, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural audio/speech coding has shown its capability to deliver a high quality
at much lower bitrates than traditional methods recently. However, existing
neural audio/speech codecs employ either acoustic features or learned blind
features with a convolutional neural network for encoding, by which there are
still temporal redundancies inside encoded features. This paper introduces
latent-domain predictive coding into the VQ-VAE framework to fully remove such
redundancies and proposes the TF-Codec for low-latency neural speech coding in
an end-to-end way. Specifically, the extracted features are encoded conditioned
on a prediction from past quantized latent frames so that temporal correlations
are further removed. What's more, we introduce a learnable compression on the
time-frequency input to adaptively adjust the attention paid on main
frequencies and details at different bitrates. A differentiable vector
quantization scheme based on distance-to-soft mapping and Gumbel-Softmax is
proposed to better model the latent distributions with rate constraint.
Subjective results on multilingual speech datasets show that with a latency of
40ms, the proposed TF-Codec at 1kbps can achieve a much better quality than
Opus 9kbps and TF-Codec at 3kbps outperforms both EVS 9.6kbps and Opus 12kbps.
Numerous studies are conducted to show the effectiveness of these techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE
  PROCESSING (TASLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retweet-<span class="highlight-title">BERT</span>: Political Leaning Detection Using Language Features and
  Information Diffusion on Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julie Jiang, Xiang Ren, Emilio Ferrara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the political leanings of social media users is a challenging and
ever more pressing problem given the increase in social media consumption. We
introduce Retweet-BERT, a simple and scalable model to estimate the political
leanings of Twitter users. Retweet-BERT leverages the retweet network structure
and the language used in users' profile descriptions. Our assumptions stem from
patterns of networks and linguistics homophily among people who share similar
ideologies. Retweet-BERT demonstrates competitive performance against other
state-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitter
datasets (a COVID-19 dataset and a 2020 United States presidential elections
dataset). We also perform manual validation to validate the performance of
Retweet-BERT on users not in the training data. Finally, in a case study of
COVID-19, we illustrate the presence of political echo chambers on Twitter and
show that it exists primarily among right-leaning users. Our code is
open-sourced and our data is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:2103.10979</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Private Convex Optimization in General Norms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, Kevin Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new framework for differentially private optimization of convex
functions which are Lipschitz in an arbitrary norm $\normx{\cdot}$. Our
algorithms are based on a regularized exponential mechanism which samples from
the density $\propto \exp(-k(F+\mu r))$ where $F$ is the empirical loss and $r$
is a regularizer which is strongly convex with respect to $\normx{\cdot}$,
generalizing a recent work of \cite{GLL22} to non-Euclidean settings. We show
that this mechanism satisfies Gaussian differential privacy and solves both
DP-ERM (empirical risk minimization) and DP-SCO (stochastic convex
optimization), by using localization tools from convex geometry. Our framework
is the first to apply to private convex optimization in general normed spaces,
and directly recovers non-private SCO rates achieved by mirror descent, as the
privacy parameter $\eps \to \infty$. As applications, for Lipschitz
optimization in $\ell_p$ norms for all $p \in (1, 2)$, we obtain the first
optimal privacy-utility tradeoffs; for $p = 1$, we improve tradeoffs obtained
by the recent works \cite{AsiFKT21, BassilyGN21} by at least a logarithmic
factor. Our $\ell_p$ norm and Schatten-$p$ norm optimization frameworks are
complemented with polynomial-time samplers whose query complexity we explicitly
bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Few Expert Queries Suffices for Sample-Efficient RL with Resets and
  Linear Value Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Amortila, Nan Jiang, Dhruv Madeka, Dean P. Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current paper studies sample-efficient Reinforcement Learning (RL) in
settings where only the optimal value function is assumed to be
linearly-realizable. It has recently been understood that, even under this
seemingly strong assumption and access to a generative model, worst-case sample
complexities can be prohibitively (i.e., exponentially) large. We investigate
the setting where the learner additionally has access to interactive
demonstrations from an expert policy, and we present a statistically and
computationally efficient algorithm (Delphi) for blending exploration with
expert queries. In particular, Delphi requires $\tilde{\mathcal{O}}(d)$ expert
queries and a $\texttt{poly}(d,H,|\mathcal{A}|,1/\varepsilon)$ amount of
exploratory samples to provably recover an $\varepsilon$-suboptimal policy.
Compared to pure RL approaches, this corresponds to an exponential improvement
in sample complexity with surprisingly-little expert input. Compared to prior
imitation learning (IL) approaches, our required number of expert
demonstrations is independent of $H$ and logarithmic in $1/\varepsilon$,
whereas all prior work required at least linear factors of both in addition to
the same dependence on $d$. Towards establishing the minimal amount of expert
queries needed, we show that, in the same setting, any learner whose
exploration budget is polynomially-bounded (in terms of $d,H,$ and
$|\mathcal{A}|$) will require at least $\tilde\Omega(\sqrt{d})$ oracle calls to
recover a policy competing with the expert's value function. Under the weaker
assumption that the expert's policy is linear, we show that the lower bound
increases to $\tilde\Omega(d)$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Fair Classification with Mostly Private Sensitive Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canyu Chen, Yueqing Liang, Xiongxiao Xu, Shangyu Xie, Yuan Hong, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have demonstrated promising performance in many
areas. However, the concerns that they can be biased against specific groups
hinder their adoption in high-stake applications. Thus it is essential to
ensure fairness in machine learning models. Most of the previous efforts
require access to sensitive attributes for mitigating bias. Nonetheless, it is
often infeasible to obtain large scale of data with sensitive attributes due to
people's increasing awareness of privacy and the legal compliance. Therefore,
an important research question is how to make fair predictions under privacy?
In this paper, we study a novel problem on fair classification in a
semi-private setting, where most of the sensitive attributes are private and
only a small amount of clean sensitive attributes are available. To this end,
we propose a novel framework FairSP that can first learn to correct the noisy
sensitive attributes under privacy guarantee via exploiting the limited clean
sensitive attributes. Then, it jointly models the corrected and clean data in
an adversarial way for debiasing and prediction. Theoretical analysis shows
that the proposed model can ensure fairness when most of the sensitive
attributes are private. Experimental results on real-world datasets demonstrate
the effectiveness of the proposed model for making fair predictions under
privacy and maintaining high accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Amortized Noisy Channel Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Yuanzhe Pang, He He, <span class="highlight-author">Kyunghyun Cho</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noisy channel models have been especially effective in neural machine
translation (NMT). However, recent approaches like "beam search and rerank"
(BSR) incur significant computation overhead during inference, making
real-world application infeasible. We aim to study if it is possible to build
an amortized noisy channel NMT model such that when we do greedy decoding
during inference, the translation accuracy matches that of BSR in terms of
reward (based on the source-to-target log probability and the target-to-source
log probability) and quality (based on BLEU and BLEURT). We attempt three
approaches to train the new model: knowledge distillation, one-step-deviation
imitation learning, and Q learning. The first approach obtains the noisy
channel signal from a pseudo-corpus, and the latter two approaches aim to
optimize toward a noisy-channel MT reward directly. For all three approaches,
the generated translations fail to achieve rewards comparable to BSR, but the
translation quality approximated by BLEU and BLEURT is similar to the quality
of BSR-produced translations. Additionally, all three approaches speed up
inference by 1-2 orders of magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INLG 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly Robust Distributionally Robust Off-Policy Evaluation and Learning <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Kallus, Xiaojie Mao, Kaiwen Wang, Zhengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-policy evaluation and learning (OPE/L) use offline observational data to
make better decisions, which is crucial in applications where online
experimentation is limited. However, depending entirely on logged data, OPE/L
is sensitive to environment distribution shifts -- discrepancies between the
data-generating environment and that where policies are deployed.
\citet{si2020distributional} proposed distributionally robust OPE/L (DROPE/L)
to address this, but the proposal relies on inverse-propensity weighting, whose
estimation error and regret will deteriorate if propensities are
nonparametrically estimated and whose variance is suboptimal even if not. For
standard, non-robust, OPE/L, this is solved by doubly robust (DR) methods, but
they do not naturally extend to the more complex DROPE/L, which involves a
worst-case expectation. In this paper, we propose the first DR algorithms for
DROPE/L with KL-divergence uncertainty sets. For evaluation, we propose
Localized Doubly Robust DROPE (LDR$^2$OPE) and show that it achieves
semiparametric efficiency under weak product rates conditions. Thanks to a
localization technique, LDR$^2$OPE only requires fitting a small number of
regressions, just like DR methods for standard OPE. For learning, we propose
Continuum Doubly Robust DROPL (CDR$^2$OPL) and show that, under a product rate
condition involving a continuum of regressions, it enjoys a fast regret rate of
$\mathcal{O}\left(N^{-1/2}\right)$ even when unknown propensities are
nonparametrically estimated. We empirically validate our algorithms in
simulations and further extend our results to general $f$-divergence
uncertainty sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short Talk at ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking Deeper into Tabular LIME 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.11092v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.11092v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien Garreau, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a thorough theoretical analysis of the default
implementation of LIME in the case of tabular data. We prove that in the large
sample limit, the interpretable coefficients provided by Tabular LIME can be
computed in an explicit way as a function of the algorithm parameters and some
expectation computations related to the black-box model. When the function to
explain has some nice algebraic structure (linear, multiplicative, or sparsely
depending on a subset of the coordinates), our analysis provides interesting
insights into the explanations provided by LIME. These can be applied to a
range of machine learning models including Gaussian kernels or CART random
forests. As an example, for linear functions we show that LIME has the
desirable property to provide explanations that are proportional to the
coefficients of the function to explain and to ignore coordinates that are not
used by the function to explain. For partition-based regressors, on the other
side, we show that LIME produces undesired artifacts that may provide
misleading explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>69 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved optimization strategies for deep Multi-Task Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.11678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.11678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Multi-Task Learning (MTL), it is a common practice to train multi-task
networks by optimizing an objective function, which is a weighted average of
the task-specific objective functions. Although the computational advantages of
this strategy are clear, the complexity of the resulting loss landscape has not
been studied in the literature. Arguably, its optimization may be more
difficult than a separate optimization of the constituting task-specific
objectives. In this work, we investigate the benefits of such an alternative,
by alternating independent gradient descent steps on the different
task-specific objective functions and we formulate a novel way to combine this
approach with state-of-the-art optimizers. As the separation of task-specific
objectives comes at the cost of increased computational time, we propose a
random task grouping as a trade-off between better optimization and
computational efficiency. Experimental results over three well-known visual MTL
datasets show better overall absolute performance on losses and standard
metrics compared to an averaged objective function and other state-of-the-art
MTL methods. In particular, our method shows the most benefits when dealing
with tasks of different nature and it enables a wider exploration of the shared
parameter space. We also show that our random grouping strategy allows to
trade-off between these benefits and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Discretization by Two-dimensional MDL-based Histogram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.01893v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.01893v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lincen Yang, Mitra Baratchi, Matthijs van Leeuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised discretization is a crucial step in many knowledge discovery
tasks. The state-of-the-art method for one-dimensional data infers locally
adaptive histograms using the minimum description length (MDL) principle, but
the multi-dimensional case is far less studied: current methods consider the
dimensions one at a time (if not independently), which result in
discretizations based on rectangular cells of adaptive size. Unfortunately,
this approach is unable to adequately characterize dependencies among
dimensions and/or results in discretizations consisting of more cells (or bins)
than is desirable.
  To address this problem, we propose an expressive model class that allows for
far more flexible partitions of two-dimensional data. We extend the state of
the art for the one-dimensional case to obtain a model selection problem based
on the normalized maximum likelihood, a form of refined MDL. As the flexibility
of our model class comes at the cost of a vast search space, we introduce a
heuristic algorithm, named PALM, which Partitions each dimension ALternately
and then Merges neighboring regions, all using the MDL principle. Experiments
on synthetic data show that PALM 1) accurately reveals ground truth partitions
that are within the model class (i.e., the search space), given a large enough
sample size; 2) approximates well a wide range of partitions outside the model
class; 3) converges, in contrast to the state-of-the-art multivariate
discretization method IPD. Finally, we apply our algorithm to three spatial
datasets, and we demonstrate that, compared to kernel density estimation (KDE),
our algorithm not only reveals more detailed density changes, but also fits
unseen data better, as measured by the log-likelihood.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>revision submitted to springer machine learning journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRVNet: A Novel Partially-Regularized Variational Autoencoders for
  Massive MIMO CSI Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.04178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.04178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Hussien, Kim Khoa Nguyen, Mohamed Cheriet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a multiple-input multiple-output frequency-division duplexing (MIMO-FDD)
system, the user equipment (UE) sends the downlink channel state information
(CSI) to the base station to report link status. Due to the complexity of MIMO
systems, the overhead incurred in sending this information negatively affects
the system bandwidth. Although this problem has been widely considered in the
literature, prior work generally assumes an ideal feedback channel. In this
paper, we introduce PRVNet, a neural network architecture inspired by
variational autoencoders (VAE) to compress the CSI matrix before sending it
back to the base station under noisy channel conditions. Moreover, we propose a
customized loss function that best suits the special characteristics of the
problem being addressed. We also introduce an additional regularization
hyperparameter for the learning objective, which is crucial for achieving
competitive performance. In addition, we provide an efficient way to tune this
hyperparameter using KL-annealing. Experimental results show the proposed model
outperforms the benchmark models including two deep learning-based models in a
noise-free feedback channel assumption. In addition, the proposed model
achieves an outstanding performance under different noise levels for additive
white Gaussian noise feedback channels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FL_PyTorch: optimization research simulator for federated learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Burlachenko, Samuel Horváth, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a promising technique for edge devices
to collaboratively learn a shared machine learning model while keeping training
data locally on the device, thereby removing the need to store and access the
full data in the cloud. However, FL is difficult to implement, test and deploy
in practice considering heterogeneity in common edge device settings, making it
fundamentally hard for researchers to efficiently prototype and test their
optimization algorithms. In this work, our aim is to alleviate this problem by
introducing FL_PyTorch : a suite of open-source software written in python that
builds on top of one the most popular research Deep Learning (DL) framework
PyTorch. We built FL_PyTorch as a research simulator for FL to enable fast
development, prototyping and experimenting with new and existing FL
optimization algorithms. Our system supports abstractions that provide
researchers with a sufficient level of flexibility to experiment with existing
and novel approaches to advance the state-of-the-art. Furthermore, FL_PyTorch
is a simple to use console system, allows to run several clients simultaneously
using local CPUs or GPU(s), and even remote compute devices without the need
for any distributed implementation provided by the user. FL_PyTorch also offers
a Graphical User Interface. For new methods, researchers only provide the
centralized implementation of their algorithm. To showcase the possibilities
and usefulness of our system, we experiment with several well-known
state-of-the-art FL algorithms and a few of the most common FL datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DistributedML '21: Proceedings of the 2nd ACM International Workshop
  on Distributed Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe reinforcement learning for multi-energy management systems with
  known constraint functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Glenn Ceusters, Luis Ramirez Camargo, Rüdiger Franke, Ann Nowé, Maarten Messagie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) is a promising optimal control technique for
multi-energy management systems. It does not require a model a priori -
reducing the upfront and ongoing project-specific engineering effort and is
capable of learning better representations of the underlying system dynamics.
However, vanilla RL does not provide constraint satisfaction guarantees -
resulting in various potentially unsafe interactions within its safety-critical
environment. In this paper, we present two novel safe RL methods, namely
SafeFallback and GiveSafe, where the safety constraint formulation is decoupled
from the RL formulation and which provides hard-constraint satisfaction
guarantees both during training a (near) optimal policy (which involves
exploratory and exploitative, i.e. greedy, steps) as well as during deployment
of any policy (e.g. random agents or offline trained RL agents). In a simulated
multi-energy systems case study we have shown that both methods start with a
significantly higher utility (i.e. useful policy) compared to a vanilla RL
benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed
SafeFallback method even can outperform the vanilla RL benchmark (102,9% to
100%). We conclude that both methods are viably safety constraint handling
techniques applicable beyond RL, as demonstrated with random policies while
still providing hard-constraint guarantees. Finally, we propose directions for
future work to i.a. improve the constraint functions itself as more data
becomes available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge
  <span class="highlight-title">Distillation</span> <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.05892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.05892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Oh, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As segmentation labels are scarce, extensive researches have been conducted
to train segmentation networks with domain adaptation, semi-supervised or
self-supervised learning techniques to utilize abundant unlabeled dataset.
However, these approaches appear different from each other, so it is not clear
how these approaches can be combined for better performance. Inspired by recent
multi-domain image translation approaches, here we propose a novel segmentation
framework using adaptive instance normalization (AdaIN), so that a single
generator is trained to perform both domain adaptation and semi-supervised
segmentation tasks via knowledge distillation by simply changing task-specific
AdaIN codes. Specifically, our framework is designed to deal with difficult
situations in chest X-ray radiograph (CXR) segmentation, where labels are only
available for normal data, but trained model should be applied to both normal
and abnormal data. The proposed network demonstrates great generalizability
under domain shift and achieves the state-of-the-art performance for abnormal
CXR segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn-Morph-Infer: a new way of solving the inverse problem for brain
  tumor modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Ezhov, Kevin Scibilia, Katharina Franitza, Felix Steinbauer, Suprosanna Shit, Lucas Zimmer, Jana Lipkova, Florian Kofler, Johannes Paetzold, Luca Canalini, Diana Waldmannstetter, Martin Menten, Marie Metz, Benedikt Wiestler, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current treatment planning of patients diagnosed with a brain tumor, such as
glioma, could significantly benefit by accessing the spatial distribution of
tumor cell concentration. Existing diagnostic modalities, e.g. magnetic
resonance imaging (MRI), contrast sufficiently well areas of high cell density.
In gliomas, however, they do not portray areas of low cell concentration, which
can often serve as a source for the secondary appearance of the tumor after
treatment. To estimate tumor cell densities beyond the visible boundaries of
the lesion, numerical simulations of tumor growth could complement imaging
information by providing estimates of full spatial distributions of tumor
cells. Over recent years a corpus of literature on medical image-based tumor
modeling was published. It includes different mathematical formalisms
describing the forward tumor growth model. Alongside, various parametric
inference schemes were developed to perform an efficient tumor model
personalization, i.e. solving the inverse problem. However, the unifying
drawback of all existing approaches is the time complexity of the model
personalization which prohibits a potential integration of the modeling into
clinical settings. In this work, we introduce a deep learning based methodology
for inferring the patient-specific spatial distribution of brain tumors from
T1Gd and FLAIR MRI medical scans. Coined as Learn-Morph-Infer the method
achieves real-time performance in the order of minutes on widely available
hardware and the compute time is stable across tumor models of different
complexity, such as reaction-diffusion and reaction-advection-diffusion models.
We believe the proposed inverse solution approach not only bridges the way for
clinical translation of brain tumor personalization but can also be adopted to
other scientific and engineering domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction-Guided <span class="highlight-title">Distillation</span> for Dense Object Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Mateusz Ochal, Amos Storkey, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world object detection models should be cheap and accurate. Knowledge
distillation (KD) can boost the accuracy of a small, cheap detection model by
leveraging useful information from a larger teacher model. However, a key
challenge is identifying the most informative features produced by the teacher
for distillation. In this work, we show that only a very small fraction of
features within a ground-truth bounding box are responsible for a teacher's
high detection performance. Based on this, we propose Prediction-Guided
Distillation (PGD), which focuses distillation on these key predictive regions
of the teacher and yields considerable gains in performance over many existing
KD baselines. In addition, we propose an adaptive weighting scheme over the key
regions to smooth out their influence and achieve even better performance. Our
proposed approach outperforms current state-of-the-art KD baselines on a
variety of advanced one-stage detection architectures. Specifically, on the
COCO dataset, our method achieves between +3.1% and +4.6% AP improvement using
ResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On
the CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,
also using these backbones. Our code is available at
https://github.com/ChenhongyiYang/PGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CANF-VC: Conditional Augmented Normalizing Flows for Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Han Ho, Chih-Peng Chang, Peng-Yu Chen, Alessandro Gnutti, Wen-Hsiao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end learning-based video compression system,
termed CANF-VC, based on conditional augmented normalizing flows (CANF). Most
learned video compression systems adopt the same hybrid-based coding
architecture as the traditional codecs. Recent research on conditional coding
has shown the sub-optimality of the hybrid-based coding and opens up
opportunities for deep generative models to take a key role in creating new
coding frameworks. CANF-VC represents a new attempt that leverages the
conditional ANF to learn a video generative model for conditional inter-frame
coding. We choose ANF because it is a special type of generative model, which
includes variational autoencoder as a special case and is able to achieve
better expressiveness. CANF-VC also extends the idea of conditional coding to
motion coding, forming a purely conditional coding framework. Extensive
experimental results on commonly used datasets confirm the superiority of
CANF-VC to the state-of-the-art methods. The source code of CANF-VC is
available at https://github.com/NYCU-MAPL/CANF-VC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computer Vision <span class="highlight-title">Self-supervised</span> Learning Methods on Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.00783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.00783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daesoo Lee, Erlend Aune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has had great success in both computer vision
and natural language processing. These approaches often rely on cleverly
crafted loss functions and training setups to avoid feature collapse. In this
study, the effectiveness of mainstream SSL frameworks from computer vision and
some SSL frameworks for time series are evaluated on the UCR, UEA and PTB-XL
datasets, and we show that computer vision SSL frameworks can be effective for
time series. In addition, we propose a new method that improves on the recently
proposed VICReg method. Our method improves on a \textit{covariance} term
proposed in VICReg, and in addition we augment the head of the architecture by
an IterNorm layer that accelerates the convergence of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On characterizations of learnability with computable learners <span class="chip">COLT 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom F. Sterkenburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study computable PAC (CPAC) learning as introduced by Agarwal et al.
(2020). First, we consider the main open question of finding characterizations
of proper and improper CPAC learning. We give a characterization of a closely
related notion of strong CPAC learning, and provide a negative answer to the
COLT open problem posed by Agarwal et al. (2021) whether all decidably
representable VC classes are improperly CPAC learnable. Second, we consider
undecidability of (computable) PAC learnability. We give a simple general
argument to exhibit such ndecidability, and initiate a study of the
arithmetical complexity of learnability. We briefly discuss the relation to the
undecidability result of Ben-David et al. (2019), that motivated the work of
Agarwal et al.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version, as accepted for COLT 2022. Changes w.r.t. previous
  arXiv version: In response to reviewer comments, major revision of the
  discussion in Section 3.2, incl. reformulation of Question 2 (that in its
  original form had an easy answer) and retraction of Definition 6 and
  Proposition 2 (that lost their relevance in the revised discussion). Minor
  textual revisions elsewhere</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the difficulty of learning chaotic dynamics with RNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas M. Mikhaeil, Zahra Monfared, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs) are wide-spread machine learning tools for
modeling sequential and time series data. They are notoriously hard to train
because their loss gradients backpropagated in time tend to saturate or diverge
during training. This is known as the exploding and vanishing gradient problem.
Previous solutions to this issue either built on rather complicated,
purpose-engineered architectures with gated memory buffers, or - more recently
- imposed constraints that ensure convergence to a fixed point or restrict (the
eigenspectrum of) the recurrence matrix. Such constraints, however, convey
severe limitations on the expressivity of the RNN. Essential intrinsic dynamics
such as multistability or chaos are disabled. This is inherently at disaccord
with the chaotic nature of many, if not most, time series encountered in nature
and society. It is particularly problematic in scientific applications where
one aims to reconstruct the underlying dynamical system. Here we offer a
comprehensive theoretical treatment of this problem by relating the loss
gradients during RNN training to the Lyapunov spectrum of RNN-generated orbits.
We mathematically prove that RNNs producing stable equilibrium or cyclic
behavior have bounded gradients, whereas the gradients of RNNs with chaotic
dynamics always diverge. Based on these analyses and insights we suggest ways
of how to optimize the training process on chaotic data according to the
system's Lyapunov spectrum, regardless of the employed RNN architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple-Modality Associative Memory: a framework for Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Simas, Luis Sa-Couto, Andreas Wichert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing from memory the face of a friend you have not seen in years is a
difficult task. However, if you happen to cross paths, you would easily
recognize each other. The biological memory is equipped with an impressive
compression algorithm that can store the essential, and then infer the details
to match perception. Willshaw's model of Associative memory is a likely
candidate for a computational model of this brain function, but its application
on real-world data is hindered by the so-called Sparse Coding Problem. Due to a
recently proposed sparse encoding prescription [31], which maps visual patterns
into binary feature maps, we were able to analyze the behavior of the Willshaw
Network (WN) on real-world data and gain key insights into the strengths of the
model. To further enhance the capabilities of the WN, we propose the
Multiple-Modality architecture. In this new setting, the memory stores several
modalities (e.g., visual, or textual) simultaneously. After training, the model
can be used to infer missing modalities when just a subset is perceived, thus
serving as a flexible framework for learning tasks. We evaluated the model on
the MNIST dataset. By storing both the images and labels as modalities, we were
able to successfully perform pattern completion, classification, and generation
with a single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QSAN: A Near-term Achievable Quantum Self-Attention Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren-xin Zhao, Jinjing Shi, Shichao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Attention Mechanism (SAM), an important component of machine learning,
has been relatively little investigated in the field of quantum machine
learning. Inspired by the Variational Quantum Algorithm (VQA) framework and
SAM, Quantum Self-Attention Network (QSAN) that can be implemented on a
near-term quantum computer is proposed.Theoretically, Quantum Self-Attention
Mechanism (QSAM), a novel interpretation of SAM with linearization and
logicalization is defined, in which Quantum Logical Similarity (QLS) is
presented firstly to impel a better execution of QSAM on quantum computers
since inner product operations are replaced with logical operations, and then a
QLS-based density matrix named Quantum Bit Self-Attention Score Matrix (QBSASM)
is deduced for representing the output distribution effectively. Moreover, QSAN
is implemented based on the QSAM framework and its practical quantum circuit is
designed with 5 modules. Finally, QSAN is tested on a quantum computer with a
small sample of data. The experimental results show that QSAN can converge
faster in the quantum natural gradient descent framework and reassign weights
to word vectors. The above illustrates that QSAN is able to provide attention
with quantum characteristics faster, laying the foundation for Quantum Natural
Language Processing (QNLP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Selection, Adaptation, and Combination for Transfer Learning in
  Wind and Photovoltaic Power Forecasts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens Schreiber, Bernhard Sick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is recent interest in using model hubs, a collection of pre-trained
models, in computer vision tasks. To utilize the model hub, we first select a
source model and then adapt the model for the target to compensate for
differences. While there is yet limited research on model selection and
adaption for computer vision tasks, this holds even more for the field of
renewable power. At the same time, it is a crucial challenge to provide
forecasts for the increasing demand for power forecasts based on weather
features from a numerical weather prediction. We close these gaps by conducting
the first thorough experiment for model selection and adaptation for transfer
learning in renewable power forecast, adopting recent results from the field of
computer vision on 667 wind and photovoltaic parks. To the best of our
knowledge, this makes it the most extensive study for transfer learning in
renewable power forecasts reducing the computational effort and improving the
forecast error. Therefore, we adopt source models based on target data from
different seasons and limit the amount of training data. As an extension of the
current state of the art, we utilize a Bayesian linear regression for
forecasting the response based on features extracted from a neural network.
This approach outperforms the baseline with only seven days of training data.
We further show how combining multiple models through ensembles can
significantly improve the model selection and adaptation approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A robust approach for deep neural networks in presence of label noise:
  relabelling and filtering instances during training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03748v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03748v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anabel Gómez-Ríos, Julián Luengo, Francisco Herrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has outperformed other machine learning algorithms in a variety
of tasks, and as a result, it is widely used. However, like other machine
learning algorithms, deep learning, and convolutional neural networks (CNNs) in
particular, perform worse when the data sets present label noise. Therefore, it
is important to develop algorithms that help the training of deep networks and
their generalization to noise-free test sets. In this paper, we propose a
robust training strategy against label noise, called RAFNI, that can be used
with any CNN. This algorithm filters and relabels instances of the training set
based on the predictions and their probabilities made by the backbone neural
network during the training process. That way, this algorithm improves the
generalization ability of the CNN on its own. RAFNI consists of three
mechanisms: two mechanisms that filter instances and one mechanism that
relabels instances. In addition, it does not suppose that the noise rate is
known nor does it need to be estimated. We evaluated our algorithm using
different data sets of several sizes and characteristics. We also compared it
with state-of-the-art models using the CIFAR10 and CIFAR100 benchmarks under
different types and rates of label noise and found that RAFNI achieves better
results in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Benchmark <span class="highlight-title">dataset</span> for predictive maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05466v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05466v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Veloso, João Gama, Rita P. Ribeiro, Pedro M. Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper describes the MetroPT data set, an outcome of a eXplainable
Predictive Maintenance (XPM) project with an urban metro public transportation
service in Porto, Portugal. The data was collected in 2022 that aimed to
evaluate machine learning methods for online anomaly detection and failure
prediction. By capturing several analogic sensor signals (pressure,
temperature, current consumption), digital signals (control signals, discrete
signals), and GPS information (latitude, longitude, and speed), we provide a
dataset that can be easily used to evaluate online machine learning methods.
This dataset contains some interesting characteristics and can be a good
benchmark for predictive maintenance models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truly Unordered Probabilistic Rule Sets for Multi-class Classification <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lincen Yang, Matthijs van Leeuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rule set learning has long been studied and has recently been frequently
revisited due to the need for interpretable models. Still, existing methods
have several shortcomings: 1) most recent methods require a binary feature
matrix as input, while learning rules directly from numeric variables is
understudied; 2) existing methods impose orders among rules, either explicitly
or implicitly, which harms interpretability; and 3) currently no method exists
for learning probabilistic rule sets for multi-class target variables (there is
only one for probabilistic rule lists).
  We propose TURS, for Truly Unordered Rule Sets, which addresses these
shortcomings. We first formalize the problem of learning truly unordered rule
sets. To resolve conflicts caused by overlapping rules, i.e., instances covered
by multiple rules, we propose a novel approach that exploits the probabilistic
properties of our rule sets. We next develop a two-phase heuristic algorithm
that learns rule sets by carefully growing rules. An important innovation is
that we use a surrogate score to take the global potential of the rule set into
account when learning a local rule.
  Finally, we empirically demonstrate that, compared to non-probabilistic and
(explicitly or implicitly) ordered state-of-the-art methods, our method learns
rule sets that not only have better interpretability but also better predictive
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ECMLPKDD 2022, with Supplementary Materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Convolutional Neural Networks in the Frequency Domain <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06718v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06718v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyue Pan, Yixin Chen, Xin Niu, Wenbo Zhou, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Secure Quantized Training for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.00501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.00501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Keller, Ke Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We implement training of neural networks in secure multi-party computation
(MPC) using quantization commonly used in said setting. We are the first to
present an MNIST classifier purely trained in MPC that comes within 0.2 percent
of the accuracy of the same convolutional neural network trained via plaintext
computation. More concretely, we have trained a network with two convolutional
and two dense layers to 99.2% accuracy in 3.5 hours (under one hour for 99%
accuracy). We have also implemented AlexNet for CIFAR-10, which converges in a
few hours. We develop novel protocols for exponentiation and inverse square
root. Finally, we present experiments in a range of MPC security models for up
to ten parties, both with honest and dishonest majority as well as semi-honest
and malicious security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic
  Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsung-Han Wu, Yi-Syuan Liou, Shao-Ji Yuan, Hsin-Ying Lee, Tung-I Chen, Kuan-Chih Huang, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present D2ADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate labeling efficiency, we design a
dynamic scheduling policy to adjust the labeling budgets between domain
exploration and model uncertainty over time. Extensive experiments show that
our method outperforms existing active learning and domain adaptation baselines
on two benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than
5% target domain annotations, our method reaches comparable results with that
of full supervision. Our code is publicly available at
https://github.com/tsunghan-wu/D2ADA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022. The code is available at
  https://github.com/tsunghan-wu/D2ADA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion Intensity and its Control for Emotional Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, Berrak Sisman, Rajib Rana, Björn W. Schuller, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Affective Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Variable Bayesian Optimization with Frequency Modulated Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyong Oh, Efstratios Gavves, Max Welling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sample efficiency of Bayesian optimization(BO) is often boosted by
Gaussian Process(GP) surrogate models. However, on mixed variable spaces,
surrogate models other than GPs are prevalent, mainly due to the lack of
kernels which can model complex dependencies across different types of
variables. In this paper, we propose the frequency modulated (FM) kernel
flexibly modeling dependencies among different types of variables, so that BO
can enjoy the further improved sample efficiency. The FM kernel uses distances
on continuous variables to modulate the graph Fourier spectrum derived from
discrete variables. However, the frequency modulation does not always define a
kernel with the similarity measure behavior which returns higher values for
pairs of more similar points. Therefore, we specify and prove conditions for FM
kernels to be positive definite and to exhibit the similarity measure behavior.
In experiments, we demonstrate the improved sample efficiency of GP BO using FM
kernels (BO-FM).On synthetic problems and hyperparameter optimization problems,
BO-FM outperforms competitors consistently. Also, the importance of the
frequency modulation principle is empirically demonstrated on the same
problems. On joint optimization of neural architectures and SGD
hyperparameters, BO-FM outperforms competitors including Regularized
evolution(RE) and BOHB. Remarkably, BO-FM performs better even than RE and BOHB
using three times as many evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Thirty-Seventh Conference on Uncertainty in
  Artificial Intelligence, PMLR 161:950-960, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Causal Structure in Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.03906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.03906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Baumann, Friedrich Solowjow, Karl H. Johansson, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical models are fundamental building blocks in the design of
dynamical control systems. As control systems are becoming increasingly complex
and networked, approaches for obtaining such models based on first principles
reach their limits. Data-driven methods provide an alternative. However,
without structural knowledge, these methods are prone to finding spurious
correlations in the training data, which can hamper generalization capabilities
of the obtained models. This can significantly lower control and prediction
performance when the system is exposed to unknown situations. A preceding
causal identification can prevent this pitfall. In this paper, we propose a
method that identifies the causal structure of control systems. We design
experiments based on the concept of controllability, which provides a
systematic way to compute input trajectories that steer the system to specific
regions in its state space. We then analyze the resulting data leveraging
powerful techniques from causal inference and extend them to control systems.
Further, we derive conditions that guarantee the discovery of the true causal
structure of the system. Experiments on a robot arm demonstrate reliable causal
identification from real-world data and enhanced generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted final versions to appear in the Transactions on Machine
  Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedNI: Federated Graph Learning with Network Inpainting for
  Population-Based Disease Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Peng, Nan Wang, Nicha Dvornek, Xiaofeng Zhu, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Neural Networks (GCNs) are widely used for graph
analysis. Specifically, in medical applications, GCNs can be used for disease
prediction on a population graph, where graph nodes represent individuals and
edges represent individual similarities. However, GCNs rely on a vast amount of
data, which is challenging to collect for a single medical institution. In
addition, a critical challenge that most medical institutions continue to face
is addressing disease prediction in isolation with incomplete data information.
To address these issues, Federated Learning (FL) allows isolated local
institutions to collaboratively train a global model without data sharing. In
this work, we propose a framework, FedNI, to leverage network inpainting and
inter-institutional data via FL. Specifically, we first federatively train
missing node and edge predictor using a graph generative adversarial network
(GAN) to complete the missing information of local networks. Then we train a
global GCN node classifier across institutions using a federated graph learning
platform. The novel design enables us to build more accurate machine learning
models by leveraging federated learning and also graph learning approaches. We
demonstrate that our federated model outperforms local and baseline FL methods
with significant margins on two public neuroimaging datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Minimization with Performative Feedback <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meena Jagadeesan, Tijana Zrnic, Celestine Mendler-Dünner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In performative prediction, the deployment of a predictive model triggers a
shift in the data distribution. As these shifts are typically unknown ahead of
time, the learner needs to deploy a model to get feedback about the
distribution it induces. We study the problem of finding near-optimal models
under performativity while maintaining low regret. On the surface, this problem
might seem equivalent to a bandit problem. However, it exhibits a fundamentally
richer feedback structure that we refer to as performative feedback: after
every deployment, the learner receives samples from the shifted distribution
rather than only bandit feedback about the reward. Our main contribution is an
algorithm that achieves regret bounds scaling only with the complexity of the
distribution shifts and not that of the reward function. The algorithm only
relies on smoothness of the shifts and does not assume convexity. Moreover, its
final iterate is guaranteed to be near-optimal. The key algorithmic idea is
careful exploration of the distribution shifts that informs a novel
construction of confidence bounds on the risk of unexplored models. More
broadly, our work establishes a conceptual approach for leveraging tools from
the bandits literature for the purpose of regret minimization with performative
feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelfReformer: Self-Refined Network with <span class="highlight-title">Transformer</span> for Salient Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11283v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11283v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ke Yun, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global and local contexts significantly contribute to the integrity of
predictions in Salient Object Detection (SOD). Unfortunately, existing methods
still struggle to generate complete predictions with fine details. There are
two major problems in conventional approaches: first, for global context,
high-level CNN-based encoder features cannot effectively catch long-range
dependencies, resulting in incomplete predictions. Second, downsampling the
ground truth to fit the size of predictions will introduce inaccuracy as the
ground truth details are lost during interpolation or pooling. Thus, in this
work, we developed a Transformer-based network and framed a supervised task for
a branch to learn the global context information explicitly. Besides, we adopt
Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the
size of ground truth instead of the reverse. Thus details in the ground truth
are untouched. In addition, we developed a two-stage Context Refinement Module
(CRM) to fuse global context and automatically locate and refine the local
details in the predictions. The proposed network can guide and correct itself
based on the global and local context generated, thus is named, Self-Refined
Transformer (SelfReformer). Extensive experiments and evaluation results on
five benchmark datasets demonstrate the outstanding performance of the network,
and we achieved the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Posterior Regularization on Bayesian Hierarchical Mixture Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.06903v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.06903v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weipeng Huang, Tin Lok James Ng, Nishma Laitonjam, Neil J. Hurley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian hierarchical mixture clustering (BHMC) improves on the traditional
Bayesian hierarchical clustering by, with regard to the parent-to-child
diffusion in the generative process, replacing the conventional
Gaussian-to-Gaussian (G2G) kernels with a Hierarchical Dirichlet Process
Mixture Model (HDPMM). However, the drawback of the BHMC lies in the
possibility of obtaining trees with comparatively high nodal variance in the
higher levels (i.e., those closer to the root node). This can be interpreted as
that the separation between the nodes, particularly those in the higher levels,
might be weak. We attempt to overcome this drawback through a recent
inferential framework named posterior regularization, which facilitates a
simple manner to impose extra constraints on a Bayesian model to address its
weakness. To enhance the separation of clusters, we apply posterior
regularization to impose max-margin constraints on the nodes at every level of
the hierarchy. In this paper, we illustrate the modeling detail of applying the
PR on BHMC and show that this solution achieves the desired improvements over
the BHMC model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning for Microcontroller-Class Hardware -- A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14550v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14550v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swapnil Sayan Saha, Sandeep Singh Sandha, Mani Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements in machine learning opened a new opportunity to bring
intelligence to the low-end Internet-of-Things nodes such as microcontrollers.
Conventional machine learning deployment has high memory and compute footprint
hindering their direct deployment on ultra resource-constrained
microcontrollers. This paper highlights the unique requirements of enabling
onboard machine learning for microcontroller class devices. Researchers use a
specialized model development workflow for resource-limited applications to
ensure the compute and latency budget is within the device limits while still
maintaining the desired performance. We characterize a closed-loop widely
applicable workflow of machine learning model development for microcontroller
class devices and show that several classes of applications adopt a specific
instance of it. We present both qualitative and numerical insights into
different stages of model development by showcasing several use cases. Finally,
we identify the open research challenges and unsolved questions demanding
careful considerations moving forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IEEE Sensors Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Fairness with Partially Known Causal Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fair machine learning aims to avoid treating individuals or sub-populations
unfavourably based on \textit{sensitive attributes}, such as gender and race.
Those methods in fair machine learning that are built on causal inference
ascertain discrimination and bias through causal effects. Though
causality-based fair learning is attracting increasing attention, current
methods assume the true causal graph is fully known. This paper proposes a
general method to achieve the notion of counterfactual fairness when the true
causal graph is unknown. To be able to select features that lead to
counterfactual fairness, we derive the conditions and algorithms to identify
ancestral relations between variables on a \textit{Partially Directed Acyclic
Graph (PDAG)}, specifically, a class of causal DAGs that can be learned from
observational data combined with domain knowledge. Interestingly, we find that
counterfactual fairness can be achieved as if the true causal graph were fully
known, when specific background knowledge is provided: the sensitive attributes
do not have ancestors in the causal graph. Results on both simulated and
real-world datasets demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Clustering in Contextual Multi-Armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.00063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.00063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Ban, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study identifying user clusters in contextual multi-armed bandits (MAB).
Contextual MAB is an effective tool for many real applications, such as content
recommendation and online advertisement. In practice, user dependency plays an
essential role in the user's actions, and thus the rewards. Clustering similar
users can improve the quality of reward estimation, which in turn leads to more
effective content recommendation and targeted advertising. Different from
traditional clustering settings, we cluster users based on the unknown bandit
parameters, which will be estimated incrementally. In particular, we define the
problem of cluster detection in contextual MAB, and propose a bandit algorithm,
LOCB, embedded with local clustering procedure. And, we provide theoretical
analysis about LOCB in terms of the correctness and efficiency of clustering
and its regret bound. Finally, we evaluate the proposed algorithm from various
aspects, which outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Weakly-Supervised Learning Methods for Classification and
  Localization in Histology Images: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.03354v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.03354v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérôme Rony, Soufiane Belharbi, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using deep learning models to diagnose cancer from histology data presents
several challenges. Cancer grading and localization of regions of interest
(ROIs) in these images normally relies on both image- and pixel-level labels,
the latter requiring a costly annotation process. Deep weakly-supervised object
localization (WSOL) methods provide different strategies for low-cost training
of deep learning models. Using only image-class annotations, these methods can
be trained to classify an image, and yield class activation maps (CAMs) for ROI
localization. This paper provides a review of state-of-art DL methods for WSOL.
We propose a taxonomy where these methods are divided into bottom-up and
top-down methods according to the information flow in models. Although the
latter have seen limited progress, recent bottom-up methods are currently
driving much progress with deep WSOL methods. Early works focused on designing
different spatial pooling functions. However, these methods reached limited
localization accuracy, and unveiled a major limitation -- the under-activation
of CAMs which leads to high false negative localization. Subsequent works aimed
to alleviate this issue and recover complete object. Representative methods
from our taxonomy are evaluated and compared in terms of classification and
localization accuracy on two challenging histology datasets. Overall, the
results indicate poor localization performance, particularly for generic
methods that were initially designed to process natural images. Methods
designed to address the challenges of histology data yielded good results.
However, all methods suffer from high false positive/negative localization.
Four key challenges are identified for the application of deep WSOL methods in
histology -- under/over activation of CAMs, sensitivity to thresholding, and
model selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Nonparametric Inference with Two-Scale Distributional Nearest
  Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1808.08469v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1808.08469v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emre Demirkaya, Yingying Fan, Lan Gao, Jinchi Lv, Patrick Vossler, Jingbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The weighted nearest neighbors (WNN) estimator has been popularly used as a
flexible and easy-to-implement nonparametric tool for mean regression
estimation. The bagging technique is an elegant way to form WNN estimators with
weights automatically generated to the nearest neighbors; we name the resulting
estimator as the distributional nearest neighbors (DNN) for easy reference.
Yet, there is a lack of distributional results for such estimator, limiting its
application to statistical inference. Moreover, when the mean regression
function has higher-order smoothness, DNN does not achieve the optimal
nonparametric convergence rate, mainly because of the bias issue. In this work,
we provide an in-depth technical analysis of the DNN, based on which we suggest
a bias reduction approach for the DNN estimator by linearly combining two DNN
estimators with different subsampling scales, resulting in the novel two-scale
DNN (TDNN) estimator. The two-scale DNN estimator has an equivalent
representation of WNN with weights admitting explicit forms and some being
negative. We prove that, thanks to the use of negative weights, the two-scale
DNN estimator enjoys the optimal nonparametric rate of convergence in
estimating the regression function under the fourth-order smoothness condition.
We further go beyond estimation and establish that the DNN and two-scale DNN
are both asymptotically normal as the subsampling scales and sample size
diverge to infinity. For the practical implementation, we also provide variance
estimators and a distribution estimator using the jackknife and bootstrap
techniques for the two-scale DNN. These estimators can be exploited for
constructing valid confidence intervals for nonparametric inference of the
regression function. The theoretical results and appealing finite-sample
performance of the suggested two-scale DNN method are illustrated with several
numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>99 pages, 2 figures, to appear in Journal of the American Statistical
  Association</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Neural Operator with Regularity Structure for Modeling Dynamics Driven
  by SPDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06255v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06255v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Hu, Qi Meng, Bingguang Chen, Shiqi Gong, Yue Wang, Wei Chen, Rongchan Zhu, Zhi-Ming Ma, <span class="highlight-author">Tie-Yan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic partial differential equations (SPDEs) are significant tools for
modeling dynamics in many areas including atmospheric sciences and physics.
Neural Operators, generations of neural networks with capability of learning
maps between infinite-dimensional spaces, are strong tools for solving
parametric PDEs. However, they lack the ability to modeling SPDEs which usually
have poor regularity due to the driving noise. As the theory of regularity
structure has achieved great successes in analyzing SPDEs and provides the
concept model feature vectors that well-approximate SPDEs' solutions, we
propose the Neural Operator with Regularity Structure (NORS) which incorporates
the feature vectors for modeling dynamics driven by SPDEs. We conduct
experiments on various of SPDEs including the dynamic Phi41 model and the 2d
stochastic Navier-Stokes equation, and the results demonstrate that the NORS is
resolution-invariant, efficient, and achieves one order of magnitude lower
error with a modest amount of data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Stochastic Gradient MCMC <span class="chip">ICML2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.09028v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.09028v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonios Alexos, Alex Boyd, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient Markov Chain Monte Carlo (SGMCMC) is considered the gold
standard for Bayesian inference in large-scale models, such as Bayesian neural
networks. Since practitioners face speed versus accuracy tradeoffs in these
models, variational inference (VI) is often the preferable option.
Unfortunately, VI makes strong assumptions on both the factorization and
functional form of the posterior. In this work, we propose a new non-parametric
variational approximation that makes no assumptions about the approximate
posterior's functional form and allows practitioners to specify the exact
dependencies the algorithm should respect or break. The approach relies on a
new Langevin-type algorithm that operates on a modified energy function, where
parts of the latent variables are averaged over samples from earlier iterations
of the Markov chain. This way, statistical dependencies can be broken in a
controlled way, allowing the chain to mix faster. This scheme can be further
modified in a "dropout" manner, leading to even more scalability. We test our
scheme for ResNet-20 on CIFAR-10, SVHN, and FMNIST. In all cases, we find
improvements in convergence speed and/or final accuracy compared to SG-MCMC and
VI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper accepted in ICML2022. Code can be found here
  https://github.com/ajboyd2/pytorch_lvi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> On the Learning and Learnablity of Quasimetrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzhou Wang, <span class="highlight-author">Phillip Isola</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our world is full of asymmetries. Gravity and wind can make reaching a place
easier than coming back. Social artifacts such as genealogy charts and citation
graphs are inherently directed. In reinforcement learning and control, optimal
goal-reaching strategies are rarely reversible (symmetrical). Distance
functions supported on these asymmetrical structures are called quasimetrics.
Despite their common appearance, little research has been done on the learning
of quasimetrics.
  Our theoretical analysis reveals that a common class of learning algorithms,
including unconstrained multilayer perceptrons (MLPs), provably fails to learn
a quasimetric consistent with training data. In contrast, our proposed Poisson
Quasimetric Embedding (PQE) is the first quasimetric learning formulation that
both is learnable with gradient-based optimization and enjoys strong
performance guarantees. Experiments on random graphs, social graphs, and
offline Q-learning demonstrate its effectiveness over many common baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ssnl.github.io/quasimetric/ Code:
  https://github.com/SsnL/poisson_quasimetric_embedding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Denoised MDPs: Learning World Models Better Than the World Itself 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongzhou Wang, Simon S. Du, Antonio Torralba, <span class="highlight-author">Phillip Isola</span>, Amy Zhang, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to separate signal from noise, and reason with clean
abstractions, is critical to intelligence. With this ability, humans can
efficiently perform real world tasks without considering all possible nuisance
factors.How can artificial agents do the same? What kind of information can
agents safely discard as noises?
  In this work, we categorize information out in the wild into four types based
on controllability and relation with reward, and formulate useful information
as that which is both controllable and reward-relevant. This framework
clarifies the kinds information removed by various prior work on representation
learning in reinforcement learning (RL), and leads to our proposed approach of
learning a Denoised MDP that explicitly factors out certain noise distractors.
Extensive experiments on variants of DeepMind Control Suite and RoboDesk
demonstrate superior performance of our denoised world model over using raw
observations alone, and over prior works, across policy optimization control
tasks as well as the non-control task of joint position regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ssnl.github.io/denoised_mdp/ Code:
  https://github.com/facebookresearch/denoised_mdp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Large Language Models Can Be Strong Differentially Private Learners <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05679v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05679v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechen Li, Florian Tramèr, <span class="highlight-author">Percy Liang</span>, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages; ICLR 2022 camera ready with additional writing
  clarification and no \vspace!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Silo Heterogeneous Model Federated Multitask Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08603v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08603v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Cao, Zonghang Li, Gang Sun, Hongfang Yu, Mohsen Guizani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a machine learning technique that enables
participants to collaboratively train high-quality models without exchanging
their private data. Participants utilizing cross-silo FL (CS-FL) settings are
independent organizations with different task needs, and they are concerned not
only with data privacy but also with independently training their unique models
due to intellectual property considerations. Most existing FL methods are
incapable of satisfying the above scenarios. In this paper, we propose a FL
method based on the pseudolabeling of unlabeled data via a process such as
cotraining. To the best of our knowledge, this is the first FL method that is
simultaneously compatible with heterogeneous tasks, heterogeneous models, and
heterogeneous training algorithms. Experimental results show that the proposed
method achieves better performance than competing ones. This is especially true
for non-independent and identically distributed (IID) settings and
heterogeneous models, where the proposed method achieves a 35% performance
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Mirror Descent for Regularized Reinforcement Learning: A
  Generalized Framework with Linear Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D. Lee, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy optimization, which finds the desired policy by maximizing value
functions via optimization techniques, lies at the heart of reinforcement
learning (RL). In addition to value maximization, other practical
considerations arise as well, including the need of encouraging exploration,
and that of ensuring certain structural properties of the learned policy due to
safety, resource and operational constraints. These can often be accounted for
via regularized RL, which augments the target value function with a
structure-promoting regularizer.
  Focusing on discounted infinite-horizon Markov decision processes, we propose
a generalized policy mirror descent (GPMD) algorithm for solving regularized
RL. As a generalization of policy mirror descent (arXiv:2102.00135), our
algorithm accommodates a general class of convex regularizers and promotes the
use of Bregman divergence in cognizant of the regularizer in use. We
demonstrate that our algorithm converges linearly to the global solution over
an entire range of learning rates, in a dimension-free fashion, even when the
regularizer lacks strong convexity and smoothness. In addition, this linear
convergence feature is provably stable in the face of inexact policy evaluation
and imperfect policy updates. Numerical experiments are provided to corroborate
the appealing performance of GPMD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Shape Rewards using a Game of Two Partners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.09159v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.09159v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez-Nieves, Yaodong Yang, Tianpei Yang, Matthew Taylor, Wenbin Song, Feifei Tong, Hui Chen, Jiangcheng Zhu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward shaping (RS) is a powerful method in reinforcement learning (RL) for
overcoming the problem of sparse or uninformative rewards. However, RS
typically relies on manually engineered shaping-reward functions whose
construction is time-consuming and error-prone. It also requires domain
knowledge which runs contrary to the goal of autonomous learning. We introduce
Reinforcement Learning Optimising Shaping Algorithm (ROSA), an automated RS
framework in which the shaping-reward function is constructed in a novel Markov
game between two agents. A reward-shaping agent (Shaper) uses switching
controls to determine which states to add shaping rewards and their optimal
values while the other agent (Controller) learns the optimal policy for the
task using these shaped rewards. We prove that ROSA, which easily adopts
existing RL algorithms, learns to construct a shaping-reward function that is
tailored to the task thus ensuring efficient convergence to high performance
policies. We demonstrate ROSA's congenial properties in three carefully
designed experiments and show its superior performance against state-of-the-art
RS algorithms in challenging sparse reward environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight Concentrations and Confidence Sequences from the Regret of
  Universal Portfolio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Orabona, Kwang-Sung Jun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A classic problem in statistics is the estimation of the expectation of
random variables from samples. This gives rise to the tightly connected
problems of deriving concentration inequalities and confidence sequences, that
is confidence intervals that hold uniformly over time. Jun and Orabona
[COLT'19] have shown how to easily convert the regret guarantee of an online
betting algorithm into a time-uniform concentration inequality. In this paper,
we show that we can go even further: We show that the regret of universal
portfolio algorithms give rise to new implicit time-uniform concentrations and
state-of-the-art empirically calculated confidence sequences. In particular,
our numerically obtained confidence sequences can be never vacuous, even with a
single sample, and satisfy the law of iterated logarithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic smoothing of the top-K calibrated hinge loss for deep
  imbalanced classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Garcin, Maximilien Servajean, Alexis Joly, Joseph Salmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern classification tasks, the number of labels is getting larger and
larger, as is the size of the datasets encountered in practice. As the number
of classes increases, class ambiguity and class imbalance become more and more
problematic to achieve high top-1 accuracy. Meanwhile, Top-K metrics (metrics
allowing K guesses) have become popular, especially for performance reporting.
Yet, proposing top-K losses tailored for deep learning remains a challenge,
both theoretically and practically. In this paper we introduce a stochastic
top-K hinge loss inspired by recent developments on top-K calibrated losses.
Our proposal is based on the smoothing of the top-K operator building on the
flexible "perturbed optimizer" framework. We show that our loss function
performs very well in the case of balanced datasets, while benefiting from a
significantly lower computational time than the state-of-the-art top-K loss
function. In addition, we propose a simple variant of our loss for the
imbalanced case. Experiments on a heavy-tailed dataset show that our loss
function significantly outperforms other baseline loss functions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Framework for Pairwise Unbiased Learning to Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Kurennoy, John Coleman, Ian Harris, Alice Lynch, Oisin Mac Fhearai, Daphne Tsatsoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pairwise debiasing is one of the most effective strategies in reducing
position bias in learning-to-rank (LTR) models. However, limiting the scope of
this strategy, are the underlying assumptions required by many pairwise
debiasing approaches. In this paper, we develop an approach based on a
minimalistic set of assumptions that can be applied to a much broader range of
user browsing patterns and arbitrary presentation layouts. We implement the
approach as a simplified version of the Unbiased LambdaMART and demonstrate
that it retains the underlying unbiasedness property in a wider variety of
settings than the original algorithm. Finally, using simulations with "golden"
relevance labels, we will show that the simplified version compares favourably
with the original Unbiased LambdaMART when the examination of different
positions in a ranked list is not assumed to be independent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ir_metadata: An Extensible Metadata Schema for IR Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Breuer, Jüri Keller, Philipp Schaer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The information retrieval (IR) community has a strong tradition of making the
computational artifacts and resources available for future reuse, allowing the
validation of experimental results. Besides the actual test collections, the
underlying run files are often hosted in data archives as part of conferences
like TREC, CLEF, or NTCIR. Unfortunately, the run data itself does not provide
much information about the underlying experiment. For instance, the single run
file is not of much use without the context of the shared task's website or the
run data archive. In other domains, like the social sciences, it is good
practice to annotate research data with metadata. In this work, we introduce
ir_metadata - an extensible metadata schema for TREC run files based on the
PRIMAD model. We propose to align the metadata annotations to PRIMAD, which
considers components of computational experiments that can affect
reproducibility. Furthermore, we outline important components and information
that should be reported in the metadata and give evidence from the literature.
To demonstrate the usefulness of these metadata annotations, we implement new
features in repro_eval that support the outlined metadata schema for the use
case of reproducibility studies. Additionally, we curate a dataset with run
files derived from experiments with different instantiations of PRIMAD
components and annotate these with the corresponding metadata. In the
experiments, we cover reproducibility experiments that are identified by the
metadata and classified by PRIMAD. With this work, we enable IR researchers to
annotate TREC run files and improve the reuse value of experimental artifacts
even further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Resource paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeLoC-ML: Semantic Low-Code Engineering for Machine Learning
  Applications in Industrial IoT <span class="chip">ISWC2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Ren, Kirill Dorofeev, Darko Anicic, Youssef Hammad, Roland Eckl, Thomas A. Runkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet of Things (IoT) is transforming the industry by bridging the gap
between Information Technology (IT) and Operational Technology (OT). Machines
are being integrated with connected sensors and managed by intelligent
analytics applications, accelerating digital transformation and business
operations. Bringing Machine Learning (ML) to industrial devices is an
advancement aiming to promote the convergence of IT and OT. However, developing
an ML application in industrial IoT (IIoT) presents various challenges,
including hardware heterogeneity, non-standardized representations of ML
models, device and ML model compatibility issues, and slow application
development. Successful deployment in this area requires a deep understanding
of hardware, algorithms, software tools, and applications. Therefore, this
paper presents a framework called Semantic Low-Code Engineering for ML
Applications (SeLoC-ML), built on a low-code platform to support the rapid
development of ML applications in IIoT by leveraging Semantic Web technologies.
SeLoC-ML enables non-experts to easily model, discover, reuse, and matchmake ML
models and devices at scale. The project code can be automatically generated
for deployment on hardware based on the matching results. Developers can
benefit from semantic application templates, called recipes, to fast prototype
end-user applications. The evaluations confirm an engineering effort reduction
by a factor of at least three compared to traditional approaches on an
industrial ML classification case study, showing the efficiency and usefulness
of SeLoC-ML. We share the code and welcome any contributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 21st International Semantic Web Conference (ISWC2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Induction enabling Recommending and Trend Analysis: A
  Corporate Research Community Use Case <span class="chip">ISWC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandana Mihindukulasooriya, Mike Sava, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Irene Yachbes, Aditya Gidh, Jillian Duckwitz, Kovit Nisar, Michael Santos, Alfio Gliozzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A research division plays an important role of driving innovation in an
organization. Drawing insights, following trends, keeping abreast of new
research, and formulating strategies are increasingly becoming more challenging
for both researchers and executives as the amount of information grows in both
velocity and volume. In this paper we present a use case of how a corporate
research community, IBM Research, utilizes Semantic Web technologies to induce
a unified Knowledge Graph from both structured and textual data obtained by
integrating various applications used by the community related to research
projects, academic papers, datasets, achievements and recognition. In order to
make the Knowledge Graph more accessible to application developers, we
identified a set of common patterns for exploiting the induced knowledge and
exposed them as APIs. Those patterns were born out of user research which
identified the most valuable use cases or user pain points to be alleviated. We
outline two distinct scenarios: recommendation and analytics for business use.
We will discuss these scenarios in detail and provide an empirical evaluation
on entity recommendation specifically. The methodology used and the lessons
learned from this work can be applied to other organizations facing similar
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISWC 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Data Augmentation for Robust Visual Question Answering <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Chen, Yuhang Zheng, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data Augmentation (DA) -- generating extra training samples beyond original
training set -- has been widely-used in today's unbiased VQA models to mitigate
the language biases. Current mainstream DA strategies are synthetic-based
methods, which synthesize new samples by either editing some visual
regions/words, or re-generating them from scratch. However, these synthetic
samples are always unnatural and error-prone. To avoid this issue, a recent DA
work composes new augmented samples by randomly pairing pristine images and
other human-written questions. Unfortunately, to guarantee augmented samples
have reasonable ground-truth answers, they manually design a set of heuristic
rules for several question types, which extremely limits its generalization
abilities. To this end, we propose a new Knowledge Distillation based Data
Augmentation for VQA, dubbed KDDAug. Specifically, we first relax the
requirements of reasonable image-question pairs, which can be easily applied to
any question types. Then, we design a knowledge distillation (KD) based answer
assignment to generate pseudo answers for all composed image-question pairs,
which are robust to both in-domain and out-of-distribution settings. Since
KDDAug is a model-agnostic DA strategy, it can be seamlessly incorporated into
any VQA architectures. Extensive ablation studies on multiple backbones and
benchmarks have demonstrated the effectiveness and generalization abilities of
KDDAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022; Codes: https://github.com/ItemZheng/KDDAug</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-dimension Geospatial feature learning for urban region function
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjia Xu, Jiuniu Wang, Yirong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban region function recognition plays a vital character in monitoring and
managing the limited urban areas. Since urban functions are complex and full of
social-economic properties, simply using remote sensing~(RS) images equipped
with physical and optical information cannot completely solve the
classification task. On the other hand, with the development of mobile
communication and the internet, the acquisition of geospatial big data~(GBD)
becomes possible. In this paper, we propose a Multi-dimension Feature Learning
Model~(MDFL) using high-dimensional GBD data in conjunction with RS images for
urban region function recognition. When extracting multi-dimension features,
our model considers the user-related information modeled by their activity, as
well as the region-based information abstracted from the region graph.
Furthermore, we propose a decision fusion network that integrates the decisions
from several neural networks and machine learning classifiers, and the final
decision is made considering both the visual cue from the RS images and the
social information from the GBD data. Through quantitative evaluation, we
demonstrate that our model achieves overall accuracy at 92.75, outperforming
the state-of-the-art by 10 percent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Display of 3D Illuminations using Flying Light Specks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahram Ghandeharizadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents techniques to display 3D illuminations using Flying Light
Specks, FLSs. Each FLS is a miniature (hundreds of micrometers) sized drone
with one or more light sources to generate different colors and textures with
adjustable brightness. It is network enabled with a processor and local
storage. Synchronized swarms of cooperating FLSs render illumination of virtual
objects in a pre-specified 3D volume, an FLS display. We present techniques to
display both static and motion illuminations. Our display techniques consider
the limited flight time of an FLS on a fully charged battery and the duration
of time to charge the FLS battery. Moreover, our techniques assume failure of
FLSs is the norm rather than an exception. We present a hardware and a software
architecture for an FLS-display along with a family of techniques to compute
flight paths of FLSs for illuminations. With motion illuminations, one
technique (ICF) minimizes the overall distance traveled by the FLSs
significantly when compared with the other techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version in the Proceedings of the 30th ACM International
  Conference on Multimedia (MM '22), October 10--14, 2022, Lisboa, Portugal,
  DOI https://dl.acm.org/doi/10.1145/3503161.3548250, ISBN
  978-1-4503-9203-7/22/10. See https://github.com/shahramg/FLS-Multimedia2022
  for experimental software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileCodec: Neural Inter-frame Video Compression on Mobile Devices <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang Le, Liang Zhang, Amir Said, Guillaume Sautiere, Yang Yang, Pranav Shrestha, Fei Yin, Reza Pourreza, Auke Wiggers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realizing the potential of neural video codecs on mobile devices is a big
technological challenge due to the computational complexity of deep networks
and the power-constrained mobile hardware. We demonstrate practical feasibility
by leveraging Qualcomm's technology and innovation, bridging the gap from
neural network-based codec simulations running on wall-powered workstations, to
real-time operation on a mobile device powered by Snapdragon technology. We
show the first-ever inter-frame neural video decoder running on a commercial
mobile phone, decoding high-definition videos in real-time while maintaining a
low bitrate and high visual quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MMSys 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive</span> <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-train</span>ing with Limited Resources <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu, Osamu Yoshie, Yubo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have
revealed the potential of aligning multi-modal representations with contrastive
learning. However, these works require a tremendous amount of data and
computational resources (e.g., billion-level web data and hundreds of GPUs),
which prevent researchers with limited resources from reproduction and further
exploration. To this end, we propose a stack of novel methods, which
significantly cut down the heavy resource dependency and allow us to conduct
dual-encoder multi-modal representation alignment with limited resources.
Besides, we provide a reproducible baseline of competitive results, namely
ZeroVL, with only 14M publicly accessible academic datasets and 8 V100 GPUs.
Additionally, we collect 100M web data for pre-training, and achieve comparable
or superior results than state-of-the-art methods, further proving the
effectiveness of our methods on large-scale data. We hope that this work will
provide useful data points and experience for future research in contrastive
vision-language pre-training. Code is available at
https://github.com/zerovl/ZeroVL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud
  Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Pengyuan Zhou, Zhi Liu, Bo Han, Pan Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud video transmission is challenging due to high encoding/decoding
complexity, high video bitrate, and low latency requirement. Consequently,
conventional adaptive streaming methodologies often find themselves
unsatisfactory to meet the requirements in threefold: 1) current algorithms
reuse existing quality of experience (QoE) definitions while overlooking the
unique features of point cloud video thus failing to provide optimal user
experience, 2) most deep learning approaches require long-span data collections
to learn sufficiently varied network conditions and result in long training
period and capacity occupation, 3) cloud training approaches pose privacy risks
caused by leakage of user reported service usage and networking conditions.
  To overcome the limitations, we present FRAS, the first federated
reinforcement learning framework, to the best of our knowledge, for adaptive
point cloud video streaming. We define a new QoE model which takes the unique
features of point cloud video into account. Each client uses reinforcement
learning (RL) to train encoding rate selection with the objective of optimizing
the user's QoE under multiple constraints. Then, a federated learning framework
is integrated with the RL algorithm to enhance training performance with
privacy preservation. Extensive simulations using real point cloud videos and
network traces reveal the superiority of the proposed scheme over baseline
schemes. We also implement a prototype that demonstrates the performance of
FRAS via real-world tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression of user generated content using denoised references <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Pavez, Enrique Perez, Xin Xiong, Antonio Ortega, Balu Adsumilli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video shared over the internet is commonly referred to as user generated
content (UGC). UGC video may have low quality due to various factors including
previous compression. UGC video is uploaded by users, and then it is re-encoded
to be made available at various levels of quality. In a traditional video
coding pipeline the encoder parameters are optimized to minimize a
rate-distortion criterion, but when the input signal has low quality, this
results in sub-optimal coding parameters optimized to preserve undesirable
artifacts. In this paper we formulate the UGC compression problem as that of
compression of a noisy/corrupted source. The noisy source coding theorem
reveals that an optimal UGC compression system is comprised of optimal
denoising of the UGC signal, followed by compression of the denoised signal.
Since optimal denoising is unattainable and users may be against modification
of their content, we propose encoding the UGC signal, and using denoised
references only to compute distortion, so the encoding process can be guided
towards perceptually better solutions. We demonstrate the effectiveness of the
proposed strategy for JPEG compression of UGC images and videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, accepted at International Conference on Image
  Processing (ICIP) 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-17T00:00:00Z">2022-07-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effectiveness of French Language Models on Abstractive <span class="highlight-title">Dialogue</span>
  Summarization Task <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhou, François Portet, Fabien Ringeval
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have established the state-of-the-art on various
natural language processing tasks, including dialogue summarization, which
allows the reader to quickly access key information from long conversations in
meetings, interviews or phone calls. However, such dialogues are still
difficult to handle with current models because the spontaneity of the language
involves expressions that are rarely present in the corpora used for
pre-training the language models. Moreover, the vast majority of the work
accomplished in this field has been focused on English. In this work, we
present a study on the summarization of spontaneous oral dialogues in French
using several language specific pre-trained models: BARThez, and BelGPT-2, as
well as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments
were performed on the DECODA (Call Center) dialogue corpus whose task is to
generate abstractive synopses from call center conversations between a caller
and one or several agents depending on the situation. Results show that the
BARThez models offer the best performance far above the previous
state-of-the-art on DECODA. We further discuss the limits of such pre-trained
models and the challenges that must be addressed for summarizing spontaneous
dialogues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yongxin Zhou, Fran\c{c}ois Portet, Fabien Ringeval. Effectiveness of
  French Language Models on Abstractive Dialogue Summarization Task. LREC 2022,
  Marseille, France, 21-23 June 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Spoken Drug Prescription <span class="highlight-title">Dataset</span> in French for Spoken Language
  Understanding <span class="chip">LREC2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Can Kocabiyikoglu, François Portet, Prudence Gibert, Hervé Blanchon, Jean-Marc Babouchkine, Gaëtan Gavazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken medical dialogue systems are increasingly attracting interest to
enhance access to healthcare services and improve quality and traceability of
patient care. In this paper, we focus on medical drug prescriptions acquired on
smartphones through spoken dialogue. Such systems would facilitate the
traceability of care and would free clinicians' time. However, there is a lack
of speech corpora to develop such systems since most of the related corpora are
in text form and in English. To facilitate the research and development of
spoken medical dialogue systems, we present, to the best of our knowledge, the
first spoken medical drug prescriptions corpus, named PxSLU. It contains 4
hours of transcribed and annotated dialogues of drug prescriptions in French
acquired through an experiment with 55 participants experts and non-experts in
prescriptions. We also present some experiments that demonstrate the interest
of this corpus for the evaluation and development of medical dialogue systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ali Can Kocabiyikoglu,Fran\c{c}ois Portet, Prudence Gibert, Herv\'e
  Blanchon, Jean-Marc Babouchkine, Ga\"etan Gavazzi. A Spoken Drug Prescription
  Dataset in French for Spoken Language Understanding. LREC2022, Marseille,
  France, 21-22-23 June 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">Overview</span> of Distant Supervision for Relation Extraction with a Focus
  on Denoising and <span class="highlight-title">Pre-train</span>ing Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Hogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation Extraction (RE) is a foundational task of natural language
processing. RE seeks to transform raw, unstructured text into structured
knowledge by identifying relational information between entity pairs found in
text. RE has numerous uses, such as knowledge graph completion, text
summarization, question-answering, and search querying. The history of RE
methods can be roughly organized into four phases: pattern-based RE,
statistical-based RE, neural-based RE, and large language model-based RE. This
survey begins with an overview of a few exemplary works in the earlier phases
of RE, highlighting limitations and shortcomings to contextualize progress.
Next, we review popular benchmarks and critically examine metrics used to
assess RE performance. We then discuss distant supervision, a paradigm that has
shaped the development of modern RE methods. Lastly, we review recent RE works
focusing on denoising and pre-training methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Learning of Image Schema 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fajrian Yunus, Chloé Clavel, Catherine Pelachaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image schema is a recurrent pattern of reasoning where one entity is mapped
into another. Image schema is similar to conceptual metaphor and is also
related to metaphoric gesture. Our main goal is to generate metaphoric gestures
for an Embodied Conversational Agent.
  We propose a technique to learn the vector representation of image schemas.
As far as we are aware of, this is the first work which addresses that problem.
Our technique uses Ravenet et al's algorithm which we use to compute the image
schemas from the text input and also BERT and SenseBERT which we use as the
base word embedding technique to calculate the final vector representation of
the image schema. Our representation learning technique works by clustering:
word embedding vectors which belong to the same image schema should be
relatively closer to each other, and thus form a cluster.
  With the image schemas representable as vectors, it also becomes possible to
have a notion that some image schemas are closer or more similar to each other
than to the others because the distance between the vectors is a proxy of the
dissimilarity between the corresponding image schemas. Therefore, after
obtaining the vector representation of the image schemas, we calculate the
distances between those vectors. Based on these, we create visualizations to
illustrate the relative distances between the different image schemas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Troll Tweet Detection Using Contextualized Word Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyhmus Yilmaz, Sultan Zavrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, many troll accounts have emerged to manipulate social media
opinion. Detecting and eradicating trolling is a critical issue for
social-networking platforms because businesses, abusers, and
nation-state-sponsored troll farms use false and automated accounts. NLP
techniques are used to extract data from social networking text, such as
Twitter tweets. In many text processing applications, word embedding
representation methods, such as BERT, have performed better than prior NLP
techniques, offering novel breaks to precisely comprehend and categorize
social-networking information for various tasks. This paper implements and
compares nine deep learning-based troll tweet detection architectures, with
three models for each BERT, ELMo, and GloVe word embedding model. Precision,
recall, F1 score, AUC, and classification accuracy are used to evaluate each
architecture. From the experimental results, most architectures using BERT
models improved troll tweet detection. A customized ELMo-based architecture
with a GRU classifier has the highest AUC for detecting troll messages. The
proposed architectures can be used by various social-based systems to detect
troll messages in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-KGD: Relation Transition Aware Knowledge-Grounded <span class="highlight-title">Dialogue</span> Generation <span class="chip">ISWC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Wang, Zhixu Li, Jiaan Wang, Jianfeng Qu, Ying He, An Liu, Lei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding dialogue system with external knowledge is a promising way to
improve the quality of responses. Most existing works adopt knowledge graphs
(KGs) as the external resources, paying attention to the contribution of
entities in the last utterance of the dialogue for context understanding and
response generation. Nevertheless, the correlations between knowledge implied
in the multi-turn context and the transition regularities between relations in
KGs are under-explored. To this end, we propose a Relation Transition aware
Knowledge-Grounded Dialogue Generation model (RT-KGD). Specifically, inspired
by the latent logic of human conversation, our model integrates dialogue-level
relation transition regularities with turn-level entity semantic information.
In this manner, the interaction between knowledge is considered to produce
abundant clues for predicting the appropriate knowledge and generating coherent
responses. The experimental results on both automatic evaluation and manual
evaluation indicate that our model outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISWC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Temporal Action Detection via <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Prompt</span>ing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing temporal action detection (TAD) methods rely on large training data
including segment-level annotations, limited to recognizing previously seen
classes alone during inference. Collecting and annotating a large training set
for each class of interest is costly and hence unscalable. Zero-shot TAD
(ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize
any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with
significantly less investigation. Inspired by the success of zero-shot image
classification aided by vision-language (ViL) models such as CLIP, we aim to
tackle the more complex TAD task. An intuitive method is to integrate an
off-the-shelf proposal detector with CLIP style classification. However, due to
the sequential localization (e.g, proposal generation) and classification
design, it is prone to localization error propagation. To overcome this
problem, in this paper we propose a novel zero-Shot Temporal Action detection
model via Vision-LanguagE prompting (STALE). Such a novel design effectively
eliminates the dependence between localization and classification by breaking
the route for error propagation in-between. We further introduce an interaction
mechanism between classification and localization for improved optimization.
Extensive experiments on standard ZS-TAD video benchmarks show that our STALE
significantly outperforms state-of-the-art alternatives. Besides, our model
also yields superior results on supervised TAD over recent strong competitors.
The PyTorch implementation of STALE is available at
https://github.com/sauradip/STALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code available at https://github.com/sauradip/STALE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End <span class="highlight-title">Spoken Language Understanding</span>: Performance analyses of a
  voice command task in a low resource setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thierry Desot, François Portet, Michel Vacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken Language Understanding (SLU) is a core task in most human-machine
interaction systems. With the emergence of smart homes, smart phones and smart
speakers, SLU has become a key technology for the industry. In a classical SLU
approach, an Automatic Speech Recognition (ASR) module transcribes the speech
signal into a textual representation from which a Natural Language
Understanding (NLU) module extracts semantic information. Recently End-to-End
SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it
benefits from the joint optimization of the ASR and the NLU parts, hence
limiting the cascade of error effect of the pipeline architecture. However,
little is known about the actual linguistic properties used by E2E models to
predict concepts and intents from speech input. In this paper, we present a
study identifying the signal features and other linguistic properties used by
an E2E model to perform the SLU task. The study is carried out in the
application domain of a smart home that has to handle non-English (here French)
voice commands. The results show that a good E2E SLU performance does not
always require a perfect ASR capability. Furthermore, the results show the
superior capabilities of the E2E model in handling background noise and
syntactic variation compared to the pipeline model. Finally, a finer-grained
analysis suggests that the E2E model uses the pitch information of the input
signal to identify voice command concepts. The results and methodology outlined
in this paper provide a springboard for further analyses of E2E models in
speech processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Thierry Desot, Fran\c{c}ois Portet, Michel Vacher, End-to-End Spoken
  Language Understanding: Performance analyses of a voice command task in a low
  resource setting, Computer Speech & Language, Volume 75, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural language processing for clusterization of genes according to
  their functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Dordiuk, Ekaterina Demicheva, Fernando Polanco Espino, Konstantin Ushenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are hundreds of methods for analysis of data obtained in
mRNA-sequencing. The most of them are focused on small number of genes. In this
study, we propose an approach that reduces the analysis of several thousand
genes to analysis of several clusters. The list of genes is enriched with
information from open databases. Then, the descriptions are encoded as vectors
using the pretrained language model (BERT) and some text processing approaches.
The encoded gene function pass through the dimensionality reduction and
clusterization. Aiming to find the most efficient pipeline, 180 cases of
pipeline with different methods in the major pipeline steps were analyzed. The
performance was evaluated with clusterization indexes and expert review of the
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ural-Siberian Conference on Computational Technologies in Cognitive
  Science, Genomics and Biomedicine 2022 (CSGB 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can large language models reason about medical questions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Liévin, Christoffer Egeberg Hother, Ole Winther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) often produce impressive outputs, they
also fail to reason and be factual. We set out to investigate how these
limitations affect the LLM's ability to answer and reason about difficult
real-world based questions. We applied the human-aligned GPT-3 (InstructGPT) to
answer multiple-choice medical exam questions (USMLE and MedMCQA) and medical
research questions (PubMedQA). We investigated Chain-of-thought (think step by
step) prompts, grounding (augmenting the prompt with search results) and
few-shot (prepending the question with question-answer exemplars). For a subset
of the USMLE questions, a medical domain expert reviewed and annotated the
model's reasoning. Overall, GPT-3 achieved a substantial improvement in
state-of-the-art machine learning performance. We observed that GPT-3 is often
knowledgeable and can reason about medical questions. GPT-3, when confronted
with a question it cannot answer, will still attempt to answer, often resulting
in a biased predictive distribution. LLMs are not on par with human performance
but our results suggest the emergence of reasoning patterns that are compatible
with medical problem-solving. We speculate that scaling model and data,
enhancing prompt alignment and allowing for better contextualization of the
completions will be sufficient for LLMs to reach human-level performance on
this type of task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 1 figure, to be submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELECTRA is a Zero-Shot Learner, Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwen Ni, Hung-Yu Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, for few-shot or even zero-shot learning, the new paradigm
"pre-train, prompt, and predict" has achieved remarkable achievements compared
with the "pre-train, fine-tune" paradigm. After the success of prompt-based
GPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)
prompt learning methods became popular and widely used. However, another
efficient pre-trained discriminative model, ELECTRA, has probably been
neglected. In this paper, we attempt to accomplish several NLP tasks in the
zero-shot scenario using a novel our proposed replaced token detection
(RTD)-based prompt learning method. Experimental results show that ELECTRA
model based on RTD-prompt learning achieves surprisingly state-of-the-art
zero-shot performance. Numerically, compared to MLM-RoBERTa-large and
MLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%
improvement on all 15 tasks. Especially on the SST-2 task, our
RTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training
data. Overall, compared to the pre-trained masked language models, the
pre-trained replaced token detection model performs better in zero-shot
learning. Therefore, ELECTRA is an excellent zero-shot learner. The source code
is available at: https://github.com/nishiwen1214/RTD-ELECTRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code is available at:
  https://github.com/nishiwen1214/RTD-ELECTRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ United States Politicians' Tone Became More Negative with 2016 Primary
  Campaigns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Külz, Andreas Spitz, Ahmad Abu-Akel, Stephan Günnemann, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a widespread belief that the tone of US political language has
become more negative recently, in particular when Donald Trump entered
politics. At the same time, there is disagreement as to whether Trump changed
or merely continued previous trends. To date, data-driven evidence regarding
these questions is scarce, partly due to the difficulty of obtaining a
comprehensive, longitudinal record of politicians' utterances. Here we apply
psycholinguistic tools to a novel, comprehensive corpus of 24 million quotes
from online news attributed to 18,627 US politicians in order to analyze how
the tone of US politicians' language evolved between 2008 and 2020. We show
that, whereas the frequency of negative emotion words had decreased
continuously during Obama's tenure, it suddenly and lastingly increased with
the 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of
the pre-campaign mean, in a pattern that emerges across parties. The effect
size drops by 40% when omitting Trump's quotes, and by 50% when averaging over
speakers rather than quotes, implying that prominent speakers, and Trump in
particular, have disproportionately, though not exclusively, contributed to the
rise in negative language. This work provides the first large-scale data-driven
evidence of a drastic shift toward a more negative political tone following
Trump's campaign start as a catalyst, with important implications for the
debate about the state of US politics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multibias-mitigated and Sentiment Knowledge Enriched <span class="highlight-title">Transformer</span> for
  Debiasing in <span class="highlight-title">Multimodal</span> <span class="highlight-title">Conversation</span>al Emotion Recognition <span class="chip">NLPCC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglin Wang, Fang Ma, Yazhou Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition in conversations (mERC) is an active research
topic in natural language processing (NLP), which aims to predict human's
emotional states in communications of multiple modalities, e,g., natural
language and facial gestures. Innumerable implicit prejudices and
preconceptions fill human language and conversations, leading to the question
of whether the current data-driven mERC approaches produce a biased error. For
example, such approaches may offer higher emotional scores on the utterances by
females than males. In addition, the existing debias models mainly focus on
gender or race, where multibias mitigation is still an unexplored task in mERC.
In this work, we take the first step to solve these issues by proposing a
series of approaches to mitigate five typical kinds of bias in textual
utterances (i.e., gender, age, race, religion and LGBTQ+) and visual
representations (i.e, gender and age), followed by a Multibias-Mitigated and
sentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive
experimental results show the effectiveness of the proposed model and prove
that the debias operation has a great impact on the classification performance
for mERC. We hope our study will benefit the development of bias mitigation in
mERC and related emotion studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, accepted to NLPCC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aspect-specific Context Modeling for Aspect-based Sentiment Analysis <span class="chip">NLPCC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Ma, Chen Zhang, Bo Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) aims at predicting sentiment polarity
(SC) or extracting opinion span (OE) expressed towards a given aspect. Previous
work in ABSA mostly relies on rather complicated aspect-specific feature
induction. Recently, pretrained language models (PLMs), e.g., BERT, have been
used as context modeling layers to simplify the feature induction structures
and achieve state-of-the-art performance. However, such PLM-based context
modeling can be not that aspect-specific. Therefore, a key question is left
under-explored: how the aspect-specific context can be better modeled through
PLMs? To answer the question, we attempt to enhance aspect-specific context
modeling with PLM in a non-intrusive manner. We propose three aspect-specific
input transformations, namely aspect companion, aspect prompt, and aspect
marker. Informed by these transformations, non-intrusive aspect-specific PLMs
can be achieved to promote the PLM to pay more attention to the aspect-specific
context in a sentence. Additionally, we craft an adversarial benchmark for ABSA
(advABSA) to see how aspect-specific modeling can impact model robustness.
Extensive experimental results on standard and adversarial benchmarks for SC
and OE demonstrate the effectiveness and robustness of the proposed method,
yielding new state-of-the-art performance on OE and competitive performance on
SC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted to NLPCC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Rui<span class="highlight-author">yang Liu</span>, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as "semantic sensitivity". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainability in NLP: Analyzing and Calculating Word Saliency
  through Word Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Dong, Zhitao Guan, Longfei Wu, Zijian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide use of black-box models in natural language processing brings great
challenges to the understanding of the decision basis, the trustworthiness of
the prediction results, and the improvement of the model performance. The words
in text samples have properties that reflect their semantics and contextual
information, such as the part of speech, the position, etc. These properties
may have certain relationships with the word saliency, which is of great help
for studying the explainability of the model predictions. In this paper, we
explore the relationships between the word saliency and the word properties.
According to the analysis results, we further establish a mapping model,
Seq2Saliency, from the words in a text sample and their properties to the
saliency values based on the idea of sequence tagging. In addition, we
establish a new dataset called PrSalM, which contains each word in the text
samples, the word properties, and the word saliency values. The experimental
evaluations are conducted to analyze the saliency of words with different
properties. The effectiveness of the Seq2Saliency model is verified.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Similarity is More Valuable than Character Similarity:
  Curriculum Learning for Chinese Spell Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding Zhang, Yinghui Li, Qingyu Zhou, Shirong Ma, Yangning Li, Yunbo Cao, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling
errors. In recent years, related researches focus on introducing the character
similarity from confusion set to enhance the CSC models, ignoring the context
of characters that contain richer information. To make better use of contextual
similarity, we propose a simple yet effective curriculum learning framework for
the CSC task. With the help of our designed model-agnostic framework, existing
CSC models will be trained from easy to difficult as humans learn Chinese
characters and achieve further performance improvements. Extensive experiments
and detailed analyses on widely used SIGHAN datasets show that our method
outperforms previous state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and
  Languages <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable evaluation benchmarks designed for replicability and
comprehensiveness have driven progress in machine learning. Due to the lack of
a multilingual benchmark, however, vision-and-language research has mostly
focused on English language tasks. To fill this gap, we introduce the
Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings
together - by both aggregating pre-existing datasets and creating new ones -
visual question answering, cross-modal retrieval, grounded reasoning, and
grounded entailment tasks across 20 diverse languages. Our benchmark enables
the evaluation of multilingual multimodal models for transfer learning, not
only in a zero-shot setting, but also in newly defined few-shot learning
setups. Based on the evaluation of the available state-of-the-art models, we
find that translate-test transfer is superior to zero-shot transfer and that
few-shot learning is hard to harness for many tasks. Moreover, downstream
performance is partially explained by the amount of available unlabelled
textual data for pretraining, and only weakly by the typological distance of
target-source languages. We hope to encourage future research efforts in this
area by releasing the benchmark to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebShop: Towards Scalable Real-World Web Interaction with Grounded
  Language Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for grounding language in interactive environments either
lack real-world linguistic elements, or prove difficult to scale up due to
substantial human involvement in the collection of data or feedback signals. To
bridge this gap, we develop WebShop -- a simulated e-commerce website
environment with $1.18$ million real-world products and $12,087$ crowd-sourced
text instructions. Given a text instruction specifying a product requirement,
an agent needs to navigate multiple types of webpages and issue diverse actions
to find, customize, and purchase an item. WebShop provides several challenges
for language grounding including understanding compositional instructions,
query (re-)formulation, comprehending and acting on noisy text in webpages, and
performing strategic exploration. We collect over $1,600$ human demonstrations
for the task, and train and evaluate a diverse range of agents using
reinforcement learning, imitation learning, and pre-trained image and language
models. Our best model achieves a task success rate of $29\%$, which
outperforms rule-based heuristics ($9.6\%$) but is far lower than human expert
performance ($59\%$). We also analyze agent and human trajectories and ablate
various model components to provide insights for developing future agents with
stronger language understanding and decision making abilities. Finally, we show
that agents trained on WebShop exhibit non-trivial sim-to-real transfer when
evaluated on amazon.com and ebay.com, indicating the potential value of WebShop
in developing practical web-based agents that can operate in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with code, data, demos: https://webshop-pnlp.github.io.
  v2 adds transfer to eBay</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GANzilla: User-Driven Direction Discovery in Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noyan Evirgen, Xiang 'Anthony' Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Network (GAN) is widely adopted in numerous
application areas, such as data preprocessing, image editing, and creativity
support. However, GAN's 'black box' nature prevents non-expert users from
controlling what data a model generates, spawning a plethora of prior work that
focused on algorithm-driven approaches to extract editing directions to control
GAN. Complementarily, we propose a GANzilla: a user-driven tool that empowers a
user with the classic scatter/gather technique to iteratively discover
directions to meet their editing goals. In a study with 12 participants,
GANzilla users were able to discover directions that (i) edited images to match
provided examples (closed-ended tasks) and that (ii) met a high-level goal,
e.g., making the face happier, while showing diversity across individuals
(open-ended tasks).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular-orbital-based Machine Learning for Open-shell and
  Multi-reference Systems with Kernel Addition Gaussian Process Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixue Cheng, Jiace Sun, J. Emiliano Deustua, Vignesh C. Bhethanabotla, Thomas F. Miller III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel machine learning strategy, kernel addition Gaussian
process regression (KA-GPR), in molecular-orbital-based machine learning
(MOB-ML) to learn the total correlation energies of general electronic
structure theories for closed- and open-shell systems by introducing a machine
learning strategy. The learning efficiency of MOB-ML (KA-GPR) is the same as
the original MOB-ML method for the smallest criegee molecule, which is a
closed-shell molecule with multi-reference characters. In addition, the
prediction accuracies of different small free radicals could reach the chemical
accuracy of 1 kcal/mol by training on one example structure. Accurate potential
energy surfaces for the H10 chain (closed-shell) and water OH bond dissociation
(open-shell) could also be generated by MOB-ML (KA-GPR). To explore the breadth
of chemical systems that KA-GPR can describe, we further apply MOB-ML to
accurately predict the large benchmark datasets for closed- (QM9, QM7b-T,
GDB-13-T) and open-shell (QMSpin) molecules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CULT: Continual Unsupervised Learning with Typicality-Based Environment
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Daniels-Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CULT (Continual Unsupervised Representation Learning with
Typicality-Based Environment Detection), a new algorithm for continual
unsupervised learning with variational auto-encoders. CULT uses a simple
typicality metric in the latent space of a VAE to detect distributional shifts
in the environment, which is used in conjunction with generative replay and an
auxiliary environmental classifier to limit catastrophic forgetting in
unsupervised representation learning. In our experiments, CULT significantly
outperforms baseline continual unsupervised learning approaches. Code for this
paper can be found here: https://github.com/oliveradk/cult
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Predicting Out-of-Domain Generalization with Local Manifold Smoothness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Ng, Neha Hulkund, <span class="highlight-author">Kyunghyun Cho</span>, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how machine learning models generalize to new environments is a
critical part of their safe deployment. Recent work has proposed a variety of
complexity measures that directly predict or theoretically bound the
generalization capacity of a model. However, these methods rely on a strong set
of assumptions that in practice are not always satisfied. Motivated by the
limited settings in which existing measures can be applied, we propose a novel
complexity measure based on the local manifold smoothness of a classifier. We
define local manifold smoothness as a classifier's output sensitivity to
perturbations in the manifold neighborhood around a given test point.
Intuitively, a classifier that is less sensitive to these perturbations should
generalize better. To estimate smoothness we sample points using data
augmentation and measure the fraction of these points classified into the
majority class. Our method only requires selecting a data augmentation method
and makes no other assumptions about the model or data distributions, meaning
it can be applied even in out-of-domain (OOD) settings where existing methods
cannot. In experiments on robustness benchmarks in image classification,
sentiment analysis, and natural language inference, we demonstrate a strong and
robust correlation between our manifold smoothness measure and actual OOD
generalization on over 3,000 models evaluated on over 100 train/test domain
pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RENs: Relevance Encoding Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krithika Iyer, Riddhish Bhalodia, Shireen Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manifold assumption for high-dimensional data assumes that the data is
generated by varying a set of parameters obtained from a low-dimensional latent
space. Deep generative models (DGMs) are widely used to learn data
representations in an unsupervised way. DGMs parameterize the underlying
low-dimensional manifold in the data space using bottleneck architectures such
as variational autoencoders (VAEs). The bottleneck dimension for VAEs is
treated as a hyperparameter that depends on the dataset and is fixed at design
time after extensive tuning. As the intrinsic dimensionality of most real-world
datasets is unknown, often, there is a mismatch between the intrinsic
dimensionality and the latent dimensionality chosen as a hyperparameter. This
mismatch can negatively contribute to the model performance for representation
learning and sample generation tasks. This paper proposes relevance encoding
networks (RENs): a novel probabilistic VAE-based framework that uses the
automatic relevance determination (ARD) prior in the latent space to learn the
data-specific bottleneck dimensionality. The relevance of each latent dimension
is directly learned from the data along with the other model parameters using
stochastic gradient descent and a reparameterization trick adapted to
non-Gaussian priors. We leverage the concept of DeepSets to capture permutation
invariant statistical properties in both data and latent spaces for relevance
determination. The proposed framework is general and flexible and can be used
for the state-of-the-art VAE models that leverage regularizers to impose
specific characteristics in the latent space (e.g., disentanglement). With
extensive experimentation on synthetic and public image datasets, we show that
the proposed model learns the relevant latent bottleneck dimensionality without
compromising the representation and generation quality of the samples.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting and Visualizing Wildlife Trafficking Events from Wildlife
  Trafficking Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devin Coughlin, Maylee Gagnon, Victoria Grasso, Guanyi Mou, Kyumin Lee, Renata Konrad, Patricia Raxter, Meredith Gore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Experts combating wildlife trafficking manually sift through articles about
seizures and arrests, which is time consuming and make identifying trends
difficult. We apply natural language processing techniques to automatically
extract data from reports published by the Eco Activists for Governance and Law
Enforcement (EAGLE). We expanded Python spaCy's pre-trained pipeline and added
a custom named entity ruler, which identified 15 fully correct and 36 partially
correct events in 15 reports against an existing baseline, which did not
identify any fully correct events. The extracted wildlife trafficking events
were inserted to a database. Then, we created visualizations to display trends
over time and across regions to support domain experts. These are accessible on
our website, Wildlife Trafficking in Africa
(https://wildlifemqp.github.io/Visualizations/).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Rui<span class="highlight-author">yang Liu</span>, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as "semantic sensitivity". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Mobility Prediction with Causal and Spatial-constrained Multi-task
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyuan Huang, Shengyuan Xu, Menghan Wang, Hansi Wu, Yanyan Xu, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling human mobility helps to understand how people are accessing
resources and physically contacting with each other in cities, and thus
contributes to various applications such as urban planning, epidemic control,
and location-based advertisement. Next location prediction is one decisive task
in individual human mobility modeling and is usually viewed as sequence
modeling, solved with Markov or RNN-based methods. However, the existing models
paid little attention to the logic of individual travel decisions and the
reproducibility of the collective behavior of population. To this end, we
propose a Causal and Spatial-constrained Long and Short-term Learner (CSLSL)
for next location prediction. CSLSL utilizes a causal structure based on
multi-task learning to explicitly model the
"when$\rightarrow$what$\rightarrow$where", a.k.a.
"time$\rightarrow$activity$\rightarrow$location" decision logic. We next
propose a spatial-constrained loss function as an auxiliary task, to ensure the
consistency between the predicted and actual spatial distribution of travelers'
destinations. Moreover, CSLSL adopts modules named Long and Short-term Capturer
(LSC) to learn the transition regularities across different time spans.
Extensive experiments on three real-world datasets show a 33.4% performance
improvement of CSLSL over baselines and confirm the effectiveness of
introducing the causality and consistency constraints. The implementation is
available at https://github.com/urbanmobility/CSLSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The experimental results in the paper need to be updated, so we
  withdraw the original version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BARS: Towards Open Benchmarking for Recommender Systems <span class="chip">SIGIR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09626v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09626v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jin<span class="highlight-author">yang Liu</span>, Guohao Cai, Xi Xiao, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past two decades have witnessed the rapid development of personalized
recommendation techniques. Despite significant progress made in both research
and practice of recommender systems, to date, there is a lack of a
widely-recognized benchmarking standard in this field. Many existing studies
perform model evaluations and comparisons in an ad-hoc manner, for example, by
employing their own private data splits or using different experimental
settings. Such conventions not only increase the difficulty in reproducing
existing studies, but also lead to inconsistent experimental results among
them. This largely limits the credibility and practical value of research
results in this field. To tackle these issues, we present an initiative project
(namely BARS) aiming for open benchmarking for recommender systems. In
comparison to some earlier attempts towards this goal, we take a further step
by setting up a standardized benchmarking pipeline for reproducible research,
which integrates all the details about datasets, source code, hyper-parameter
settings, running logs, and evaluation results. The benchmark is designed with
comprehensiveness and sustainability in mind. It covers both matching and
ranking tasks, and also enables researchers to easily follow and contribute to
the research in this field. This project will not only reduce the redundant
efforts of researchers to re-implement or re-run existing baselines, but also
drive more solid and reproducible research on recommender systems. We would
like to call upon everyone to use the BARS benchmark for future evaluation, and
contribute to the project through the portal at:
https://openbenchmark.github.io/BARS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2022. Note that version v5 is updated to keep
  consistency with the ACM camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Benchmark Suite for Template Detection and Content Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1409.6182v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1409.6182v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julián Alarte, Josep Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Template detection and content extraction are two of the main areas of
information retrieval applied to the Web. They perform different analyses over
the structure and content of webpages to extract some part of the document.
However, their objective is different. While template detection identifies the
template of a webpage (usually comparing with other webpages of the same
website), content extraction identifies the main content of the webpage
discarding the other part. Therefore, they are somehow complementary, because
the main content is not part of the template. It has been measured that
templates represent between 40% and 50% of data on the Web. Therefore,
identifying templates is essential for indexing tasks because templates usually
contain irrelevant information such as advertisements, menus and banners.
Processing and storing this information is likely to lead to a waste of
resources (storage space, bandwidth, etc.). Similarly, identifying the main
content is essential for many information retrieval tasks. In this paper, we
present a benchmark suite to test different approaches for template detection
and content extraction. The suite is public, and it contains real heterogeneous
webpages that have been labelled so that different techniques can be suitable
(and automatically) compared.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Temporal Action Detection via <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Prompt</span>ing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing temporal action detection (TAD) methods rely on large training data
including segment-level annotations, limited to recognizing previously seen
classes alone during inference. Collecting and annotating a large training set
for each class of interest is costly and hence unscalable. Zero-shot TAD
(ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize
any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with
significantly less investigation. Inspired by the success of zero-shot image
classification aided by vision-language (ViL) models such as CLIP, we aim to
tackle the more complex TAD task. An intuitive method is to integrate an
off-the-shelf proposal detector with CLIP style classification. However, due to
the sequential localization (e.g, proposal generation) and classification
design, it is prone to localization error propagation. To overcome this
problem, in this paper we propose a novel zero-Shot Temporal Action detection
model via Vision-LanguagE prompting (STALE). Such a novel design effectively
eliminates the dependence between localization and classification by breaking
the route for error propagation in-between. We further introduce an interaction
mechanism between classification and localization for improved optimization.
Extensive experiments on standard ZS-TAD video benchmarks show that our STALE
significantly outperforms state-of-the-art alternatives. Besides, our model
also yields superior results on supervised TAD over recent strong competitors.
The PyTorch implementation of STALE is available at
https://github.com/sauradip/STALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code available at https://github.com/sauradip/STALE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action-conditioned On-demand Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiujing Lu, Yipeng Zhang, Mingjian Lu, Vwani Roychowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel framework, On-Demand MOtion Generation (ODMO), for
generating realistic and diverse long-term 3D human motion sequences
conditioned only on action types with an additional capability of
customization. ODMO shows improvements over SOTA approaches on all traditional
motion evaluation metrics when evaluated on three public datasets (HumanAct12,
UESTC, and MoCap). Furthermore, we provide both qualitative evaluations and
quantitative metrics demonstrating several first-known customization
capabilities afforded by our framework, including mode discovery,
interpolation, and trajectory customization. These capabilities significantly
widen the spectrum of potential applications of such motion generation models.
The novel on-demand generative capabilities are enabled by innovations in both
the encoder and decoder architectures: (i) Encoder: Utilizing contrastive
learning in low-dimensional latent space to create a hierarchical embedding of
motion sequences, where not only the codes of different action types form
different groups, but within an action type, codes of similar inherent patterns
(motion styles) cluster together, making them readily discoverable; (ii)
Decoder: Using a hierarchical decoding strategy where the motion trajectory is
reconstructed first and then used to reconstruct the whole motion sequence.
Such an architecture enables effective trajectory control. Our code is released
on the Github page: https://github.com/roychowdhuryresearch/ODMO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM 2022, 13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-16T00:00:00Z">2022-07-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Referential Games to Learn Compositional Learning Behaviours <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Denamganaï, Sondess Missaoui, James Alfred Walker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human beings use compositionality to generalise from past experiences to
actual or fictive, novel experiences. To do so, we separate our experiences
into fundamental atomic components. These atomic components can then be
recombined in novel ways to support our ability to imagine and engage with
novel experiences. We frame this as the ability to learn to generalise
compositionally. And, we will refer to behaviours making use of this ability as
compositional learning behaviours (CLBs).
  A central problem to learning CLBs is the resolution of a binding problem
(BP) (by learning to, firstly, segregate the supportive stimulus components
from the observation of multiple stimuli, and then, combine them in a single
episodic experience). While it is another feat of intelligence that human
beings perform with ease, it is not the case for state-of-the-art artificial
agents.
  Thus, in order to build artificial agents able to collaborate with human
beings, we propose to develop a novel benchmark to investigate agents'
abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We
take inspiration from the language emergence and grounding framework of
referential games and propose a meta-learning extension of referential games,
entitled Meta-Referential Games, and use this framework to build our benchmark,
that we name Symbolic Behaviour Benchmark (S2B).
  While it has the potential to test for more symbolic behaviours, rather than
solely CLBs, in the present paper, though, we solely focus on the single-agent
language grounding task that tests for CLBs. We provide baseline results for
it, using state-of-the-art RL agents, and show that our proposed benchmark is a
compelling challenge that we hope will spur the research community towards
developing more capable artificial agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress / under review at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> <span class="highlight-title">Dialog</span> Systems with Dual Knowledge-enhanced Generative
  <span class="highlight-title">Pretrain</span>ed Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolin Chen, Xuemeng Song, Liqiang Jing, Shuo Li, Linmei Hu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text response generation for multimodal task-oriented dialog systems, which
aims to generate the proper text response given the multimodal context, is an
essential yet challenging task. Although existing efforts have achieved
compelling success, they still suffer from two pivotal limitations: 1) overlook
the benefit of generative pre-training, and 2) ignore the textual context
related knowledge. To address these limitations, we propose a novel dual
knowledge-enhanced generative pretrained language model for multimodal
task-oriented dialog systems (DKMD), consisting of three key components: dual
knowledge selection, dual knowledge-enhanced context learning, and
knowledge-enhanced response generation. To be specific, the dual knowledge
selection component aims to select the related knowledge according to both
textual and visual modalities of the given context. Thereafter, the dual
knowledge-enhanced context learning component targets seamlessly integrating
the selected knowledge into the multimodal context learning from both global
and local perspectives, where the cross-modal semantic relation is also
explored. Moreover, the knowledge-enhanced response generation component
comprises a revised BART decoder, where an additional dot-product
knowledge-decoder attention sub-layer is introduced for explicitly utilizing
the knowledge to advance the text response generation. Extensive experiments on
a public dataset verify the superiority of the proposed DKMD over
state-of-the-art competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sotto Voce: Federated Speech Recognition with Differential Privacy
  Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Shoemate, Kevin Jett, Ethan Cowan, Sean Colbath, James Honaker, Prasanna Muthukumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech data is expensive to collect, and incredibly sensitive to its sources.
It is often the case that organizations independently collect small datasets
for their own use, but often these are not performant for the demands of
machine learning. Organizations could pool these datasets together and jointly
build a strong ASR system; sharing data in the clear, however, comes with
tremendous risk, in terms of intellectual property loss as well as loss of
privacy of the individuals who exist in the dataset. In this paper, we offer a
potential solution for learning an ML model across multiple organizations where
we can provide mathematical guarantees limiting privacy loss. We use a
Federated Learning approach built on a strong foundation of Differential
Privacy techniques. We apply these to a senone classification prototype and
demonstrate that the model improves with the addition of private data while
still respecting privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Selective Differential Privacy for Language Modeling <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.12944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.12944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, <span class="highlight-author">Zhou Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing applications of language models, it has become crucial to
protect these models from leaking private information. Previous work has
attempted to tackle this challenge by training RNN-based language models with
differential privacy guarantees. However, applying classical differential
privacy to language models leads to poor model performance as the underlying
privacy notion is over-pessimistic and provides undifferentiated protection for
all tokens in the data. Given that the private information in natural language
is sparse (for example, the bulk of an email might not carry personally
identifiable information), we propose a new privacy notion, selective
differential privacy, to provide rigorous privacy guarantees on the sensitive
portion of the data to improve model utility. To realize such a new notion, we
develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based
language models. Besides language modeling, we also apply the method to a more
concrete application--dialog systems. Experiments on both language modeling and
dialog system building show that the proposed privacy-preserving mechanism
achieves better utilities while remaining safe under various privacy attacks
compared to the baselines. The data and code are released at
https://github.com/wyshi/lm_privacy to facilitate future research .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Event Linking to Wikidata <span class="chip">NAACL
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adithya Pratapa, Rishubh Gupta, Teruko Mitamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a task of multilingual linking of events to a knowledge base. We
automatically compile a large-scale dataset for this task, comprising of 1.8M
mentions across 44 languages referring to over 10.9K events from Wikidata. We
propose two variants of the event linking task: 1) multilingual, where event
descriptions are from the same language as the mention, and 2) crosslingual,
where all event descriptions are in English. On the two proposed tasks, we
compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and
multilingual adaptations of the biencoder and crossencoder architectures from
BLINK (Wu et al., 2020). In our experiments on the two task variants, we find
both biencoder and crossencoder models significantly outperform the BM25+
baseline. Our results also indicate that the crosslingual task is in general
more challenging than the multilingual task. To test the out-of-domain
generalization of the proposed linking systems, we additionally create a
Wikinews-based evaluation set. We present qualitative analysis highlighting
various aspects captured by the proposed dataset, including the need for
temporal reasoning over context and tackling diverse event descriptions across
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for Multilingual Information Access workshop at NAACL
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation for Dementia Detection in Spoken Language <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Hlédiková, Dominika Woszczyk, Alican Akman, Soteris Demetriou, Björn Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dementia is a growing problem as our society ages, and detection methods are
often invasive and expensive. Recent deep-learning techniques can offer a
faster diagnosis and have shown promising results. However, they require large
amounts of labelled data which is not easily available for the task of dementia
detection. One effective solution to sparse data problems is data augmentation,
though the exact methods need to be selected carefully. To date, there has been
no empirical study of data augmentation on Alzheimer's disease (AD) datasets
for NLP and speech processing. In this work, we investigate data augmentation
techniques for the task of AD detection and perform an empirical evaluation of
the different approaches on two kinds of models for both the text and audio
domains. We use a transformer-based model for both domains, and SVM and Random
Forest models for the text and audio domains, respectively. We generate
additional samples using traditional as well as deep learning based methods and
show that data augmentation improves performance for both the text- and
audio-based models and that such results are comparable to state-of-the-art
results on the popular ADReSS set, with carefully crafted architectures and
features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models (Mostly) Know What They Know 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, Jared Kaplan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability "P(True)" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict "P(IK)", the probability
that "I know" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and in the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23+17 pages; refs added, typos fixed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of
  <span class="highlight-title">Transformer</span>s for Patronizing and Condescending Language Detection <span class="chip">SemEval-2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dou Hu, Mengyuan Zhou, Xiyang Du, Mengfei Yuan, Meizhi Jin, Lianxin Jiang, Yang Mo, Xiaofeng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patronizing and condescending language (PCL) has a large harmful impact and
is difficult to detect, both for human judges and existing NLP systems. At
SemEval-2022 Task 4, we propose a novel Transformer-based model and its
ensembles to accurately understand such language context for PCL detection. To
facilitate comprehension of the subtle and subjective nature of PCL, two
fine-tuning strategies are applied to capture discriminative features from
diverse linguistic behaviour and categorical distribution. The system achieves
remarkable results on the official ranking, including 1st in Subtask 1 and 5th
in Subtask 2. Extensive experiments on the task demonstrate the effectiveness
of our system and its strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, submitted in SemEval-2022 Workshop (co-located with NAACL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Compression of Generative <span class="highlight-title">Pre-train</span>ed Language Models via Quantization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, <span class="highlight-author">Qun Liu</span>, Ping Luo, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Retrieve Videos by Asking Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.05739v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.05739v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Madasu, Junier Oliva, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of traditional text-to-video retrieval systems operate in static
environments, i.e., there is no interaction between the user and the agent
beyond the initial textual query provided by the user. This can be sub-optimal
if the initial query has ambiguities, which would lead to many falsely
retrieved videos. To overcome this limitation, we propose a novel framework for
Video Retrieval using Dialog (ViReD), which enables the user to interact with
an AI agent via multiple rounds of dialog, where the user refines retrieved
results by answering questions generated by an AI agent. Our novel multimodal
question generator learns to ask questions that maximize the subsequent video
retrieval performance using (i) the video candidates retrieved during the last
round of interaction with the user and (ii) the text-based dialog history
documenting all previous interactions, to generate questions that incorporate
both visual and linguistic cues relevant to video retrieval. Furthermore, to
generate maximally informative questions, we propose an Information-Guided
Supervision (IGS), which guides the question generator to ask questions that
would boost subsequent video retrieval accuracy. We validate the effectiveness
of our interactive ViReD framework on the AVSD dataset, showing that our
interactive method performs significantly better than traditional
non-interactive video retrieval systems. We also demonstrate that our proposed
approach generalizes to the real-world settings that involve interactions with
real humans, thus, demonstrating the robustness and generality of our framework
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IAM: A Comprehensive and Large-Scale <span class="highlight-title">Dataset</span> for Integrated Argument
  Mining Tasks <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liying Cheng, Lidong Bing, Ruidan He, Qian Yu, Yan Zhang, Luo Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, a debate usually requires a manual preparation process,
including reading plenty of articles, selecting the claims, identifying the
stances of the claims, seeking the evidence for the claims, etc. As the AI
debate attracts more attention these years, it is worth exploring the methods
to automate the tedious process involved in the debating system. In this work,
we introduce a comprehensive and large dataset named IAM, which can be applied
to a series of argument mining tasks, including claim extraction, stance
classification, evidence extraction, etc. Our dataset is collected from over 1k
articles related to 123 topics. Near 70k sentences in the dataset are fully
annotated based on their argument properties (e.g., claims, stances, evidence,
etc.). We further propose two new integrated argument mining tasks associated
with the debate preparation process: (1) claim extraction with stance
classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a
pipeline approach and an end-to-end method for each integrated task separately.
Promising experimental results are reported to show the values and challenges
of our proposed tasks, and motivate future research on argument mining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, accepted by ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Span-based Joint Entity and Relation Extraction via Squence
  Tagging Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.10080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.10080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Ji, Shasha Li, Jie Yu, Jun Ma, Huijun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Span-based joint extraction simultaneously conducts named entity recognition
(NER) and relation extraction (RE) in text span form. Recent studies have shown
that token labels can convey crucial task-specific information and enrich token
semantics. However, as far as we know, due to completely abstain from sequence
tagging mechanism, all prior span-based work fails to use token label
in-formation. To solve this problem, we pro-pose Sequence Tagging enhanced
Span-based Network (STSN), a span-based joint extrac-tion network that is
enhanced by token BIO label information derived from sequence tag-ging based
NER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral
architecture to build STSN, and each atten-tion layer consists of three basic
attention units. The deep neural architecture first learns seman-tic
representations for token labels and span-based joint extraction, and then
constructs in-formation interactions between them, which also realizes
bidirectional information interac-tions between span-based NER and RE.
Fur-thermore, we extend the BIO tagging scheme to make STSN can extract
overlapping en-tity. Experiments on three benchmark datasets show that our
model consistently outperforms previous optimal models by a large margin,
creating new state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Ji, Shasha Li, Jie Yu, Jun Ma, Huijun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For Named Entity Recognition (NER), sequence labeling-based and span-based
paradigms are quite different. Previous research has demonstrated that the two
paradigms have clear complementary advantages, but few models have attempted to
leverage these advantages in a single NER model as far as we know. In our
previous work, we proposed a paradigm known as Bundling Learning (BL) to
address the above problem. The BL paradigm bundles the two NER paradigms,
enabling NER models to jointly tune their parameters by weighted summing each
paradigm's training loss. However, three critical issues remain unresolved:
When does BL work? Why does BL work? Can BL enhance the existing
state-of-the-art (SOTA) NER models? To address the first two issues, we
implement three NER models, involving a sequence labeling-based model--SeqNER,
a span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER
together. We draw two conclusions regarding the two issues based on the
experimental results on eleven NER datasets from five domains. We then apply BL
to five existing SOTA NER models to investigate the third issue, consisting of
three sequence labeling-based models and two span-based models. Experimental
results indicate that BL consistently enhances their performance, suggesting
that it is possible to construct a new SOTA NER system by incorporating BL into
the current SOTA system. Moreover, we find that BL reduces both entity boundary
and type prediction errors. In addition, we compare two commonly used labeling
tagging methods as well as three types of span semantic representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A <span class="highlight-title">Survey</span> of <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-Train</span>ed Models <span class="chip">IJCAI-2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Du, Zi<span class="highlight-author">kang Liu</span>, Junyi Li, <span class="highlight-author">Wayne Xin Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As transformer evolves, pre-trained models have advanced at a breakneck pace
in recent years. They have dominated the mainstream techniques in natural
language processing (NLP) and computer vision (CV). How to adapt pre-training
to the field of Vision-and-Language (V-L) learning and improve downstream task
performance becomes a focus of multimodal learning. In this paper, we review
the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the
core content, we first briefly introduce several ways to encode raw images and
texts to single-modal embeddings before pre-training. Then, we dive into the
mainstream architectures of VL-PTMs in modeling the interaction between text
and image representations. We further present widely-used pre-training tasks,
and then we introduce some common downstream tasks. We finally conclude this
paper and present some promising research directions. Our survey aims to
provide researchers with synthesis and pointer to related research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI-2022 survey track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HQANN: Efficient and Robust Similarity Search for Hybrid Queries with
  Structured and Unstructured Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Junlin He, Yu Qiao, Guoheng Fu, Li Liu, Jin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The in-memory approximate nearest neighbor search (ANNS) algorithms have
achieved great success for fast high-recall query processing, but are extremely
inefficient when handling hybrid queries with unstructured (i.e., feature
vectors) and structured (i.e., related attributes) constraints. In this paper,
we present HQANN, a simple yet highly efficient hybrid query processing
framework which can be easily embedded into existing proximity graph-based ANNS
algorithms. We guarantee both low latency and high recall by leveraging
navigation sense among attributes and fusing vector similarity search with
attribute filtering. Experimental results on both public and in-house datasets
demonstrate that HQANN is 10x faster than the state-of-the-art hybrid ANNS
solutions to reach the same recall quality and its performance is hardly
affected by the complexity of attributes. It can reach 99\% recall@10 in just
around 50 microseconds On GLOVE-1.2M with thousands of attribute constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BCRLSP: An Offline Reinforcement Learning Framework for Sequential
  Targeted Promotion <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanglin Chen, Xiao Liu, Bo Tang, Feiyu Xiong, Serim Hwang, Guomian Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We utilize an offline reinforcement learning (RL) model for sequential
targeted promotion in the presence of budget constraints in a real-world
business environment. In our application, the mobile app aims to boost customer
retention by sending cash bonuses to customers and control the costs of such
cash bonuses during each time period. To achieve the multi-task goal, we
propose the Budget Constrained Reinforcement Learning for Sequential Promotion
(BCRLSP) framework to determine the value of cash bonuses to be sent to users.
We first find out the target policy and the associated Q-values that maximizes
the user retention rate using an RL model. A linear programming (LP) model is
then added to satisfy the constraints of promotion costs. We solve the LP
problem by maximizing the Q-values of actions learned from the RL model given
the budget constraints. During deployment, we combine the offline RL model with
the LP model to generate a robust policy under the budget constraints. Using
both online and offline experiments, we demonstrate the efficacy of our
approach by showing that BCRLSP achieves a higher long-term customer retention
rate and a lower cost than various baselines. Taking advantage of the near
real-time cost control method, the proposed framework can easily adapt to data
with a noisy behavioral policy and/or meet flexible budget constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, DRL4IR@SIGIR</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually-aware Acoustic Event Detection using Heterogeneous Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shirian, Krishna Somandepalli, Victor Sanchez, Tanaya Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception of auditory events is inherently multimodal relying on both audio
and visual cues. A large number of existing multimodal approaches process each
modality using modality-specific models and then fuse the embeddings to encode
the joint information. In contrast, we employ heterogeneous graphs to
explicitly capture the spatial and temporal relationships between the
modalities and represent detailed information about the underlying signal.
Using heterogeneous graph approaches to address the task of visually-aware
acoustic event classification, which serves as a compact, efficient and
scalable way to represent data in the form of graphs. Through heterogeneous
graphs, we show efficiently modelling of intra- and inter-modality
relationships both at spatial and temporal scales. Our model can easily be
adapted to different scales of events through relevant hyperparameters.
Experiments on AudioSet, a large benchmark, shows that our model achieves
state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> <span class="highlight-title">Dialog</span> Systems with Dual Knowledge-enhanced Generative
  <span class="highlight-title">Pretrain</span>ed Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolin Chen, Xuemeng Song, Liqiang Jing, Shuo Li, Linmei Hu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text response generation for multimodal task-oriented dialog systems, which
aims to generate the proper text response given the multimodal context, is an
essential yet challenging task. Although existing efforts have achieved
compelling success, they still suffer from two pivotal limitations: 1) overlook
the benefit of generative pre-training, and 2) ignore the textual context
related knowledge. To address these limitations, we propose a novel dual
knowledge-enhanced generative pretrained language model for multimodal
task-oriented dialog systems (DKMD), consisting of three key components: dual
knowledge selection, dual knowledge-enhanced context learning, and
knowledge-enhanced response generation. To be specific, the dual knowledge
selection component aims to select the related knowledge according to both
textual and visual modalities of the given context. Thereafter, the dual
knowledge-enhanced context learning component targets seamlessly integrating
the selected knowledge into the multimodal context learning from both global
and local perspectives, where the cross-modal semantic relation is also
explored. Moreover, the knowledge-enhanced response generation component
comprises a revised BART decoder, where an additional dot-product
knowledge-decoder attention sub-layer is introduced for explicitly utilizing
the knowledge to advance the text response generation. Extensive experiments on
a public dataset verify the superiority of the proposed DKMD over
state-of-the-art competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New Framework for Code-Mapping-based Reversible Data Hiding in JPEG
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.15984v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.15984v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Du, Zhaoxia Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code mapping (CM) is an efficient technique for reversible data hiding (RDH)
in JPEG images, which embeds data by constructing a mapping relationship
between the used and unused codes in the JPEG bitstream. This study presents a
new framework for designing a CM-based RDH method. First, a new code mapping
strategy is proposed to suppress file size expansion and improve applicability.
Based on our proposed strategy, the mapped codes are redefined by creating a
new Huffman table rather than selecting them from the unused codes in the
original Huffman table. The critical issue of designing the CM-based RDH
method, that is, constructing code mapping, is converted into a combinatorial
optimization problem. This study proposes a novel CM-based RDH method that
utilizes a genetic algorithm (GA). The experimental results demonstrate that
the proposed method achieves a high embedding capacity with no signal
distortion while suppressing file size expansion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AVA-AVD: Audio-Visual Speaker Diarization in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.14448v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.14448v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zhongcong Xu, Zeyang Song, Satoshi Tsutsui, Chao Feng, Mang Ye, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speaker diarization aims at detecting "who spoke when" using
both auditory and visual signals. Existing audio-visual diarization datasets
are mainly focused on indoor environments like meeting rooms or news studios,
which are quite different from in-the-wild videos in many scenarios such as
movies, documentaries, and audience sitcoms. To develop diarization methods for
these challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)
dataset. Our experiments demonstrate that adding AVA-AVD into training set can
produce significantly better diarization models for in-the-wild videos despite
that the data is relatively small. Moreover, this benchmark is challenging due
to the diverse scenes, complicated acoustic conditions, and completely
off-screen speakers. As a first step towards addressing the challenges, we
design the Audio-Visual Relation Network (AVR-Net) which introduces a simple
yet effective modality mask to capture discriminative information based on face
visibility. Experiments show that our method not only can outperform
state-of-the-art methods but is more robust as varying the ratio of off-screen
speakers. Our data and code has been made publicly available at
https://github.com/showlab/AVA-AVD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACMMM 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-15T00:00:00Z">2022-07-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning about Actions over Visual and Linguistic Modalities: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailaja Keyur Sampat, Maitreya Patel, Subhasish Das, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  'Actions' play a vital role in how humans interact with the world and enable
them to achieve desired goals. As a result, most common sense (CS) knowledge
for humans revolves around actions. While 'Reasoning about Actions & Change'
(RAC) has been widely studied in the Knowledge Representation community, it has
recently piqued the interest of NLP and computer vision researchers. This paper
surveys existing tasks, benchmark datasets, various techniques and models, and
their respective performance concerning advancements in RAC in the vision and
language domain. Towards the end, we summarize our key takeaways, discuss the
present challenges facing this research area, and outline potential directions
for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures; This survey will be periodically updated with the
  latest works in this area</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prerona Tarannum, Firoj Alam, Md. Arid Hasan, Sheak Rashed Haider Noori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide use of social media and digital technologies facilitates sharing
various news and information about events and activities. Despite sharing
positive information misleading and false information is also spreading on
social media. There have been efforts in identifying such misleading
information both manually by human experts and automatic tools. Manual effort
does not scale well due to the high volume of information, containing factual
claims, are appearing online. Therefore, automatically identifying check-worthy
claims can be very useful for human experts. In this study, we describe our
participation in Subtask-1A: Check-worthiness of tweets (English, Dutch and
Spanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing
steps and applied different models to identify whether a given text is worthy
of fact checking or not. We use the oversampling technique to balance the
dataset and applied SVM and Random Forest (RF) with TF-IDF representations. We
also used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models
for the experiments. We used BERT-m for the official submissions and our
systems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,
respectively. In further experiments, our evaluation shows that transformer
models (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and
English languages where a different scenario is observed for Spanish.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CLEF 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting <span class="highlight-title">Multi-Modal</span> E-commerce Attribute Value Extraction via Unified
  Learning Scheme and Dynamic Range Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyin Liu, Chao Zhu, Hongyu Gao, Weibo Gu, Hongfa Wang, Wei Liu, Xu-cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prosperity of e-commerce industry, various modalities, e.g., vision
and language, are utilized to describe product items. It is an enormous
challenge to understand such diversified data, especially via extracting the
attribute-value pairs in text sequences with the aid of helpful image regions.
Although a series of previous works have been dedicated to this task, there
remain seldomly investigated obstacles that hinder further improvements: 1)
Parameters from up-stream single-modal pretraining are inadequately applied,
without proper jointly fine-tuning in a down-stream multi-modal task. 2) To
select descriptive parts of images, a simple late fusion is widely applied,
regardless of priori knowledge that language-related information should be
encoded into a common linguistic embedding space by stronger encoders. 3) Due
to diversity across products, their attribute sets tend to vary greatly, but
current approaches predict with an unnecessary maximal range and lead to more
potential false positives. To address these issues, we propose in this paper a
novel approach to boost multi-modal e-commerce attribute value extraction via
unified learning scheme and dynamic range minimization: 1) Firstly, a unified
scheme is designed to jointly train a multi-modal task with pretrained
single-modal parameters. 2) Secondly, a text-guided information range
minimization method is proposed to adaptively encode descriptive parts of each
modality into an identical space with a powerful pretrained linguistic model.
3) Moreover, a prototype-guided attribute range minimization method is proposed
to first determine the proper attribute set of the current product, and then
select prototypes to guide the prediction of the chosen attributes. Experiments
on the popular multi-modal e-commerce benchmarks show that our approach
achieves superior performance over the other state-of-the-art techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Flexible Schema-Guided <span class="highlight-title">Dialogue</span> Management Framework: From Friendly
  Peer to Virtual Standardized Cancer Patient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Kane, Catherine Giugno, Lenhart Schubert, Kurtis Haut, Caleb Wohn, Ehsan Hoque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A schema-guided approach to dialogue management has been shown in recent work
to be effective in creating robust customizable virtual agents capable of
acting as friendly peers or task assistants. However, successful applications
of these methods in open-ended, mixed-initiative domains remain elusive --
particularly within medical domains such as virtual standardized patients,
where such complex interactions are commonplace -- and require more extensive
and flexible dialogue management capabilities than previous systems provide. In
this paper, we describe a general-purpose schema-guided dialogue management
framework used to develop SOPHIE, a virtual standardized cancer patient that
allows a doctor to conveniently practice for interactions with patients. We
conduct a crowdsourced evaluation of conversations between medical students and
SOPHIE. Our agent is judged to produce responses that are natural, emotionally
appropriate, and consistent with her role as a cancer patient. Furthermore, it
significantly outperforms an end-to-end neural model fine-tuned on a human
standardized patient corpus, attesting to the advantages of a schema-guided
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Non-Cooperative <span class="highlight-title">Dialogue</span>: Theoretical and Empirical Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Sicilia, Tristan Maidment, Pat Healy, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating cooperativity of interlocutors is central in studying
pragmatics of dialogue. Models of conversation that only assume cooperative
agents fail to explain the dynamics of strategic conversations. Thus, we
investigate the ability of agents to identify non-cooperative interlocutors
while completing a concurrent visual-dialogue task. Within this novel setting,
we study the optimality of communication strategies for achieving this
multi-task objective. We use the tools of learning theory to develop a
theoretical model for identifying non-cooperative interlocutors and apply this
theory to analyze different communication strategies. We also introduce a
corpus of non-cooperative conversations about images in the GuessWhat?! dataset
proposed by De Vries et al. (2017). We use reinforcement learning to implement
multiple communication strategies in this context and find empirical results
validate our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LineCap: Line Charts for Data Visualization Captioning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anita Mahinpei, Zona Kostic, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data visualization captions help readers understand the purpose of a
visualization and are crucial for individuals with visual impairments. The
prevalence of poor figure captions and the successful application of deep
learning approaches to image captioning motivate the use of similar techniques
for automated figure captioning. However, research in this field has been
stunted by the lack of suitable datasets. We introduce LineCap, a novel figure
captioning dataset of 3,528 figures, and we provide insights from curating this
dataset and using end-to-end deep learning models for automated figure
captioning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Semantic Grounding in Language Models of Code with
  Representational Similarity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shounak Naik, Rajaswa Patil, Swati Agarwal, Veeky Baths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representational Similarity Analysis is a method from cognitive neuroscience,
which helps in comparing representations from two different sources of data. In
this paper, we propose using Representational Similarity Analysis to probe the
semantic grounding in language models of code. We probe representations from
the CodeBERT model for semantic grounding by using the data from the IBM
CodeNet dataset. Through our experiments, we show that current pre-training
methods do not induce semantic grounding in language models of code, and
instead focus on optimizing form-based patterns. We also show that even a
little amount of fine-tuning on semantically relevant tasks increases the
semantic grounding in CodeBERT significantly. Our ablations with the input
modality to the CodeBERT model show that using bimodal inputs (code and natural
language) over unimodal inputs (only code) gives better semantic grounding and
sample efficiency during semantic fine-tuning. Finally, our experiments with
semantic perturbations in code reveal that CodeBERT is able to robustly
distinguish between semantically correct and incorrect code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ADMA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Start to Finish: Latency Reduction Strategies for Incremental
  Speech Synthesis in Simultaneous Speech-to-Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Liu, Changhan Wang, Hongyu Gong, Xutai Ma, Yun Tang, Juan Pino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-to-speech translation (S2ST) converts input speech to speech in
another language. A challenge of delivering S2ST in real time is the
accumulated delay between the translation and speech synthesis modules. While
recently incremental text-to-speech (iTTS) models have shown large quality
improvements, they typically require additional future text inputs to reach
optimal performance. In this work, we minimize the initial waiting time of iTTS
by adapting the upstream speech translator to generate high-quality pseudo
lookahead for the speech synthesizer. After mitigating the initial delay, we
demonstrate that the duration of synthesized speech also plays a crucial role
on latency. We formalize this as a latency metric and then present a simple yet
effective duration-scaling approach for latency reduction. Our approaches
consistently reduce latency by 0.2-0.5 second without sacrificing speech
translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HLT-MT: High-resource Language-specific Training for Multilingual Neural
  Machine Translation <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Yang, Yuwei Yin, Shuming Ma, Dongdong Zhang, Zhoujun Li, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual neural machine translation (MNMT) trained in multiple language
pairs has attracted considerable attention due to fewer model parameters and
lower training costs by sharing knowledge among multiple languages.
Nonetheless, multilingual training is plagued by language interference
degeneration in shared parameters because of the negative interference among
different translation directions, especially on high-resource languages. In
this paper, we propose the multilingual translation model with the
high-resource language-specific training (HLT-MT) to alleviate the negative
interference, which adopts the two-stage training with the language-specific
selection mechanism. Specifically, we first train the multilingual model only
with the high-resource pairs and select the language-specific modules at the
top of the decoder to enhance the translation quality of high-resource
directions. Next, the model is further trained on all available corpora to
transfer knowledge from high-resource languages (HRLs) to low-resource
languages (LRLs). Experimental results show that HLT-MT outperforms various
strong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic
experiments validate the effectiveness of our method in mitigating the negative
interference in multilingual training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, IJCAI-ECAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear-time calculation of the expected sum of edge lengths in planar
  linearizations of trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lluís Alemany-Puig, Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependency graphs have proven to be a very successful model to represent the
syntactic structure of sentences of human languages. In these graphs, widely
accepted to be trees, vertices are words and arcs connect
syntactically-dependent words. The tendency of these dependencies to be short
has been demonstrated using random baselines for the sum of the lengths of the
edges or its variants. A ubiquitous baseline is the expected sum in projective
orderings (wherein edges do not cross and the root word of the sentence is not
covered by any edge). It was shown that said expected value can be computed in
$O(n)$ time. In this article we focus on planar orderings (where the root word
can be covered) and present two main results. First, we show the relationship
between the expected sum in planar arrangements and the expected sum in
projective arrangements. Second, we also derive a $O(n)$-time algorithm to
calculate the expected value of the sum of edge lengths. These two results stem
from another contribution of the present article, namely a characterization of
planarity that, given a sentence, yields either the number of planar
permutations or an efficient algorithm to generate uniformly random planar
permutations of the words. Our research paves the way for replicating past
research on dependency distance minimization using random planar linearizations
as random baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated with comments from a colleague</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibration of Natural Language Understanding Models with Venn--ABERS
  Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrizio Giovannotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, currently the state-of-the-art in natural language
understanding (NLU) tasks, are prone to generate uncalibrated predictions or
extreme probabilities, making the process of taking different decisions based
on their output relatively difficult. In this paper we propose to build several
inductive Venn--ABERS predictors (IVAP), which are guaranteed to be well
calibrated under minimal assumptions, based on a selection of pre-trained
transformers. We test their performance over a set of diverse NLU tasks and
show that they are capable of producing well-calibrated probabilistic
predictions that are uniformly spread over the [0,1] interval -- all while
retaining the original model's predictive accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 11th Symposium on Conformal and Probabilistic
  Prediction with Applications - COPA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Speech Recognition for Speech Assessment of Persian Preschool
  Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12886v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12886v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Abaskohi, Fatemeh Mortazavi, Hadi Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about a children's growth and development. The
coronavirus pandemic has highlighted the necessity of online assessment for
preschool children. One of the areas that should be tested is the ability to
speak. Because of the differences between children's and adults' voices,
employing Automatic Speech Recognition(ASR) systems is difficult since they are
pre-trained on adults' voices. We constructed an ASR for our cognitive test
system to solve this issue using the Wav2Vec 2.0 model with a new pre-training
objective called Random Frequency Pitch(RFP). In addition, we used our new
dataset to fine-tune our model for Meaningless Words(MW) and Rapid Automatic
Naming(RAN) tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on
the Persian section of the CommonVoice dataset. Furthermore, our novel
methodology produces positive outcomes in zero- and few-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entailment Graph Learning with Textual Entailment and Soft Transitivity <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typed entailment graphs try to learn the entailment relations between
predicates from text and model them as edges between predicate nodes. The
construction of entailment graphs usually suffers from severe sparsity and
unreliability of distributional similarity. We propose a two-stage method,
Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns
local entailment relations by recognizing possible textual entailment between
template sentences formed by typed CCG-parsed predicates. Based on the
generated local graph, EGT2 then uses three novel soft transitivity constraints
to consider the logical transitivity in entailment structures. Experiments on
benchmark datasets show that EGT2 can well model the transitivity in entailment
graph to alleviate the sparsity issue, and lead to significant improvement over
current state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted to ACL 2022 (main conference). 10 pages
  for version 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based
  Sentiment Analysis <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, Zhiyong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the "conj" relation between "great" and "dreadful" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Visual-Language Models for Efficient Video Understanding <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.04478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.04478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based visual-language (I-VL) pre-training has shown great success for
learning joint visual-textual representations from large-scale web data,
revealing remarkable ability for zero-shot generalisation. This paper presents
a simple but strong baseline to efficiently adapt the pre-trained I-VL model,
and exploit its powerful ability for resource-hungry video understanding tasks,
with minimal training. Specifically, we propose to optimise a few random
vectors, termed as continuous prompt vectors, that convert video-related tasks
into the same format as the pre-training objectives. In addition, to bridge the
gap between static images and videos, temporal information is encoded with
lightweight Transformers stacking on top of frame-wise visual features.
Experimentally, we conduct extensive ablation studies to analyse the critical
components. On 10 public benchmarks of action recognition, action localisation,
and text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,
we achieve competitive or state-of-the-art performance to existing methods,
despite optimising significantly fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project page: https://ju-chen.github.io/efficient-prompt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual Contrast Stretching on Target Feature for Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.17152v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.17152v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Chao, Cheng Yu, Szu-Wei Fu, Xugang Lu, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement (SE) performance has improved considerably owing to the
use of deep learning models as a base function. Herein, we propose a perceptual
contrast stretching (PCS) approach to further improve SE performance. The PCS
is derived based on the critical band importance function and is applied to
modify the targets of the SE model. Specifically, the contrast of target
features is stretched based on perceptual importance, thereby improving the
overall SE performance. Compared with post-processing-based implementations,
incorporating PCS into the training phase preserves performance and reduces
online computation. Notably, PCS can be combined with different SE model
architectures and training criteria. Furthermore, PCS does not affect the
causality or convergence of SE model training. Experimental results on the
VoiceBank-DEMAND dataset show that the proposed method can achieve
state-of-the-art performance on both causal (PESQ score = 3.07) and noncausal
(PESQ score = 3.35) SE tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Injection: Parameterization of Fixed Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunbi Choi, Yongrae Jo, Joel Jang, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that attaching prompts to the input is effective at
conditioning Language Models (LM) to perform specific tasks. However, prompts
are always included in the input text during inference, thus incurring
substantial computational and memory overhead. Also, there is currently no
straightforward method of utilizing prompts that are longer than the maximum
input length of the LMs without incurring additional costs during inference. We
propose Prompt Injection (PI), a novel formulation of injecting the prompt into
the parameters of an LM to be an efficient alternative to attaching fixed
prompts to the input. We show that in scenarios with long fixed prompts, PI can
be up to 280 times more efficient in terms of total FLOPs than previous
approaches. We further explore methodologies for PI and show promising results
in persona-dependent conversation, semantic parsing, and zero-shot learning
with task instructions. Through these explorations, we show that PI can be a
promising direction for conditioning language models, especially in scenarios
with long and fixed prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PING results in Table 2 updated (bug fixed)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence-aware <span class="highlight-title">multimodal</span> page classification of Brazilian legal
  documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro H. Luz de Araujo, Ana Paula G. S. de Almeida, Fabricio A. Braz, Nilton C. da Silva, Flavio de Barros Vidal, Teofilo E. de Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Brazilian Supreme Court receives tens of thousands of cases each
semester. Court employees spend thousands of hours to execute the initial
analysis and classification of those cases -- which takes effort away from
posterior, more complex stages of the case management workflow. In this paper,
we explore multimodal classification of documents from Brazil's Supreme Court.
We train and evaluate our methods on a novel multimodal dataset of 6,510
lawsuits (339,478 pages) with manual annotation assigning each page to one of
six classes. Each lawsuit is an ordered sequence of pages, which are stored
both as an image and as a corresponding text extracted through optical
character recognition. We first train two unimodal classifiers: a ResNet
pre-trained on ImageNet is fine-tuned on the images, and a convolutional
network with filters of multiple kernel sizes is trained from scratch on
document texts. We use them as extractors of visual and textual features, which
are then combined through our proposed Fusion Module. Our Fusion Module can
handle missing textual or visual input by using learned embeddings for missing
data. Moreover, we experiment with bi-directional Long Short-Term Memory
(biLSTM) networks and linear-chain conditional random fields to model the
sequential nature of the pages. The multimodal approaches outperform both
textual and visual classifiers, especially when leveraging the sequential
nature of the pages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures. This preprint, which was originally written on 8
  April 2021, has not undergone peer review or any post-submission improvements
  or corrections. The Version of Record of this article is published in the
  International Journal on Document Analysis and Recognition, and is available
  online at https://doi.org/10.1007/s10032-022-00406-7 and
  https://rdcu.be/cRvvV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Long-term Dependencies and Short-term Correlations in Patient
  Journey Data with Temporal Attention Networks for Health Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Liu, Zhenhao Zhang, Antonio Jimeno Yepes, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building models for health prediction based on Electronic Health Records
(EHR) has become an active research area. EHR patient journey data consists of
patient time-ordered clinical events/visits from patients. Most existing
studies focus on modeling long-term dependencies between visits, without
explicitly taking short-term correlations between consecutive visits into
account, where irregular time intervals, incorporated as auxiliary information,
are fed into health prediction models to capture latent progressive patterns of
patient journeys. We present a novel deep neural network with four modules to
take into account the contributions of various variables for health prediction:
i) the Stacked Attention module strengthens the deep semantics in clinical
events within each patient journey and generates visit embeddings, ii) the
Short-Term Temporal Attention module models short-term correlations between
consecutive visit embeddings while capturing the impact of time intervals
within those visit embeddings, iii) the Long-Term Temporal Attention module
models long-term dependencies between visit embeddings while capturing the
impact of time intervals within those visit embeddings, iv) and finally, the
Coupled Attention module adaptively aggregates the outputs of Short-Term
Temporal Attention and Long-Term Temporal Attention modules to make health
predictions. Experimental results on MIMIC-III demonstrate superior predictive
accuracy of our model compared to existing state-of-the-art methods, as well as
the interpretability and robustness of this approach. Furthermore, we found
that modeling short-term correlations contributes to local priors generation,
leading to improved predictive modeling of patient journeys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, accepted at ACM BCB 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Task Sampling for Few-shot <span class="highlight-title">Vision-Language</span> Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Hang Yu, Manling Li, Han Zhao, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite achieving state-of-the-art zero-shot performance, existing
vision-language models still fall short of few-shot transfer ability on
domain-specific problems. Classical fine-tuning often fails to prevent highly
expressive models from exploiting spurious correlations. Although
model-agnostic meta-learning (MAML) presents as a natural alternative for
few-shot transfer learning, the expensive computation due to implicit
second-order optimization limits its use on large-scale vision-language models
such as CLIP. While much literature has been devoted to exploring alternative
optimization strategies, we identify another essential aspect towards effective
few-shot transfer learning, task sampling, which is previously only be viewed
as part of data pre-processing in MAML. To show the impact of task sampling, we
propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which
differentiates classical fine-tuning only on uniformly sampling multiple tasks.
Despite its simplicity, we show that MAMF consistently outperforms classical
fine-tuning on five few-shot vision-language classification tasks. We further
show that the effectiveness of the bi-level optimization in MAML is highly
sensitive to the zero-shot performance of a task in the context of few-shot
vision-language classification. The goal of this paper is to provide new
insights on what makes few-shot learning work, and encourage more research into
investigating better task sampling strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> Open-Vocabulary Video Classification via <span class="highlight-title">Pre-Train</span>ed Vision
  and Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, Yin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing vision and language models (VLMs) pre-trained on large-scale
image-text pairs is becoming a promising paradigm for open-vocabulary visual
recognition. In this work, we extend this paradigm by leveraging motion and
audio that naturally exist in video. We present \textbf{MOV}, a simple yet
effective method for \textbf{M}ultimodal \textbf{O}pen-\textbf{V}ocabulary
video classification. In MOV, we directly use the vision encoder from
pre-trained VLMs with minimal modifications to encode video, optical flow and
audio spectrogram. We design a cross-modal fusion mechanism to aggregate
complimentary multimodal information. Experiments on Kinetics-700 and VGGSound
show that introducing flow or audio modality brings large performance gains
over the pre-trained VLM and existing methods. Specifically, MOV greatly
improves the accuracy on base classes, while generalizes better on novel
classes. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video
classification benchmarks, significantly outperforming both traditional
zero-shot methods and recent methods based on VLMs. Code and models will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Is a Caption Worth a Thousand Images? A Controlled Study for
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibani Santurkar, Yann Dubois, Rohan Taori, <span class="highlight-author">Percy Liang</span>, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of CLIP [Radford et al., 2021] has sparked a debate on
whether language supervision can result in vision models with more transferable
representations than traditional image-only methods. Our work studies this
question through a carefully controlled comparison of two approaches in terms
of their ability to learn representations that generalize to downstream
classification tasks. We find that when the pre-training dataset meets certain
criteria -- it is sufficiently large and contains descriptive captions with low
variability -- image-only methods do not match CLIP's transfer performance,
even when they are trained with more image data. However, contrary to what one
might expect, there are practical settings in which these criteria are not met,
wherein added supervision through captions is actually detrimental. Motivated
by our findings, we devise simple prescriptions to enable CLIP to better
leverage the language information present in existing pre-training datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUSOT: Green and Unsupervised Single Object Tracking for Long Video
  Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiruo Zhou, Hongyu Fu, Suya You, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised and unsupervised deep trackers that rely on deep learning
technologies are popular in recent years. Yet, they demand high computational
complexity and a high memory cost. A green unsupervised single-object tracker,
called GUSOT, that aims at object tracking for long videos under a
resource-constrained environment is proposed in this work. Built upon a
baseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT
contains two additional new modules: 1) lost object recovery, and 2)
color-saliency-based shape proposal. They help resolve the tracking loss
problem and offer a more flexible object proposal, respectively. Thus, they
enable GUSOT to achieve higher tracking accuracy in the long run. We conduct
experiments on the large-scale dataset LaSOT with long video sequences, and
show that GUSOT offers a lightweight high-performance tracking solution that
finds applications in mobile and edge computing platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain MRI study for glioma segmentation using convolutional neural
  networks and original post-processing techniques with low computational
  demand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Gerardo Suárez-García Javier Miguel Hernández-López, Eduardo Moreno-Barbosa, Benito de Celis-Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gliomas are brain tumors composed of different highly heterogeneous
histological subregions. Image analysis techniques to identify relevant tumor
substructures have high potential for improving patient diagnosis, treatment
and prognosis. However, due to the high heterogeneity of gliomas, the
segmentation task is currently a major challenge in the field of medical image
analysis. In the present work, the database of the Brain Tumor Segmentation
(BraTS) Challenge 2018, composed of multimodal MRI scans of gliomas, was
studied. A segmentation methodology based on the design and application of
convolutional neural networks (CNNs) combined with original post-processing
techniques with low computational demand was proposed. The post-processing
techniques were the main responsible for the results obtained in the
segmentations. The segmented regions were the whole tumor, the tumor core, and
the enhancing tumor core, obtaining averaged Dice coefficients equal to 0.8934,
0.8376, and 0.8113, respectively. These results reached the state of the art in
glioma segmentation determined by the winners of the challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 12 tables, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MegaPortraits: One-shot Megapixel Neural Head Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, Egor Zakharov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we advance the neural head avatar technology to the megapixel
resolution while focusing on the particularly challenging task of cross-driving
synthesis, i.e., when the appearance of the driving image is substantially
different from the animated source image. We propose a set of new neural
architectures and training methods that can leverage both medium-resolution
video data and high-resolution image data to achieve the desired levels of
rendered image quality and generalization to novel views and motion. We
demonstrate that suggested architectures and methods produce convincing
high-resolution neural avatars, outperforming the competitors in the
cross-driving scenario. Lastly, we show how a trained high-resolution neural
avatar model can be distilled into a lightweight student model which runs in
real-time and locks the identities of neural avatars to several dozens of
pre-defined source images. Real-time operation and identity lock are essential
for many practical applications head avatar systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Non-Anatomical Graph Structure for isolated hand gesture separation in
  continuous gesture sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razieh Rastgoo, Kourosh Kiani, Sergio Escalera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous Hand Gesture Recognition (CHGR) has been extensively studied by
researchers in the last few decades. Recently, one model has been presented to
deal with the challenge of the boundary detection of isolated gestures in a
continuous gesture video [17]. To enhance the model performance and also
replace the handcrafted feature extractor in the presented model in [17], we
propose a GCN model and combine it with the stacked Bi-LSTM and Attention
modules to push the temporal information in the video stream. Considering the
breakthroughs of GCN models for skeleton modality, we propose a two-layer GCN
model to empower the 3D hand skeleton features. Finally, the class
probabilities of each isolated gesture are fed to the post-processing module,
borrowed from [17]. Furthermore, we replace the anatomical graph structure with
some non-anatomical graph structures. Due to the lack of a large dataset,
including both the continuous gesture sequences and the corresponding isolated
gestures, three public datasets in Dynamic Hand Gesture Recognition (DHGR),
RKS-PERSIANSIGN, and ASLVID, are used for evaluation. Experimental results show
the superiority of the proposed model in dealing with isolated gesture
boundaries detection in continuous gesture sequences
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Prediction as an Effective <span class="highlight-title">Pretrain</span>ing Strategy <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana Likhomanenko, Joseph Yitan Cheng, Walter Talbott, Chen Huang, Hanlin Goh, Joshua Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have gained increasing popularity in a wide range of
applications, including Natural Language Processing (NLP), Computer Vision and
Speech Recognition, because of their powerful representational capacity.
However, harnessing this representational capacity effectively requires a large
amount of data, strong regularization, or both, to mitigate overfitting.
Recently, the power of the Transformer has been unlocked by self-supervised
pretraining strategies based on masked autoencoders which rely on
reconstructing masked inputs, directly, or contrastively from unmasked content.
This pretraining strategy which has been used in BERT models in NLP, Wav2Vec
models in Speech and, recently, in MAE models in Vision, forces the model to
learn about relationships between the content in different parts of the input
using autoencoding related objectives. In this paper, we propose a novel, but
surprisingly simple alternative to content reconstruction~-- that of predicting
locations from content, without providing positional information for it. Doing
so requires the Transformer to understand the positional relationships between
different parts of the input, from their content alone. This amounts to an
efficient implementation where the pretext task is a classification problem
among all possible positions for each input token. We experiment on both Vision
and Speech benchmarks, where our approach brings improvements over strong
supervised training baselines and is comparable to modern
unsupervised/self-supervised pretraining methods. Our method also enables
Transformers trained without position embeddings to outperform ones trained
with full position information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOLPHINS: <span class="highlight-title">Dataset</span> for Collaborative Perception enabled Harmonious and
  Interconnected Self-driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqing Mao, Jingyu Guo, Yukuan Jia, Yuxuan Sun, Sheng Zhou, Zhisheng Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-Everything (V2X) network has enabled collaborative perception in
autonomous driving, which is a promising solution to the fundamental defect of
stand-alone intelligence including blind zones and long-range perception.
However, the lack of datasets has severely blocked the development of
collaborative perception algorithms. In this work, we release DOLPHINS: Dataset
for cOllaborative Perception enabled Harmonious and INterconnected
Self-driving, as a new simulated large-scale various-scenario multi-view
multi-modality autonomous driving dataset, which provides a ground-breaking
benchmark platform for interconnected autonomous driving. DOLPHINS outperforms
current datasets in six dimensions: temporally-aligned images and point clouds
from both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle
(V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6
typical scenarios with dynamic weather conditions make the most various
interconnected autonomous driving dataset; meticulously selected viewpoints
providing full coverage of the key areas and every object; 42376 frames and
292549 objects, as well as the corresponding 3D annotations, geo-positions, and
calibrations, compose the largest dataset for collaborative perception; Full-HD
images and 64-line LiDARs construct high-resolution data with sufficient
details; well-organized APIs and open-source codes ensure the extensibility of
DOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and
multi-view collaborative perception tasks on DOLPHINS. The experiment results
show that the raw-level fusion scheme through V2X communication can help to
improve the precision as well as to reduce the necessity of expensive LiDAR
equipment on vehicles when RSUs exist, which may accelerate the popularity of
interconnected self-driving vehicles. DOLPHINS is now available on
https://dolphins-dataset.net/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image and Texture Independent Deep Learning Noise Estimation using
  Multiple Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikmet Kirmizitas, Nurettin Besli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, a novel multiple-frame based image and texture independent
convolutional Neural Network (CNN) noise estimator is introduced. The estimator
works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal
  Feature Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing autonomous driving paradigms involve a multi-stage discrete
pipeline of tasks. To better predict the control signals and enhance user
safety, an end-to-end approach that benefits from joint spatial-temporal
feature learning is desirable. While there are some pioneering works on
LiDAR-based input or implicit design, in this paper we formulate the problem in
an interpretable vision-based setting. In particular, we propose a
spatial-temporal feature learning scheme towards a set of more representative
features for perception, prediction and planning tasks simultaneously, which is
called ST-P3. Specifically, an egocentric-aligned accumulation technique is
proposed to preserve geometry information in 3D space before the bird's eye
view transformation for perception; a dual pathway modeling is devised to take
past motion variations into account for future prediction; a temporal-based
refinement unit is introduced to compensate for recognizing vision-based
elements for planning. To the best of our knowledge, we are the first to
systematically investigate each part of an interpretable end-to-end
vision-based autonomous driving system. We benchmark our approach against
previous state-of-the-arts on both open-loop nuScenes dataset as well as
closed-loop CARLA simulation. The results show the effectiveness of our method.
Source code, model and protocol details are made publicly available at
https://github.com/OpenPerceptionX/ST-P3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/OpenPerceptionX/ST-P3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Keystroke Biometrics Using <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Stragapede, Paula Delgado-Santos, Ruben Tolosana, Ruben Vera-Rodriguez, Richard Guest, Aythami Morales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Behavioural biometrics have proven to be effective against identity theft as
well as be considered user-friendly authentication methods. One of the most
popular traits in the literature is keystroke dynamics due to the large
deployment of computers and mobile devices in our society. This paper focuses
on improving keystroke biometric systems on the free-text scenario. This
scenario is characterised as very challenging due to the uncontrolled text
conditions, the influential of the user's emotional and physical state, and the
in-use application. To overcome these drawbacks, methods based on deep learning
such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks
(RNNs) have been proposed in the literature, outperforming traditional machine
learning methods. However, these architectures still have aspects that need to
be reviewed and improved. To the best of our knowledge, this is the first study
that proposes keystroke biometric systems based on Transformers. The proposed
Transformer architecture has achieved Equal Error Rate (EER) values of 3.84% in
the popular Aalto mobile keystroke database using only 5 enrolment sessions,
outperforming in large margin other state-of-the-art approaches in the
literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CheXplaining in Style: Counterfactual Explanations for Chest X-rays
  using StyleGAN <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Atad, Vitalii Dmytrenko, Yitong Li, Xin<span class="highlight-author">yue Zhang</span>, Matthias Keicher, Jan Kirschke, Bene Wiestler, Ashkan Khakzar, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models used in medical image analysis are prone to raising
reliability concerns due to their black-box nature. To shed light on these
black-box models, previous works predominantly focus on identifying the
contribution of input features to the diagnosis, i.e., feature attribution. In
this work, we explore counterfactual explanations to identify what patterns the
models rely on for diagnosis. Specifically, we investigate the effect of
changing features within chest X-rays on the classifier's output to understand
its decision mechanism. We leverage a StyleGAN-based approach (StyleEx) to
create counterfactual explanations for chest X-rays by manipulating specific
latent directions in their latent space. In addition, we propose EigenFind to
significantly reduce the computation time of generated explanations. We
clinically evaluate the relevancy of our counterfactual explanations with the
help of radiologists. Our code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ICML 2022 Interpretable Machine Learning in
  Healthcare (IMLH) Workshop ----- Project website:
  http://github.com/CAMP-eXplain-AI/Style-CheXplain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Mu, Wenjie Ruan, Leandro S. Marcolino, Qiang Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud models are widely applied in safety-critical scenes, which
delivers an urgent need to obtain more solid proofs to verify the robustness of
models. Existing verification method for point cloud model is time-expensive
and computationally unattainable on large networks. Additionally, they cannot
handle the complete PointNet model with joint alignment network (JANet) that
contains multiplication layers, which effectively boosts the performance of 3D
models. This motivates us to design a more efficient and general framework to
verify various architectures of point cloud models. The key challenges in
verifying the large-scale complete PointNet models are addressed as dealing
with the cross-non-linearity operations in the multiplication layers and the
high computational complexity of high-dimensional point cloud inputs and added
layers. Thus, we propose an efficient verification framework, 3DVerifier, to
tackle both challenges by adopting a linear relaxation function to bound the
multiplication layer and combining forward and backward propagation to compute
the certified bounds of the outputs of the point cloud models. Our
comprehensive experiments demonstrate that 3DVerifier outperforms existing
verification algorithms for 3D models in terms of both efficiency and accuracy.
Notably, our approach achieves an orders-of-magnitude improvement in
verification efficiency for the large network, and the obtained certified
bounds are also significantly tighter than the state-of-the-art verifiers. We
release our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use
by the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow
  Estimation <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wencan Cheng, Jong Hwan Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at European Conference on Computer
  Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect Out-of-Distribution (OOD) data is important in
safety-critical applications of deep learning. The aim is to separate
In-Distribution (ID) data drawn from the training distribution from OOD data
using a measure of uncertainty extracted from a deep neural network. Deep
Ensembles are a well-established method of improving the quality of uncertainty
estimates produced by deep neural networks, and have been shown to have
superior OOD detection performance compared to single models. An existing
intuition in the literature is that the diversity of Deep Ensemble predictions
indicates distributional shift, and so measures of diversity such as Mutual
Information (MI) should be used for OOD detection. We show experimentally that
this intuition is not valid on ImageNet-scale OOD detection -- using MI leads
to 30-40% worse %FPR@95 compared to single-model entropy on some OOD datasets.
We suggest an alternative explanation for Deep Ensembles' better OOD detection
performance -- OOD detection is binary classification and we are ensembling
diverse classifiers. As such we show that practically, even better OOD
detection performance can be achieved for Deep Ensembles by averaging
task-specific detection scores such as Energy over the ensemble.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Softmax Information for Selective Classification with
  Out-of-Distribution Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a task that is receiving an
increasing amount of research attention in the domain of deep learning for
computer vision. However, the performance of detection methods is generally
evaluated on the task in isolation, rather than also considering potential
downstream tasks in tandem. In this work, we examine selective classification
in the presence of OOD data (SCOD). That is to say, the motivation for
detecting OOD samples is to reject them so their impact on the quality of
predictions is reduced. We show under this task specification, that existing
post-hoc methods perform quite differently compared to when evaluated only on
OOD detection. This is because it is no longer an issue to conflate
in-distribution (ID) data with OOD data if the ID data is going to be
misclassified. However, the conflation within ID data of correct and incorrect
predictions becomes undesirable. We also propose a novel method for SCOD,
Softmax Information Retaining Combination (SIRC), that augments softmax-based
confidence scores with feature-agnostic information such that their ability to
identify OOD samples is improved without sacrificing separation between correct
and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale
datasets and convolutional neural network architectures show that SIRC is able
to consistently match or outperform the baseline for SCOD, whilst existing OOD
detection methods fail to do so.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USegScene: Unsupervised Learning of Depth, Optical Flow and Ego-Motion
  with Semantic Guidance and Coupled Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Vertens, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose USegScene, a framework for semantically guided
unsupervised learning of depth, optical flow and ego-motion estimation for
stereo camera images using convolutional neural networks. Our framework
leverages semantic information for improved regularization of depth and optical
flow maps, multimodal fusion and occlusion filling considering dynamic rigid
object motions as independent SE(3) transformations. Furthermore, complementary
to pure photo-metric matching, we propose matching of semantic features,
pixel-wise classes and object instance borders between the consecutive images.
In contrast to previous methods, we propose a network architecture that jointly
predicts all outputs using shared encoders and allows passing information
across the task-domains, e.g., the prediction of optical flow can benefit from
the prediction of the depth. Furthermore, we explicitly learn the depth and
optical flow occlusion maps inside the network, which are leveraged in order to
improve the predictions in therespective regions. We present results on the
popular KITTI dataset and show that our approach outperforms other methods by a
large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSolar tracker: towards unsupervised assessment with open-source data
  of the accuracy of deep learning-based distributed PV mapping <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Kasmi, Laurent Dubus, Philippe Blanc, Yves-Marie Saint-Drenan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photovoltaic (PV) energy is key to mitigating the current energy crisis.
However, distributed PV generation, which amounts to half of the PV energy
generation, makes it increasingly difficult for transmission system operators
(TSOs) to balance the load and supply and avoid grid congestions. Indeed, in
the absence of measurements, estimating the distributed PV generation is tough.
In recent years, many remote sensing-based approaches have been proposed to map
distributed PV installations. However, to be applicable in industrial settings,
one needs to assess the accuracy of the mapping over the whole deployment area.
We build on existing work to propose an automated PV registry pipeline. This
pipeline automatically generates a dataset recording all distributed PV
installations' location, area, installed capacity, and tilt angle. It only
requires aerial orthoimagery and topological data, both of which are freely
accessible online. In order to assess the accuracy of the registry, we propose
an unsupervised method based on the {\it Registre national d'installation}
(RNI), that centralizes all individual PV systems aggregated at communal level,
enabling practitioners to assess the accuracy of the registry and eventually
remove outliers. We deploy our model on 9 French {\it d\'epartements} covering
more than 50 000 square kilometers, providing the largest mapping of
distributed PV panels with this level of detail to date. We then demonstrate
how practitioners can use our unsupervised accuracy assessment method to assess
the accuracy of the outputs. In particular, we show how it can easily identify
outliers in the detections. Overall, our approach paves the way for a safer
integration of deep learning-based pipelines for remote PV mapping. Code is
available at {\tt https://github.com/gabrielkasmi/dsfrance}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 3 tables. Accepted for Workshop on Machine
  Learning for Earth Observation (MACLEAN), in Conjunction with the ECML/PKDD
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Object Tracking and Segmentation via Neural Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillem Braso, Orcun Cetintas, Laura Leal-Taixe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs offer a natural way to formulate Multiple Object Tracking (MOT) and
Multiple Object Tracking and Segmentation (MOTS) within the
tracking-by-detection paradigm. However, they also introduce a major challenge
for learning methods, as defining a model that can operate on such structured
domain is not trivial. In this work, we exploit the classical network flow
formulation of MOT to define a fully differentiable framework based on Message
Passing Networks (MPNs). By operating directly on the graph domain, our method
can reason globally over an entire set of detections and exploit contextual
features. It then jointly predicts both final solutions for the data
association problem and segmentation masks for all objects in the scene while
exploiting synergies between the two tasks. We achieve state-of-the-art results
for both tracking and segmentation in several publicly available datasets. Our
code is available at github.com/ocetintas/MPNTrackSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:1912.07515</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds
  Representing Laparoscopic Scenes <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Alt, Christian Kunz, Darko Katic, Rayan Younis, Rainer Jäkel, Beat Peter Müller-Stich, Martin Wagner, Franziska Mathis-Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic segmentation of surgical scenes is a prerequisite for task
automation in robot assisted interventions. We propose LapSeg3D, a novel
DNN-based approach for the voxel-wise annotation of point clouds representing
surgical scenes. As the manual annotation of training data is highly time
consuming, we introduce a semi-autonomous clustering-based pipeline for the
annotation of the gallbladder, which is used to generate segmented labels for
the DNN. When evaluated against manually annotated data, LapSeg3D achieves an
F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo
porcine livers. We show LapSeg3D to generalize accurately across different
gallbladders and datasets recorded with different RGB-D camera systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted at the 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual-Masked Auto-Encoder for Robust Motion Capture with
  Spatial-Temporal Skeletal Token Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkun Jiang, Jie Chen, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-person motion capture can be challenging due to ambiguities caused by
severe occlusion, fast body movement, and complex interactions. Existing
frameworks build on 2D pose estimations and triangulate to 3D coordinates via
reasoning the appearance, trajectory, and geometric consistencies among
multi-camera observations. However, 2D joint detection is usually incomplete
and with wrong identity assignments due to limited observation angle, which
leads to noisy 3D triangulation results. To overcome this issue, we propose to
explore the short-range autoregressive characteristics of skeletal motion using
transformer. First, we propose an adaptive, identity-aware triangulation module
to reconstruct 3D joints and identify the missing joints for each identity. To
generate complete 3D skeletal motion, we then propose a Dual-Masked
Auto-Encoder (D-MAE) which encodes the joint status with both
skeletal-structural and temporal position encoding for trajectory completion.
D-MAE's flexible masking and encoding mechanism enable arbitrary skeleton
definitions to be conveniently deployed under the same framework. In order to
demonstrate the proposed model's capability in dealing with severe data loss
scenarios, we contribute a high-accuracy and challenging motion capture dataset
of multi-person interactions with severe occlusion. Evaluations on both
benchmark and our new dataset demonstrate the efficiency of our proposed model,
as well as its advantage against the other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Instances as 1D Kernels <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao, Weicai Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a 3D instance representation, termed instance kernels, where
instances are represented by one-dimensional vectors that encode the semantic,
positional, and shape information of 3D instances. We show that instance
kernels enable easy mask inference by simply scanning kernels over the entire
scenes, avoiding the heavy reliance on proposals or heuristic clustering
algorithms in standard 3D instance segmentation pipelines. The idea of instance
kernel is inspired by recent success of dynamic convolutions in 2D/3D instance
segmentation. However, we find it non-trivial to represent 3D instances due to
the disordered and unstructured nature of point cloud data, e.g., poor instance
localization can significantly degrade instance representation. To remedy this,
we construct a novel 3D instance encoding paradigm. First, potential instance
centroids are localized as candidates. Then, a candidate merging scheme is
devised to simultaneously aggregate duplicated candidates and collect context
around the merged centroids to form the instance kernels. Once instance kernels
are available, instance masks can be reconstructed via dynamic convolutions
whose weights are conditioned on instance kernels. The whole pipeline is
instantiated with a dynamic kernel network (DKNet). Results show that DKNet
outperforms the state of the arts on both ScanNetV2 and S3DIS datasets with
better instance localization. Code is available:
https://github.com/W1zheng/DKNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in ECCV, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CKD-TransBTS: Clinical Knowledge-Driven Hybrid <span class="highlight-title">Transformer</span> with
  Modality-Correlated Cross-Attention for Brain Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Lin, Jiatai Lin, Cheng Lu, Hao Chen, Huan Lin, Bingchao Zhao, Zhenwei Shi, Bingjiang Qiu, Xipeng Pan, Zeyan Xu, Biao Huang, Changhong Liang, Guoqiang Han, Zaiyi Liu, Chu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor segmentation (BTS) in magnetic resonance image (MRI) is crucial
for brain tumor diagnosis, cancer management and research purposes. With the
great success of the ten-year BraTS challenges as well as the advances of CNN
and Transformer algorithms, a lot of outstanding BTS models have been proposed
to tackle the difficulties of BTS in different technical aspects. However,
existing studies hardly consider how to fuse the multi-modality images in a
reasonable manner. In this paper, we leverage the clinical knowledge of how
radiologists diagnose brain tumors from multiple MRI modalities and propose a
clinical knowledge-driven brain tumor segmentation model, called CKD-TransBTS.
Instead of directly concatenating all the modalities, we re-organize the input
modalities by separating them into two groups according to the imaging
principle of MRI. A dual-branch hybrid encoder with the proposed
modality-correlated cross-attention block (MCCA) is designed to extract the
multi-modality image features. The proposed model inherits the strengths from
both Transformer and CNN with the local feature representation ability for
precise lesion boundaries and long-range feature extraction for 3D volumetric
images. To bridge the gap between Transformer and CNN features, we propose a
Trans&CNN Feature Calibration block (TCFC) in the decoder. We compare the
proposed model with five CNN-based models and six transformer-based models on
the BraTS 2021 challenge dataset. Extensive experiments demonstrate that the
proposed model achieves state-of-the-art brain tumor segmentation performance
compared with all the competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trainable Joint Bilateral Filters for Enhanced Prediction Stability in
  Low-dose CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Wagner, Mareike Thies, Felix Denzinger, Mingxuan Gu, Mayank Patwari, Stefan Ploner, Noah Maul, Laura Pfaff, Yixing Huang, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-dose computed tomography (CT) denoising algorithms aim to enable reduced
patient dose in routine CT acquisitions while maintaining high image quality.
Recently, deep learning~(DL)-based methods were introduced, outperforming
conventional denoising algorithms on this task due to their high model
capacity. However, for the transition of DL-based denoising to clinical
practice, these data-driven approaches must generalize robustly beyond the seen
training data. We, therefore, propose a hybrid denoising approach consisting of
a set of trainable joint bilateral filters (JBFs) combined with a convolutional
DL-based denoising network to predict the guidance image. Our proposed
denoising pipeline combines the high model capacity enabled by DL-based feature
extraction with the reliability of the conventional JBF. The pipeline's ability
to generalize is demonstrated by training on abdomen CT scans without metal
implants and testing on abdomen scans with metal implants as well as on head CT
data. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in
our pipeline, the denoising performance is improved by $10\,\%$/$82\,\%$ (RMSE)
and $3\,\%$/$81\,\%$ (PSNR) in regions containing metal and by $6\,\%$/$78\,\%$
(RMSE) and $2\,\%$/$4\,\%$ (PSNR) on head CT data, compared to the respective
vanilla model. Concluding, the proposed trainable JBFs limit the error bound of
deep neural networks to facilitate the applicability of DL-based denoisers in
low-dose CT pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Registration based Few-Shot Anomaly Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, Yan-Feng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers few-shot anomaly detection (FSAD), a practical yet
under-studied setting for anomaly detection (AD), where only a limited number
of normal images are provided for each category at training. So far, existing
FSAD studies follow the one-model-per-category learning paradigm used for
standard AD, and the inter-category commonality has not been explored. Inspired
by how humans detect anomalies, i.e., comparing an image in question to normal
images, we here leverage registration, an image alignment task that is
inherently generalizable across categories, as the proxy task, to train a
category-agnostic anomaly detection model. During testing, the anomalies are
identified by comparing the registered features of the test image and its
corresponding support (normal) images. As far as we know, this is the first
FSAD method that trains a single generalizable model and requires no
re-training or parameter fine-tuning for new categories. Experimental results
have shown that the proposed method outperforms the state-of-the-art FSAD
methods by 3%-8% in AUC on the MVTec and MPDD benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral; Code is available at
  https://github.com/MediaBrain-SJTU/RegAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Human Motion Prediction via Gumbel-Softmax Sampling from an
  Auxiliary Space <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingwei Dang, Yongwei Nie, Chengjiang Long, Qing Zhang, Guiqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diverse human motion prediction aims at predicting multiple possible future
pose sequences from a sequence of observed poses. Previous approaches usually
employ deep generative networks to model the conditional distribution of data,
and then randomly sample outcomes from the distribution. While different
results can be obtained, they are usually the most likely ones which are not
diverse enough. Recent work explicitly learns multiple modes of the conditional
distribution via a deterministic network, which however can only cover a fixed
number of modes within a limited range. In this paper, we propose a novel
sampling strategy for sampling very diverse results from an imbalanced
multimodal distribution learned by a deep generative model. Our method works by
generating an auxiliary space and smartly making randomly sampling from the
auxiliary space equivalent to the diverse sampling from the target
distribution. We propose a simple yet effective network architecture that
implements this novel sampling strategy, which incorporates a Gumbel-Softmax
coefficient matrix sampling method and an aggressive diversity promoting hinge
loss function. Extensive experiments demonstrate that our method significantly
improves both the diversity and accuracy of the samplings compared with
previous state-of-the-art sampling approaches. Code and pre-trained models are
available at https://github.com/Droliven/diverse_sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper and Supp of our work accepted by ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feasibility of Inconspicuous GAN-generated Adversarial Patches against
  Object Detection <span class="chip">IJCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Pavlitskaya, Bianca-Marina Codău, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard approaches for adversarial patch generation lead to noisy
conspicuous patterns, which are easily recognizable by humans. Recent research
has proposed several approaches to generate naturalistic patches using
generative adversarial networks (GANs), yet only a few of them were evaluated
on the object detection use case. Moreover, the state of the art mostly focuses
on suppressing a single large bounding box in input by overlapping it with the
patch directly. Suppressing objects near the patch is a different, more complex
task. In this work, we have evaluated the existing approaches to generate
inconspicuous patches. We have adapted methods, originally developed for
different computer vision tasks, to the object detection use case with YOLOv3
and the COCO dataset. We have evaluated two approaches to generate naturalistic
patches: by incorporating patch generation into the GAN training process and by
using the pretrained GAN. For both cases, we have assessed a trade-off between
performance and naturalistic patch appearance. Our experiments have shown, that
using a pre-trained GAN helps to gain realistic-looking patches while
preserving the performance similar to conventional adversarial patches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IJCAI 2022 AISafety workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel
  Splitting in the Frequency Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Mi, Yuge Huang, Jiazhen Ji, Hongquan Liu, Xingkun Xu, Shouhong Ding, Shuigeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the wide application of face recognition systems, there is rising
concern that original face images could be exposed to malicious intents and
consequently cause personal privacy breaches. This paper presents DuetFace, a
novel privacy-preserving face recognition method that employs collaborative
inference in the frequency domain. Starting from a counterintuitive discovery
that face recognition can achieve surprisingly good performance with only
visually indistinguishable high-frequency channels, this method designs a
credible split of frequency channels by their cruciality for visualization and
operates the server-side model on non-crucial channels. However, the model
degrades in its attention to facial features due to the missing visual
information. To compensate, the method introduces a plug-in interactive block
to allow attention transfer from the client-side by producing a feature mask.
The mask is further refined by deriving and overlaying a facial region of
interest (ROI). Extensive experiments on multiple datasets validate the
effectiveness of the proposed method in protecting face images from undesired
visual inspection, reconstruction, and identification while maintaining high
task availability and performance. Results show that the proposed method
achieves a comparable recognition accuracy and computation cost to the
unprotected ArcFace and outperforms the state-of-the-art privacy-preserving
methods. The source code is available at
https://github.com/Tencent/TFace/tree/master/recognition/tasks/duetface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Parallax <span class="highlight-title">Transformer</span> Network for Stereo Image JPEG Artifacts
  Removal <span class="chip">ACM MM2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhao Jiang, Weimin Tan, Ri Cheng, Shili Zhou, Bo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under stereo settings, the performance of image JPEG artifacts removal can be
further improved by exploiting the additional information provided by a second
view. However, incorporating this information for stereo image JPEG artifacts
removal is a huge challenge, since the existing compression artifacts make
pixel-level view alignment difficult. In this paper, we propose a novel
parallax transformer network (PTNet) to integrate the information from stereo
image pairs for stereo image JPEG artifacts removal. Specifically, a
well-designed symmetric bi-directional parallax transformer module is proposed
to match features with similar textures between different views instead of
pixel-level view alignment. Due to the issues of occlusions and boundaries, a
confidence-based cross-view fusion module is proposed to achieve better feature
fusion for both views, where the cross-view features are weighted with
confidence maps. Especially, we adopt a coarse-to-fine design for the
cross-view interaction, leading to better performance. Comprehensive
experimental results demonstrate that our PTNet can effectively remove
compression artifacts and achieves superior performance than other testing
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, ACM MM2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rain Rate Estimation with SAR using NEXRAD measurements with
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurélien Colin, Pierre Tandeo, Charles Peureux, Romain Husson, Nicolas Longepe, Ronan Fablet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing of rainfall events is critical for both operational and
scientific needs, including for example weather forecasting, extreme flood
mitigation, water cycle monitoring, etc. Ground-based weather radars, such as
NOAA's Next-Generation Radar (NEXRAD), provide reflectivity and precipitation
measurements of rainfall events. However, the observation range of such radars
is limited to a few hundred kilometers, prompting the exploration of other
remote sensing methods, paricularly over the open ocean, that represents large
areas not covered by land-based radars. For a number of decades, C-band SAR
imagery such a such as Sentinel-1 imagery has been known to exhibit rainfall
signatures over the sea surface. However, the development of SAR-derived
rainfall products remains a challenge. Here we propose a deep learning approach
to extract rainfall information from SAR imagery. We demonstrate that a
convolutional neural network, such as U-Net, trained on a colocated and
preprocessed Sentinel-1/NEXRAD dataset clearly outperforms state-of-the-art
filtering schemes. Our results indicate high performance in segmenting
precipitation regimes, delineated by thresholds at 1, 3, and 10 mm/h. Compared
to current methods that rely on Koch filters to draw binary rainfall maps,
these multi-threshold learning-based models can provide rainfall estimation for
higher wind speeds and thus may be of great interest for data assimilation
weather forecasting or for improving the qualification of SAR-derived wind
field data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo Co-capture System for Recording and Tracking Fish with Frame- and
  Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedhelm Hamann, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a co-capture system for multi-animal visual data
acquisition using conventional cameras and event cameras. Event cameras offer
multiple advantages over frame-based cameras, such as a high temporal
resolution and temporal redundancy suppression, which enable us to efficiently
capture the fast and erratic movements of fish. We furthermore present an
event-based multi-animal tracking algorithm, which proves the feasibility of
the approach and sets the baseline for further exploration of combining the
advantages of event cameras and conventional cameras for multi-animal tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancement by Your Aesthetic: An Intelligible Unsupervised Personalized
  Enhancer for Low-Light Images <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naishan Zheng, Jie Huang, Qi Zhu, Man Zhou, Feng Zhao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement is an inherently subjective process whose targets
vary with the user's aesthetic. Motivated by this, several personalized
enhancement methods have been investigated. However, the enhancement process
based on user preferences in these techniques is invisible, i.e., a "black
box". In this work, we propose an intelligible unsupervised personalized
enhancer (iUPEnhancer) for low-light images, which establishes the correlations
between the low-light and the unpaired reference images with regard to three
user-friendly attributions (brightness, chromaticity, and noise). The proposed
iUP-Enhancer is trained with the guidance of these correlations and the
corresponding unsupervised loss functions. Rather than a "black box" process,
our iUP-Enhancer presents an intelligible enhancement process with the above
attributions. Extensive experiments demonstrate that the proposed algorithm
produces competitive qualitative and quantitative results while maintaining
excellent flexibility and scalability. This can be validated by personalization
with single/multiple references, cross-attribution references, or merely
adjusting parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Face Recognition with Learnable Privacy Budgets in
  Frequency Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhen Ji, Huan Wang, Yuge Huang, Jiaxiang Wu, Xingkun Xu, Shouhong Ding, ShengChuan Zhang, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Privacy-Preserving Person Re-identification via Person Identify
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuguang Dou, Xinyang Jiang, Qingsong Zhao, Dongsheng Li, Cairong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently privacy concerns of person re-identification (ReID) raise more and
more attention and preserving the privacy of the pedestrian images used by ReID
methods become essential. De-identification (DeID) methods alleviate privacy
issues by removing the identity-related of the ReID data. However, most of the
existing DeID methods tend to remove all personal identity-related information
and compromise the usability of de-identified data on the ReID task. In this
paper, we aim to develop a technique that can achieve a good trade-off between
privacy protection and data usability for person ReID. To achieve this, we
propose a novel de-identification method designed explicitly for person ReID,
named Person Identify Shift (PIS). PIS removes the absolute identity in a
pedestrian image while preserving the identity relationship between image
pairs. By exploiting the interpolation property of variational auto-encoder,
PIS shifts each pedestrian image from the current identity to another with a
new identity, resulting in images still preserving the relative identities.
Experimental results show that our method has a better trade-off between
privacy-preserving and model performance than existing de-identification
methods and can defend against human and model attacks for data privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Dermoscopic Image Feature Representation Learning for
  Melanoma Classification <span class="chip">ICONIP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChengHui Yu, MingKang Tang, ShengGe Yang, MingQing Wang, Zhe Xu, JiangPeng Yan, HanMo Chen, Yu Yang, Xiao-Jun Zeng, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based melanoma classification with dermoscopic images has
recently shown great potential in automatic early-stage melanoma diagnosis.
However, limited by the significant data imbalance and obvious extraneous
artifacts, i.e., the hair and ruler markings, discriminative feature extraction
from dermoscopic images is very challenging. In this study, we seek to resolve
these problems respectively towards better representation learning for lesion
features. Specifically, a GAN-based data augmentation (GDA) strategy is adapted
to generate synthetic melanoma-positive images, in conjunction with the
proposed implicit hair denoising (IHD) strategy. Wherein the hair-related
representations are implicitly disentangled via an auxiliary classifier network
and reversely sent to the melanoma-feature extraction backbone for better
melanoma-specific representation learning. Furthermore, to train the IHD
module, the hair noises are additionally labeled on the ISIC2020 dataset,
making it the first large-scale dermoscopic dataset with annotation of
hair-like artifacts. Extensive experiments demonstrate the superiority of the
proposed framework as well as the effectiveness of each component. The improved
dataset publicly avaliable at https://github.com/kirtsy/DermoscopicDataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICONIP 2021 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Deep Compressive Sensing with Recurrent-Residual Structural
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep compressive sensing (CS) methods either ignore adaptive online
optimization or depend on costly iterative optimizer during reconstruction.
This work explores a novel image CS framework with recurrent-residual
structural constraint, termed as R$^2$CS-NET. The R$^2$CS-NET first
progressively optimizes the acquired samplings through a novel recurrent neural
network. The cascaded residual convolutional network then fully reconstructs
the image from optimized latent representation. As the first deep CS framework
efficiently bridging adaptive online optimization, the R$^2$CS-NET integrates
the robustness of online optimization with the efficiency and nonlinear
capacity of deep learning methods. Signal correlation has been addressed
through the network architecture. The adaptive sensing nature further makes it
an ideal candidate for color image CS via leveraging channel correlation.
Numerical experiments verify the proposed recurrent latent optimization design
not only fulfills the adaptation motivation, but also outperforms classic long
short-term memory (LSTM) architecture in the same scenario. The overall
framework demonstrates hardware implementation feasibility, with leading
robustness and generalization capability among existing deep CS benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengping Yang, Zhe Wang, Ziqiu Chi, Wenyi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022, Code
  Link:https://github.com/kobeshegu/ECCV2022_WaveGAN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-CLIP: End-to-End Multi-grained <span class="highlight-title">Contrastive Learning</span> for Video-Text
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has been a crucial and fundamental task in multi-modal
research. The development of video-text retrieval has been considerably
promoted by large-scale multi-modal contrastive pre-training, which primarily
focuses on coarse-grained or fine-grained contrast. However, cross-grained
contrast, which is the contrast between coarse-grained representations and
fine-grained representations, has rarely been explored in prior research.
Compared with fine-grained or coarse-grained contrasts, cross-grained contrast
calculate the correlation between coarse-grained features and each fine-grained
feature, and is able to filter out the unnecessary fine-grained features guided
by the coarse-grained feature during similarity calculation, thus improving the
accuracy of retrieval. To this end, this paper presents a novel multi-grained
contrastive model, namely X-CLIP, for video-text retrieval. However, another
challenge lies in the similarity aggregation problem, which aims to aggregate
fine-grained and cross-grained similarity matrices to instance-level
similarity. To address this challenge, we propose the Attention Over Similarity
Matrix (AOSM) module to make the model focus on the contrast between essential
frames and words, thus lowering the impact of unnecessary frames and words on
retrieval results. With multi-grained contrast and the proposed AOSM module,
X-CLIP achieves outstanding performance on five widely-used video-text
retrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1
R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous
state-of-theart by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on
these benchmarks, demonstrating the superiority of multi-grained contrast and
AOSM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, ACMMM22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameterization of Cross-Token Relations with Relative Positional
  Encoding for Vision MLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicai Wang, Yanbin Hao, Xingyu Gao, Hao Zhang, Shuo Wang, Tingting Mu, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision multi-layer perceptrons (MLPs) have shown promising performance in
computer vision tasks, and become the main competitor of CNNs and vision
Transformers. They use token-mixing layers to capture cross-token interactions,
as opposed to the multi-head self-attention mechanism used by Transformers.
However, the heavily parameterized token-mixing layers naturally lack
mechanisms to capture local information and multi-granular non-local relations,
thus their discriminative power is restrained. To tackle this issue, we propose
a new positional spacial gating unit (PoSGU). It exploits the attention
formulations used in the classical relative positional encoding (RPE), to
efficiently encode the cross-token relations for token mixing. It can
successfully reduce the current quadratic parameter complexity $O(N^2)$ of
vision MLPs to $O(N)$ and $O(1)$. We experiment with two RPE mechanisms, and
further propose a group-wise extension to improve their expressive power with
the accomplishment of multi-granular contexts. These then serve as the key
building blocks of a new type of vision MLP, referred to as PosMLP. We evaluate
the effectiveness of the proposed approach by conducting thorough experiments,
demonstrating an improved or comparable performance with reduced parameter
complexity. For instance, for a model trained on ImageNet1K, we achieve a
performance improvement from 72.14\% to 74.02\% and a learnable parameter
reduction from $19.4M$ to $18.2M$. Code could be found at
\href{https://github.com/Zhicaiwww/PosMLP}{https://github.com/Zhicaiwww/PosMLP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting <span class="highlight-title">Multi-Modal</span> E-commerce Attribute Value Extraction via Unified
  Learning Scheme and Dynamic Range Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyin Liu, Chao Zhu, Hongyu Gao, Weibo Gu, Hongfa Wang, Wei Liu, Xu-cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prosperity of e-commerce industry, various modalities, e.g., vision
and language, are utilized to describe product items. It is an enormous
challenge to understand such diversified data, especially via extracting the
attribute-value pairs in text sequences with the aid of helpful image regions.
Although a series of previous works have been dedicated to this task, there
remain seldomly investigated obstacles that hinder further improvements: 1)
Parameters from up-stream single-modal pretraining are inadequately applied,
without proper jointly fine-tuning in a down-stream multi-modal task. 2) To
select descriptive parts of images, a simple late fusion is widely applied,
regardless of priori knowledge that language-related information should be
encoded into a common linguistic embedding space by stronger encoders. 3) Due
to diversity across products, their attribute sets tend to vary greatly, but
current approaches predict with an unnecessary maximal range and lead to more
potential false positives. To address these issues, we propose in this paper a
novel approach to boost multi-modal e-commerce attribute value extraction via
unified learning scheme and dynamic range minimization: 1) Firstly, a unified
scheme is designed to jointly train a multi-modal task with pretrained
single-modal parameters. 2) Secondly, a text-guided information range
minimization method is proposed to adaptively encode descriptive parts of each
modality into an identical space with a powerful pretrained linguistic model.
3) Moreover, a prototype-guided attribute range minimization method is proposed
to first determine the proper attribute set of the current product, and then
select prototypes to guide the prediction of the chosen attributes. Experiments
on the popular multi-modal e-commerce benchmarks show that our approach
achieves superior performance over the other state-of-the-art techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Supervised Video Salient Object Detection via Point Supervision <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyong Gao, Haozhe Xing, Wei Zhang, Yan Wang, Qianyu Guo, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video salient object detection models trained on pixel-wise dense annotation
have achieved excellent performance, yet obtaining pixel-by-pixel annotated
datasets is laborious. Several works attempt to use scribble annotations to
mitigate this problem, but point supervision as a more labor-saving annotation
method (even the most labor-saving method among manual annotation methods for
dense prediction), has not been explored. In this paper, we propose a strong
baseline model based on point supervision. To infer saliency maps with temporal
information, we mine inter-frame complementary information from short-term and
long-term perspectives, respectively. Specifically, we propose a hybrid token
attention module, which mixes optical flow and image information from
orthogonal directions, adaptively highlighting critical optical flow
information (channel dimension) and critical token information (spatial
dimension). To exploit long-term cues, we develop the Long-term Cross-Frame
Attention module (LCFA), which assists the current frame in inferring salient
objects based on multi-frame tokens. Furthermore, we label two point-supervised
datasets, P-DAVIS and P-DAVSOD, by relabeling the DAVIS and the DAVSOD dataset.
Experiments on the six benchmark datasets illustrate our method outperforms the
previous state-of-the-art weakly supervised methods and even is comparable with
some fully supervised approaches. Source code and datasets are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Vision <span class="highlight-title">Transformer</span> with Cross Feature Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youpeng Zhao, Huadong Tang, Yingying Jiang, Yong A, Qiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in vision transformers (ViTs) have achieved great performance
in visual recognition tasks. Convolutional neural networks (CNNs) exploit
spatial inductive bias to learn visual representations, but these networks are
spatially local. ViTs can learn global representations with their
self-attention mechanism, but they are usually heavy-weight and unsuitable for
mobile devices. In this paper, we propose cross feature attention (XFA) to
bring down computation cost for transformers, and combine efficient mobile CNNs
to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can
serve as a general-purpose backbone to learn both global and local
representation. Experimental results show that XFormer outperforms numerous CNN
and ViT-based models across different tasks and datasets. On ImageNet1K
dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,
which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT
(ViT-based) for similar number of parameters. Our model also performs well when
transferring to object detection and semantic segmentation tasks. On MS COCO
dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -> 33.2 AP) in YOLOv3
framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with
only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,
surpassing state-of-the-art lightweight segmentation networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaleNet: Searching for the Model to Scale <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyang Xie, Xiu Su, Shan You, Zhanyu Ma, Fei Wang, Chen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, community has paid increasing attention on model scaling and
contributed to developing a model family with a wide spectrum of scales.
Current methods either simply resort to a one-shot NAS manner to construct a
non-structural and non-scalable model family or rely on a manual yet fixed
scaling strategy to scale an unnecessarily best base model. In this paper, we
bridge both two components and propose ScaleNet to jointly search base model
and scaling strategy so that the scaled large model can have more promising
performance. Concretely, we design a super-supernet to embody models with
different spectrum of sizes (e.g., FLOPs). Then, the scaling strategy can be
learned interactively with the base model via a Markov chain-based evolution
algorithm and generalized to develop even larger models. To obtain a decent
super-supernet, we design a hierarchical sampling strategy to enhance its
training sufficiency and alleviate the disturbance. Experimental results show
our scaled networks enjoy significant performance superiority on various FLOPs,
but with at least 2.53x reduction on search cost. Codes are available at
https://github.com/luminolx/ScaleNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Recognition from Detection: Single Shot Self-Reliant Scene
  Text Spotter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Wu, Pengyuan Lyu, Guangming Lu, Chengquan Zhang, Kun Yao, Wenjie Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typical text spotters follow the two-stage spotting strategy: detect the
precise boundary for a text instance first and then perform text recognition
within the located text region. While such strategy has achieved substantial
progress, there are two underlying limitations. 1) The performance of text
recognition depends heavily on the precision of text detection, resulting in
the potential error propagation from detection to recognition. 2) The RoI
cropping which bridges the detection and recognition brings noise from
background and leads to information loss when pooling or interpolating from
feature maps. In this work we propose the single shot Self-Reliant Scene Text
Spotter (SRSTS), which circumvents these limitations by decoupling recognition
from detection. Specifically, we conduct text detection and recognition in
parallel and bridge them by the shared positive anchor point. Consequently, our
method is able to recognize the text instances correctly even though the
precise text boundaries are challenging to detect. Additionally, our method
reduces the annotation cost for text detection substantially. Extensive
experiments on regular-shaped benchmark and arbitrary-shaped benchmark
demonstrate that our SRSTS compares favorably to previous state-of-the-art
spotters in terms of both accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LineCap: Line Charts for Data Visualization Captioning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anita Mahinpei, Zona Kostic, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data visualization captions help readers understand the purpose of a
visualization and are crucial for individuals with visual impairments. The
prevalence of poor figure captions and the successful application of deep
learning approaches to image captioning motivate the use of similar techniques
for automated figure captioning. However, research in this field has been
stunted by the lack of suitable datasets. We introduce LineCap, a novel figure
captioning dataset of 3,528 figures, and we provide insights from curating this
dataset and using end-to-end deep learning models for automated figure
captioning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Bark Beetle-Induced Forest Tree Mortality using Deep
  Learning <span class="chip">ICPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudraksh Kapil, Seyed Mojtaba Marvasti-Zadeh, Devin Goodsman, Nilanjan Ray, Nadir Erbilgin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bark beetle outbreaks can dramatically impact forest ecosystems and services
around the world. For the development of effective forest policies and
management plans, the early detection of infested trees is essential. Despite
the visual symptoms of bark beetle infestation, this task remains challenging,
considering overlapping tree crowns and non-homogeneity in crown foliage
discolouration. In this work, a deep learning based method is proposed to
effectively classify different stages of bark beetle attacks at the individual
tree level. The proposed method uses RetinaNet architecture (exploiting a
robust feature extraction backbone pre-trained for tree crown detection) to
train a shallow subnetwork for classifying the different attack stages of
images captured by unmanned aerial vehicles (UAVs). Moreover, various data
augmentation strategies are examined to address the class imbalance problem,
and consequently, the affine transformation is selected to be the most
effective one for this purpose. Experimental evaluations demonstrate the
effectiveness of the proposed method by achieving an average accuracy of
98.95%, considerably outperforming the baseline method by approximately 10%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract submitted to VAIB Worskhop at ICPR 2022. 4 pages, 6
  figures. The code and results are publicly available at
  https://github.com/rudrakshkapil09/BarkBeetle-DamageClassification-DL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EKTVQA: Generalized use of External Knowledge to empower Scene Text in
  Text-VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.09717v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.09717v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arka Ujjal Dey, Ernest Valveny, Gaurav Harit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The open-ended question answering task of Text-VQA often requires reading and
reasoning about rarely seen or completely unseen scene-text content of an
image. We address this zero-shot nature of the problem by proposing the
generalized use of external knowledge to augment our understanding of the scene
text. We design a framework to extract, validate, and reason with knowledge
using a standard multimodal transformer for vision language understanding
tasks. Through empirical evidence and qualitative results, we demonstrate how
external knowledge can highlight instance-only cues and thus help deal with
training data bias, improve answer entity type correctness, and detect
multiword named entities. We generate results comparable to the
state-of-the-art on three publicly available datasets, under the constraints of
similar upstream OCR systems and training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> learning in non-small cell lung cancer discovers novel
  morphological clusters linked to patient outcome and molecular phenotypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Xinyu Yang, Luis Chiriboga, Afreen Karimkhan, Navneet Narula, Harvey Pass, Andre L. Moreira, John Le Quesne, Aristotelis Tsirigos, Ke Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathological images provide the definitive source of cancer diagnosis,
containing information used by pathologists to identify and subclassify
malignant disease, and to guide therapeutic choices. These images contain vast
amounts of information, much of which is currently unavailable to human
interpretation. Supervised deep learning approaches have been powerful for
classification tasks, but they are inherently limited by the cost and quality
of annotations. Therefore, we developed Histomorphological Phenotype Learning,
an unsupervised methodology, which requires no annotations and operates via the
self-discovery of discriminatory image features in small image tiles. Tiles are
grouped into morphologically similar clusters which appear to represent
recurrent modes of tumor growth emerging under natural selection. These
clusters have distinct features which can be identified using orthogonal
methods. Applied to lung cancer tissues, we show that they align closely with
patient outcomes, with histopathologically recognised tumor types and growth
patterns, and with transcriptomic measures of immunophenotype.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable
  Prototypes <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.15000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.15000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Donnelly, Alina Jade Barnett, Chaofan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deformable prototypical part network (Deformable ProtoPNet), an
interpretable image classifier that integrates the power of deep learning and
the interpretability of case-based reasoning. This model classifies input
images by comparing them with prototypes learned during training, yielding
explanations in the form of "this looks like that." However, while previous
methods use spatially rigid prototypes, we address this shortcoming by
proposing spatially flexible prototypes. Each prototype is made up of several
prototypical parts that adaptively change their relative spatial positions
depending on the input image. Consequently, a Deformable ProtoPNet can
explicitly capture pose variations and context, improving both model accuracy
and the richness of explanations provided. Compared to other case-based
interpretable models using prototypes, our approach achieves state-of-the-art
accuracy and gives an explanation with greater context. The code is available
at https://github.com/jdonnelly36/Deformable-ProtoPNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This was published in CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual
  Simultaneous Localization and Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mátyás Szántó, György R. Bogár, László Vajta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Periodica Polytechnica Electrical Engineering 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Highly Accurate Dichotomous Image Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03041v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03041v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a systematic study on a new task called dichotomous image
segmentation (DIS) , which aims to segment highly accurate objects from natural
images. To this end, we collected the first large-scale DIS dataset, called
DIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images
covering camouflaged, salient, or meticulous objects in various backgrounds.
DIS is annotated with extremely fine-grained labels. Besides, we introduce a
simple intermediate supervision baseline (IS-Net) using both feature-level and
mask-level guidance for DIS model training. IS-Net outperforms various
cutting-edge baselines on the proposed DIS5K, making it a general self-learned
supervision network that can facilitate future research in DIS. Further, we
design a new metric called human correction efforts (HCE) which approximates
the number of mouse clicking operations required to correct the false positives
and false negatives. HCE is utilized to measure the gap between models and
real-world applications and thus can complement existing metrics. Finally, we
conduct the largest-scale benchmark, evaluating 16 representative segmentation
models, providing a more insightful discussion regarding object complexities,
and showing several potential applications (e.g., background removal, art
design, 3D reconstruction). Hoping these efforts can open up promising
directions for both academic and industries. Project page:
https://xuebinqin.github.io/dis/index.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 18 figures, ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HODOR: High-level Object Descriptors for Object Re-segmentation in Video
  Learned from Static Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Athar, Jonathon Luiten, Alexander Hermans, Deva Ramanan, Bastian Leibe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing state-of-the-art methods for Video Object Segmentation (VOS) learn
low-level pixel-to-pixel correspondences between frames to propagate object
masks across video. This requires a large amount of densely annotated video
data, which is costly to annotate, and largely redundant since frames within a
video are highly correlated. In light of this, we propose HODOR: a novel method
that tackles VOS by effectively leveraging annotated static images for
understanding object appearance and scene context. We encode object instances
and scene information from an image frame into robust high-level descriptors
which can then be used to re-segment those objects in different frames. As a
result, HODOR achieves state-of-the-art performance on the DAVIS and
YouTube-VOS benchmarks compared to existing methods trained without video
annotations. Without any architectural modification, HODOR can also learn from
video context around single annotated video frames by utilizing cyclic
consistency, whereas other methods rely on dense, temporally consistent
annotations. Source code is available at: https://github.com/Ali2500/HODOR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODFNet: Using orientation distribution functions to characterize 3D
  point clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.04708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.04708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf H. Sahin, Alican Mertan, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning new representations of 3D point clouds is an active research area in
3D vision, as the order-invariant point cloud structure still presents
challenges to the design of neural network architectures. Recent works explored
learning either global or local features or both for point clouds, however none
of the earlier methods focused on capturing contextual shape information by
analysing local orientation distribution of points. In this paper, we leverage
on point orientation distributions around a point in order to obtain an
expressive local neighborhood representation for point clouds. We achieve this
by dividing the spherical neighborhood of a given point into predefined cone
volumes, and statistics inside each volume are used as point features. In this
way, a local patch can be represented by not only the selected point's nearest
neighbors, but also considering a point density distribution defined along
multiple orientations around the point. We are then able to construct an
orientation distribution function (ODF) neural network that involves an
ODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet
model achieves state-of the-art accuracy for object classification on
ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under consideration at Computer Vision and Image
  Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multimodal</span> Token Fusion for Vision <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.08721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.08721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iColoriT: Towards Propagating Local Hint to the Right Region in
  Interactive Colorization by Leveraging Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyeon Lee, Jooyeol Yun, Minho Park, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-interactive image colorization aims to colorize grayscale images when a
user provides the colors for specific locations. It is essential for
point-interactive colorization methods to appropriately propagate user-provided
colors (i.e., user hints) in the entire image to obtain a reasonably colorized
image with minimal user effort. However, existing approaches often produce
partially colorized results due to the inefficient design of stacking
convolutional layers to propagate hints to distant relevant regions. To address
this problem, we present iColoriT, a novel point-interactive colorization
Vision Transformer capable of propagating user hints to relevant regions,
leveraging the global receptive field of Transformers. The self-attention
mechanism of Transformers enables iColoriT to selectively colorize relevant
regions with only a few local hints. Our approach colorizes images in real-time
by utilizing pixel shuffling, an efficient upsampling technique that replaces
the decoder architecture. Also, in order to mitigate the artifacts caused by
pixel shuffling with large upsampling ratios, we present the local stabilizing
layer. Extensive quantitative and qualitative results demonstrate that our
approach highly outperforms existing methods for point-interactive
colorization, producing accurately colorized images with a user's minimal
effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recur, Attend or Convolve? On Whether Temporal Modeling Matters for
  Cross-Domain Robustness in Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12175v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12175v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Broomé, Ernest Pokropek, Boyu Li, Hedvig Kjellström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most action recognition models today are highly parameterized, and evaluated
on datasets with predominantly spatially distinct classes. It has also been
shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward
texture rather than shape in still image recognition tasks. Taken together,
this raises suspicion that large video models partly learn spurious
correlations rather than to track relevant shapes over time to infer
generalizable semantics from their movement. A natural way to avoid parameter
explosion when learning visual patterns over time is to make use of recurrence.
In this article, we empirically study whether the choice of low-level temporal
modeling has consequences for texture bias and cross-domain robustness. In
order to enable a light-weight and systematic assessment of the ability to
capture temporal structure, not revealed from single frames, we provide the
Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing
for the investigation of texture bias for video models. We find that across a
variety of model sizes, convolutional-recurrent and attention-based models show
better out-of-domain robustness on TS than 3D CNNs. In domain shift experiments
on Diving48, our experiments indicate that 3D CNNs and attention-based models
exhibit more texture bias than convolutional-recurrent models. Moreover,
qualitative examples suggest that convolutional-recurrent models learn more
correct class attributes from the diving data when compared to the other two
types of models at the same global validation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ clDice -- A Novel Topology-Preserving Loss Function for Tubular
  Structure Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.07311v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.07311v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suprosanna Shit, Johannes C. Paetzold, Anjany Sekuboyina, Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien P. W. Pluim, Ulrich Bauer, Bjoern H. Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of tubular, network-like structures, such as vessels,
neurons, or roads, is relevant to many fields of research. For such structures,
the topology is their most important characteristic; particularly preserving
connectedness: in the case of vascular networks, missing a connected vessel
entirely alters the blood-flow dynamics. We introduce a novel similarity
measure termed centerlineDice (short clDice), which is calculated on the
intersection of the segmentation masks and their (morphological) skeleta. We
theoretically prove that clDice guarantees topology preservation up to homotopy
equivalence for binary 2D and 3D segmentation. Extending this, we propose a
computationally efficient, differentiable loss function (soft-clDice) for
training arbitrary neural segmentation networks. We benchmark the soft-clDice
loss on five public datasets, including vessels, roads and neurons (2D and 3D).
Training on soft-clDice leads to segmentation with more accurate connectivity
information, higher graph similarity, and better volumetric scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>* The authors Suprosanna Shit and Johannes C. Paetzold contributed
  equally to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Neural Representations for Variable Length Human Motion
  Generation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, Koichi Shinoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an action-conditional human motion generation method using
variational implicit neural representations (INR). The variational formalism
enables action-conditional distributions of INRs, from which one can easily
sample representations to generate novel human motion sequences. Our method
offers variable-length sequence generation by construction because a part of
INR is optimized for a whole sequence of arbitrary length with temporal
embeddings. In contrast, previous works reported difficulties with modeling
variable-length sequences. We confirm that our method with a Transformer
decoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC
datasets in terms of realism and diversity of generated motions. Surprisingly,
even our method with an MLP decoder consistently outperforms the
state-of-the-art Transformer-based auto-encoder. In particular, we show that
variable-length motions generated by our method are better than fixed-length
motions generated by the state-of-the-art method in terms of realism and
diversity. Code at https://github.com/PACerv/ImplicitMotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying the Adversarial Robustness of Random Transformation
  Defenses <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chawin Sitawarin, Zachary Golan-Strieb, David Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks' lack of robustness against attacks raises concerns in
security-sensitive settings such as autonomous vehicles. While many
countermeasures may look promising, only a few withstand rigorous evaluation.
Defenses using random transformations (RT) have shown impressive results,
particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of
defense has not been rigorously evaluated, leaving its robustness properties
poorly understood. Their stochastic properties make evaluation more challenging
and render many proposed attacks on deterministic models inapplicable. First,
we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation
is ineffective and likely overestimates its robustness. We then attempt to
construct the strongest possible RT defense through the informed selection of
transformations and Bayesian optimization for tuning their parameters.
Furthermore, we create the strongest possible attack to evaluate our RT
defense. Our new attack vastly outperforms the baseline, reducing the accuracy
by 83% compared to the 19% reduction by the commonly used EoT attack
($4.3\times$ improvement). Our result indicates that the RT defense on the
Imagenette dataset (a ten-class subset of ImageNet) is not robust against
adversarial examples. Extending the study further, we use our new attack to
adversarially train RT defense (called AdvRT), resulting in a large robustness
gain. Code is available at
https://github.com/wagner-group/demystify-random-transform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022 (short presentation), AAAI 2022 AdvML Workshop (best paper,
  oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PanoFlow: Learning 360° Optical Flow for Surrounding Temporal
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Yifan Zhou, Kailun Yang, Xiaoting Yin, Ze Wang, Yaozu Ye, Zhe Yin, Shi Meng, Peng Li, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow estimation is a basic task in self-driving and robotics systems,
which enables to temporally interpret traffic scenes. Autonomous vehicles
clearly benefit from the ultra-wide Field of View (FoV) offered by 360{\deg}
panoramic sensors. However, due to the unique imaging process of panoramic
cameras, models designed for pinhole images do not directly generalize
satisfactorily to 360{\deg} panoramic images. In this paper, we put forward a
novel network framework--PanoFlow, to learn optical flow for panoramic images.
To overcome the distortions introduced by equirectangular projection in
panoramic transformation, we design a Flow Distortion Augmentation (FDA)
method, which contains radial flow distortion (FDA-R) or equirectangular flow
distortion (FDA-E). We further look into the definition and properties of
cyclic optical flow for panoramic videos, and hereby propose a Cyclic Flow
Estimation (CFE) method by leveraging the cyclicity of spherical images to
infer 360{\deg} optical flow and converting large displacement to relatively
small displacement. PanoFlow is applicable to any existing flow estimation
method and benefits from the progress of narrow-FoV flow estimation. In
addition, we create and release a synthetic panoramic dataset Flow360 based on
CARLA to facilitate training and quantitative analysis. PanoFlow achieves
state-of-the-art performance on the public OmniFlowNet and the established
Flow360 benchmarks. Our proposed approach reduces the End-Point-Error (EPE) on
Flow360 by 27.3%. On OmniFlowNet, PanoFlow achieves an EPE of 3.17 pixels, a
55.5% error reduction from the best published result. We also qualitatively
validate our method via a collection vehicle and a public real-world OmniPhotos
dataset, indicating strong potential and robustness for real-world navigation
applications. Code and dataset are publicly available at
https://github.com/MasterHow/PanoFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and dataset are publicly available at
  https://github.com/MasterHow/PanoFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SALAD: Source-free Active Label-Agnostic Domain Adaptation for
  Classification, Segmentation and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method, SALAD, for the challenging vision task of adapting
a pre-trained "source" domain network to a "target" domain, with a small budget
for annotation in the "target" domain and a shift in the label space. Further,
the task assumes that the source data is not available for adaptation, due to
privacy concerns or otherwise. We postulate that such systems need to jointly
optimize the dual task of (i) selecting fixed number of samples from the target
domain for annotation and (ii) transfer of knowledge from the pre-trained
network to the target domain. To do this, SALAD consists of a novel Guided
Attention Transfer Network (GATN) and an active learning function, HAL. The
GATN enables feature distillation from pre-trained network to the target
network, complemented with the target samples mined by HAL using
transfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it
is task-agnostic, and can be applied across various visual tasks such as
classification, segmentation and detection; (ii) it can handle shifts in output
label space from the pre-trained source network to the target domain; (iii) it
does not require access to source data for adaptation. We conduct extensive
experiments across 3 visual tasks, viz. digits classification (MNIST, SVHN,
VISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document
layout detection (PubLayNet to DSSE). We show that our source-free approach,
SALAD, results in an improvement of 0.5%-31.3%(across datasets and tasks) over
prior adaptation methods that assume access to large amounts of annotated
source data for adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking Objects as Pixel-wise Distributions <span class="chip">ECCV22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Zhao, Ze Wu, Yueqing Zhuang, Boxun Li, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object tracking (MOT) requires detecting and associating objects
through frames. Unlike tracking via detected bounding boxes or tracking objects
as points, we propose tracking objects as pixel-wise distributions. We
instantiate this idea on a transformer-based architecture, P3AFormer, with
pixel-wise propagation, prediction, and association. P3AFormer propagates
pixel-wise features guided by flow information to pass messages between frames.
Furthermore, P3AFormer adopts a meta-architecture to produce multi-scale object
feature maps. During inference, a pixel-wise association procedure is proposed
to recover object connections through frames based on the pixel-wise
prediction. P3AFormer yields 81.2\% in terms of MOTA on the MOT17 benchmark --
the first among all transformer networks to reach 80\% MOTA in literature.
P3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV22 as an oral presentation paper. The code&project
  page is at
  https://github.com/dvlab-research/ECCV22-P3AFormer-Tracking-Objects-as-Pixel-wise-Distributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentive Fine-Grained Structured Sparsity for Image Restoration <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghun Oh, Heewon Kim, Seungjun Nah, Cheeun Hong, Jonghyun Choi, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration tasks have witnessed great performance improvement in
recent years by developing large deep models. Despite the outstanding
performance, the heavy computation demanded by the deep models has restricted
the application of image restoration. To lift the restriction, it is required
to reduce the size of the networks while maintaining accuracy. Recently, N:M
structured pruning has appeared as one of the effective and practical pruning
approaches for making the model efficient with the accuracy constraint.
However, it fails to account for different computational complexities and
performance requirements for different layers of an image restoration network.
To further optimize the trade-off between the efficiency and the restoration
accuracy, we propose a novel pruning method that determines the pruning ratio
for N:M structured sparsity at each layer. Extensive experimental results on
super-resolution and deblurring tasks demonstrate the efficacy of our method
which outperforms previous pruning methods significantly. PyTorch
implementation for the proposed methods will be publicly available at
https://github.com/JungHunOh/SLS_CVPR2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Visual-Language Models for Efficient Video Understanding <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.04478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.04478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based visual-language (I-VL) pre-training has shown great success for
learning joint visual-textual representations from large-scale web data,
revealing remarkable ability for zero-shot generalisation. This paper presents
a simple but strong baseline to efficiently adapt the pre-trained I-VL model,
and exploit its powerful ability for resource-hungry video understanding tasks,
with minimal training. Specifically, we propose to optimise a few random
vectors, termed as continuous prompt vectors, that convert video-related tasks
into the same format as the pre-training objectives. In addition, to bridge the
gap between static images and videos, temporal information is encoded with
lightweight Transformers stacking on top of frame-wise visual features.
Experimentally, we conduct extensive ablation studies to analyse the critical
components. On 10 public benchmarks of action recognition, action localisation,
and text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,
we achieve competitive or state-of-the-art performance to existing methods,
despite optimising significantly fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022. Project page: https://ju-chen.github.io/efficient-prompt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative
  Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.07555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.07555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ru Li, Shuaicheng Liu, Guangfu Wang, Guanghui Liu, Bing Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a solution based on Generative Adversarial Network (GAN)
for solving jigsaw puzzles. The problem assumes that an image is divided into
equal square pieces, and asks to recover the image according to information
provided by the pieces. Conventional jigsaw puzzle solvers often determine the
relationships based on the boundaries of pieces, which ignore the important
semantic information. In this paper, we propose JigsawGAN, a GAN-based
auxiliary learning method for solving jigsaw puzzles with unpaired images (with
no prior knowledge of the initial images). We design a multi-task pipeline that
includes, (1) a classification branch to classify jigsaw permutations, and (2)
a GAN branch to recover features to images in correct orders. The
classification branch is constrained by the pseudo-labels generated according
to the shuffled pieces. The GAN branch concentrates on the image semantic
information, where the generator produces the natural images to fool the
discriminator, while the discriminator distinguishes whether a given image
belongs to the synthesized or the real target domain. These two branches are
connected by a flow-based warp module that is applied to warp features to
correct the order according to the classification results. The proposed method
can solve jigsaw puzzles more efficiently by utilizing both semantic
information and boundary information simultaneously. Qualitative and
quantitative comparisons against several representative jigsaw puzzle solvers
demonstrate the superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Image Processing (TIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging
  with Unpaired Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ru Li, Chuan Wang, Jue Wang, Guanghui Liu, Heng-Yu Zhang, Bing Zeng, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a method to effectively fuse multi-exposure inputs and
generate high-quality high dynamic range (HDR) images with unpaired datasets.
Deep learning-based HDR image generation methods rely heavily on paired
datasets. The ground truth images play a leading role in generating reasonable
HDR images. Datasets without ground truth are hard to be applied to train deep
neural networks. Recently, Generative Adversarial Networks (GAN) have
demonstrated their potentials of translating images from source domain X to
target domain Y in the absence of paired examples. In this paper, we propose a
GAN-based network for solving such problems while generating enjoyable HDR
results, named UPHDR-GAN. The proposed method relaxes the constraint of the
paired dataset and learns the mapping from the LDR domain to the HDR domain.
Although the pair data are missing, UPHDR-GAN can properly handle the ghosting
artifacts caused by moving objects or misalignments with the help of the
modified GAN loss, the improved discriminator network and the useful
initialization phase. The proposed method preserves the details of important
regions and improves the total image perceptual quality. Qualitative and
quantitative comparisons against the representative methods demonstrate the
superiority of the proposed UPHDR-GAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative
  Characterization of SLAM <span class="highlight-title">Dataset</span>s <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.11312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.11312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Islam Ali, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliability of SLAM systems is considered one of the critical requirements in
modern autonomous systems. This directed the efforts to developing many
state-of-the-art systems, creating challenging datasets, and introducing
rigorous metrics to measure SLAM performance. However, the link between
datasets and performance in the robustness/resilience context has rarely been
explored. In order to fill this void, characterization of the operating
conditions of SLAM systems is essential in order to provide an environment for
quantitative measurement of robustness and resilience. In this paper, we argue
that for proper evaluation of SLAM performance, the characterization of SLAM
datasets serves as a critical first step. The study starts by reviewing
previous efforts for quantitative characterization of SLAM datasets. Then, the
problem of perturbation characterization is discussed and the linkage to SLAM
robustness/resilience is established. After that, we propose a novel, generic
and extendable framework for quantitative analysis and comparison of SLAM
datasets. Additionally, a description of different characterization parameters
is provided. Finally, we demonstrate the application of our framework by
presenting the characterization results of three SLAM datasets: KITTI,
EuroC-MAV, and TUM-VI highlighting the level of insights achieved by the
proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Accepted to IROS 2022, updating to the latest submitted
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Shadow Generation Using Pixel Height Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Sheng, Yifan Liu, Jianming Zhang, Wei Yin, A. Cengiz Oztireli, He Zhang, Zhe Lin, Eli Shechtman, Bedrich Benes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shadows are essential for realistic image compositing. Physics-based shadow
rendering methods require 3D geometries, which are not always available. Deep
learning-based shadow synthesis methods learn a mapping from the light
information to an object's shadow without explicitly modeling the shadow
geometry. Still, they lack control and are prone to visual artifacts. We
introduce pixel heigh, a novel geometry representation that encodes the
correlations between objects, ground, and camera pose. The pixel height can be
calculated from 3D geometries, manually annotated on 2D images, and can also be
predicted from a single-view RGB image by a supervised approach. It can be used
to calculate hard shadows in a 2D image based on the projective geometry,
providing precise control of the shadows' direction and shape. Furthermore, we
propose a data-driven soft shadow generator to apply softness to a hard shadow
based on a softness input parameter. Qualitative and quantitative evaluations
demonstrate that the proposed pixel height significantly improves the quality
of the shadow generation while allowing for controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Sequence Feature Alignment for Domain Adaptive Detection
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.12636v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.12636v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-Jun Zha, Yonggang Wen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix a typo in Eq. 13</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMix: Unveiling the Power of Mixup for Stronger Classifiers <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.13027v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.13027v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Liu, Siyuan Li, Di Wu, Zihan Liu, Zhiyuan Chen, Lirong Wu, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data mixing augmentation have proved to be effective for improving the
generalization ability of deep neural networks. While early methods mix samples
by hand-crafted policies (e.g., linear interpolation), recent methods utilize
saliency information to match the mixed samples and labels via complex offline
optimization. However, there arises a trade-off between precise mixing policies
and optimization complexity. To address this challenge, we propose a novel
automatic mixup (AutoMix) framework, where the mixup policy is parameterized
and serves the ultimate classification goal directly. Specifically, AutoMix
reformulates the mixup classification into two sub-tasks (i.e., mixed sample
generation and mixup classification) with corresponding sub-networks and solves
them in a bi-level optimization framework. For the generation, a learnable
lightweight mixup generator, Mix Block, is designed to generate mixed samples
by modeling patch-wise relationships under the direct supervision of the
corresponding mixed labels. To prevent the degradation and instability of
bi-level optimization, we further introduce a momentum pipeline to train
AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks
prove the superiority of AutoMix compared with state-of-the-arts in various
classification scenarios and downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 (Oral representation paper). The source code is available
  at https://github.com/Westlake-AI/openmixup</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HoVer-Trans: Anatomy-aware HoVer-<span class="highlight-title">Transformer</span> for ROI-free Breast Cancer
  Diagnosis in Ultrasound Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Mo, Chu Han, Yu Liu, Min Liu, Zhenwei Shi, Jiatai Lin, Bingchao Zhao, Chunwang Huang, Bingjiang Qiu, Yanfen Cui, Lei Wu, Xipeng Pan, Zeyan Xu, Xiaomei Huang, Zaiyi Liu, Ying Wang, Changhong Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasonography is an important routine examination for breast cancer
diagnosis, due to its non-invasive, radiation-free and low-cost properties.
However, the diagnostic accuracy of breast cancer is still limited due to its
inherent limitations. It would be a tremendous success if we can precisely
diagnose breast cancer by breast ultrasound images (BUS). Many learning-based
computer-aided diagnostic methods have been proposed to achieve breast cancer
diagnosis/lesion classification. However, most of them require a pre-define ROI
and then classify the lesion inside the ROI. Conventional classification
backbones, such as VGG16 and ResNet50, can achieve promising classification
results with no ROI requirement. But these models lack interpretability, thus
restricting their use in clinical practice. In this study, we propose a novel
ROI-free model for breast cancer diagnosis in ultrasound images with
interpretable feature representations. We leverage the anatomical prior
knowledge that malignant and benign tumors have different spatial relationships
between different tissue layers, and propose a HoVer-Transformer to formulate
this prior knowledge. The proposed HoVer-Trans block extracts the inter- and
intra-layer spatial information horizontally and vertically. We conduct and
release an open dataset GDPH&SYSUCC for breast cancer diagnosis in BUS. The
proposed model is evaluated in three datasets by comparing with four CNN-based
models and two vision transformer models via five-fold cross validation. It
achieves state-of-the-art classification performance with the best model
interpretability. In the meanwhile, our proposed model outperforms two senior
sonographers on the breast cancer diagnosis when only one BUS image is given.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ deepNIR: <span class="highlight-title">Dataset</span>s for generating synthetic NIR images and improved fruit
  detection system using deep learning techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inkyu Sa, JongYoon Lim, Ho Seok Ahn, Bruce MacDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents datasets utilised for synthetic near-infrared (NIR) image
generation and bounding-box level fruit detection systems. It is undeniable
that high-calibre machine learning frameworks such as Tensorflow or Pytorch,
and large-scale ImageNet or COCO datasets with the aid of accelerated GPU
hardware have pushed the limit of machine learning techniques for more than
decades. Among these breakthroughs, a high-quality dataset is one of the
essential building blocks that can lead to success in model generalisation and
the deployment of data-driven deep neural networks. In particular, synthetic
data generation tasks often require more training samples than other supervised
approaches. Therefore, in this paper, we share the NIR+RGB datasets that are
re-processed from two public datasets (i.e., nirscene and SEN12MS) and our
novel NIR+RGB sweet pepper(capsicum) dataset. We quantitatively and
qualitatively demonstrate that these NIR+RGB datasets are sufficient to be used
for synthetic NIR image generation. We achieved Frechet Inception Distance
(FID) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper
datasets respectively. In addition, we release manual annotations of 11 fruit
bounding boxes that can be exported as various formats using cloud service.
Four newly added fruits [blueberry, cherry, kiwi, and wheat] compound 11 novel
bounding box datasets on top of our previous work presented in the deepFruits
project [apple, avocado, capsicum, mango, orange, rockmelon, strawberry]. The
total number of bounding box instances of the dataset is 162k and it is ready
to use from cloud service. For the evaluation of the dataset, Yolov5 single
stage detector is exploited and reported impressive
mean-average-precision,mAP[0.5:0.95] results of[min:0.49, max:0.812]. We hope
these datasets are useful and serve as a baseline for the future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 27 figures, published in MDPI Remote Sensing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Task Sampling for Few-shot <span class="highlight-title">Vision-Language</span> Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Hang Yu, Manling Li, Han Zhao, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite achieving state-of-the-art zero-shot performance, existing
vision-language models still fall short of few-shot transfer ability on
domain-specific problems. Classical fine-tuning often fails to prevent highly
expressive models from exploiting spurious correlations. Although
model-agnostic meta-learning (MAML) presents as a natural alternative for
few-shot transfer learning, the expensive computation due to implicit
second-order optimization limits its use on large-scale vision-language models
such as CLIP. While much literature has been devoted to exploring alternative
optimization strategies, we identify another essential aspect towards effective
few-shot transfer learning, task sampling, which is previously only be viewed
as part of data pre-processing in MAML. To show the impact of task sampling, we
propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which
differentiates classical fine-tuning only on uniformly sampling multiple tasks.
Despite its simplicity, we show that MAMF consistently outperforms classical
fine-tuning on five few-shot vision-language classification tasks. We further
show that the effectiveness of the bi-level optimization in MAML is highly
sensitive to the zero-shot performance of a task in the context of few-shot
vision-language classification. The goal of this paper is to provide new
insights on what makes few-shot learning work, and encourage more research into
investigating better task sampling strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Omni-Vision Representation through the Lens of Visual
  Realms <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though impressive performance has been achieved in specific visual realms
(e.g. faces, dogs, and places), an omni-vision representation generalizing to
many natural visual domains is highly desirable. But, existing benchmarks are
biased and inefficient to evaluate the omni-vision representation -- these
benchmarks either only include several specific realms, or cover most realms at
the expense of subsuming numerous datasets that have extensive realm
overlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It
includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.
Without semantic overlapping, these datasets cover most visual realms
comprehensively and meanwhile efficiently. In addition, we propose a new
supervised contrastive learning framework, namely Relational Contrastive
learning (ReCo), for a better omni-vision representation. Beyond pulling two
instances from the same concept closer -- the typical supervised contrastive
learning framework -- ReCo also pulls two instances from the same semantic
realm closer, encoding the semantic relation between concepts, and facilitating
omni-vision representation learning. We benchmark ReCo and other advances in
omni-vision representation studies that are different in architectures (from
CNNs to transformers) and in learning paradigms (from supervised learning to
self-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo
to other supervised contrastive learning methods and reveal multiple practical
observations to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ECCV 2022; The project page at
  https://zhangyuanhan-ai.github.io/OmniBenchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STDAN: Deformable Attention Network for Space-Time Video
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Wang, Xiaoyu Xiang, Yapeng Tian, Wenming Yang, Qingmin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The target of space-time video super-resolution (STVSR) is to increase the
spatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)
videos. Recent approaches based on deep learning have made significant
improvements, but most of them only use two adjacent frames, that is,
short-term features, to synthesize the missing frame embedding, which cannot
fully explore the information flow of consecutive input LR frames. In addition,
existing STVSR models hardly exploit the temporal contexts explicitly to assist
high-resolution (HR) frame reconstruction. To address these issues, in this
paper, we propose a deformable attention network called STDAN for STVSR. First,
we devise a long-short term feature interpolation (LSTFI) module, which is
capable of excavating abundant content from more neighboring input frames for
the interpolation process through a bidirectional RNN structure. Second, we put
forward a spatial-temporal deformable feature aggregation (STDFA) module, in
which spatial and temporal contexts in dynamic video frames are adaptively
captured and aggregated to enhance SR reconstruction. Experimental results on
several datasets demonstrate that our approach outperforms state-of-the-art
STVSR methods. The code is available at
https://github.com/littlewhitesea/STDAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVFNet: Real-time 3D Object Detection by Learning Cross View Features <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Gu, Zhiyu Xiang, Pan Zhao, Tingming Bai, Lingxuan Wang, Xijun Zhao, Zhiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years 3D object detection from LiDAR point clouds has made great
progress thanks to the development of deep learning technologies. Although
voxel or point based methods are popular in 3D object detection, they usually
involve time-consuming operations such as 3D convolutions on voxels or ball
query among points, making the resulting network inappropriate for time
critical applications. On the other hand, 2D view-based methods feature high
computing efficiency while usually obtaining inferior performance than the
voxel or point based methods. In this work, we present a real-time view-based
single stage 3D object detector, namely CVFNet to fulfill this task. To
strengthen the cross-view feature learning under the condition of demanding
efficiency, our framework extracts the features of different views and fuses
them in an efficient progressive way. We first propose a novel Point-Range
feature fusion module that deeply integrates point and range view features in
multiple stages. Then, a special Slice Pillar is designed to well maintain the
3D geometry when transforming the obtained deep point-view features into bird's
eye view. To better balance the ratio of samples, a sparse pillar detection
head is presented to focus the detection on the nonempty grids. We conduct
experiments on the popular KITTI and NuScenes benchmark, and state-of-the-art
performances are achieved in terms of both accuracy and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, accepted by IROS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Inter-Class Distance for Semantic Segmentation <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengbo Zhang, Chunluan Zhou, Zhigang Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is widely adopted in semantic segmentation to reduce
the computation cost.The previous knowledge distillation methods for semantic
segmentation focus on pixel-wise feature alignment and intra-class feature
variation distillation, neglecting to transfer the knowledge of the inter-class
distance in the feature space, which is important for semantic segmentation. To
address this issue, we propose an Inter-class Distance Distillation (IDD)
method to transfer the inter-class distance in the feature space from the
teacher network to the student network. Furthermore, semantic segmentation is a
position-dependent task,thus we exploit a position information distillation
module to help the student network encode more position information. Extensive
experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show
that our method is helpful to improve the accuracy of semantic segmentation
models and achieves the state-of-the-art performance. E.g. it boosts the
benchmark model("PSPNet+ResNet18") by 7.50% in accuracy on the Cityscapes
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI-ECAI2022 Long Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Low-Resolution <span class="highlight-title">Distillation</span> for Cost-Efficient End-to-End Text
  Spotting <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Chen, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end text spotting has attached great attention recently due to its
benefits on global optimization and high maintainability for real applications.
However, the input scale has always been a tough trade-off since recognizing a
small text instance usually requires enlarging the whole image, which brings
high computational costs. In this paper, to address this problem, we propose a
novel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting
framework, which aims to infer images in different small but recognizable
resolutions and achieve a better balance between accuracy and efficiency.
Concretely, we adopt a resolution selector to dynamically decide the input
resolutions for different images, which is constraint by both inference
accuracy and computational cost. Another sequential knowledge distillation
strategy is conducted on the text recognition branch, making the low-res input
obtains comparable performance to a high-res image. The proposed method can be
optimized end-to-end and adopted in any current text spotting framework to
improve the practicability. Extensive experiments on several text spotting
benchmarks show that the proposed method vastly improves the usability of
low-res models. The code is available at
https://github.com/hikopensource/DAVAR-Lab-OCR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> Open-Vocabulary Video Classification via <span class="highlight-title">Pre-Train</span>ed Vision
  and Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, Yin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing vision and language models (VLMs) pre-trained on large-scale
image-text pairs is becoming a promising paradigm for open-vocabulary visual
recognition. In this work, we extend this paradigm by leveraging motion and
audio that naturally exist in video. We present \textbf{MOV}, a simple yet
effective method for \textbf{M}ultimodal \textbf{O}pen-\textbf{V}ocabulary
video classification. In MOV, we directly use the vision encoder from
pre-trained VLMs with minimal modifications to encode video, optical flow and
audio spectrogram. We design a cross-modal fusion mechanism to aggregate
complimentary multimodal information. Experiments on Kinetics-700 and VGGSound
show that introducing flow or audio modality brings large performance gains
over the pre-trained VLM and existing methods. Specifically, MOV greatly
improves the accuracy on base classes, while generalizes better on novel
classes. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video
classification benchmarks, significantly outperforming both traditional
zero-shot methods and recent methods based on VLMs. Code and models will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Probabilistic Autoencoder for Type Ia Supernovae Spectral Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Stein, Uros Seljak, Vanessa Bohm, G. Aldering, P. Antilogus, C. Aragon, S. Bailey, C. Baltay, S. Bongard, K. Boone, C. Buton, Y. Copin, S. Dixon, D. Fouchez, E. Gangler, R. Gupta, B. Hayden, W. Hillebrandt, M. Karmen, A. G. Kim, M. Kowalski, D. Kusters, P. F. Leget, F. Mondon, J. Nordin, R. Pain, E. Pecontal, R. Pereira, S. Perlmutter, K. A. Ponder, D. Rabinowitz, M. Rigault, D. Rubin, K. Runge, C. Saunders, G. Smadja, N. Suzuki, C. Tao, R. C. Thomas, M. Vincenzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We construct a physically-parameterized probabilistic autoencoder (PAE) to
learn the intrinsic diversity of type Ia supernovae (SNe Ia) from a sparse set
of spectral time series. The PAE is a two-stage generative model, composed of
an Auto-Encoder (AE) which is interpreted probabilistically after training
using a Normalizing Flow (NF). We demonstrate that the PAE learns a
low-dimensional latent space that captures the nonlinear range of features that
exists within the population, and can accurately model the spectral evolution
of SNe Ia across the full range of wavelength and observation times directly
from the data. By introducing a correlation penalty term and multi-stage
training setup alongside our physically-parameterized network we show that
intrinsic and extrinsic modes of variability can be separated during training,
removing the need for the additional models to perform magnitude
standardization. We then use our PAE in a number of downstream tasks on SNe Ia
for increasingly precise cosmological analyses, including automatic detection
of SN outliers, the generation of samples consistent with the data
distribution, and solving the inverse problem in the presence of noisy and
incomplete data to constrain cosmological distance measurements. We find that
the optimal number of intrinsic model parameters appears to be three, in line
with previous studies, and show that we can standardize our test sample of SNe
Ia with an RMS of $0.091 \pm 0.010$ mag, which corresponds to $0.074 \pm 0.010$
mag if peculiar velocity contributions are removed. Trained models and codes
are released at
\href{https://github.com/georgestein/suPAErnova}{github.com/georgestein/suPAErnova}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 Figures, 1 Table. Accepted to ApJ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Is a Caption Worth a Thousand Images? A Controlled Study for
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibani Santurkar, Yann Dubois, Rohan Taori, <span class="highlight-author">Percy Liang</span>, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of CLIP [Radford et al., 2021] has sparked a debate on
whether language supervision can result in vision models with more transferable
representations than traditional image-only methods. Our work studies this
question through a carefully controlled comparison of two approaches in terms
of their ability to learn representations that generalize to downstream
classification tasks. We find that when the pre-training dataset meets certain
criteria -- it is sufficiently large and contains descriptive captions with low
variability -- image-only methods do not match CLIP's transfer performance,
even when they are trained with more image data. However, contrary to what one
might expect, there are practical settings in which these criteria are not met,
wherein added supervision through captions is actually detrimental. Motivated
by our findings, we devise simple prescriptions to enable CLIP to better
leverage the language information present in existing pre-training datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing-In-Memory Neural Network Accelerators for Safety-Critical
  Systems: Can Small Device Variations Be Disastrous? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyu Yan, Xiaobo Sharon Hu, Yiyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computing-in-Memory (CiM) architectures based on emerging non-volatile memory
(NVM) devices have demonstrated great potential for deep neural network (DNN)
acceleration thanks to their high energy efficiency. However, NVM devices
suffer from various non-idealities, especially device-to-device variations due
to fabrication defects and cycle-to-cycle variations due to the stochastic
behavior of devices. As such, the DNN weights actually mapped to NVM devices
could deviate significantly from the expected values, leading to large
performance degradation. To address this issue, most existing works focus on
maximizing average performance under device variations. This objective would
work well for general-purpose scenarios. But for safety-critical applications,
the worst-case performance must also be considered. Unfortunately, this has
been rarely explored in the literature. In this work, we formulate the problem
of determining the worst-case performance of CiM DNN accelerators under the
impact of device variations. We further propose a method to effectively find
the specific combination of device variation in the high-dimensional space that
leads to the worst-case performance. We find that even with very small device
variations, the accuracy of a DNN can drop drastically, causing concerns when
deploying CiM accelerators in safety-critical applications. Finally, we show
that surprisingly none of the existing methods used to enhance average DNN
performance in CiM accelerators are very effective when extended to enhance the
worst-case performance, and further research down the road is needed to address
this problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feed-Forward Source-Free Latent Domain Adaptation via Cross-Attention <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondrej Bohdal, Da Li, Shell Xu Hu, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the highly practical but comparatively under-studied problem of
latent-domain adaptation, where a source model should be adapted to a target
dataset that contains a mixture of unlabelled domain-relevant and
domain-irrelevant examples. Furthermore, motivated by the requirements for data
privacy and the need for embedded and resource-constrained devices of all kinds
to adapt to local data distributions, we focus on the setting of feed-forward
source-free domain adaptation, where adaptation should not require access to
the source dataset, and also be back propagation-free. Our solution is to
meta-learn a network capable of embedding the mixed-relevance target dataset
and dynamically adapting inference for target examples using cross-attention.
The resulting framework leads to consistent improvement on strong ERM
baselines. We also show that our framework sometimes even improves on the upper
bound of domain-supervised adaptation, where only domain-relevant instances are
provided for adaptation. This suggests that human annotated domain labels may
not always be optimal, and raises the possibility of doing better through
automated instance selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shorter version accepted at the First Workshop of Pre-training:
  Perspectives, Pitfalls, and Paths Forward at ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blessing of Nonconvexity in Deep Linear Models: Depth Flattens the
  Optimization Landscape Around the True Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Ma, Salar Fattahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work characterizes the effect of depth on the optimization landscape of
linear regression, showing that, despite their nonconvexity, deeper models have
more desirable optimization landscape. We consider a robust and
over-parameterized setting, where a subset of measurements are grossly
corrupted with noise and the true linear model is captured via an $N$-layer
linear neural network. On the negative side, we show that this problem
\textit{does not} have a benign landscape: given any $N\geq 1$, with constant
probability, there exists a solution corresponding to the ground truth that is
neither local nor global minimum. However, on the positive side, we prove that,
for any $N$-layer model with $N\geq 2$, a simple sub-gradient method becomes
oblivious to such ``problematic'' solutions; instead, it converges to a
balanced solution that is not only close to the ground truth but also enjoys a
flat local landscape, thereby eschewing the need for "early stopping". Lastly,
we empirically verify that the desirable optimization landscape of deeper
models extends to other robust learning tasks, including deep matrix recovery
and deep ReLU networks with $\ell_1$-loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Prediction as an Effective <span class="highlight-title">Pretrain</span>ing Strategy <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana Likhomanenko, Joseph Yitan Cheng, Walter Talbott, Chen Huang, Hanlin Goh, Joshua Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have gained increasing popularity in a wide range of
applications, including Natural Language Processing (NLP), Computer Vision and
Speech Recognition, because of their powerful representational capacity.
However, harnessing this representational capacity effectively requires a large
amount of data, strong regularization, or both, to mitigate overfitting.
Recently, the power of the Transformer has been unlocked by self-supervised
pretraining strategies based on masked autoencoders which rely on
reconstructing masked inputs, directly, or contrastively from unmasked content.
This pretraining strategy which has been used in BERT models in NLP, Wav2Vec
models in Speech and, recently, in MAE models in Vision, forces the model to
learn about relationships between the content in different parts of the input
using autoencoding related objectives. In this paper, we propose a novel, but
surprisingly simple alternative to content reconstruction~-- that of predicting
locations from content, without providing positional information for it. Doing
so requires the Transformer to understand the positional relationships between
different parts of the input, from their content alone. This amounts to an
efficient implementation where the pretext task is a classification problem
among all possible positions for each input token. We experiment on both Vision
and Speech benchmarks, where our approach brings improvements over strong
supervised training baselines and is comparable to modern
unsupervised/self-supervised pretraining methods. Our method also enables
Transformers trained without position embeddings to outperform ones trained
with full position information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithms to estimate Shapley value feature attributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugh Chen, Ian C. Covert, Scott M. Lundberg, Su-In Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature attributions based on the Shapley value are popular for explaining
machine learning models; however, their estimation is complex from both a
theoretical and computational standpoint. We disentangle this complexity into
two factors: (1)~the approach to removing feature information, and (2)~the
tractable estimation strategy. These two factors provide a natural lens through
which we can better understand and compare 24 distinct algorithms. Based on the
various feature removal approaches, we describe the multiple types of Shapley
value feature attributions and methods to calculate each one. Then, based on
the tractable estimation strategies, we characterize two distinct families of
approaches: model-agnostic and model-specific approximations. For the
model-agnostic approximations, we benchmark a wide class of estimation
approaches and tie them to alternative yet equivalent characterizations of the
Shapley value. For the model-specific approximations, we clarify the
assumptions crucial to each method's tractability for linear, tree, and deep
models. Finally, we identify gaps in the literature and promising future
research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A two-step machine learning approach to statistical post-processing of
  weather forecasts for power generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ágnes Baran, Sándor Baran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By the end of 2021, the renewable energy share of the global electricity
capacity reached 38.3% and the new installations are dominated by wind and
solar energy, showing global increases of 12.7% and 18.5%, respectively.
However, both wind and photovoltaic energy sources are highly volatile making
planning difficult for grid operators, so accurate forecasts of the
corresponding weather variables are essential for reliable electricity
predictions. The most advanced approach in weather prediction is the ensemble
method, which opens the door for probabilistic forecasting; though ensemble
forecast are often underdispersive and subject to systematic bias. Hence, they
require some form of statistical post-processing, where parametric models
provide full predictive distributions of the weather variables at hand. We
propose a general two-step machine learning-based approach to calibrating
ensemble weather forecasts, where in the first step improved point forecasts
are generated, which are then together with various ensemble statistics serve
as input features of the neural network estimating the parameters of the
predictive distribution. In two case studies based of 100m wind speed and
global horizontal irradiance forecasts of the operational ensemble pre diction
system of the Hungarian Meteorological Service, the predictive performance of
this novel method is compared with the forecast skill of the raw ensemble and
the state-of-the-art parametric approaches. Both case studies confirm that at
least up to 48h statistical post-processing substantially improves the
predictive performance of the raw ensemble for all considered forecast
horizons. The investigated variants of the proposed two-step method outperform
in skill their competitors and the suggested new approach is well applicable
for different weather quantities and for a fair range of predictive
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outlier detection of vital sign trajectories from COVID-19 patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Summerton, Ann Tivey, Rohan Shotton, Gavin Brown, Oliver C. Redfern, Rachel Oakley, John Radford, David C. Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in continuous wearable vital sign sensors for
monitoring patients remotely at home. These monitors are usually coupled to an
alerting system, which is triggered when vital sign measurements fall outside a
predefined normal range. Trends in vital signs, such as an increasing heart
rate, are often indicative of deteriorating health, but are rarely incorporated
into alerting systems. In this work, we present a novel outlier detection
algorithm to identify such abnormal vital sign trends. We introduce a
distance-based measure to compare vital sign trajectories. For each patient in
our dataset, we split vital sign time series into 180 minute, non-overlapping
epochs. We then calculated a distance between all pairs of epochs using the
dynamic time warp distance. Each epoch was characterized by its mean pairwise
distance (average link distance) to all other epochs, with large distances
considered as outliers. We applied this method to a pilot dataset collected
over 1561 patient-hours from 8 patients who had recently been discharged from
hospital after contracting COVID-19. We show that outlier epochs correspond
well with patients who were subsequently readmitted to hospital. We also show,
descriptively, how epochs transition from normal to abnormal for one such
patient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, 1 table. Submitted to IEEE BHI 2022, decision
  pending</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> The Nature of Temporal Difference Errors in Multi-step Distributional
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun<span class="highlight-author">hao Tan</span>g, Mark Rowland, Rémi Munos, Bernardo Ávila Pires, Will Dabney, Marc G. Bellemare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the multi-step off-policy learning approach to distributional RL.
Despite the apparent similarity between value-based RL and distributional RL,
our study reveals intriguing and fundamental differences between the two cases
in the multi-step setting. We identify a novel notion of path-dependent
distributional TD error, which is indispensable for principled multi-step
distributional RL. The distinction from the value-based case bears important
implications on concepts such as backward-view algorithms. Our work provides
the first theoretical guarantees on multi-step off-policy distributional RL
algorithms, including results that apply to the small number of existing
approaches to multi-step distributional RL. In addition, we derive a novel
algorithm, Quantile Regression-Retrace, which leads to a deep RL agent
QR-DQN-Retrace that shows empirical improvements over QR-DQN on the Atari-57
benchmark. Collectively, we shed light on how unique challenges in multi-step
distributional RL can be addressed both in theory and practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skill-based Model-based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Xiaoyang Shi, Joseph J. Lim, Youngwoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based reinforcement learning (RL) is a sample-efficient way of learning
complex behaviors by leveraging a learned single-step dynamics model to plan
actions in imagination. However, planning every action for long-horizon tasks
is not practical, akin to a human planning out every muscle movement. Instead,
humans efficiently plan with high-level skills to solve complex tasks. From
this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that
enables planning in the skill space using a skill dynamics model, which
directly predicts the skill outcomes, rather than predicting all small details
in the intermediate states, step by step. For accurate and efficient long-term
planning, we jointly learn the skill dynamics model and a skill repertoire from
prior experience. We then harness the learned skill dynamics model to
accurately simulate and plan over long horizons in the skill space, which
enables efficient downstream learning of long-horizon, sparse reward tasks.
Experimental results in navigation and manipulation domains show that SkiMo
extends the temporal horizon of model-based approaches and improves the sample
efficiency for both model-based RL and skill-based RL. Code and videos are
available at \url{https://clvrai.com/skimo}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: \url{https://clvrai.com/skimo}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CheXplaining in Style: Counterfactual Explanations for Chest X-rays
  using StyleGAN <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Atad, Vitalii Dmytrenko, Yitong Li, Xin<span class="highlight-author">yue Zhang</span>, Matthias Keicher, Jan Kirschke, Bene Wiestler, Ashkan Khakzar, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models used in medical image analysis are prone to raising
reliability concerns due to their black-box nature. To shed light on these
black-box models, previous works predominantly focus on identifying the
contribution of input features to the diagnosis, i.e., feature attribution. In
this work, we explore counterfactual explanations to identify what patterns the
models rely on for diagnosis. Specifically, we investigate the effect of
changing features within chest X-rays on the classifier's output to understand
its decision mechanism. We leverage a StyleGAN-based approach (StyleEx) to
create counterfactual explanations for chest X-rays by manipulating specific
latent directions in their latent space. In addition, we propose EigenFind to
significantly reduce the computation time of generated explanations. We
clinically evaluate the relevancy of our counterfactual explanations with the
help of radiologists. Our code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ICML 2022 Interpretable Machine Learning in
  Healthcare (IMLH) Workshop ----- Project website:
  http://github.com/CAMP-eXplain-AI/Style-CheXplain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pick your Neighbor: Local Gauss-Southwell Rule for Fast Asynchronous
  Decentralized Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Costantini, Nikolaos Liakopoulos, Panayotis Mertikopoulos, Thrasyvoulos Spyropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In decentralized optimization environments, each agent $i$ in a network of
$n$ optimization nodes possesses a private function $f_i$, and nodes
communicate with their neighbors to cooperatively minimize the aggregate
objective $\sum_{i=1}^n f_i$. In this setting, synchronizing the nodes' updates
incurs significant communication overhead and computational costs, so much of
the recent literature has focused on the analysis and design of asynchronous
optimization algorithms where agents activate and communicate at arbitrary
times, without requiring a global synchronization enforcer. Nonetheless, in
most of the work on the topic, active nodes select a neighbor to contact based
on a fixed probability (e.g., uniformly at random), a choice that ignores the
optimization landscape at the moment of activation. Instead, in this work we
introduce an optimization-aware selection rule that chooses the neighbor with
the highest dual cost improvement (a quantity related to a consensus-based
dualization of the problem at hand). This scheme is related to the coordinate
descent (CD) method with a Gauss-Southwell (GS) rule for coordinate updates; in
our setting however, only a subset of coordinates is accessible at each
iteration (because each node is constrained to communicate only with its direct
neighbors), so the existing literature on GS methods does not apply. To
overcome this difficulty, we develop a new analytical framework for smooth and
strongly convex $f_i$ that covers the class of set-wise CD algorithms -- a
class that directly applies to decentralized scenarios, but is not limited to
them -- and we show that the proposed set-wise GS rule achieves a speedup by a
factor of up to the maximum degree in the network (which is of the order of
$\Theta(n)$ in highly connected graphs). The speedup predicted by our
theoretical analysis is subsequently validated in numerical experiments with
synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Mu, Wenjie Ruan, Leandro S. Marcolino, Qiang Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud models are widely applied in safety-critical scenes, which
delivers an urgent need to obtain more solid proofs to verify the robustness of
models. Existing verification method for point cloud model is time-expensive
and computationally unattainable on large networks. Additionally, they cannot
handle the complete PointNet model with joint alignment network (JANet) that
contains multiplication layers, which effectively boosts the performance of 3D
models. This motivates us to design a more efficient and general framework to
verify various architectures of point cloud models. The key challenges in
verifying the large-scale complete PointNet models are addressed as dealing
with the cross-non-linearity operations in the multiplication layers and the
high computational complexity of high-dimensional point cloud inputs and added
layers. Thus, we propose an efficient verification framework, 3DVerifier, to
tackle both challenges by adopting a linear relaxation function to bound the
multiplication layer and combining forward and backward propagation to compute
the certified bounds of the outputs of the point cloud models. Our
comprehensive experiments demonstrate that 3DVerifier outperforms existing
verification algorithms for 3D models in terms of both efficiency and accuracy.
Notably, our approach achieves an orders-of-magnitude improvement in
verification efficiency for the large network, and the obtained certified
bounds are also significantly tighter than the state-of-the-art verifiers. We
release our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use
by the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selection of the Most Probable Best 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeho Kim, Kyoung-kuk Kim, Eunhye Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an expected-value ranking and selection problem where all k
solutions' simulation outputs depend on a common uncertain input model. Given
that the uncertainty of the input model is captured by a probability simplex on
a finite support, we define the most probable best (MPB) to be the solution
whose probability of being optimal is the largest. To devise an efficient
sampling algorithm to find the MPB, we first derive a lower bound to the large
deviation rate of the probability of falsely selecting the MPB, then formulate
an optimal computing budget allocation (OCBA) problem to find the optimal
static sampling ratios for all solution-input model pairs that maximize the
lower bound. We devise a series of sequential algorithms that apply
interpretable and computationally efficient sampling rules and prove their
sampling ratios achieve the optimality conditions for the OCBA problem as the
simulation budget increases. The algorithms are benchmarked against a
state-of-the-art sequential sampling algorithm designed for contextual ranking
and selection problems and demonstrated to have superior empirical performances
at finding the MPB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Quality and Machine Learning Pipelines through Extended Feature
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giordano d'Aloisio, Antinisca Di Marco, Giovanni Stilo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently increased complexity of Machine Learning (ML) methods, led to
the necessity to lighten both the research and industry development processes.
ML pipelines have become an essential tool for experts of many domains, data
scientists and researchers, allowing them to easily put together several ML
models to cover the full analytic process starting from raw datasets. Over the
years, several solutions have been proposed to automate the building of ML
pipelines, most of them focused on semantic aspects and characteristics of the
input dataset. However, an approach taking into account the new quality
concerns needed by ML systems (like fairness, interpretability, privacy, etc.)
is still missing. In this paper, we first identify, from the literature, key
quality attributes of ML systems. Further, we propose a new engineering
approach for quality ML pipeline by properly extending the Feature Models
meta-model. The presented approach allows to model ML pipelines, their quality
requirements (on the whole pipeline and on single phases), and quality
characteristics of algorithms used to implement each pipeline phase. Finally,
we demonstrate the expressiveness of our model considering the classification
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heuristic-free Optimization of Force-Controlled Robot Search Strategies
  in Stochastic Environments <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Alt, Darko Katic, Rainer Jäkel, Michael Beetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In both industrial and service domains, a central benefit of the use of
robots is their ability to quickly and reliably execute repetitive tasks.
However, even relatively simple peg-in-hole tasks are typically subject to
stochastic variations, requiring search motions to find relevant features such
as holes. While search improves robustness, it comes at the cost of increased
runtime: More exhaustive search will maximize the probability of successfully
executing a given task, but will significantly delay any downstream tasks. This
trade-off is typically resolved by human experts according to simple
heuristics, which are rarely optimal. This paper introduces an automatic,
data-driven and heuristic-free approach to optimize robot search strategies. By
training a neural model of the search strategy on a large set of simulated
stochastic environments, conditioning it on few real-world examples and
inverting the model, we can infer search strategies which adapt to the
time-variant characteristics of the underlying probability distributions, while
requiring very few real-world measurements. We evaluate our approach on two
different industrial robots in the context of spiral and probe search for THT
electronics assembly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, accepted to the 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan For
  code and data, see https://github.com/benjaminalt/dpse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual
  Reality with Redirected Walking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Lemic, Jakob Struye, Jeroen Famaey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-immersive multiuser Virtual Reality (VR) envisions supporting
unconstrained mobility of the users in the virtual worlds, while at the same
time constraining their physical movements inside VR setups through redirected
walking. For enabling delivery of high data rate video content in real-time,
the supporting wireless networks will leverage highly directional communication
links that will "track" the users for maintaining the Line-of-Sight (LoS)
connectivity. Recurrent Neural Networks (RNNs) and in particular Long
Short-Term Memory (LSTM) networks have historically presented themselves as a
suitable candidate for near-term movement trajectory prediction for natural
human mobility, and have also recently been shown as applicable in predicting
VR users' mobility under the constraints of redirected walking. In this work,
we extend these initial findings by showing that Gated Recurrent Unit (GRU)
networks, another candidate from the RNN family, generally outperform the
traditionally utilized LSTMs. Second, we show that context from a virtual world
can enhance the accuracy of the prediction if used as an additional input
feature in comparison to the more traditional utilization of solely the
historical physical movements of the VR users. Finally, we show that the
prediction system trained on a static number of coexisting VR users be scaled
to a multi-user system without significant accuracy degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect Out-of-Distribution (OOD) data is important in
safety-critical applications of deep learning. The aim is to separate
In-Distribution (ID) data drawn from the training distribution from OOD data
using a measure of uncertainty extracted from a deep neural network. Deep
Ensembles are a well-established method of improving the quality of uncertainty
estimates produced by deep neural networks, and have been shown to have
superior OOD detection performance compared to single models. An existing
intuition in the literature is that the diversity of Deep Ensemble predictions
indicates distributional shift, and so measures of diversity such as Mutual
Information (MI) should be used for OOD detection. We show experimentally that
this intuition is not valid on ImageNet-scale OOD detection -- using MI leads
to 30-40% worse %FPR@95 compared to single-model entropy on some OOD datasets.
We suggest an alternative explanation for Deep Ensembles' better OOD detection
performance -- OOD detection is binary classification and we are ensembling
diverse classifiers. As such we show that practically, even better OOD
detection performance can be achieved for Deep Ensembles by averaging
task-specific detection scores such as Energy over the ensemble.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Relational Reasoning with Object-Centric Representations <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex F. Spies, Alessandra Russo, Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the composability of soft-rules learned by relational neural
architectures when operating over object-centric (slot-based) representations,
under a variety of sparsity-inducing constraints. We find that increasing
sparsity, especially on features, improves the performance of some models and
leads to simpler relations. Additionally, we observe that object-centric
representations can be detrimental when not all objects are fully captured; a
failure mode to which CNNs are less prone. These findings demonstrate the
trade-offs between interpretability and performance, even for models designed
to tackle relational tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022, DyNN Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Softmax Information for Selective Classification with
  Out-of-Distribution Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a task that is receiving an
increasing amount of research attention in the domain of deep learning for
computer vision. However, the performance of detection methods is generally
evaluated on the task in isolation, rather than also considering potential
downstream tasks in tandem. In this work, we examine selective classification
in the presence of OOD data (SCOD). That is to say, the motivation for
detecting OOD samples is to reject them so their impact on the quality of
predictions is reduced. We show under this task specification, that existing
post-hoc methods perform quite differently compared to when evaluated only on
OOD detection. This is because it is no longer an issue to conflate
in-distribution (ID) data with OOD data if the ID data is going to be
misclassified. However, the conflation within ID data of correct and incorrect
predictions becomes undesirable. We also propose a novel method for SCOD,
Softmax Information Retaining Combination (SIRC), that augments softmax-based
confidence scores with feature-agnostic information such that their ability to
identify OOD samples is improved without sacrificing separation between correct
and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale
datasets and convolutional neural network architectures show that SIRC is able
to consistently match or outperform the baseline for SCOD, whilst existing OOD
detection methods fail to do so.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-bit Shift Network for End-to-End <span class="highlight-title">Spoken Language Understanding</span> <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anderson R. Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, Xiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNN) have achieved impressive success in multiple
domains. Over the years, the accuracy of these models has increased with the
proliferation of deeper and more complex architectures. Thus, state-of-the-art
solutions are often computationally expensive, which makes them unfit to be
deployed on edge computing platforms. In order to mitigate the high
computation, memory, and power requirements of inferring convolutional neural
networks (CNNs), we propose the use of power-of-two quantization, which
quantizes continuous parameters into low-bit power-of-two values. This reduces
computational complexity by removing expensive multiplication operations and
with the use of low-bit weights. ResNet is adopted as the building block of our
solution and the proposed model is evaluated on a spoken language understanding
(SLU) task. Experimental results show improved performance for shift neural
network architectures, with our low-bit quantization achieving 98.76 \% on the
test set which is comparable performance to its full-precision counterpart and
state-of-the-art solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Diffusion Strategy for Performance Improvement
  of Federated Learning with Non-IID Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyoung Ahn, Soohyeong Kim, Yongseok Kwon, Joohan Park, Jiseung Youn, Sunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a novel learning paradigm that addresses the
privacy leakage challenge of centralized learning. However, in FL, users with
non-independent and identically distributed (non-IID) characteristics can
deteriorate the performance of the global model. Specifically, the global model
suffers from the weight divergence challenge owing to non-IID data. To address
the aforementioned challenge, we propose a novel diffusion strategy of the
machine learning (ML) model (FedDif) to maximize the FL performance with
non-IID data. In FedDif, users spread local models to neighboring users over
D2D communications. FedDif enables the local model to experience different
distributions before parameter aggregation. Furthermore, we theoretically
demonstrate that FedDif can circumvent the weight divergence challenge. On the
theoretical basis, we propose the communication-efficient diffusion strategy of
the ML model, which can determine the trade-off between the learning
performance and communication cost based on auction theory. The performance
evaluation results show that FedDif improves the test accuracy of the global
model by 11% compared to the baseline FL with non-IID settings. Moreover,
FedDif improves communication efficiency in perspective of the number of
transmitted sub-frames and models by 2.77 folds than the latest methods
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic <span class="highlight-title">Review</span> and Replicability Study of <span class="highlight-title">BERT</span>4Rec for Sequential
  Recommendation <span class="chip">RecSys
  '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Petrov, Craig Macdonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  BERT4Rec is an effective model for sequential recommendation based on the
Transformer architecture. In the original publication, BERT4Rec claimed
superiority over other available sequential recommendation approaches (e.g.
SASRec), and it is now frequently being used as a state-of-the art baseline for
sequential recommendations. However, not all subsequent publications confirmed
this result and proposed other models that were shown to outperform BERT4Rec in
effectiveness. In this paper we systematically review all publications that
compare BERT4Rec with another popular Transformer-based model, namely SASRec,
and show that BERT4Rec results are not consistent within these publications. To
understand the reasons behind this inconsistency, we analyse the available
implementations of BERT4Rec and show that we fail to reproduce results of the
original BERT4Rec publication when using their default configuration
parameters. However, we are able to replicate the reported results with the
original code if training for a much longer amount of time (up to 30x) compared
to the default configuration. We also propose our own implementation of
BERT4Rec based on the Hugging Face Transformers library, which we demonstrate
replicates the originally reported results on 3 out 4 datasets, while requiring
up to 95% less training time to converge. Overall, from our systematic review
and detailed experiments, we conclude that BERT4Rec does indeed exhibit
state-of-the-art effectiveness for sequential recommendation, but only when
trained for a sufficient amount of time. Additionally, we show that our
implementation can further benefit from adapting other Transformer
architectures that are available in the Hugging Face Transformers library (e.g.
using disentangled attention, as provided by DeBERTa, or larger hidden layer
size cf. ALBERT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the Reproducibility track of the ACM RecSys
  '22 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mechanical Neural Network(MNN) -- A physical implementation of a
  multilayer perceptron for education and hands-on experimentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Schaffland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper the Mechanical Neural Network(MNN) is introduced, a physical
implementation of a multilayer perceptron(MLP) with ReLU activation functions,
two input neurons, four hidden neurons and two output neurons. This physical
model of a MLP is used in education to give a hands on experience and allow
students to experience the effect of changing the parameters of the network on
the output. Neurons are small wooden levers which are connected by threads.
Students can adapt the weights between the neurons by moving the clamps
connecting a neuron via a thread to the next. The MNN can model real valued
functions and logical operators including XOR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>short video (30sec): https://youtu.be/zMxh3Io3hFE, full presentation
  video: https://youtu.be/cEzk8JKDzy4; 8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Invariant Models via Koopman Spectra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Konishi, Yoshinobu Kawahara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weight-tied models have attracted attention in the modern development of
neural networks. The deep equilibrium model (DEQ) represents infinitely deep
neural networks with weight-tying, and recent studies have shown the potential
of this type of approach. DEQs are needed to iteratively solve root-finding
problems in training and are built on the assumption that the underlying
dynamics determined by the models converge to a fixed point. In this paper, we
present the stable invariant model (SIM), a new class of deep models that in
principle approximates DEQs under stability and extends the dynamics to more
general ones converging to an invariant set (not restricted in a fixed point).
The key ingredient in deriving SIMs is a representation of the dynamics with
the spectra of the Koopman and Perron--Frobenius operators. This perspective
approximately reveals stable dynamics with DEQs and then derives two variants
of SIMs. We also propose an implementation of SIMs that can be learned in the
same way as feedforward models. We illustrate the empirical performance of SIMs
with experiments and demonstrate that SIMs achieve comparative or superior
performance against DEQs in several learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating an Explainable Intrusion Detection System Using Self Organizing
  Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Ables, Thomas Kirby, William Anderson, Sudip Mittal, Shahram Rahimi, Ioana Banicescu, Maria Seale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Artificial Intelligence (AI) enabled Intrusion Detection Systems (IDS)
are complex black boxes. This means that a security analyst will have little to
no explanation or clarification on why an IDS model made a particular
prediction. A potential solution to this problem is to research and develop
Explainable Intrusion Detection Systems (X-IDS) based on current capabilities
in Explainable Artificial Intelligence (XAI). In this paper, we create a Self
Organizing Maps (SOMs) based X-IDS system that is capable of producing
explanatory visualizations. We leverage SOM's explainability to create both
global and local explanations. An analyst can use global explanations to get a
general idea of how a particular IDS model computes predictions. Local
explanations are generated for individual datapoints to explain why a certain
prediction value was computed. Furthermore, our SOM based X-IDS was evaluated
on both explanation generation and traditional accuracy tests using the NSL-KDD
and the CIC-IDS-2017 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Application of the Target Trial Causal Framework and Machine
  Learning Modeling to Optimize Antibiotic Therapy: Use Case on Acute Bacterial
  Skin and Skin Structure Infections due to Methicillin-resistant
  Staphylococcus aureus <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inyoung Jun, Simone Marini, Christina A. Boucher, J. Glenn Morris, Jiang Bian, Mattia Prosperi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bacterial infections are responsible for high mortality worldwide.
Antimicrobial resistance underlying the infection, and multifaceted patient's
clinical status can hamper the correct choice of antibiotic treatment.
Randomized clinical trials provide average treatment effect estimates but are
not ideal for risk stratification and optimization of therapeutic choice, i.e.,
individualized treatment effects (ITE). Here, we leverage large-scale
electronic health record data, collected from Southern US academic clinics, to
emulate a clinical trial, i.e., 'target trial', and develop a machine learning
model of mortality prediction and ITE estimation for patients diagnosed with
acute bacterial skin and skin structure infection (ABSSSI) due to
methicillin-resistant Staphylococcus aureus (MRSA). ABSSSI-MRSA is a
challenging condition with reduced treatment options - vancomycin is the
preferred choice, but it has non-negligible side effects. First, we use
propensity score matching to emulate the trial and create a treatment
randomized (vancomycin vs. other antibiotics) dataset. Next, we use this data
to train various machine learning methods (including boosted/LASSO logistic
regression, support vector machines, and random forest) and choose the best
model in terms of area under the receiver characteristic (AUC) through
bootstrap validation. Lastly, we use the models to calculate ITE and identify
possible averted deaths by therapy change. The out-of-bag tests indicate that
SVM and RF are the most accurate, with AUC of 81% and 78%, respectively, but
BLR/LASSO is not far behind (76%). By calculating the counterfactuals using the
BLR/LASSO, vancomycin increases the risk of death, but it shows a large
variation (odds ratio 1.2, 95% range 0.4-3.8) and the contribution to outcome
probability is modest. Instead, the RF exhibits stronger changes in ITE,
suggesting more complex treatment heterogeneity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the Proceedings of the KDD workshop on Applied Data Science
  for Healthcare (DSHealth 2022), which was held on Washington D.C, August 14
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds
  Representing Laparoscopic Scenes <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Alt, Christian Kunz, Darko Katic, Rayan Younis, Rainer Jäkel, Beat Peter Müller-Stich, Martin Wagner, Franziska Mathis-Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic segmentation of surgical scenes is a prerequisite for task
automation in robot assisted interventions. We propose LapSeg3D, a novel
DNN-based approach for the voxel-wise annotation of point clouds representing
surgical scenes. As the manual annotation of training data is highly time
consuming, we introduce a semi-autonomous clustering-based pipeline for the
annotation of the gallbladder, which is used to generate segmented labels for
the DNN. When evaluated against manually annotated data, LapSeg3D achieves an
F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo
porcine livers. We show LapSeg3D to generalize accurately across different
gallbladders and datasets recorded with different RGB-D camera systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted at the 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Rank Approximation for General Tensor Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvind V. Mahankali, David P. Woodruff, Ziyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of approximating a given tensor with $q$ modes $A \in
\mathbb{R}^{n \times \ldots \times n}$ with an arbitrary tensor network of rank
$k$ -- that is, a graph $G = (V, E)$, where $|V| = q$, together with a
collection of tensors $\{U_v \mid v \in V\}$ which are contracted in the manner
specified by $G$ to obtain a tensor $T$. For each mode of $U_v$ corresponding
to an edge incident to $v$, the dimension is $k$, and we wish to find $U_v$
such that the Frobenius norm distance between $T$ and $A$ is minimized. This
generalizes a number of well-known tensor network decompositions, such as the
Tensor Train, Tensor Ring, Tucker, and PEPS decompositions. We approximate $A$
by a binary tree network $T'$ with $O(q)$ cores, such that the dimension on
each edge of this network is at most $\widetilde{O}(k^{O(dt)} \cdot
q/\varepsilon)$, where $d$ is the maximum degree of $G$ and $t$ is its
treewidth, such that $\|A - T'\|_F^2 \leq (1 + \varepsilon) \|A - T\|_F^2$. The
running time of our algorithm is $O(q \cdot \text{nnz}(A)) + n \cdot
\text{poly}(k^{dt}q/\varepsilon)$, where $\text{nnz}(A)$ is the number of
nonzero entries of $A$. Our algorithm is based on a new dimensionality
reduction technique for tensor decomposition which may be of independent
interest.
  We also develop fixed-parameter tractable $(1 + \varepsilon)$-approximation
algorithms for Tensor Train and Tucker decompositions, improving the running
time of Song, Woodruff and Zhong (SODA, 2019) and avoiding the use of generic
polynomial system solvers. We show that our algorithms have a nearly optimal
dependence on $1/\varepsilon$ assuming that there is no $O(1)$-approximation
algorithm for the $2 \to 4$ norm with better running time than brute force.
Finally, we give additional results for Tucker decomposition with robust loss
functions, and fixed-parameter tractable CP decomposition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plex: Towards Reliability using <span class="highlight-title">Pretrain</span>ed Large Model Extensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Tran, Jeremiah Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, Neil Band, Tim G. J. Rudner, Karan Singhal, Zachary Nado, Joost van Amersfoort, Andreas Kirsch, Rodolphe Jenatton, Nithum Thain, Honglin Yuan, Kelly Buchanan, Kevin Murphy, D. Sculley, Yarin Gal, Zoubin Ghahramani, Jasper Snoek, Balaji Lakshminarayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent trend in artificial intelligence is the use of pretrained models for
language and vision tasks, which have achieved extraordinary performance but
also puzzling failures. Probing these models' abilities in diverse ways is
therefore critical to the field. In this paper, we explore the reliability of
models, where we define a reliable model as one that not only achieves strong
predictive performance but also performs well consistently over many
decision-making tasks involving uncertainty (e.g., selective prediction, open
set recognition), robust generalization (e.g., accuracy and proper scoring
rules such as log-likelihood on in- and out-of-distribution datasets), and
adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of
tasks over 40 datasets in order to evaluate different aspects of reliability on
both vision and language domains. To improve reliability, we developed ViT-Plex
and T5-Plex, pretrained large model extensions for vision and language
modalities, respectively. Plex greatly improves the state-of-the-art across
reliability tasks, and simplifies the traditional protocol as it improves the
out-of-the-box performance and does not require designing scores or tuning the
model for each task. We demonstrate scaling effects over model sizes up to 1B
parameters and pretraining dataset sizes up to 4B examples. We also demonstrate
Plex's capabilities on challenging tasks including zero-shot open set
recognition, active learning, and uncertainty in conversational language
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://goo.gle/plex-code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pathGCN: Learning General Graph Spatial Operators from Paths <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Eliasof, Eldad Haber, Eran Treister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCNs), similarly to Convolutional Neural
Networks (CNNs), are typically based on two main operations - spatial and
point-wise convolutions. In the context of GCNs, differently from CNNs, a
pre-determined spatial operator based on the graph Laplacian is often chosen,
allowing only the point-wise operations to be learnt. However, learning a
meaningful spatial operator is critical for developing more expressive GCNs for
improved performance. In this paper we propose pathGCN, a novel approach to
learn the spatial operator from random paths on the graph. We analyze the
convergence of our method and its difference from existing GCNs. Furthermore,
we discuss several options of combining our learnt spatial operator with
point-wise convolutions. Our extensive experiments on numerous datasets suggest
that by properly learning both the spatial and point-wise convolutions,
phenomena like over-smoothing can be inherently avoided, and new
state-of-the-art performance is achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Approach for Link Prediction in Directed Complex Networks based on
  Asymmetric Similarity-Popularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafida Benhidour, Lama Almeshkhas, Said Kerrache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex networks are graphs representing real-life systems that exhibit
unique characteristics not found in purely regular or completely random graphs.
The study of such systems is vital but challenging due to the complexity of the
underlying processes. This task has nevertheless been made easier in recent
decades thanks to the availability of large amounts of networked data. Link
prediction in complex networks aims to estimate the likelihood that a link
between two nodes is missing from the network. Links can be missing due to
imperfections in data collection or simply because they are yet to appear.
Discovering new relationships between entities in networked data has attracted
researchers' attention in various domains such as sociology, computer science,
physics, and biology. Most existing research focuses on link prediction in
undirected complex networks. However, not all real-life systems can be
faithfully represented as undirected networks. This simplifying assumption is
often made when using link prediction algorithms but inevitably leads to loss
of information about relations among nodes and degradation in prediction
performance. This paper introduces a link prediction method designed explicitly
for directed networks. It is based on the similarity-popularity paradigm, which
has recently proven successful in undirected networks. The presented algorithms
handle the asymmetry in node relationships by modeling it as asymmetry in
similarity and popularity. Given the observed network topology, the algorithms
approximate the hidden similarities as shortest path distances using edge
weights that capture and factor out the links' asymmetry and nodes' popularity.
The proposed approach is evaluated on real-life networks, and the experimental
results demonstrate its effectiveness in predicting missing links across a
broad spectrum of networked data types and sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error analysis for deep neural network approximations of parametric
  hyperbolic conservation laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim De Ryck, Siddhartha Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive rigorous bounds on the error resulting from the approximation of
the solution of parametric hyperbolic scalar conservation laws with ReLU neural
networks. We show that the approximation error can be made as small as desired
with ReLU neural networks that overcome the curse of dimensionality. In
addition, we provide an explicit upper bound on the generalization error in
terms of the training error, number of training samples and the neural network
size. The theoretical results are illustrated by numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feasibility of Inconspicuous GAN-generated Adversarial Patches against
  Object Detection <span class="chip">IJCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svetlana Pavlitskaya, Bianca-Marina Codău, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard approaches for adversarial patch generation lead to noisy
conspicuous patterns, which are easily recognizable by humans. Recent research
has proposed several approaches to generate naturalistic patches using
generative adversarial networks (GANs), yet only a few of them were evaluated
on the object detection use case. Moreover, the state of the art mostly focuses
on suppressing a single large bounding box in input by overlapping it with the
patch directly. Suppressing objects near the patch is a different, more complex
task. In this work, we have evaluated the existing approaches to generate
inconspicuous patches. We have adapted methods, originally developed for
different computer vision tasks, to the object detection use case with YOLOv3
and the COCO dataset. We have evaluated two approaches to generate naturalistic
patches: by incorporating patch generation into the GAN training process and by
using the pretrained GAN. For both cases, we have assessed a trade-off between
performance and naturalistic patch appearance. Our experiments have shown, that
using a pre-trained GAN helps to gain realistic-looking patches while
preserving the performance similar to conventional adversarial patches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IJCAI 2022 AISafety workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-sensitive neocortical neurons transform the effectiveness and
  efficiency of neural information processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahsan Adeel, Mario Franco, Mohsin Raza, Khubaib Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is ample neurobiological evidence that context-sensitive neocortical
neurons use their apical inputs as context to amplify the transmission of
coherent feedforward (FF) inputs. However, it has not been demonstrated until
now how this known mechanism can provide useful neural computation. Here we
show for the first time that the processing and learning capabilities of this
form of neural information processing are well-matched to the abilities of
mammalian neocortex. Specifically, we show that a network composed of such
local processors restricts the transmission of conflicting information to
higher levels and greatly reduces the amount of activity required to process
large amounts of heterogeneous real-world data e.g., when processing
audiovisual speech, these local processors use seen lip movements to
selectively amplify FF transmission of the auditory information that those
movements generate and vice versa. As this mechanism is shown to be far more
effective and efficient than the best available forms of deep neural nets, it
offers a step-change in understanding the brain's mysterious energy-saving
mechanism and inspires advances in designing enhanced forms of biologically
plausible machine learning algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pattern Analysis of Money Flow in the Bitcoin Blockchain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natkamon Tovanich, Rémy Cazabet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bitcoin is the first and highest valued cryptocurrency that stores
transactions in a publicly distributed ledger called the blockchain.
Understanding the activity and behavior of Bitcoin actors is a crucial research
topic as they are pseudonymous in the transaction network. In this article, we
propose a method based on taint analysis to extract taint flows --dynamic
networks representing the sequence of Bitcoins transferred from an initial
source to other actors until dissolution. Then, we apply graph embedding
methods to characterize taint flows. We evaluate our embedding method with
taint flows from top mining pools and show that it can classify mining pools
with high accuracy. We also found that taint flows from the same period show
high similarity. Our work proves that tracing the money flows can be a
promising approach to classifying source actors and characterizing different
money flow patterns
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prerona Tarannum, Firoj Alam, Md. Arid Hasan, Sheak Rashed Haider Noori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide use of social media and digital technologies facilitates sharing
various news and information about events and activities. Despite sharing
positive information misleading and false information is also spreading on
social media. There have been efforts in identifying such misleading
information both manually by human experts and automatic tools. Manual effort
does not scale well due to the high volume of information, containing factual
claims, are appearing online. Therefore, automatically identifying check-worthy
claims can be very useful for human experts. In this study, we describe our
participation in Subtask-1A: Check-worthiness of tweets (English, Dutch and
Spanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing
steps and applied different models to identify whether a given text is worthy
of fact checking or not. We use the oversampling technique to balance the
dataset and applied SVM and Random Forest (RF) with TF-IDF representations. We
also used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models
for the experiments. We used BERT-m for the official submissions and our
systems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,
respectively. In further experiments, our evaluation shows that transformer
models (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and
English languages where a different scenario is observed for Spanish.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CLEF 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with
  Unknown Number of Sound Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Yin, Meng Ge, Yanjie Fu, Gaoyan Zhang, Longbiao Wang, Lei Zhang, Lin Qiu, Jianwu Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent neural network based Direction of Arrival (DoA) estimation algorithms
have performed well on unknown number of sound sources scenarios. These
algorithms are usually achieved by mapping the multi-channel audio input to the
single output (i.e. overall spatial pseudo-spectrum (SPS) of all sources), that
is called MISO. However, such MISO algorithms strongly depend on empirical
threshold setting and the angle assumption that the angles between the sound
sources are greater than a fixed angle. To address these limitations, we
propose a novel multi-channel input and multiple outputs DoA network called
MIMO-DoAnet. Unlike the general MISO algorithms, MIMO-DoAnet predicts the SPS
coding of each sound source with the help of the informative spatial covariance
matrix. By doing so, the threshold task of detecting the number of sound
sources becomes an easier task of detecting whether there is a sound source in
each output, and the serious interaction between sound sources disappears
during inference stage. Experimental results show that MIMO-DoAnet achieves
relative 18.6% and absolute 13.3%, relative 34.4% and absolute 20.2% F1 score
improvement compared with the MISO baseline system in 3, 4 sources scenes. The
results also demonstrate MIMO-DoAnet alleviates the threshold setting problem
and solves the angle assumption problem effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Dermoscopic Image Feature Representation Learning for
  Melanoma Classification <span class="chip">ICONIP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChengHui Yu, MingKang Tang, ShengGe Yang, MingQing Wang, Zhe Xu, JiangPeng Yan, HanMo Chen, Yu Yang, Xiao-Jun Zeng, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based melanoma classification with dermoscopic images has
recently shown great potential in automatic early-stage melanoma diagnosis.
However, limited by the significant data imbalance and obvious extraneous
artifacts, i.e., the hair and ruler markings, discriminative feature extraction
from dermoscopic images is very challenging. In this study, we seek to resolve
these problems respectively towards better representation learning for lesion
features. Specifically, a GAN-based data augmentation (GDA) strategy is adapted
to generate synthetic melanoma-positive images, in conjunction with the
proposed implicit hair denoising (IHD) strategy. Wherein the hair-related
representations are implicitly disentangled via an auxiliary classifier network
and reversely sent to the melanoma-feature extraction backbone for better
melanoma-specific representation learning. Furthermore, to train the IHD
module, the hair noises are additionally labeled on the ISIC2020 dataset,
making it the first large-scale dermoscopic dataset with annotation of
hair-like artifacts. Extensive experiments demonstrate the superiority of the
proposed framework as well as the effectiveness of each component. The improved
dataset publicly avaliable at https://github.com/kirtsy/DermoscopicDataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICONIP 2021 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direction-Aware Adaptive Online Neural Speech Enhancement with an
  Augmented Reality Headset in Real Noisy <span class="highlight-title">Conversation</span>al Environments <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kouhei Sekiguchi, Aditya Arie Nugraha, Yicheng Du, Yoshiaki Bando, Mathieu Fontaine, Kazuyoshi Yoshii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the practical response- and performance-aware
development of online speech enhancement for an augmented reality (AR) headset
that helps a user understand conversations made in real noisy echoic
environments (e.g., cocktail party). One may use a state-of-the-art blind
source separation method called fast multichannel nonnegative matrix
factorization (FastMNMF) that works well in various environments thanks to its
unsupervised nature. Its heavy computational cost, however, prevents its
application to real-time processing. In contrast, a supervised beamforming
method that uses a deep neural network (DNN) for estimating spatial information
of speech and noise readily fits real-time processing, but suffers from drastic
performance degradation in mismatched conditions. Given such complementary
characteristics, we propose a dual-process robust online speech enhancement
method based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF
(back end) is performed in a mini-batch style and the noisy and enhanced speech
pairs are used together with the original parallel training data for updating
the direction-aware DNN (front end) with backpropagation at a
computationally-allowable interval. This method is used with a blind
dereverberation method called weighted prediction error (WPE) for transcribing
the noisy reverberant speech of a speaker, which can be detected from video or
selected by a user's hand gesture or eye gaze, in a streaming manner and
spatially showing the transcriptions with an AR technique. Our experiment
showed that the word error rate was improved by more than 10 points with the
run-time adaptation using only twelve minutes of observation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/RSJ IROS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Riemannian Natural Gradient Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Hu, Ruicheng Ao, Anthony Man-Cho So, Minghan Yang, Zaiwen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies large-scale optimization problems on Riemannian manifolds
whose objective function is a finite sum of negative log-probability losses.
Such problems arise in various machine learning and signal processing
applications. By introducing the notion of Fisher information matrix in the
manifold setting, we propose a novel Riemannian natural gradient method, which
can be viewed as a natural extension of the natural gradient method from the
Euclidean setting to the manifold setting. We establish the almost-sure global
convergence of our proposed method under standard assumptions. Moreover, we
show that if the loss function satisfies certain convexity and smoothness
conditions and the input-output map satisfies a Riemannian Jacobian stability
condition, then our proposed method enjoys a local linear -- or, under the
Lipschitz continuity of the Riemannian Jacobian of the input-output map, even
quadratic -- rate of convergence. We then prove that the Riemannian Jacobian
stability condition will be satisfied by a two-layer fully connected neural
network with batch normalization with high probability, provided that the width
of the network is sufficiently large. This demonstrates the practical relevance
of our convergence rate result. Numerical experiments on applications arising
from machine learning demonstrate the advantages of the proposed method over
state-of-the-art ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direction-Aware Joint Adaptation of Neural Speech Enhancement and
  Recognition in Real Multiparty <span class="highlight-title">Conversation</span>al Environments <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Du, Aditya Arie Nugraha, Kouhei Sekiguchi, Yoshiaki Bando, Mathieu Fontaine, Kazuyoshi Yoshii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes noisy speech recognition for an augmented reality
headset that helps verbal communication within real multiparty conversational
environments. A major approach that has actively been studied in simulated
environments is to sequentially perform speech enhancement and automatic speech
recognition (ASR) based on deep neural networks (DNNs) trained in a supervised
manner. In our task, however, such a pretrained system fails to work due to the
mismatch between the training and test conditions and the head movements of the
user. To enhance only the utterances of a target speaker, we use beamforming
based on a DNN-based speech mask estimator that can adaptively extract the
speech components corresponding to a head-relative particular direction. We
propose a semi-supervised adaptation method that jointly updates the mask
estimator and the ASR model at run-time using clean speech signals with
ground-truth transcriptions and noisy speech signals with highly-confident
estimated transcriptions. Comparative experiments using the state-of-the-art
distant speech recognition system show that the proposed method significantly
improves the ASR performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set-based value operators for non-stationary Markovian environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah H. Q. Li, Assalé Adjé, Pierre-Loïc Garoche, Behçet Açıkmeşe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes finite state Markov Decision Processes (MDPs) with
uncertain parameters in compact sets and re-examines results from robust MDP
via set-based fixed point theory. We generalize the Bellman and policy
evaluation operators to operators that contract on the space of value functions
and denote them as \emph{value operators}. We generalize these value operators
to act on the space of value function sets and denote them as \emph{set-based
value operators}. We prove that these set-based value operators are
contractions in the space of compact value function sets. Leveraging insights
from set theory, we generalize the rectangularity condition for the Bellman
operator from classic robust MDP literature to a \emph{containment condition}
for a generic value operator, which is weaker and can be applied to a larger
set of parameter-uncertain MDPs and contractive operators in dynamic
programming and reinforcement learning. We prove that both the rectangularity
condition and the containment condition sufficiently ensure that the set-based
value operator's fixed point set contains its own supremum and infimum
elements. For convex and compact sets of uncertain MDP parameters, we show
equivalence between the classic robust value function and the supremum of the
fixed point set of the set-based Bellman operator. Under dynamically changing
MDP parameters in compact sets, we prove a set convergence result for value
iteration, which otherwise may not converge to a single value function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaleNet: Searching for the Model to Scale <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyang Xie, Xiu Su, Shan You, Zhanyu Ma, Fei Wang, Chen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, community has paid increasing attention on model scaling and
contributed to developing a model family with a wide spectrum of scales.
Current methods either simply resort to a one-shot NAS manner to construct a
non-structural and non-scalable model family or rely on a manual yet fixed
scaling strategy to scale an unnecessarily best base model. In this paper, we
bridge both two components and propose ScaleNet to jointly search base model
and scaling strategy so that the scaled large model can have more promising
performance. Concretely, we design a super-supernet to embody models with
different spectrum of sizes (e.g., FLOPs). Then, the scaling strategy can be
learned interactively with the base model via a Markov chain-based evolution
algorithm and generalized to develop even larger models. To obtain a decent
super-supernet, we design a hierarchical sampling strategy to enhance its
training sufficiency and alleviate the disturbance. Experimental results show
our scaled networks enjoy significant performance superiority on various FLOPs,
but with at least 2.53x reduction on search cost. Codes are available at
https://github.com/luminolx/ScaleNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Probabilistic Marching Cubes by Deep Learning for
  Time-Varying Scalar Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjiao Han, Tushar M. Athawale, David Pugmire, Chris R. Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualizing the uncertainty of ensemble simulations is challenging due to the
large size and multivariate and temporal features of ensemble data sets. One
popular approach to studying the uncertainty of ensembles is analyzing the
positional uncertainty of the level sets. Probabilistic marching cubes is a
technique that performs Monte Carlo sampling of multivariate Gaussian noise
distributions for positional uncertainty visualization of level sets. However,
the technique suffers from high computational time, making interactive
visualization and analysis impossible to achieve. This paper introduces a
deep-learning-based approach to learning the level-set uncertainty for
two-dimensional ensemble data with a multivariate Gaussian noise assumption. We
train the model using the first few time steps from time-varying ensemble data
in our workflow. We demonstrate that our trained model accurately infers
uncertainty in level sets for new time steps and is up to 170X faster than that
of the original probabilistic model with serial computation and 10X faster than
that of the original parallel computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, IEEE Vis 2022 Short Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Task-free Continual Learning by Distributionally Robust Memory
  Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Tiehang Duan, Mingchen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-free continual learning (CL) aims to learn a non-stationary data stream
without explicit task definitions and not forget previous knowledge. The widely
adopted memory replay approach could gradually become less effective for long
data streams, as the model may memorize the stored examples and overfit the
memory buffer. Second, existing methods overlook the high uncertainty in the
memory data distribution since there is a big gap between the memory data
distribution and the distribution of all the previous data examples. To address
these problems, for the first time, we propose a principled memory evolution
framework to dynamically evolve the memory data distribution by making the
memory buffer gradually harder to be memorized with distributionally robust
optimization (DRO). We then derive a family of methods to evolve the memory
buffer data in the continuous probability measure space with Wasserstein
gradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory
data distribution, thus guarantees the model performance and learns
significantly more robust features than existing memory-replay-based methods.
Extensive experiments on existing benchmarks demonstrate the effectiveness of
the proposed methods for alleviating forgetting. As a by-product of the
proposed framework, our method is more robust to adversarial examples than
existing task-free CL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Non-Cooperative <span class="highlight-title">Dialogue</span>: Theoretical and Empirical Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Sicilia, Tristan Maidment, Pat Healy, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating cooperativity of interlocutors is central in studying
pragmatics of dialogue. Models of conversation that only assume cooperative
agents fail to explain the dynamics of strategic conversations. Thus, we
investigate the ability of agents to identify non-cooperative interlocutors
while completing a concurrent visual-dialogue task. Within this novel setting,
we study the optimality of communication strategies for achieving this
multi-task objective. We use the tools of learning theory to develop a
theoretical model for identifying non-cooperative interlocutors and apply this
theory to analyze different communication strategies. We also introduce a
corpus of non-cooperative conversations about images in the GuessWhat?! dataset
proposed by De Vries et al. (2017). We use reinforcement learning to implement
multiple communication strategies in this context and find empirical results
validate our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Super-exponential Quantum Speedup of Equivariant Quantum Machine
  Learning Algorithms with SU($d$) Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zheng, Zimu Li, Junyu Liu, Sergii Strelchuk, Risi Kondor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework of the equivariant convolutional algorithms which is
tailored for a number of machine-learning tasks on physical systems with
arbitrary SU($d$) symmetries. It allows us to enhance a natural model of
quantum computation--permutational quantum computing (PQC) [Quantum Inf.
Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While
PQC was shown to be effectively classically simulatable, we exhibit a problem
which can be efficiently solved on PQC+ machine, whereas the best known
classical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence
against PQC+ being classically simulatable. We further discuss practical
quantum machine learning algorithms which can be carried out in the paradigm of
PQC+.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version established based on arXiv:2112.07611, presented in
  TQC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LineCap: Line Charts for Data Visualization Captioning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anita Mahinpei, Zona Kostic, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data visualization captions help readers understand the purpose of a
visualization and are crucial for individuals with visual impairments. The
prevalence of poor figure captions and the successful application of deep
learning approaches to image captioning motivate the use of similar techniques
for automated figure captioning. However, research in this field has been
stunted by the lack of suitable datasets. We introduce LineCap, a novel figure
captioning dataset of 3,528 figures, and we provide insights from curating this
dataset and using end-to-end deep learning models for automated figure
captioning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Acoustic scene classification using auditory <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.13450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.13450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayesh Kumpawat, Shubhajit Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The approach used not only challenges some of the fundamental mathematical
techniques used so far in early experiments of the same trend but also
introduces new scopes and new horizons for interesting results. The physics
governing spectrograms have been optimized in the project along with exploring
how it handles the intense requirements of the problem at hand. Major
contributions and developments brought under the light, through this project
involve using better mathematical techniques and problem-specific machine
learning methods. Improvised data analysis and data augmentation for audio
datasets like frequency masking and random frequency-time stretching are used
in the project and hence are explained in this paper. In the used methodology,
the audio transforms principle were also tried and explored, and indeed the
insights gained were used constructively in the later stages of the project.
Using a deep learning principle is surely one of them. Also, in this paper, the
potential scopes and upcoming research openings in both short and long term
tunnel of time has been presented. Although much of the results gained are
domain-specific as of now, they are surely potent enough to produce novel
solutions in various different domains of diverse backgrounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Learning in Infinite-Width Neural Networks <span class="chip">ICML 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.14522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.14522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Greg Yang, Edward J. Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As its width tends to infinity, a deep neural network's behavior under
gradient descent can become simplified and predictable (e.g. given by the
Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK
parametrization). However, we show that the standard and NTK parametrizations
of a neural network do not admit infinite-width limits that can learn features,
which is crucial for pretraining and transfer learning such as with BERT. We
propose simple modifications to the standard parametrization to allow for
feature learning in the limit. Using the *Tensor Programs* technique, we derive
explicit formulas for such limits. On Word2Vec and few-shot learning on
Omniglot via MAML, two canonical tasks that rely crucially on feature learning,
we compute these limits exactly. We find that they outperform both NTK
baselines and finite-width networks, with the latter approaching the
infinite-width feature learning performance as width increases.
  More generally, we classify a natural space of neural network
parametrizations that generalizes standard, NTK, and Mean Field
parametrizations. We show 1) any parametrization in this space either admits
feature learning or has an infinite-width training dynamics given by kernel
gradient descent, but not both; 2) any such infinite-width limit can be
computed using the Tensor Programs technique. Code for our experiments can be
found at github.com/edwardjhu/TP4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th paper in the Tensor Programs series. Appearing in ICML 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Networks to Solve Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Liu, Yuzhe Lu, Ali Abbasi, Meiyi Li, Javad Mohammadi, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging machine learning to facilitate the optimization process is an
emerging field that holds the promise to bypass the fundamental computational
bottleneck caused by classic iterative solvers in critical applications
requiring near-real-time optimization. The majority of existing approaches
focus on learning data-driven optimizers that lead to fewer iterations in
solving an optimization. In this paper, we take a different approach and
propose to replace the iterative solvers altogether with a trainable parametric
set function, that outputs the optimal arguments/parameters of an optimization
problem in a single feed forward. We denote our method as Learning to Optimize
the Optimization Process (LOOP). We show the feasibility of learning such
parametric (set) functions to solve various classic optimization problems
including linear/nonlinear regression, principal component analysis,
transport-based coreset, and quadratic programming in supply management
applications. In addition, we propose two alternative approaches for learning
such parametric functions, with and without a solver in the LOOP. Finally,
through various numerical experiments, we show that the trained solvers could
be orders of magnitude faster than the classic iterative solvers while
providing near optimal solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Calibration: Learning of Model Calibration Using Differentiable
  Expected Calibration Error <span class="chip">ICML 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.09613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.09613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondrej Bohdal, Yongxin Yang, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibration of neural networks is a topical problem that is becoming more and
more important as neural networks increasingly underpin real-world
applications. The problem is especially noticeable when using modern neural
networks, for which there is a significant difference between the confidence of
the model and the probability of correct prediction. Various strategies have
been proposed to improve calibration, yet accurate calibration remains
challenging. We propose a novel framework with two contributions: introducing a
differentiable surrogate for expected calibration error (DECE) that allows
calibration quality to be directly optimised, and a meta-learning framework
that uses DECE to optimise for validation set calibration with respect to model
hyper-parameters. The results show that we achieve competitive performance with
state-of-the-art calibration approaches. Our framework opens up a new avenue
and toolset for tackling calibration, which we believe will inspire further
work in this important challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shorter version was presented at ICML 2021 Workshop on Uncertainty
  and Robustness in Deep Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> learning in non-small cell lung cancer discovers novel
  morphological clusters linked to patient outcome and molecular phenotypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Xinyu Yang, Luis Chiriboga, Afreen Karimkhan, Navneet Narula, Harvey Pass, Andre L. Moreira, John Le Quesne, Aristotelis Tsirigos, Ke Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathological images provide the definitive source of cancer diagnosis,
containing information used by pathologists to identify and subclassify
malignant disease, and to guide therapeutic choices. These images contain vast
amounts of information, much of which is currently unavailable to human
interpretation. Supervised deep learning approaches have been powerful for
classification tasks, but they are inherently limited by the cost and quality
of annotations. Therefore, we developed Histomorphological Phenotype Learning,
an unsupervised methodology, which requires no annotations and operates via the
self-discovery of discriminatory image features in small image tiles. Tiles are
grouped into morphologically similar clusters which appear to represent
recurrent modes of tumor growth emerging under natural selection. These
clusters have distinct features which can be identified using orthogonal
methods. Applied to lung cancer tissues, we show that they align closely with
patient outcomes, with histopathologically recognised tumor types and growth
patterns, and with transcriptomic measures of immunophenotype.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable
  Prototypes <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.15000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.15000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Donnelly, Alina Jade Barnett, Chaofan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deformable prototypical part network (Deformable ProtoPNet), an
interpretable image classifier that integrates the power of deep learning and
the interpretability of case-based reasoning. This model classifies input
images by comparing them with prototypes learned during training, yielding
explanations in the form of "this looks like that." However, while previous
methods use spatially rigid prototypes, we address this shortcoming by
proposing spatially flexible prototypes. Each prototype is made up of several
prototypical parts that adaptively change their relative spatial positions
depending on the input image. Consequently, a Deformable ProtoPNet can
explicitly capture pose variations and context, improving both model accuracy
and the richness of explanations provided. Compared to other case-based
interpretable models using prototypes, our approach achieves state-of-the-art
accuracy and gives an explanation with greater context. The code is available
at https://github.com/jdonnelly36/Deformable-ProtoPNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This was published in CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Continual Learning for Embedded Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler L. Hayes, Christopher Kanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Conference on Lifelong Learning Agents (CoLLAs-2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved conformalized quantile regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martim Sousa, Ana Maria Tomé, José Moreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformalized quantile regression is a procedure that inherits the advantages
of conformal prediction and quantile regression. That is, we use quantile
regression to estimate the true conditional quantile and then apply a conformal
step on a calibration set to ensure marginal coverage. In this way, we get
adaptive prediction intervals that account for heteroscedasticity. However, the
aforementioned conformal step lacks adaptiveness as described in (Romano et
al., 2019). To overcome this limitation, instead of applying a single conformal
step after estimating conditional quantiles with quantile regression, we
propose to cluster the explanatory variables weighted by their permutation
importance with an optimized k-means and apply k conformal steps. To show that
this improved version outperforms the classic version of conformalized quantile
regression and is more adaptive to heteroscedasticity, we extensively compare
the prediction intervals of both in open datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Benchmark <span class="highlight-title">dataset</span> for predictive maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Veloso, João Gama, Rita P. Ribeiro, Pedro M. Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper describes the MetroPT data set, an outcome of a Predictive
Maintenance project with an urban metro public transportation service in Porto,
Portugal. The data was collected between 2020 and 2022 that aimed to develop
machine learning methods for online anomaly detection and failure prediction.
By capturing several analogic sensor signals (pressure, temperature, current
consumption), digital signals (control signals, discrete signals), and GPS
information (latitude, longitude, and speed), we provide a framework that can
be easily used and developed for the new machine learning methods. We believe
this dataset contains some interesting characteristics and can be a good
benchmark for predictive maintenance models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective and Efficient Training for Sequential Recommendation using
  Recency Sampling <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Petrov, Craig Macdonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern sequential recommender systems use deep neural networks, which
can effectively estimate the relevance of items but require a lot of time to
train. Slow training increases expenses, hinders product development timescales
and prevents the model from being regularly updated to adapt to changing user
preferences. Training such sequential models involves appropriately sampling
past user interactions to create a realistic training objective. The existing
training objectives have limitations. For instance, next item prediction never
uses the beginning of the sequence as a learning target, thereby potentially
discarding valuable data. On the other hand, the item masking used by BERT4Rec
is only weakly related to the goal of the sequential recommendation; therefore,
it requires much more time to obtain an effective model. Hence, we propose a
novel Recency-based Sampling of Sequences training objective that addresses
both limitations. We apply our method to various recent and state-of-the-art
model architectures - such as GRU4Rec, Caser, and SASRec. We show that the
models enhanced with our method can achieve performances exceeding or very
close to stateof-the-art BERT4Rec, but with much less training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This full research paper is accepted at 16th ACM Conference on
  Recommender Systems (ACM RecSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of Batch Asynchronous Stochastic Approximation With
  Applications to Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeeva L. Karandikar, M. Vidyasagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The stochastic approximation algorithm is a widely used probabilistic method
for finding a zero of a vector-valued funtion, when only noisy measurements of
the function are available. In the literature to date, one can make a
distinction between "synchronous" updating, whereby every component of the
current guess is updated at each time, and `"synchronous" updating, whereby
only one component is updated. In principle, it is also possible to update, at
each time instant, some but not all components of $\theta_t$, which might be
termed as "batch asynchronous stochastic approximation" (BASA). Also, one can
also make a distinction between using a "local" clock versus a "global" clock.
  In this paper, we propose a unified formulation of batch asynchronous
stochastic approximation (BASA) algorithms, and develop a general methodology
for proving that such algorithms converge, irrespective of whether global or
local clocks are used. These convergence proofs make use of weaker hypotheses
than existing results. For example: existing convergence proofs when a local
clock is used require that the measurement noise is an i.i.d sequence. Here, it
is assumed that the measurement errors form a martingale difference sequence.
Also, all results to date assume that the stochastic step sizes satisfy a
probabilistic analog of the Robbins-Monro conditions. We replace this by a
purely deterministic condition on the irreducibility of the underlying Markov
processes.
  As specific applications to Reinforcement Learning, we introduce ``batch''
versions of the temporal difference algorithm $TD(0)$ for value iteration, and
the $Q$-learning algorithm for finding the optimal action-value function, and
also permit the use of local clocks instead of a global clock. In all cases, we
establish the convergence of these algorithms, under milder conditions than in
the existing literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HLT-MT: High-resource Language-specific Training for Multilingual Neural
  Machine Translation <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Yang, Yuwei Yin, Shuming Ma, Dongdong Zhang, Zhoujun Li, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual neural machine translation (MNMT) trained in multiple language
pairs has attracted considerable attention due to fewer model parameters and
lower training costs by sharing knowledge among multiple languages.
Nonetheless, multilingual training is plagued by language interference
degeneration in shared parameters because of the negative interference among
different translation directions, especially on high-resource languages. In
this paper, we propose the multilingual translation model with the
high-resource language-specific training (HLT-MT) to alleviate the negative
interference, which adopts the two-stage training with the language-specific
selection mechanism. Specifically, we first train the multilingual model only
with the high-resource pairs and select the language-specific modules at the
top of the decoder to enhance the translation quality of high-resource
directions. Next, the model is further trained on all available corpora to
transfer knowledge from high-resource languages (HRLs) to low-resource
languages (LRLs). Experimental results show that HLT-MT outperforms various
strong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic
experiments validate the effectiveness of our method in mitigating the negative
interference in multilingual training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, IJCAI-ECAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAC Reinforcement Learning for Predictive State Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study online Reinforcement Learning (RL) in partially
observable dynamical systems. We focus on the Predictive State Representations
(PSRs) model, which is an expressive model that captures other well-known
models such as Partially Observable Markov Decision Processes (POMDP). PSR
represents the states using a set of predictions of future observations and is
defined entirely using observable quantities. We develop a novel model-based
algorithm for PSRs that can learn a near optimal policy in sample complexity
scaling polynomially with respect to all the relevant parameters of the
systems. Our algorithm naturally works with function approximation to extend to
systems with potentially large state and observation spaces. We show that given
a realizable model class, the sample complexity of learning the near optimal
policy only scales polynomially with respect to the statistical complexity of
the model class, without any explicit polynomial dependence on the size of the
state and observation spaces. Notably, our work is the first work that shows
polynomial sample complexities to compete with the globally optimal policy in
PSRs. Finally, we demonstrate how our general theorem can be directly used to
derive sample complexity bounds for special models including $m$-step weakly
revealing and $m$-step decodable tabular POMDPs, POMDPs with low-rank latent
transition, and POMDPs with linear emission and latent transition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A note on large deviations for interacting particle dynamics for finding
  mixed equilibria in zero-sum games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Nilsson, Pierre Nyquist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding equilibria points in continuous minimax games has become a key
problem within machine learning, in part due to its connection to the training
of generative adversarial networks. Because of existence and robustness issues,
recent developments have shifted from pure equilibria to focusing on mixed
equilibria points. In this note we consider a method proposed by Domingo-Enrich
et al. for finding mixed equilibria in two-layer zero-sum games. The method is
based on entropic regularisation and the two competing strategies are
represented by two sets of interacting particles. We show that the sequence of
empirical measures of the particle system satisfies a large deviation principle
as the number of particles grows to infinity, and how this implies convergence
of the empirical measure and the associated Nikaid\^o-Isoda error,
complementing existing law of large numbers results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised section 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias Mitigation for Machine Learning Classifiers: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Hort, Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive survey of bias mitigation methods for
achieving fairness in Machine Learning (ML) models. We collect a total of 234
publications concerning bias mitigation for ML classifiers. These methods can
be distinguished based on their intervention procedure (i.e., pre-processing,
in-processing, post-processing) and the technology they apply. We investigate
how existing bias mitigation methods are evaluated in the literature. In
particular, we consider datasets, metrics and benchmarking. Based on the
gathered insights (e.g., what is the most popular fairness metric? How many
datasets are used for evaluating bias mitigation methods?). We hope to support
practitioners in making informed choices when developing and evaluating new
bias mitigation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theory of Acceleration of Decision Making by Correlated Time Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16004v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16004v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Norihiro Okada, Tomoki Yamagami, Nicolas Chauvet, Yusuke Ito, Mikio Hasegawa, Makoto Naruse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photonic accelerators have been intensively studied to provide enhanced
information processing capability to benefit from the unique attributes of
physical processes. Recently, it has been reported that chaotically oscillating
ultrafast time series from a laser, called laser chaos, provide the ability to
solve multi-armed bandit (MAB) problems or decision-making problems at GHz
order. Furthermore, it has been confirmed that the negatively correlated
time-domain structure of laser chaos contributes to the acceleration of
decision-making. However, the underlying mechanism of why decision-making is
accelerated by correlated time series is unknown. In this study, we demonstrate
a theoretical model to account for accelerating decision-making by correlated
time sequence. We first confirm the effectiveness of the negative
autocorrelation inherent in time series for solving two-armed bandit problems
using Fourier transform surrogate methods. We propose a theoretical model that
concerns the correlated time series subjected to the decision-making system and
the internal status of the system therein in a unified manner, inspired by
correlated random walks. We demonstrate that the performance derived
analytically by the theory agrees well with the numerical simulations, which
confirms the validity of the proposed model and leads to optimal system design.
The present study paves the way for improving the effectiveness of correlated
time series for decision-making, impacting artificial intelligence and other
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibration of Natural Language Understanding Models with Venn--ABERS
  Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrizio Giovannotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, currently the state-of-the-art in natural language
understanding (NLU) tasks, are prone to generate uncalibrated predictions or
extreme probabilities, making the process of taking different decisions based
on their output relatively difficult. In this paper we propose to build several
inductive Venn--ABERS predictors (IVAP), which are guaranteed to be well
calibrated under minimal assumptions, based on a selection of pre-trained
transformers. We test their performance over a set of diverse NLU tasks and
show that they are capable of producing well-calibrated probabilistic
predictions that are uniformly spread over the [0,1] interval -- all while
retaining the original model's predictive accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 11th Symposium on Conformal and Probabilistic
  Prediction with Applications - COPA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODFNet: Using orientation distribution functions to characterize 3D
  point clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.04708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.04708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf H. Sahin, Alican Mertan, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning new representations of 3D point clouds is an active research area in
3D vision, as the order-invariant point cloud structure still presents
challenges to the design of neural network architectures. Recent works explored
learning either global or local features or both for point clouds, however none
of the earlier methods focused on capturing contextual shape information by
analysing local orientation distribution of points. In this paper, we leverage
on point orientation distributions around a point in order to obtain an
expressive local neighborhood representation for point clouds. We achieve this
by dividing the spherical neighborhood of a given point into predefined cone
volumes, and statistics inside each volume are used as point features. In this
way, a local patch can be represented by not only the selected point's nearest
neighbors, but also considering a point density distribution defined along
multiple orientations around the point. We are then able to construct an
orientation distribution function (ODF) neural network that involves an
ODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet
model achieves state-of the-art accuracy for object classification on
ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under consideration at Computer Vision and Image
  Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ clDice -- A Novel Topology-Preserving Loss Function for Tubular
  Structure Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.07311v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.07311v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suprosanna Shit, Johannes C. Paetzold, Anjany Sekuboyina, Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien P. W. Pluim, Ulrich Bauer, Bjoern H. Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of tubular, network-like structures, such as vessels,
neurons, or roads, is relevant to many fields of research. For such structures,
the topology is their most important characteristic; particularly preserving
connectedness: in the case of vascular networks, missing a connected vessel
entirely alters the blood-flow dynamics. We introduce a novel similarity
measure termed centerlineDice (short clDice), which is calculated on the
intersection of the segmentation masks and their (morphological) skeleta. We
theoretically prove that clDice guarantees topology preservation up to homotopy
equivalence for binary 2D and 3D segmentation. Extending this, we propose a
computationally efficient, differentiable loss function (soft-clDice) for
training arbitrary neural segmentation networks. We benchmark the soft-clDice
loss on five public datasets, including vessels, roads and neurons (2D and 3D).
Training on soft-clDice leads to segmentation with more accurate connectivity
information, higher graph similarity, and better volumetric scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>* The authors Suprosanna Shit and Johannes C. Paetzold contributed
  equally to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring
  Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.00378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.00378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pau Ferrer-Cid, Jose M. Barcelo-Ordinas, Jorge Garcia-Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air pollution monitoring platforms play a very important role in preventing
and mitigating the effects of pollution. Recent advances in the field of graph
signal processing have made it possible to describe and analyze air pollution
monitoring networks using graphs. One of the main applications is the
reconstruction of the measured signal in a graph using a subset of sensors.
Reconstructing the signal using information from sensor neighbors can help
improve the quality of network data, examples are filling in missing data with
correlated neighboring nodes, or correcting a drifting sensor with neighboring
sensors that are more accurate. This paper compares the use of various types of
graph signal reconstruction methods applied to real data sets of Spanish air
pollution reference stations. The methods considered are Laplacian
interpolation, graph signal processing low-pass based graph signal
reconstruction, and kernel-based graph signal reconstruction, and are compared
on actual air pollution data sets measuring O3, NO2, and PM10. The ability of
the methods to reconstruct the signal of a pollutant is shown, as well as the
computational cost of this reconstruction. The results indicate the superiority
of methods based on kernel-based graph signal reconstruction, as well as the
difficulties of the methods to scale in an air pollution monitoring network
with a large number of low-cost sensors. However, we show that scalability can
be overcome with simple methods, such as partitioning the network using a
clustering algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying the Adversarial Robustness of Random Transformation
  Defenses <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chawin Sitawarin, Zachary Golan-Strieb, David Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks' lack of robustness against attacks raises concerns in
security-sensitive settings such as autonomous vehicles. While many
countermeasures may look promising, only a few withstand rigorous evaluation.
Defenses using random transformations (RT) have shown impressive results,
particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of
defense has not been rigorously evaluated, leaving its robustness properties
poorly understood. Their stochastic properties make evaluation more challenging
and render many proposed attacks on deterministic models inapplicable. First,
we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation
is ineffective and likely overestimates its robustness. We then attempt to
construct the strongest possible RT defense through the informed selection of
transformations and Bayesian optimization for tuning their parameters.
Furthermore, we create the strongest possible attack to evaluate our RT
defense. Our new attack vastly outperforms the baseline, reducing the accuracy
by 83% compared to the 19% reduction by the commonly used EoT attack
($4.3\times$ improvement). Our result indicates that the RT defense on the
Imagenette dataset (a ten-class subset of ImageNet) is not robust against
adversarial examples. Extending the study further, we use our new attack to
adversarially train RT defense (called AdvRT), resulting in a large robustness
gain. Code is available at
https://github.com/wagner-group/demystify-random-transform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022 (short presentation), AAAI 2022 AdvML Workshop (best paper,
  oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12747v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12747v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Mohammed Saifuddin, Briana Bumgardner, Farhan Tanvir, Esra Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in
the worst scenario, they may lead to adverse drug reactions (ADRs). Predicting
all DDIs is a challenging and critical problem. Most existing computational
models integrate drug-centric information from different sources and leverage
them as features in machine learning classifiers to predict DDIs. However,
these models have a high chance of failure, especially for the new drugs when
all the information is not available. This paper proposes a novel Hypergraph
Neural Network (HyGNN) model based on only the SMILES string of drugs,
available for any drug, for the DDI prediction problem. To capture the drug
similarities, we create a hypergraph from drugs' chemical substructures
extracted from the SMILES strings. Then, we develop HyGNN consisting of a novel
attention-based hypergraph edge encoder to get the representation of drugs as
hyperedges and a decoder to predict the interactions between drug pairs.
Furthermore, we conduct extensive experiments to evaluate our model and compare
it with several state-of-the-art methods. Experimental results demonstrate that
our proposed HyGNN model effectively predicts DDIs and impressively outperforms
the baselines with a maximum ROC-AUC and PR-AUC of 97.9% and 98.1%,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some new experiments have been added. One more dataset has been
  considered. Theoretical part has been updated too</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Assistance in Novel Decision Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastiaan De Peuter, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of creating assistants that can help agents - often
humans - solve novel sequential decision problems, assuming the agent is not
able to specify the reward function explicitly to the assistant. Instead of
aiming to automate, and act in place of the agent as in current approaches, we
give the assistant an advisory role and keep the agent in the loop as the main
decision maker. The difficulty is that we must account for potential biases
induced by limitations or constraints of the agent which may cause it to
seemingly irrationally reject advice. To do this we introduce a novel
formalization of assistance that models these biases, allowing the assistant to
infer and adapt to them. We then introduce a new method for planning the
assistant's advice which can scale to large decision making problems. Finally,
we show experimentally that our approach adapts to these agent biases, and
results in higher cumulative reward for the agent than automation-based
alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient and Privacy Preserving Group Signature for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneha Kanchan, Jae Won Jang, Jun Yong Yoon, Bong Jun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a Machine Learning (ML) technique that aims to
reduce the threats to user data privacy. Training is done using the raw data on
the users' device, called clients, and only the training results, called
gradients, are sent to the server to be aggregated and generate an updated
model. However, we cannot assume that the server can be trusted with private
information, such as metadata related to the owner or source of the data. So,
hiding the client information from the server helps reduce privacy-related
attacks. Therefore, the privacy of the client's identity, along with the
privacy of the client's data, is necessary to make such attacks more difficult.
This paper proposes an efficient and privacy-preserving protocol for FL based
on group signature. A new group signature for federated learning, called GSFL,
is designed to not only protect the privacy of the client's data and identity
but also significantly reduce the computation and communication costs
considering the iterative process of federated learning. We show that GSFL
outperforms existing approaches in terms of computation, communication, and
signaling costs. Also, we show that the proposed protocol can handle various
security attacks in the federated learning environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking Objects as Pixel-wise Distributions <span class="chip">ECCV22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Zhao, Ze Wu, Yueqing Zhuang, Boxun Li, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object tracking (MOT) requires detecting and associating objects
through frames. Unlike tracking via detected bounding boxes or tracking objects
as points, we propose tracking objects as pixel-wise distributions. We
instantiate this idea on a transformer-based architecture, P3AFormer, with
pixel-wise propagation, prediction, and association. P3AFormer propagates
pixel-wise features guided by flow information to pass messages between frames.
Furthermore, P3AFormer adopts a meta-architecture to produce multi-scale object
feature maps. During inference, a pixel-wise association procedure is proposed
to recover object connections through frames based on the pixel-wise
prediction. P3AFormer yields 81.2\% in terms of MOTA on the MOT17 benchmark --
the first among all transformer networks to reach 80\% MOTA in literature.
P3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV22 as an oral presentation paper. The code&project
  page is at
  https://github.com/dvlab-research/ECCV22-P3AFormer-Tracking-Objects-as-Pixel-wise-Distributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Rates for Spectral Algorithms with Least-Squares Regression over
  Hil<span class="highlight-title">bert</span> Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1801.06720v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1801.06720v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study regression problems over a separable Hilbert space
with the square loss, covering non-parametric regression over a reproducing
kernel Hilbert space. We investigate a class of spectral/regularized
algorithms, including ridge regression, principal component regression, and
gradient methods. We prove optimal, high-probability convergence results in
terms of variants of norms for the studied algorithms, considering a capacity
assumption on the hypothesis space and a general source condition on the target
function. Consequently, we obtain almost sure convergence results with optimal
rates. Our results improve and generalize previous results, filling a
theoretical gap for the non-attainable cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updating acknowledgments; Journal version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Deep Learning: Interpretation, Interpretability,
  Trustworthiness, and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.10689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.10689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, Dejing Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been well-known for their superb handling of
various machine learning and artificial intelligence tasks. However, due to
their over-parameterized black-box nature, it is often difficult to understand
the prediction results of deep models. In recent years, many interpretation
tools have been proposed to explain or reveal how deep models make decisions.
In this paper, we review this line of research and try to make a comprehensive
survey. Specifically, we first introduce and clarify two basic concepts --
interpretations and interpretability -- that people usually get confused about.
To address the research efforts in interpretations, we elaborate the designs of
a number of interpretation algorithms, from different perspectives, by
proposing a new taxonomy. Then, to understand the interpretation results, we
also survey the performance metrics for evaluating interpretation algorithms.
Further, we summarize the current works in evaluating models' interpretability
using "trustworthy" interpretation algorithms. Finally, we review and discuss
the connections between deep models' interpretations and other factors, such as
adversarial robustness and learning from interpretations, and we introduce
several open-source libraries for interpretation algorithms and evaluation
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel Conjugate Gradient Methods with Random Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.01760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.01760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhong Lin, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and study kernel conjugate gradient methods (KCGM) with random
projections for least-squares regression over a separable Hilbert space.
Considering two types of random projections generated by randomized sketches
and Nystr\"{o}m subsampling, we prove optimal statistical results with respect
to variants of norms for the algorithms under a suitable stopping rule.
Particularly, our results show that if the projection dimension is proportional
to the effective dimension of the problem, KCGM with randomized sketches can
generalize optimally, while achieving a computational advantage. As a
corollary, we derive optimal rates for classic KCGM in the well-conditioned
regimes for the case that the target function may not be in the hypothesis
space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updating acknowledgments; Accepted version for Applied and
  Computational Harmonic Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Injection: Parameterization of Fixed Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunbi Choi, Yongrae Jo, Joel Jang, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that attaching prompts to the input is effective at
conditioning Language Models (LM) to perform specific tasks. However, prompts
are always included in the input text during inference, thus incurring
substantial computational and memory overhead. Also, there is currently no
straightforward method of utilizing prompts that are longer than the maximum
input length of the LMs without incurring additional costs during inference. We
propose Prompt Injection (PI), a novel formulation of injecting the prompt into
the parameters of an LM to be an efficient alternative to attaching fixed
prompts to the input. We show that in scenarios with long fixed prompts, PI can
be up to 280 times more efficient in terms of total FLOPs than previous
approaches. We further explore methodologies for PI and show promising results
in persona-dependent conversation, semantic parsing, and zero-shot learning
with task instructions. Through these explorations, we show that PI can be a
promising direction for conditioning language models, especially in scenarios
with long and fixed prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PING results in Table 2 updated (bug fixed)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Population Mobility Data-Based Community-Level COVID-19
  Prediction Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Ling Chen, Dandan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the number of infections in the anti-epidemic process is extremely
beneficial to the government in developing anti-epidemic strategies, especially
in fine-grained geographic units. Previous works focus on low spatial
resolution prediction, e.g., county-level, and preprocess data to the same
geographic level, which loses some useful information. In this paper, we
propose a fine-grained population mobility data-based model (FGC-COVID)
utilizing data of two geographic levels for community-level COVID-19
prediction. We use the population mobility data between Census Block Groups
(CBGs), which is a finer-grained geographic level than community, to build the
graph and capture the dependencies between CBGs using graph neural networks
(GNNs). To mine as finer-grained patterns as possible for prediction, a spatial
weighted aggregation module is introduced to aggregate the embeddings of CBGs
to community level based on their geographic affiliation and spatial
autocorrelation. Extensive experiments on 300 days LA city COVID-19 data
indicate our model outperforms existing forecasting models on community-level
COVID-19 prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Cybernetics and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Sequence Feature Alignment for Domain Adaptive Detection
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.12636v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.12636v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-Jun Zha, Yonggang Wen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix a typo in Eq. 13</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Long-term Dependencies and Short-term Correlations in Patient
  Journey Data with Temporal Attention Networks for Health Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Liu, Zhenhao Zhang, Antonio Jimeno Yepes, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building models for health prediction based on Electronic Health Records
(EHR) has become an active research area. EHR patient journey data consists of
patient time-ordered clinical events/visits from patients. Most existing
studies focus on modeling long-term dependencies between visits, without
explicitly taking short-term correlations between consecutive visits into
account, where irregular time intervals, incorporated as auxiliary information,
are fed into health prediction models to capture latent progressive patterns of
patient journeys. We present a novel deep neural network with four modules to
take into account the contributions of various variables for health prediction:
i) the Stacked Attention module strengthens the deep semantics in clinical
events within each patient journey and generates visit embeddings, ii) the
Short-Term Temporal Attention module models short-term correlations between
consecutive visit embeddings while capturing the impact of time intervals
within those visit embeddings, iii) the Long-Term Temporal Attention module
models long-term dependencies between visit embeddings while capturing the
impact of time intervals within those visit embeddings, iv) and finally, the
Coupled Attention module adaptively aggregates the outputs of Short-Term
Temporal Attention and Long-Term Temporal Attention modules to make health
predictions. Experimental results on MIMIC-III demonstrate superior predictive
accuracy of our model compared to existing state-of-the-art methods, as well as
the interpretability and robustness of this approach. Furthermore, we found
that modeling short-term correlations contributes to local priors generation,
leading to improved predictive modeling of patient journeys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, accepted at ACM BCB 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic categories, dynamic operads: From deep learning to prediction
  markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Shapiro, David I. Spivak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural organized systems adapt to internal and external pressures and this
seems to happens all the way down. Wanting to think clearly about this idea
motivates our paper, and so the idea is elaborated extensively in the
introduction, which should be broadly accessible to a
philosophically-interested audience.
  In the remaining sections, we turn to more compressed category theory. We
define the monoidal double category $\mathbf{Org}$ of dynamic organizations, we
provide definitions of $\mathbf{Org}$-enriched, or "dynamic", categorical
structures -- e.g. dynamic categories, operads, and monoidal categories -- and
we show how they instantiate the motivating philosophical ideas. We give two
examples of dynamic categorical structures: prediction markets as a dynamic
operad and deep learning as a dynamic monoidal category.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages + two appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Sparse Fixed-Structure Gaussian Bayesian Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.10450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.10450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Bhattacharyya, Davin Choo, Rishikesh Gajjala, Sutanu Gayen, Yuhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation
models) are widely used to model causal interactions among continuous
variables. In this work, we study the problem of learning a fixed-structure
Gaussian Bayesian network up to a bounded error in total variation distance. We
analyze the commonly used node-wise least squares regression (LeastSquares) and
prove that it has a near-optimal sample complexity. We also study a couple of
new algorithms for the problem:
  - BatchAvgLeastSquares takes the average of several batches of least squares
solutions at each node, so that one can interpolate between the batch size and
the number of batches. We show that BatchAvgLeastSquares also has near-optimal
sample complexity.
  - CauchyEst takes the median of solutions to several batches of linear
systems at each node. We show that the algorithm specialized to polytrees,
CauchyEstTree, has near-optimal sample complexity.
  Experimentally, we show that for uncontaminated, realizable data, the
LeastSquares algorithm performs best, but in the presence of contamination or
DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares
respectively perform better.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal No-regret Learning in Repeated First-price Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.09795v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.09795v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Han, Zhengyuan Zhou, Tsachy Weissman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study online learning in repeated first-price auctions with censored
feedback, where a bidder, only observing the winning bid at the end of each
auction, learns to adaptively bid in order to maximize her cumulative payoff.
To achieve this goal, the bidder faces a challenging dilemma: if she wins the
bid--the only way to achieve positive payoffs--then she is not able to observe
the highest bid of the other bidders, which we assume is iid drawn from an
unknown distribution. This dilemma, despite being reminiscent of the
exploration-exploitation trade-off in contextual bandits, cannot directly be
addressed by the existing UCB or Thompson sampling algorithms.
  In this paper, by exploiting the structural properties of first-price
auctions, we develop the first learning algorithm that achieves
$O(\sqrt{T}\log^{2.5} T)$ regret bound, which is minimax optimal up to $\log$
factors, when the bidder's private values are stochastically generated. We do
so by providing an algorithm on a general class of problems, called the
partially ordered contextual bandits, which combine the graph feedback across
actions, the cross learning across contexts, and a partial order over the
contexts. We establish both strengths and weaknesses of this framework, by
showing a curious separation that a regret nearly independent of the
action/context sizes is possible under stochastic contexts, but is impossible
under adversarial contexts. Despite the limitation of this general framework,
we further exploit the structure of first-price auctions and develop a learning
algorithm that operates sample-efficiently (and computationally efficiently) in
the presence of adversarially generated private values. We establish an
$O(\sqrt{T}\log^3 T)$ regret bound for this algorithm, hence providing a
complete characterization of optimal learning guarantees for first-price
auctions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version (v5) has significantly revised the previous version by
  adding a new Section 5, and multiple new results to Section 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Separate Voices by Spatial Regions <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongweiyang Xu, Romit Roy Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of audio voice separation for binaural applications,
such as earphones and hearing aids. While today's neural networks perform
remarkably well (separating $4+$ sources with 2 microphones) they assume a
known or fixed maximum number of sources, K. Moreover, today's models are
trained in a supervised manner, using training data synthesized from generic
sources, environments, and human head shapes.
  This paper intends to relax both these constraints at the expense of a slight
alteration in the problem definition. We observe that, when a received mixture
contains too many sources, it is still helpful to separate them by region,
i.e., isolating signal mixtures from each conical sector around the user's
head. This requires learning the fine-grained spatial properties of each
region, including the signal distortions imposed by a person's head. We propose
a two-stage self-supervised framework in which overheard voices from earphones
are pre-processed to extract relatively clean personalized signals, which are
then used to train a region-wise separation model. Results show promising
performance, underscoring the importance of personalization over a generic
supervised approach. (audio samples available at our project website:
https://uiuc-earable-computing.github.io/binaural/. We believe this result
could help real-world applications in selective hearing, noise cancellation,
and audio augmented reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022. For associated audio samples, see
  https://uiuc-earable-computing.github.io/binaural</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Classification Confidence Using Kernel Densities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Salamon, David Salamon, V. Adrian Cantu, Michelle An, Tyler Perry, Robert A. Edwards, Anca M. Segall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the post-hoc calibration of confidence for
"exploratory" machine learning classification problems. The difficulty in these
problems stems from the continuing desire to push the boundaries of which
categories have enough examples to generalize from when curating datasets, and
confusion regarding the validity of those categories. We argue that for such
problems the "one-versus-all" approach (top-label calibration) must be used
rather than the "calibrate-the-full-response-matrix" approach advocated
elsewhere in the literature. We introduce and test four new algorithms designed
to handle the idiosyncrasies of category-specific confidence estimation. Chief
among these methods is the use of kernel density ratios for confidence
calibration including a novel, bulletproof algorithm for choosing the
bandwidth. We test our claims and explore the limits of calibration on a
bioinformatics application (PhANNs) as well as the classic MNIST benchmark.
Finally, our analysis argues that post-hoc calibration should always be
performed, should be based only on the test dataset, and should be
sanity-checked visually.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Breaking Feedback Loops in Recommender Systems with Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Krauth, Yixin Wang, <span class="highlight-author">Michael I. Jordan</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a key role in shaping modern web ecosystems. These
systems alternate between (1) making recommendations (2) collecting user
responses to these recommendations, and (3) retraining the recommendation
algorithm based on this feedback. During this process the recommender system
influences the user behavioral data that is subsequently used to update it,
thus creating a feedback loop. Recent work has shown that feedback loops may
compromise recommendation quality and homogenize user behavior, raising ethical
and performance concerns when deploying recommender systems. To address these
issues, we propose the Causal Adjustment for Feedback Loops (CAFL), an
algorithm that provably breaks feedback loops using causal inference and can be
applied to any recommendation algorithm that optimizes a training loss. Our
main observation is that a recommender system does not suffer from feedback
loops if it reasons about causal quantities, namely the intervention
distributions of recommendations on user ratings. Moreover, we can calculate
this intervention distribution from observational data by adjusting for the
recommender system's predictions of user preferences. Using simulated
environments, we demonstrate that CAFL improves recommendation quality when
compared to prior correction methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Budget Throttling in Repeated Second-Price Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohua Chen, Chang Wang, Qian Wang, Yuqi Pan, Zhuming Shi, Chuyue Tang, Zheng Cai, Yukun Ren, Zhihua Zhu, Xiaotie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throttling is one of the most popular budget control methods in today's
online advertising markets. When a budget-constrained advertiser employs
throttling, she can choose whether or not to participate in an auction after
the advertising platform recommends a bid. This paper focuses on the dynamic
budget throttling process in repeated second-price auctions from a theoretical
view. An essential feature of the underlying problem is that the advertiser
does not know the distribution of the highest competing bid upon entering the
market. To model the difficulty of eliminating such uncertainty, we consider
two different information structures. The advertiser could obtain the highest
competing bid in each round with full-information feedback. Meanwhile, with
partial information feedback, the advertiser could only have access to the
highest competing bid in the auctions she participates in. We propose the
OGD-CB algorithm, which involves simultaneous distribution learning and revenue
optimization. In both settings, we demonstrate that this algorithm guarantees
an $O(\sqrt{T\log T})$ regret with probability $1 - O(1/T)$ relative to the
fluid adaptive throttling benchmark. By proving a lower bound of
$\Omega(\sqrt{T})$ on the minimal regret for even the hindsight optimum, we
establish the near optimality of our algorithm. Finally, we compare the fluid
optimum of throttling to that of pacing, another widely adopted budget control
method. The numerical relationship of these benchmarks sheds new light on the
understanding of different online algorithms for revenue maximization under
budget constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An original model for multi-target learning of logical rules for
  knowledge graph reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Wei, Haotian Li, Guodong Xin, Yao Wang, Bailing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale knowledge graphs provide structured representations of human
knowledge. However, as it is impossible to collect all knowledge, knowledge
graphs are usually incomplete. Reasoning based on existing facts paves a way to
discover missing facts. In this paper, we study the problem of learning logical
rules for reasoning on knowledge graphs for completing missing factual
triplets. Learning logical rules equips a model with strong interpretability as
well as the ability to generalize to similar tasks. We propose a model able to
fully use training data which also considers multi-target scenarios. In
addition, considering the deficiency in evaluating the performance of models
and the quality of mined rules, we further propose two novel indicators to help
with the problem. Experimental results empirically demonstrate that our model
outperforms state-of-the-art methods on five benchmark datasets. The results
also prove the effectiveness of the indicators.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic <span class="highlight-title">Review</span> and Replicability Study of <span class="highlight-title">BERT</span>4Rec for Sequential
  Recommendation <span class="chip">RecSys
  '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Petrov, Craig Macdonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  BERT4Rec is an effective model for sequential recommendation based on the
Transformer architecture. In the original publication, BERT4Rec claimed
superiority over other available sequential recommendation approaches (e.g.
SASRec), and it is now frequently being used as a state-of-the art baseline for
sequential recommendations. However, not all subsequent publications confirmed
this result and proposed other models that were shown to outperform BERT4Rec in
effectiveness. In this paper we systematically review all publications that
compare BERT4Rec with another popular Transformer-based model, namely SASRec,
and show that BERT4Rec results are not consistent within these publications. To
understand the reasons behind this inconsistency, we analyse the available
implementations of BERT4Rec and show that we fail to reproduce results of the
original BERT4Rec publication when using their default configuration
parameters. However, we are able to replicate the reported results with the
original code if training for a much longer amount of time (up to 30x) compared
to the default configuration. We also propose our own implementation of
BERT4Rec based on the Hugging Face Transformers library, which we demonstrate
replicates the originally reported results on 3 out 4 datasets, while requiring
up to 95% less training time to converge. Overall, from our systematic review
and detailed experiments, we conclude that BERT4Rec does indeed exhibit
state-of-the-art effectiveness for sequential recommendation, but only when
trained for a sufficient amount of time. Additionally, we show that our
implementation can further benefit from adapting other Transformer
architectures that are available in the Hugging Face Transformers library (e.g.
using disentangled attention, as provided by DeBERTa, or larger hidden layer
size cf. ALBERT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the Reproducibility track of the ACM RecSys
  '22 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Multi-interest News Sequence for News Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongyao Wang, Wenpeng Lu, Xueping Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A session-based news recommender system recommends the next news to a user by
modeling the potential interests embedded in a sequence of news read/clicked by
her/him in a session. Generally, a user's interests are diverse, namely there
are multiple interests corresponding to different types of news, e.g., news of
distinct topics, within a session. %Modeling such multiple interests is
critical for precise news recommendation. However, most of existing methods
typically overlook such important characteristic and thus fail to distinguish
and model the potential multiple interests of a user, impeding accurate
recommendation of the next piece of news. Therefore, this paper proposes
multi-interest news sequence (MINS) model for news recommendation. In MINS, a
news encoder based on self-attention is devised on learn an informative
embedding for each piece of news, and then a novel parallel interest network is
devised to extract the potential multiple interests embedded in the news
sequence in preparation for the subsequent next-news recommendations. The
experimental results on a real-world dataset demonstrate that our model can
achieve better performance than the state-of-the-art compared models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparison of Source Distribution and Result Overlap in Web Search
  Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurce Yagci, Sebastian Sünkler, Helena Häußler, Dirk Lewandowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When it comes to search engines, users generally prefer Google. Our study
aims to find the differences between the results found in Google compared to
other search engines. We compared the top 10 results from Google, Bing,
DuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from
Germany and the US. Google displays more unique domains in the top results than
its competitors. Wikipedia and news websites are the most popular sources
overall. With some top sources dominating search results, the distribution of
domains is also consistent across all search engines. The overlap between
Google and Bing is always under 32%, while Metager has a higher overlap with
Bing than DuckDuckGo, going up to 78%. This study shows that the use of another
search engine, especially in addition to Google, provides a wider variety in
sources and might lead the user to find new perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 85th Annual Meeting of the Association for
  Information Science & Technology and will be published in the conference
  proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomalous behaviour in loss-gradient based interpretability methods <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinod Subramanian, Siddharth Gururani, Emmanouil Benetos, Mark Sandler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loss-gradients are used to interpret the decision making process of deep
learning models. In this work, we evaluate loss-gradient based attribution
methods by occluding parts of the input and comparing the performance of the
occluded input to the original input. We observe that the occluded input has
better performance than the original across the test dataset under certain
conditions. Similar behaviour is observed in sound and image recognition tasks.
We explore different loss-gradient attribution methods, occlusion levels and
replacement values to explain the phenomenon of performance improvement under
occlusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR RobustML workshop 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Semantic Grounding in Language Models of Code with
  Representational Similarity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shounak Naik, Rajaswa Patil, Swati Agarwal, Veeky Baths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representational Similarity Analysis is a method from cognitive neuroscience,
which helps in comparing representations from two different sources of data. In
this paper, we propose using Representational Similarity Analysis to probe the
semantic grounding in language models of code. We probe representations from
the CodeBERT model for semantic grounding by using the data from the IBM
CodeNet dataset. Through our experiments, we show that current pre-training
methods do not induce semantic grounding in language models of code, and
instead focus on optimizing form-based patterns. We also show that even a
little amount of fine-tuning on semantically relevant tasks increases the
semantic grounding in CodeBERT significantly. Our ablations with the input
modality to the CodeBERT model show that using bimodal inputs (code and natural
language) over unimodal inputs (only code) gives better semantic grounding and
sample efficiency during semantic fine-tuning. Finally, our experiments with
semantic perturbations in code reveal that CodeBERT is able to robustly
distinguish between semantically correct and incorrect code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ADMA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective and Efficient Training for Sequential Recommendation using
  Recency Sampling <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Petrov, Craig Macdonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern sequential recommender systems use deep neural networks, which
can effectively estimate the relevance of items but require a lot of time to
train. Slow training increases expenses, hinders product development timescales
and prevents the model from being regularly updated to adapt to changing user
preferences. Training such sequential models involves appropriately sampling
past user interactions to create a realistic training objective. The existing
training objectives have limitations. For instance, next item prediction never
uses the beginning of the sequence as a learning target, thereby potentially
discarding valuable data. On the other hand, the item masking used by BERT4Rec
is only weakly related to the goal of the sequential recommendation; therefore,
it requires much more time to obtain an effective model. Hence, we propose a
novel Recency-based Sampling of Sequences training objective that addresses
both limitations. We apply our method to various recent and state-of-the-art
model architectures - such as GRU4Rec, Caser, and SASRec. We show that the
models enhanced with our method can achieve performances exceeding or very
close to stateof-the-art BERT4Rec, but with much less training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This full research paper is accepted at 16th ACM Conference on
  Recommender Systems (ACM RecSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-Class Representation of Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichao Zhang, Jiaye Li, Wenzhen Zhang, Yongsong Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data representation is usually a natural form with their attribute values. On
this basis, data processing is an attribute-centered calculation. However,
there are three limitations in the attribute-centered calculation, saying,
inflexible calculation, preference computation, and unsatisfactory output. To
attempt the issues, a new data representation, named as hyper-classes
representation, is proposed for improving recommendation. First, the cross
entropy, KL divergence and JS divergence of features in data are defined. And
then, the hyper-classes in data can be discovered with these three parameters.
Finally, a kind of recommendation algorithm is used to evaluate the proposed
hyper-class representation of data, and shows that the hyper-class
representation is able to provide truly useful reference information for
recommendation systems and makes recommendations much better than existing
algorithms, i.e., this approach is efficient and promising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention over Self-attention:Intention-aware Re-ranking with Dynamic
  <span class="highlight-title">Transformer</span> Encoders for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Sheng Zang, Rundong Wang, Zhu Sun, J. Senthilnath, Chi Xu, Chee-Keong Kwoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Re-ranking models refine item recommendation lists generated by the prior
global ranking model, which have demonstrated their effectiveness in improving
the recommendation quality. However, most existing re-ranking solutions only
learn from implicit feedback with a shared prediction model, which regrettably
ignore inter-item relationships under diverse user intentions. In this paper,
we propose a novel Intention-aware Re-ranking Model with Dynamic Transformer
Encoder (RAISE), aiming to perform user-specific prediction for each individual
user based on her intentions. Specifically, we first propose to mine latent
user intentions from text reviews with an intention discovering module (IDM).
By differentiating the importance of review information with a co-attention
network, the latent user intention can be explicitly modeled for each user-item
pair. We then introduce a dynamic transformer encoder (DTE) to capture
user-specific inter-item relationships among item candidates by seamlessly
accommodating the learned latent user intentions via IDM. As such, one can not
only achieve more personalized recommendations but also obtain corresponding
explanations by constructing RAISE upon existing recommendation engines.
Empirical study on four public datasets shows the superiority of our proposed
RAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by
Precision@5, MAP@5, and NDCG@5 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Breaking Feedback Loops in Recommender Systems with Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Krauth, Yixin Wang, <span class="highlight-author">Michael I. Jordan</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a key role in shaping modern web ecosystems. These
systems alternate between (1) making recommendations (2) collecting user
responses to these recommendations, and (3) retraining the recommendation
algorithm based on this feedback. During this process the recommender system
influences the user behavioral data that is subsequently used to update it,
thus creating a feedback loop. Recent work has shown that feedback loops may
compromise recommendation quality and homogenize user behavior, raising ethical
and performance concerns when deploying recommender systems. To address these
issues, we propose the Causal Adjustment for Feedback Loops (CAFL), an
algorithm that provably breaks feedback loops using causal inference and can be
applied to any recommendation algorithm that optimizes a training loss. Our
main observation is that a recommender system does not suffer from feedback
loops if it reasons about causal quantities, namely the intervention
distributions of recommendations on user ratings. Moreover, we can calculate
this intervention distribution from observational data by adjusting for the
recommender system's predictions of user preferences. Using simulated
environments, we demonstrate that CAFL improves recommendation quality when
compared to prior correction methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual
  Reality with Redirected Walking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Lemic, Jakob Struye, Jeroen Famaey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-immersive multiuser Virtual Reality (VR) envisions supporting
unconstrained mobility of the users in the virtual worlds, while at the same
time constraining their physical movements inside VR setups through redirected
walking. For enabling delivery of high data rate video content in real-time,
the supporting wireless networks will leverage highly directional communication
links that will "track" the users for maintaining the Line-of-Sight (LoS)
connectivity. Recurrent Neural Networks (RNNs) and in particular Long
Short-Term Memory (LSTM) networks have historically presented themselves as a
suitable candidate for near-term movement trajectory prediction for natural
human mobility, and have also recently been shown as applicable in predicting
VR users' mobility under the constraints of redirected walking. In this work,
we extend these initial findings by showing that Gated Recurrent Unit (GRU)
networks, another candidate from the RNN family, generally outperform the
traditionally utilized LSTMs. Second, we show that context from a virtual world
can enhance the accuracy of the prediction if used as an additional input
feature in comparison to the more traditional utilization of solely the
historical physical movements of the VR users. Finally, we show that the
prediction system trained on a static number of coexisting VR users be scaled
to a multi-user system without significant accuracy degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud
  Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Pengyuan Zhou, Zhi Liu, Bo Han, Hui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud video transmission is challenging due to high encoding/decoding
complexity, high video bitrate, and low latency requirement. Consequently,
conventional adaptive streaming methodologies often find themselves
unsatisfactory to meet the requirements in threefold: 1) current algorithms
reuse existing quality of experience (QoE) definitions while overlooking the
unique features of point cloud video thus failing to provide optimal user
experience, 2) most deep learning approaches require long-span data collections
to learn sufficiently varied network conditions and result in long training
period and capacity occupation, 3) cloud training approaches pose privacy risks
caused by leakage of user reported service usage and networking conditions.
  To overcome the limitations, we present FRAS, the first federated
reinforcement learning framework, to the best of our knowledge, for adaptive
point cloud video streaming. We define a new QoE model which takes the unique
features of point cloud video into account. Each client uses reinforcement
learning (RL) to train encoding rate selection with the objective of optimizing
the user's QoE under multiple constraints. Then, a federated learning framework
is integrated with the RL algorithm to enhance training performance with
privacy preservation. Extensive simulations using real point cloud videos and
network traces reveal the superiority of the proposed scheme over baseline
schemes. We also implement a prototype that demonstrates the performance of
FRAS via real-world tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChoreoGraph: Music-conditioned Automatic Dance Choreography over a Style
  and Tempo Consistent Dynamic Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Yin Au, Jie Chen, Junkun Jiang, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To generate dance that temporally and aesthetically matches the music is a
challenging problem, as the following factors need to be considered. First, the
aesthetic styles and messages conveyed by the motion and music should be
consistent. Second, the beats of the generated motion should be locally aligned
to the musical features. And finally, basic choreomusical rules should be
observed, and the motion generated should be diverse. To address these
challenges, we propose ChoreoGraph, which choreographs high-quality dance
motion for a given piece of music over a Dynamic Graph. A data-driven learning
strategy is proposed to evaluate the aesthetic style and rhythmic connections
between music and motion in a progressively learned cross-modality embedding
space. The motion sequences will be beats-aligned based on the music segments
and then incorporated as nodes of a Dynamic Motion Graph. Compatibility factors
such as the style and tempo consistency, motion context connection, action
completeness, and transition smoothness are comprehensively evaluated to
determine the node transition in the graph. We demonstrate that our
repertoire-based framework can generate motions with aesthetic consistency and
robustly extensible in diversity. Both quantitative and qualitative experiment
results show that our proposed model outperforms other baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting <span class="highlight-title">Multi-Modal</span> E-commerce Attribute Value Extraction via Unified
  Learning Scheme and Dynamic Range Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyin Liu, Chao Zhu, Hongyu Gao, Weibo Gu, Hongfa Wang, Wei Liu, Xu-cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prosperity of e-commerce industry, various modalities, e.g., vision
and language, are utilized to describe product items. It is an enormous
challenge to understand such diversified data, especially via extracting the
attribute-value pairs in text sequences with the aid of helpful image regions.
Although a series of previous works have been dedicated to this task, there
remain seldomly investigated obstacles that hinder further improvements: 1)
Parameters from up-stream single-modal pretraining are inadequately applied,
without proper jointly fine-tuning in a down-stream multi-modal task. 2) To
select descriptive parts of images, a simple late fusion is widely applied,
regardless of priori knowledge that language-related information should be
encoded into a common linguistic embedding space by stronger encoders. 3) Due
to diversity across products, their attribute sets tend to vary greatly, but
current approaches predict with an unnecessary maximal range and lead to more
potential false positives. To address these issues, we propose in this paper a
novel approach to boost multi-modal e-commerce attribute value extraction via
unified learning scheme and dynamic range minimization: 1) Firstly, a unified
scheme is designed to jointly train a multi-modal task with pretrained
single-modal parameters. 2) Secondly, a text-guided information range
minimization method is proposed to adaptively encode descriptive parts of each
modality into an identical space with a powerful pretrained linguistic model.
3) Moreover, a prototype-guided attribute range minimization method is proposed
to first determine the proper attribute set of the current product, and then
select prototypes to guide the prediction of the chosen attributes. Experiments
on the popular multi-modal e-commerce benchmarks show that our approach
achieves superior performance over the other state-of-the-art techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality Assessment of Image Super-Resolution: Balancing Deterministic
  and Statistical Fidelity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a growing interest in developing image super-resolution (SR)
algorithms that convert low-resolution (LR) to higher resolution images, but
automatically evaluating the visual quality of super-resolved images remains a
challenging problem. Here we look at the problem of SR image quality assessment
(SR IQA) in a two-dimensional (2D) space of deterministic fidelity (DF) versus
statistical fidelity (SF). This allows us to better understand the advantages
and disadvantages of existing SR algorithms, which produce images at different
clusters in the 2D space of (DF, SF). Specifically, we observe an interesting
trend from more traditional SR algorithms that are typically inclined to
optimize for DF while losing SF, to more recent generative adversarial network
(GAN) based approaches that by contrast exhibit strong advantages in achieving
high SF but sometimes appear weak at maintaining DF. Furthermore, we propose an
uncertainty weighting scheme based on content-dependent sharpness and texture
assessment that merges the two fidelity measures into an overall quality
prediction named the Super Resolution Image Fidelity (SRIF) index, which
demonstrates superior performance against state-of-the-art IQA models when
tested on subject-rated datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM2022 https://github.com/weizhou-geek/SRIF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EKTVQA: Generalized use of External Knowledge to empower Scene Text in
  Text-VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.09717v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.09717v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arka Ujjal Dey, Ernest Valveny, Gaurav Harit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The open-ended question answering task of Text-VQA often requires reading and
reasoning about rarely seen or completely unseen scene-text content of an
image. We address this zero-shot nature of the problem by proposing the
generalized use of external knowledge to augment our understanding of the scene
text. We design a framework to extract, validate, and reason with knowledge
using a standard multimodal transformer for vision language understanding
tasks. Through empirical evidence and qualitative results, we demonstrate how
external knowledge can highlight instance-only cues and thus help deal with
training data bias, improve answer entity type correctness, and detect
multiword named entities. We generate results comparable to the
state-of-the-art on three publicly available datasets, under the constraints of
similar upstream OCR systems and training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Information-Theoretic Bounds for Steganography in Multimedia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Y. El Arsh, Amr Abdelaziz, Ahmed Elliethy, Hussein A. Aly, T. Aaron Gulliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganography in multimedia aims to embed secret data into an innocent
looking multimedia cover object. This embedding introduces some distortion to
the cover object and produces a corresponding stego object. The embedding
distortion is measured by a cost function that determines the detection
probability of the existence of the embedded secret data. A cost function
related to the maximum embedding rate is typically employed to evaluate a
steganographic system. In addition, the distribution of multimedia sources
follows the Gibbs distribution which is a complex statistical model that
restricts analysis. Thus, previous multimedia steganographic approaches either
assume a relaxed distribution or presume a proposition on the maximum embedding
rate and then try to prove it is correct. Conversely, this paper introduces an
analytic approach to determining the maximum embedding rate in multimedia cover
objects through a constrained optimization problem concerning the relationship
between the maximum embedding rate and the probability of detection by any
steganographic detector. The KL-divergence between the distributions for the
cover and stego objects is used as the cost function as it upper bounds the
performance of the optimal steganographic detector. An equivalence between the
Gibbs and correlated-multivariate-quantized-Gaussian distributions is
established to solve this optimization problem. The solution provides an
analytic form for the maximum embedding rate in terms of the WrightOmega
function. Moreover, it is proven that the maximum embedding rate is in
agreement with the commonly used Square Root Law (SRL) for steganography, but
the solution presented here is more accurate. Finally, the theoretical results
obtained are verified experimentally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2111.04960</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Task Sampling for Few-shot <span class="highlight-title">Vision-Language</span> Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Hang Yu, Manling Li, Han Zhao, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite achieving state-of-the-art zero-shot performance, existing
vision-language models still fall short of few-shot transfer ability on
domain-specific problems. Classical fine-tuning often fails to prevent highly
expressive models from exploiting spurious correlations. Although
model-agnostic meta-learning (MAML) presents as a natural alternative for
few-shot transfer learning, the expensive computation due to implicit
second-order optimization limits its use on large-scale vision-language models
such as CLIP. While much literature has been devoted to exploring alternative
optimization strategies, we identify another essential aspect towards effective
few-shot transfer learning, task sampling, which is previously only be viewed
as part of data pre-processing in MAML. To show the impact of task sampling, we
propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which
differentiates classical fine-tuning only on uniformly sampling multiple tasks.
Despite its simplicity, we show that MAMF consistently outperforms classical
fine-tuning on five few-shot vision-language classification tasks. We further
show that the effectiveness of the bi-level optimization in MAML is highly
sensitive to the zero-shot performance of a task in the context of few-shot
vision-language classification. The goal of this paper is to provide new
insights on what makes few-shot learning work, and encourage more research into
investigating better task sampling strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Separate Voices by Spatial Regions <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongweiyang Xu, Romit Roy Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of audio voice separation for binaural applications,
such as earphones and hearing aids. While today's neural networks perform
remarkably well (separating $4+$ sources with 2 microphones) they assume a
known or fixed maximum number of sources, K. Moreover, today's models are
trained in a supervised manner, using training data synthesized from generic
sources, environments, and human head shapes.
  This paper intends to relax both these constraints at the expense of a slight
alteration in the problem definition. We observe that, when a received mixture
contains too many sources, it is still helpful to separate them by region,
i.e., isolating signal mixtures from each conical sector around the user's
head. This requires learning the fine-grained spatial properties of each
region, including the signal distortions imposed by a person's head. We propose
a two-stage self-supervised framework in which overheard voices from earphones
are pre-processed to extract relatively clean personalized signals, which are
then used to train a region-wise separation model. Results show promising
performance, underscoring the importance of personalization over a generic
supervised approach. (audio samples available at our project website:
https://uiuc-earable-computing.github.io/binaural/. We believe this result
could help real-world applications in selective hearing, noise cancellation,
and audio augmented reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022. For associated audio samples, see
  https://uiuc-earable-computing.github.io/binaural</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-07-14T00:00:00Z">2022-07-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient <span class="highlight-title">Prompt</span> Tuning Makes Generalized and Calibrated
  Neural Text Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning attempts to update few task-specific parameters in pre-trained
models. It has achieved comparable performance to fine-tuning of the full
parameter set on both language understanding and generation tasks. In this
work, we study the problem of prompt tuning for neural text retrievers. We
introduce parameter-efficient prompt tuning for text retrieval across
in-domain, cross-domain, and cross-topic settings. Through an extensive
analysis, we show that the strategy can mitigate the two issues --
parameter-inefficiency and weak generalizability -- faced by fine-tuning based
retrieval methods. Notably, it can significantly improve the out-of-domain
zero-shot generalization of the retrieval models. By updating only 0.1% of the
model parameters, the prompt tuning strategy can help retrieval models achieve
better generalization performance than traditional methods in which all
parameters are updated. Finally, to facilitate research on retrievers'
cross-topic generalizability, we curate and release an academic retrieval
dataset with 18K query-results pairs in 87 topics, making it the largest
topic-specific one to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confident Adaptive Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, Donald Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language models show human-like content effects on reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract reasoning is a key ability for an intelligent system. Large language
models achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect, and depends on our knowledge and beliefs about the content of the
reasoning problem. For example, humans reason much more reliably about logical
rules that are grounded in everyday situations than arbitrary rules about
abstract attributes. The training experiences of language models similarly
endow them with prior expectations that reflect human knowledge and beliefs. We
therefore hypothesized that language models would show human-like content
effects on abstract reasoning problems. We explored this hypothesis across
three logical reasoning tasks: natural language inference, judging the logical
validity of syllogisms, and the Wason selection task (Wason, 1968). We find
that state of the art large language models (with 7 or 70 billion parameters;
Hoffman et al., 2022) reflect many of the same patterns observed in humans
across these tasks -- like humans, models reason more effectively about
believable situations than unrealistic or abstract ones. Our findings have
implications for understanding both these cognitive effects, and the factors
that contribute to language model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Single <span class="highlight-title">Self-Supervised</span> Model for Many Speech Modalities Enables
  Zero-Shot Modality Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Ning Hsu, Bowen Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While audio-visual speech models can yield superior performance and
robustness compared to audio-only models, their development and adoption are
hindered by the lack of labeled and unlabeled audio-visual data and the cost to
deploy one model per modality. In this paper, we present u-HuBERT, a
self-supervised pre-training framework that can leverage both multimodal and
unimodal speech with a unified masked cluster prediction objective. By
utilizing modality dropout during pre-training, we demonstrate that a single
fine-tuned model can achieve performance on par or better than the
state-of-the-art modality-specific models. Moreover, our model fine-tuned only
on audio can perform well with audio-visual and visual speech input, achieving
zero-shot modality generalization for speech recognition and speaker
verification. In particular, our single model yields 1.2%/1.4%/27.2% speech
recognition word error rate on LRS3 with audio-visual/audio/visual input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to translate by learning to communicate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. M. Downey, Leo Z. Liu, Xuhui Zhou, Shane Steinert-Threlkeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formulate and test a technique to use Emergent Communication (EC) with a
pretrained multilingual model to improve on modern Unsupervised NMT systems,
especially for low-resource languages. It has been argued that the currently
dominant paradigm in NLP of pretraining on text-only corpora will not yield
robust natural language understanding systems, and the need for grounded,
goal-oriented, and interactive language learning has been highlighted. In our
approach, we embed a modern multilingual model (mBART, Liu et. al. 2020) into
an EC image-reference game, in which the model is incentivized to use
multilingual generations to accomplish a vision-grounded task, with the
hypothesis that this will align multiple languages to a shared task space. We
present two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022), one
of which outperforms a backtranslation-based baseline in 6/8 translation
settings, and proves especially beneficial for the very low-resource languages
of Nepali and Sinhala.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Modelling with Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches, instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT,
further confirming the benefits of modelling language with pixels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene Text Recognition with Permuted Autoregressive Sequence Models <span class="chip">ECCV
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darwin Bautista, Rowel Atienza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-aware STR methods typically use internal autoregressive (AR) language
models (LM). Inherent limitations of AR models motivated two-stage methods
which employ an external LM. The conditional independence of the external LM on
the input image may cause it to erroneously rectify correct predictions,
leading to significant inefficiencies. Our method, PARSeq, learns an ensemble
of internal AR LMs with shared weights using Permutation Language Modeling. It
unifies context-free non-AR and context-aware AR inference, and iterative
refinement using bidirectional context. Using synthetic training data, PARSeq
achieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and
more challenging datasets. It establishes new SOTA results (96.0% accuracy)
when trained on real data. PARSeq is optimal on accuracy vs parameter count,
FLOPS, and latency because of its simple, unified structure and parallel token
processing. Due to its extensive use of attention, it is robust on
arbitrarily-oriented text which is common in real-world images. Code,
pretrained weights, and data are available at: https://github.com/baudm/parseq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 17th European Conference on Computer Vision (ECCV
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forming Trees with Treeformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilay Patel, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular models such as Transformers and LSTMs use tokens as its unit of
information. That is, each token is encoded into a vector representation, and
those vectors are used directly in a computation. However, humans frequently
consider spans of tokens (i.e., phrases) instead of their constituent tokens.
In this paper we introduce Treeformer, an architecture inspired by the CKY
algorithm and Transformer which learns a composition operator and pooling
function in order to construct hierarchical encodings for phrases and
sentences. Our extensive experiments demonstrate the benefits of incorporating
a hierarchical structure into the Transformer, and show significant
improvements compared to a baseline Transformer in machine translation,
abstractive summarization, and various natural language understanding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beware the Rationalization Trap! When Language Model Explainability
  Diverges from our Mental Models of Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rita Sevastjanova, Mennatallah El-Assady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models learn and represent language differently than humans; they
learn the form and not the meaning. Thus, to assess the success of language
model explainability, we need to consider the impact of its divergence from a
user's mental model of language. In this position paper, we argue that in order
to avoid harmful rationalization and achieve truthful understanding of language
models, explanation processes must satisfy three main conditions: (1)
explanations have to truthfully represent the model behavior, i.e., have a high
fidelity; (2) explanations must be complete, as missing information distorts
the truth; and (3) explanations have to take the user's mental model into
account, progressively verifying a person's knowledge and adapting their
understanding. We introduce a decision tree model to showcase potential reasons
why current explanations fail to reach their objectives. We further emphasize
the need for human-centered design to explain the model from multiple
perspectives, progressively adapting explanations to changing user
expectations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically
  Ambiguous Settings for Low Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Pandey, Swayatta Daw, Narendra Babu Unnam, Vikram Pudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We leverage pre-trained language models to solve the task of complex NER for
two low-resource languages: Chinese and Spanish. We use the technique of Whole
Word Masking(WWM) to boost the performance of masked language modeling
objective on large and unsupervised corpora. We experiment with multiple neural
network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on
top of a fine-tuned BERT layer. All our models outperform the baseline by a
significant margin and our best performing model obtains a competitive position
on the evaluation leaderboard for the blind test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent Memory <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models show their effectiveness across multiple domains and
tasks. The self-attention allows to combine information from all sequence
elements into context-aware representations. However, global and local
information has to be stored mostly in the same element-wise representations.
Moreover, the length of an input sequence is limited by quadratic computational
complexity of self-attention.
  In this work, we propose and study a memory-augmented segment-level recurrent
Transformer (Recurrent Memory Transformer). Memory allows to store and process
local and global information as well as to pass information between segments of
the long sequence with the help of recurrence. We implement a memory mechanism
with no changes to Transformer model by adding special memory tokens to the
input or output sequence. Then Transformer is trained to control both memory
operations and sequence representations processing.
  Results of experiments show that our model performs on par with the
Transformer-XL on language modeling for smaller memory sizes and outperforms it
for tasks that require longer sequence processing. We show that adding memory
tokens to Tr-XL is able to improve it performance. This makes Recurrent Memory
Transformer a promising architecture for applications that require learning of
long-term dependencies and general purpose in memory processing, such as
algorithmic tasks and reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation for Low-Resource Quechua ASR Improvement <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodolfo Zevallos, Nuria Bel, Guillermo Cámbara, Mireia Farrús, Jordi Luque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) is a key element in new services that
helps users to interact with an automated system. Deep learning methods have
made it possible to deploy systems with word error rates below 5% for ASR of
English. However, the use of these methods is only available for languages with
hundreds or thousands of hours of audio and their corresponding transcriptions.
For the so-called low-resource languages to speed up the availability of
resources that can improve the performance of their ASR systems, methods of
creating new resources on the basis of existing ones are being investigated. In
this paper we describe our data augmentation approach to improve the results of
ASR models for low-resource and agglutinative languages. We carry out
experiments developing an ASR for Quechua using the wav2letter++ model. We
reduced WER by 8.73% through our approach to the base model. The resulting ASR
model obtained 22.75% WER and was trained with 99 hours of original resources
and 99 hours of synthetic data obtained with a combination of text augmentation
and synthetic speech generati
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2022. arXiv admin note: substantial text
  overlap with arXiv:2204.00291</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic
  Knowledge <span class="highlight-title">Distillation</span> of <span class="highlight-title">Self-Supervised</span> Speech Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takanori Ashihara, Takafumi Moriya, Kohei Matsuura, Tomohiro Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is seen as a very promising approach with high
performance for several speech downstream tasks. Since the parameters of SSL
models are generally so large that training and inference require a lot of
memory and computational cost, it is desirable to produce compact SSL models
without a significant performance degradation by applying compression methods
such as knowledge distillation (KD). Although the KD approach is able to shrink
the depth and/or width of SSL model structures, there has been little research
on how varying the depth and width impacts the internal representation of the
small-footprint model. This paper provides an empirical study that addresses
the question. We investigate the performance on SUPERB while varying the
structure and KD methods so as to keep the number of parameters constant; this
allows us to analyze the contribution of the representation introduced by
varying the model architecture. Experiments demonstrate that a certain depth is
essential for solving content-oriented tasks (e.g. automatic speech
recognition) accurately, whereas a certain width is necessary for achieving
high performance on several speaker-oriented tasks (e.g. speaker
identification). Based on these observations, we identify, for SUPERB, a more
compressed model with better performance than previous studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Data-to-Text Generation Based on Small <span class="highlight-title">Dataset</span>s: Comparing the
  Added Value of Two Semi-Supervised Learning Approaches on Top of a Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris van der Lee, Thiago Castro Ferreira, Chris Emmery, Travis Wiltshire, Emiel Krahmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study discusses the effect of semi-supervised learning in combination
with pretrained language models for data-to-text generation. It is not known
whether semi-supervised learning is still helpful when a large-scale language
model is also supplemented. This study aims to answer this question by
comparing a data-to-text system only supplemented with a language model, to two
data-to-text systems that are additionally enriched by a data augmentation or a
pseudo-labeling semi-supervised learning approach.
  Results show that semi-supervised learning results in higher scores on
diversity metrics. In terms of output quality, extending the training set of a
data-to-text system with a language model using the pseudo-labeling approach
did increase text quality scores, but the data augmentation approach yielded
similar scores to the system without training set extension. These results
indicate that semi-supervised learning approaches can bolster output quality
and diversity, even when a language model is also present.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (excluding bibliography and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">BERT</span>IN: Efficient <span class="highlight-title">Pre-Train</span>ing of a Spanish Language Model using
  Perplexity Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier de la Rosa, Eduardo G. Ponferrada, Paulo Villegas, Pablo Gonzalez de Prado Salas, Manu Romero, Marıa Grandury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-training of large language models usually requires massive amounts of
resources, both in terms of computation and data. Frequently used web sources
such as Common Crawl might contain enough noise to make this pre-training
sub-optimal. In this work, we experiment with different sampling methods from
the Spanish version of mC4, and present a novel data-centric technique which we
name $\textit{perplexity sampling}$ that enables the pre-training of language
models in roughly half the amount of steps and using one fifth of the data. The
resulting models are comparable to the current state-of-the-art, and even
achieve better results for certain tasks. Our work is proof of the versatility
of Transformers, and paves the way for small teams to train their models on a
limited budget. Our models are available at this
$\href{https://huggingface.co/bertin-project}{URL}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Procesamiento del Lenguaje Natural</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform
  Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei He, Artur Podobas, Måns I. Andersson, Stefano Markidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete Fourier Transform (DFT) libraries are one of the most critical
software components for scientific computing. Inspired by FFTW, a widely used
library for DFT HPC calculations, we apply compiler technologies for the
development of HPC Fourier transform libraries. In this work, we introduce
FFTc, a domain-specific language, based on Multi-Level Intermediate
Representation (MLIR), for expressing Fourier Transform algorithms. We present
the initial design, implementation, and preliminary results of FFTc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Terminology Management and Sharing Toolkit for Federation of
  Terminology Databases <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andis Lagzdiņš, Uldis Siliņš, Mārcis Pinnis, Toms Bergmanis, Artūrs Vasiļevskis, Andrejs Vasiļjevs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consolidated access to current and reliable terms from different subject
fields and languages is necessary for content creators and translators.
Terminology is also needed in AI applications such as machine translation,
speech recognition, information extraction, and other natural language
processing tools. In this work, we facilitate standards-based sharing and
management of terminology resources by providing an open terminology management
solution - the EuroTermBank Toolkit. It allows organisations to manage and
search their terms, create term collections, and share them within and outside
the organisation by participating in the network of federated databases. The
data curated in the federated databases are automatically shared with
EuroTermBank, the largest multilingual terminology resource in Europe, allowing
translators and language service providers as well as researchers and students
to access terminology resources in their most current version.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layout-Aware Information Extraction for Document-Grounded <span class="highlight-title">Dialogue</span>:
  <span class="highlight-title">Dataset</span>, Method and Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Zhang, Bowen Yu, Haiyang Yu, Tingwen Liu, Cheng Fu, Jingyang Li, Chengguang Tang, Jian Sun, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building document-grounded dialogue systems have received growing interest as
documents convey a wealth of human knowledge and commonly exist in enterprises.
Wherein, how to comprehend and retrieve information from documents is a
challenging research problem. Previous work ignores the visual property of
documents and treats them as plain text, resulting in incomplete modality. In
this paper, we propose a Layout-aware document-level Information Extraction
dataset, LIE, to facilitate the study of extracting both structural and
semantic knowledge from visually rich documents (VRDs), so as to generate
accurate responses in dialogue systems. LIE contains 62k annotations of three
extraction tasks from 4,061 pages in product and official documents, becoming
the largest VRD-based information extraction dataset to the best of our
knowledge. We also develop benchmark methods that extend the token-based
language model to consider layout features like humans. Empirical results show
that layout is critical for VRD-based extraction, and system demonstration also
verifies that the extracted knowledge can help locate the answers that users
care about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia (MM) Industry Track 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of Abusive and Threatening Language Detection in Urdu at FIRE
  2021 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maaz Amjad, Alisa Zhila, Grigori Sidorov, Andrey Labunets, Sabur Butta, Hamza Imam Amjad, Oxana Vitman, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growth of social media platform influence, the effect of their
misuse becomes more and more impactful. The importance of automatic detection
of threatening and abusive language can not be overestimated. However, most of
the existing studies and state-of-the-art methods focus on English as the
target language, with limited work on low- and medium-resource languages. In
this paper, we present two shared tasks of abusive and threatening language
detection for the Urdu language which has more than 170 million speakers
worldwide. Both are posed as binary classification tasks where participating
systems are required to classify tweets in Urdu into two classes, namely: (i)
Abusive and Non-Abusive for the first task, and (ii) Threatening and
Non-Threatening for the second. We present two manually annotated datasets
containing tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening
and Non-Threatening. The abusive dataset contains 2400 annotated tweets in the
train part and 1100 annotated tweets in the test part. The threatening dataset
contains 6000 annotated tweets in the train part and 3950 annotated tweets in
the test part. We also provide logistic regression and BERT-based baseline
classifiers for both tasks. In this shared task, 21 teams from six countries
registered for participation (India, Pakistan, China, Malaysia, United Arab
Emirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is
Abusive Language Detection and 9 teams submitted their runs for Subtask B,
which is Threatening Language detection, and seven teams submitted their
technical reports. The best performing system achieved an F1-score value of
0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based
transformer model showed the best performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Pass Low Latency End-to-End <span class="highlight-title">Spoken Language Understanding</span> <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end (E2E) models are becoming increasingly popular for spoken language
understanding (SLU) systems and are beginning to achieve competitive
performance to pipeline-based approaches. However, recent work has shown that
these models struggle to generalize to new phrasings for the same intent
indicating that models cannot understand the semantic content of the given
utterance. In this work, we incorporated language models pre-trained on
unlabeled text data inside E2E-SLU frameworks to build strong semantic
representations. Incorporating both semantic and acoustic information can
increase the inference time, leading to high latency when deployed for
applications like voice assistants. We developed a 2-pass SLU system that makes
low latency prediction using acoustic information from the few seconds of the
audio in the first pass and makes higher quality prediction in the second pass
by combining semantic and acoustic representations. We take inspiration from
prior work on 2-pass end-to-end speech recognition systems that attends on both
audio and first-pass hypothesis using a deliberation network. The proposed
2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech
Commands Challenge Set and SLURP dataset and reduces latency, thus improving
user experience. Our code and models are publicly available as part of the
ESPnet-SLU toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A tool to overcome technical barriers for bias assessment in human
  language technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Alonso Alemany, Luciana Benotti, Lucía González, Jorge Sánchez, Beatriz Busaniche, Alexia Halvorsen, Matías Bordone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic processing of language is becoming pervasive in our lives, often
taking central roles in our decision making, like choosing the wording for our
messages and mails, translating our readings, or even having full conversations
with us. Word embeddings are a key component of modern natural language
processing systems. They provide a representation of words that has boosted the
performance of many applications, working as a semblance of meaning. Word
embeddings seem to capture a semblance of the meaning of words from raw text,
but, at the same time, they also distill stereotypes and societal biases which
are subsequently relayed to the final applications. Such biases can be
discriminatory. It is very important to detect and mitigate those biases, to
prevent discriminatory behaviors of automated processes, which can be much more
harmful than in the case of humans because their of their scale. There are
currently many tools and techniques to detect and mitigate biases in word
embeddings, but they present many barriers for the engagement of people without
technical skills. As it happens, most of the experts in bias, either social
scientists or people with deep knowledge of the context where bias is harmful,
do not have such skills, and they cannot engage in the processes of bias
detection because of the technical barriers. We have studied the barriers in
existing tools and have explored their possibilities and limitations with
different kinds of users. With this exploration, we propose to develop a tool
that is specially aimed to lower the technical barriers and provide the
exploration power to address the requirements of experts, scientists and people
in general who are willing to audit these technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Recognition in <span class="highlight-title">Conversation</span> using Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eriq Augustine, Pegah Jandaghi, Alon Albalak, Connor Pryor, Charles Dickens, William Wang, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating agents that can both appropriately respond to conversations and
understand complex human linguistic tendencies and social cues has been a long
standing challenge in the NLP community. A recent pillar of research revolves
around emotion recognition in conversation (ERC); a sub-field of emotion
recognition that focuses on conversations or dialogues that contain two or more
utterances. In this work, we explore an approach to ERC that exploits the use
of neural embeddings along with complex structures in dialogues. We implement
our approach in a framework called Probabilistic Soft Logic (PSL), a
declarative templating language that uses first-order like logical rules, that
when combined with data, define a particular class of graphical model.
Additionally, PSL provides functionality for the incorporation of results from
neural models into PSL models. This allows our model to take advantage of
advanced neural methods, such as sentence embeddings, and logical reasoning
over the structure of a dialogue. We compare our method with state-of-the-art
purely neural ERC systems, and see almost a 20% improvement. With these
results, we provide an extensive qualitative and quantitative analysis over the
DailyDialog conversation dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Meta-learning for Low-resource Text Classification and
  Generation via Memory Imitation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng, Dongkyu Lee, Yiping Song, Jian Sun, Nevin L. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building models of natural language processing (NLP) is challenging in
low-resource scenarios where only limited data are available.
Optimization-based meta-learning algorithms achieve promising results in
low-resource scenarios by adapting a well-generalized model initialization to
handle new tasks. Nonetheless, these approaches suffer from the memorization
overfitting issue, where the model tends to memorize the meta-training tasks
while ignoring support sets when adapting to new tasks. To address this issue,
we propose a memory imitation meta-learning (MemIML) method that enhances the
model's reliance on support sets for task adaptation. Specifically, we
introduce a task-specific memory module to store support set information and
construct an imitation module to force query sets to imitate the behaviors of
some representative support-set samples stored in the memory. A theoretical
analysis is provided to prove the effectiveness of our method, and empirical
results also demonstrate that our method outperforms competitive baselines on
both text classification and generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Camera Ready; modified emails</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToxiGen: A Large-Scale Machine-Generated <span class="highlight-title">Dataset</span> for Adversarial and
  Implicit Hate Speech Detection <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09509v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09509v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toxic language detection systems often falsely flag text that contains
minority group mentions as toxic, as those groups are often the targets of
online hate. Such over-reliance on spurious correlations also causes systems to
struggle with detecting implicitly toxic language. To help mitigate these
issues, we create ToxiGen, a new large-scale and machine-generated dataset of
274k toxic and benign statements about 13 minority groups. We develop a
demonstration-based prompting framework and an adversarial
classifier-in-the-loop decoding method to generate subtly toxic and benign text
with a massive pretrained language model. Controlling machine generation in
this way allows ToxiGen to cover implicitly toxic text at a larger scale, and
about more demographic groups, than previous resources of human-written text.
We conduct a human evaluation on a challenging subset of ToxiGen and find that
annotators struggle to distinguish machine-generated text from human-written
language. We also find that 94.5% of toxic examples are labeled as hate speech
by human annotators. Using three publicly-available datasets, we show that
finetuning a toxicity classifier on our data improves its performance on
human-written data substantially. We also demonstrate that ToxiGen can be used
to fight machine-generated toxicity as finetuning improves the classifier
significantly on our evaluation subset. Our code and data can be found at
https://github.com/microsoft/ToxiGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a long paper at ACL 2022. Code:
  https://github.com/microsoft/TOXIGEN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comparison of latent semantic analysis and correspondence analysis of
  document-term matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Qi, David J. Hessen, Tejaswini Deoskar, Peter G. M. van der Heijden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent semantic analysis (LSA) and correspondence analysis (CA) are two
techniques that use a singular value decomposition (SVD) for dimensionality
reduction. LSA has been extensively used to obtain low-dimensional
representations that capture relationships among documents and terms. In this
article, we present a theoretical analysis and comparison of the two techniques
in the context of document-term matrices. We show that CA has some attractive
properties as compared to LSA, for instance that effects of margins arising
from differing document-lengths and term-frequencies are effectively
eliminated, so that the CA solution is optimally suited to focus on
relationships among documents and terms. A unifying framework is proposed that
includes both CA and LSA as special cases. We empirically compare CA to various
LSA based methods on text categorization in English and authorship attribution
on historical Dutch texts, and find that CA performs significantly better. We
also apply CA to a long-standing question regarding the authorship of the Dutch
national anthem Wilhelmus and provide further support that it can be attributed
to the author Datheen, amongst several contenders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language with Vision: a Study on Grounded Word and Sentence Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Shahmohammadi, Maria Heitmeier, Elnaz Shafaei-Bajestan, Hendrik P. A. Lensch, Harald Baayen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language grounding to vision is an active field of research aiming to enrich
text-based representations of word meanings by leveraging perceptual knowledge
from vision. Despite many attempts at language grounding, it is still unclear
how to effectively inject visual knowledge into the word embeddings of a
language in such a way that a proper balance of textual and visual knowledge is
maintained. Some common concerns are the following. Is visual grounding
beneficial for abstract words or is its contribution only limited to concrete
words? What is the optimal way of bridging the gap between text and vision? How
much do we gain by visually grounding textual embeddings? The present study
addresses these questions by proposing a simple yet very effective grounding
approach for pre-trained word embeddings. Our model aligns textual embeddings
with vision while largely preserving the distributional statistics that
characterize word use in text corpora. By applying a learned alignment, we are
able to generate visually grounded embeddings for unseen words, including
abstract words. A series of evaluations on word similarity benchmarks shows
that visual grounding is beneficial not only for concrete words, but also for
abstract words. We also show that our method for visual grounding offers
advantages for contextualized embeddings, but only when these are trained on
corpora of relatively modest size. Code and grounded embeddings for English are
available at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Parallelize in a Shared-Memory Environment with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12835v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12835v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Re'em Harel, Yuval Pinter, Gal Oren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In past years, the world has switched to many-core and multi-core shared
memory architectures. As a result, there is a growing need to utilize these
architectures by introducing shared memory parallelization schemes to software
applications. OpenMP is the most comprehensive API that implements such
schemes, characterized by a readable interface. Nevertheless, introducing
OpenMP into code is challenging due to pervasive pitfalls in management of
parallel shared memory. To facilitate the performance of this task, many
source-to-source (S2S) compilers have been created over the years, tasked with
inserting OpenMP directives into code automatically. In addition to having
limited robustness to their input format, these compilers still do not achieve
satisfactory coverage and precision in locating parallelizable code and
generating appropriate directives. In this work, we propose leveraging recent
advances in ML techniques, specifically in natural language processing (NLP),
to replace S2S compilers altogether. We create a database (corpus), Open-OMP,
specifically for this goal. Open-OMP contains over 28,000 code snippets, half
of which contain OpenMP directives while the other half do not need
parallelization at all with high probability. We use the corpus to train
systems to automatically classify code segments in need of parallelization, as
well as suggest individual OpenMP clauses. We train several transformer models,
named PragFormer, for these tasks, and show that they outperform
statistically-trained baselines and automatic S2S parallelization compilers in
both classifying the overall need for an OpenMP directive and the introduction
of private and reduction clauses.
  Our source code and database are available at:
https://github.com/Scientific-Computing-Lab-NRCN/PragFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Security Analysis Based on Random Geometry Theory for
  Satellite-Terrestrial-Vehicle Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Li, Ye Fan, Rugui Yao, Peng Wang, Nan Qi, Xiaoya Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by B5G and 6G technologies, multi-network fusion is an indispensable
tendency for future communications. In this paper, we focus on and analyze the
\emph{security performance} (SP) of the \emph{satellite-terrestrial downlink
transmission} (STDT). Here, the STDT is composed of a satellite network and a
vehicular network with a legitimate mobile receiver and an mobile eavesdropper
distributing. To theoretically analyze the SP of this system from the
perspective of mobile terminals better, the random geometry theory is adopted,
which assumes that both terrestrial vehicles are distributed stochastically in
one beam of the satellite. Furthermore, based on this theory, the closed-form
analytical expressions for two crucial and specific indicators in the STDT are
derived, respectively, the secrecy outage probability and the ergodic secrecy
capacity. Additionally, several related variables restricting the SP of the
STDT are discussed, and specific schemes are presented to enhance the SP. Then,
the asymptotic property is investigated in the high signal-to-noise ratio
scenario, and accurate and asymptotic closed-form expressions are given.
Finally, simulation results show that, under the precondition of guaranteeing
the reliability of the STDT, the asymptotic solutions outperform the
corresponding accurate results significantly in the effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The theoretical analysis in the original manuscript is insufficient,
  and the system model is not convincing. With the consideration of these
  flaws, we decide to withdraw our work for further improvement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2020 U.S. presidential election in swing states: Gender differences in
  Twitter <span class="highlight-title">conversation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.09416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.09416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Karami, Spring B. Clark, Anderson Mackenzie, Dorathea Lee, Michael Zhu, Hannah R. Boyajieff, Bailey Goldschmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media is commonly used by the public during election campaigns to
express their opinions regarding different issues. Among various social media
channels, Twitter provides an efficient platform for researchers and
politicians to explore public opinion regarding a wide range of topics such as
the economy and foreign policy. Current literature mainly focuses on analyzing
the content of tweets without considering the gender of users. This research
collects and analyzes a large number of tweets and uses computational, human
coding, and statistical analyses to identify topics in more than 300,000 tweets
posted during the 2020 U.S. presidential election and to compare female and
male users regarding the average weight of the discussed topics. Our findings
are based upon a wide range of topics, such as tax, climate change, and the
COVID-19 pandemic. Out of the topics, there exists a significant difference
between female and male users for more than 70% of topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Parametric Domain Adaptation for End-to-End Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11211v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11211v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichao Du, Weizhi Wang, Zhirui Zhang, Boxing Chen, Tong Xu, Jun Xie, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-End Speech Translation (E2E-ST) has received increasing attention due
to the potential of its less error propagation, lower latency, and fewer
parameters. However, the effectiveness of neural-based approaches to this task
is severely limited by the available training corpus, especially for domain
adaptation where in-domain triplet training data is scarce or nonexistent. In
this paper, we propose a novel non-parametric method that leverages
domain-specific text translation corpus to achieve domain adaptation for the
E2E-ST system. To this end, we first incorporate an additional encoder into the
pre-trained E2E-ST model to realize text translation modelling, and then unify
the decoder's output representation for text and speech translation tasks by
reducing the correspondent representation mismatch in available triplet
training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier
is introduced to produce the final translation distribution using the external
datastore built by the domain-specific text translation corpus, while the
universal output representation is adopted to perform a similarity search.
Experiments on the Europarl-ST benchmark demonstrate that when in-domain text
translation data is involved only, our proposed approach significantly improves
baseline by 12.82 BLEU on average in all translation directions, even
outperforming the strong in-domain fine-tuning method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech-enhanced and Noise-aware Networks for Robust Speech Recognition <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung-Shin Lee, Pin-Yuan Chen, Yao-Fei Cheng, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compensation for channel mismatch and noise interference is essential for
robust automatic speech recognition. Enhanced speech has been introduced into
the multi-condition training of acoustic models to improve their generalization
ability. In this paper, a noise-aware training framework based on two cascaded
neural structures is proposed to jointly optimize speech enhancement and speech
recognition. The feature enhancement module is composed of a multi-task
autoencoder, where noisy speech is decomposed into clean speech and noise. By
concatenating its enhanced, noise-aware, and noisy features for each frame, the
acoustic-modeling module maps each feature-augmented frame into a triphone
state by optimizing the lattice-free maximum mutual information and cross
entropy between the predicted and actual state sequences. On top of the
factorized time delay neural network (TDNN-F) and its convolutional variant
(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error
rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared
with the best existing systems that use bigram and trigram language models for
decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction
of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based
system also outperforms the baseline CNN-TDNNF system on the AMI task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ISCSLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretically Grounded Benchmark for Evaluating Machine Commonsense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrique Santos, Ke Shen, Alice M. Mulvehill, Yasaman Razeghi, Deborah L. McGuinness, Mayank Kejriwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming machines with commonsense reasoning (CSR) abilities is a
longstanding challenge in the Artificial Intelligence community. Current CSR
benchmarks use multiple-choice (and in relatively fewer cases, generative)
question-answering instances to evaluate machine commonsense. Recent progress
in transformer-based language representation models suggest that considerable
progress has been made on existing benchmarks. However, although tens of CSR
benchmarks currently exist, and are growing, it is not evident that the full
suite of commonsense capabilities have been systematically evaluated.
Furthermore, there are doubts about whether language models are 'fitting' to a
benchmark dataset's training partition by picking up on subtle, but normatively
irrelevant (at least for CSR), statistical features to achieve good performance
on the testing partition. To address these challenges, we propose a benchmark
called Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based
on discriminative question answering, but with questions designed to evaluate
diverse aspects of commonsense, such as space, time, and world states. TG-CSR
is based on a subset of commonsense categories first proposed as a viable
theory of commonsense by Gordon and Hobbs. The benchmark is also designed to be
few-shot (and in the future, zero-shot), with only a few training and
validation examples provided. This report discusses the structure and
construction of the benchmark. Preliminary results suggest that the benchmark
is challenging even for advanced language representation models designed for
discriminative CSR question answering tasks.
  Benchmark access and leaderboard:
https://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:
https://usc-isi-i2.github.io/TGCSR/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Fine-tuning of Language Models <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give simpler, sparser, and faster algorithms for differentially private
fine-tuning of large-scale pre-trained language models, which achieve the
state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.
We propose a meta-framework for this problem, inspired by the recent success of
highly parameter-efficient methods for fine-tuning. Our experiments show that
differentially private adaptations of these approaches outperform previous
private algorithms in three important dimensions: utility, privacy, and the
computational and memory cost of private training. On many commonly studied
datasets, the utility of private models approaches that of non-private models.
For example, on the MNLI dataset we achieve an accuracy of $87.8\%$ using
RoBERTa-Large and $83.5\%$ using RoBERTa-Base with a privacy budget of
$\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large
achieves an accuracy of $90.2\%$. Our findings are similar for natural language
generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,
GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8
respectively (privacy budget of $\epsilon = 6.8,\delta=$ 1e-5) whereas the
non-private baseline is $48.1$. All our experiments suggest that larger models
are better suited for private fine-tuning: while they are well known to achieve
superior accuracy non-privately, we find that they also better maintain their
accuracy when privacy is introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022. Code available at
  https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making the Most of Text Semantics to Improve Biomedical Vision--Language
  Processing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, Ozan Oktay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:
  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Streaming Multi-Talker ASR with Token-Level Serialized Output Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00842v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00842v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoyuki Kanda, Jian Wu, Yu Wu, Xiong Xiao, Zhong Meng, Xiaofei Wang, Yashesh Gaur, Zhuo Chen, Jinyu Li, Takuya Yoshioka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a token-level serialized output training (t-SOT), a novel
framework for streaming multi-talker automatic speech recognition (ASR). Unlike
existing streaming multi-talker ASR models using multiple output branches, the
t-SOT model has only a single output branch that generates recognition tokens
(e.g., words, subwords) of multiple speakers in chronological order based on
their emission times. A special token that indicates the change of ``virtual''
output channels is introduced to keep track of the overlapping utterances.
Compared to the prior streaming multi-talker ASR models, the t-SOT model has
the advantages of less inference cost and a simpler model architecture.
Moreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the
t-SOT-based transformer transducer model achieves the state-of-the-art word
error rates by a significant margin to the prior results. For non-overlapping
speech, the t-SOT model is on par with a single-talker ASR model in terms of
both accuracy and computational cost, opening the door for deploying one model
for both single- and multi-talker scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 7 tables, v2: minor fixes, v3: Appendix D has been
  added, v4: citation to [27] has been added, v5: citations to [28][29][30]
  have been added with minor fixes, short version accepted for presentation at
  Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoyuki Kanda, Jian Wu, Yu Wu, Xiong Xiao, Zhong Meng, Xiaofei Wang, Yashesh Gaur, Zhuo Chen, Jinyu Li, Takuya Yoshioka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a streaming speaker-attributed automatic speech
recognition (SA-ASR) model that can recognize ``who spoke what'' with low
latency even when multiple people are speaking simultaneously. Our model is
based on token-level serialized output training (t-SOT) which was recently
proposed to transcribe multi-talker speech in a streaming fashion. To further
recognize speaker identities, we propose an encoder-decoder based speaker
embedding extractor that can estimate a speaker representation for each
recognized token not only from non-overlapping speech but also from overlapping
speech. The proposed speaker embedding, named t-vector, is extracted
synchronously with the t-SOT ASR model, enabling joint execution of speaker
identification (SID) or speaker diarization (SD) with the multi-talker
transcription with low latency. We evaluate the proposed model for a joint task
of ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed
model achieves substantially better accuracy than a prior streaming model and
shows comparable or sometimes even superior results to the state-of-the-art
offline SA-ASR model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Bootstrapped Masked Autoencoders for Vision <span class="highlight-title">BERT</span> <span class="highlight-title">Pretrain</span>ing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, <span class="highlight-author">Lu Yuan</span>, Dong Chen, Fang Wen, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose bootstrapped masked autoencoders (BootMAE), a new approach for
vision BERT pretraining. BootMAE improves the original masked autoencoders
(MAE) with two core designs: 1) momentum encoder that provides online feature
as extra BERT prediction targets; 2) target-aware decoder that tries to reduce
the pressure on the encoder to memorize target-specific information in BERT
pretraining. The first design is motivated by the observation that using a
pretrained MAE to extract the features as the BERT prediction target for masked
tokens can achieve better pretraining performance. Therefore, we add a momentum
encoder in parallel with the original MAE encoder, which bootstraps the
pretraining performance by using its own representation as the BERT prediction
target. In the second design, we introduce target-specific information (e.g.,
pixel values of unmasked patches) from the encoder directly to the decoder to
reduce the pressure on the encoder of memorizing the target-specific
information. Thus, the encoder focuses on semantic modeling, which is the goal
of BERT pretraining, and does not need to waste its capacity in memorizing the
information of unmasked tokens related to the prediction target. Through
extensive experiments, our BootMAE achieves $84.2\%$ Top-1 accuracy on
ImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\%$ under the same
pre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic
segmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object
detection and segmentation on COCO dataset. Code is released at
https://github.com/LightDXY/BootMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, code is available at https://github.com/LightDXY/BootMAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin
  Memory Model <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Kei Cheng, Alexander G. Schwing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present XMem, a video object segmentation architecture for long videos
with unified feature memory stores inspired by the Atkinson-Shiffrin memory
model. Prior work on video object segmentation typically only uses one type of
feature memory. For videos longer than a minute, a single feature memory model
tightly links memory consumption and accuracy. In contrast, following the
Atkinson-Shiffrin model, we develop an architecture that incorporates multiple
independent yet deeply-connected feature memory stores: a rapidly updated
sensory memory, a high-resolution working memory, and a compact thus sustained
long-term memory. Crucially, we develop a memory potentiation algorithm that
routinely consolidates actively used working memory elements into the long-term
memory, which avoids memory explosion and minimizes performance decay for
long-term prediction. Combined with a new memory reading mechanism, XMem
greatly exceeds state-of-the-art performance on long-video datasets while being
on par with state-of-the-art methods (that do not work on long videos) on
short-video datasets. Code is available at https://hkchengrex.github.io/XMem
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022. Project page:
  https://hkchengrex.github.io/XMem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Few-shot Recognition by Deep Object Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengkai Zhu, Ruizhao Zhu, Samarth Mishra, Venkatesh Saligrama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our framework, an object is made up of K distinct parts or units, and we
parse a test instance by inferring the K parts, where each part occupies a
distinct location in the feature space, and the instance features at this
location, manifest as an active subset of part templates shared across all
instances. We recognize test instances by comparing its active templates and
the relative geometry of its part locations against those of the presented
few-shot instances. We propose an end-to-end training method to learn part
templates on-top of a convolutional backbone. To combat visual distortions such
as orientation, pose and size, we learn multi-scale templates, and at test-time
parse and match instances across these scales. We show that our method is
competitive with the state-of-the-art, and by virtue of parsing enjoys
interpretability as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Omni-Vision Representation through the Lens of Visual
  Realms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though impressive performance has been achieved in specific visual realms
(e.g. faces, dogs, and places), an omni-vision representation generalizing to
many natural visual domains is highly desirable. But, existing benchmarks are
biased and inefficient to evaluate the omni-vision representation -- these
benchmarks either only include several specific realms, or cover most realms at
the expense of subsuming numerous datasets that have extensive realm
overlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It
includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.
Without semantic overlapping, these datasets cover most visual realms
comprehensively and meanwhile efficiently. In addition, we propose a new
supervised contrastive learning framework, namely Relational Contrastive
learning (ReCo), for a better omni-vision representation. Beyond pulling two
instances from the same concept closer -- the typical supervised contrastive
learning framework -- ReCo also pulls two instances from the same semantic
realm closer, encoding the semantic relation between concepts, and facilitating
omni-vision representation learning. We benchmark ReCo and other advances in
omni-vision representation studies that are different in architectures (from
CNNs to transformers) and in learning paradigms (from supervised learning to
self-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo
to other supervised contrastive learning methods and reveal multiple practical
observations to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page at https://zhangyuanhan-ai.github.io/OmniBenchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relighting4D: Neural Relightable Human from Videos <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxi Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human relighting is a highly desirable yet challenging task. Existing works
either require expensive one-light-at-a-time (OLAT) captured data using light
stage or cannot freely change the viewpoints of the rendered body. In this
work, we propose a principled framework, Relighting4D, that enables
free-viewpoints relighting from only human videos under unknown illuminations.
Our key insight is that the space-time varying geometry and reflectance of the
human body can be decomposed as a set of neural fields of normal, occlusion,
diffuse, and specular maps. These neural fields are further integrated into
reflectance-aware physically based rendering, where each vertex in the neural
field absorbs and reflects the light from the environment. The whole framework
can be learned from videos in a self-supervised manner, with physically
informed priors designed for regularization. Extensive experiments on both real
and synthetic datasets demonstrate that our framework is capable of relighting
dynamic human actors with free-viewpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Project Page
  https://frozenburning.github.io/projects/relighting4d Codes are available at
  https://github.com/FrozenBurning/Relighting4D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReAct: Temporal Action Detection with Relational Queries <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingfeng Shi, Yujie Zhong, Qiong Cao, Jing Zhang, Lin Ma, Jia Li, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims at advancing temporal action detection (TAD) using an
encoder-decoder framework with action queries, similar to DETR, which has shown
great success in object detection. However, the framework suffers from several
problems if directly applied to TAD: the insufficient exploration of
inter-query relation in the decoder, the inadequate classification training due
to a limited number of training samples, and the unreliable classification
scores at inference. To this end, we first propose a relational attention
mechanism in the decoder, which guides the attention among queries based on
their relations. Moreover, we propose two losses to facilitate and stabilize
the training of action classification. Lastly, we propose to predict the
localization quality of each action query at inference in order to distinguish
high-quality queries. The proposed method, named ReAct, achieves the
state-of-the-art performance on THUMOS14, with much lower computational costs
than previous methods. Besides, extensive ablation studies are conducted to
verify the effectiveness of each proposed component. The code is available at
https://github.com/sssste/React.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Image Enhancement Black-Box Methods through a Path Planning
  Based Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Cotogni, Claudio Cusano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, image-to-image translation methods, are the state of the art for
the enhancement of natural images. Even if they usually show high performance
in terms of accuracy, they often suffer from several limitations such as the
generation of artifacts and the scalability to high resolutions. Moreover,
their main drawback is the completely black-box approach that does not allow to
provide the final user with any insight about the enhancement processes
applied. In this paper we present a path planning algorithm which provides a
step-by-step explanation of the output produced by state of the art enhancement
methods, overcoming black-box limitation. This algorithm, called eXIE, uses a
variant of the A* algorithm to emulate the enhancement process of another
method through the application of an equivalent sequence of enhancing
operators. We applied eXIE to explain the output of several state-of-the-art
models trained on the Five-K dataset, obtaining sequences of enhancing
operators able to produce very similar results in terms of performance and
overcoming the huge limitation of poor interpretability of the best performing
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse
  Representation Based Domain Adaption to Energy Efficient Abnormal Beat
  Detection for Practical ECG Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Yamaç, Mert Duman, İlke Adalıoğlu, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a low-cost and highly accurate ECG-monitoring system
intended for personalized early arrhythmia detection for wearable mobile
sensors. Earlier supervised approaches for personalized ECG monitoring require
both abnormal and normal heartbeats for the training of the dedicated
classifier. However, in a real-world scenario where the personalized algorithm
is embedded in a wearable device, such training data is not available for
healthy people with no cardiac disorder history. In this study, (i) we propose
a null space analysis on the healthy signal space obtained via sparse
dictionary learning, and investigate how a simple null space projection or
alternatively regularized least squares-based classification methods can reduce
the computational complexity, without sacrificing the detection accuracy, when
compared to sparse representation-based classification. (ii) Then we introduce
a sparse representation-based domain adaptation technique in order to project
other existing users' abnormal and normal signals onto the new user's signal
space, enabling us to train the dedicated classifier without having any
abnormal heartbeat of the new user. Therefore, zero-shot learning can be
achieved without the need for synthetic abnormal heartbeat generation. An
extensive set of experiments performed on the benchmark MIT-BIH ECG dataset
shows that when this domain adaptation-based training data generator is used
with a simple 1-D CNN classifier, the method outperforms the prior work by a
significant margin. (iii) Then, by combining (i) and (ii), we propose an
ensemble classifier that further improves the performance. This approach for
zero-shot arrhythmia detection achieves an average accuracy level of 98.2% and
an F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring
scheme is proposed using the above-mentioned innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Software implementation: https://github.com/MertDuman/Zero-Shot-ECG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Asymmetric <span class="highlight-title">Contrastive</span> Loss for Handling Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentino Vito, Lim Yohanes Stefanus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning is a representation learning method performed by
contrasting a sample to other similar samples so that they are brought closely
together, forming clusters in the feature space. The learning process is
typically conducted using a two-stage training architecture, and it utilizes
the contrastive loss (CL) for its feature learning. Contrastive learning has
been shown to be quite successful in handling imbalanced datasets, in which
some classes are overrepresented while some others are underrepresented.
However, previous studies have not specifically modified CL for imbalanced
datasets. In this work, we introduce an asymmetric version of CL, referred to
as ACL, in order to directly address the problem of class imbalance. In
addition, we propose the asymmetric focal contrastive loss (AFCL) as a further
generalization of both ACL and focal contrastive loss (FCL). Results on the
FMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of
outperforming CL and FCL in terms of both weighted and unweighted
classification accuracies. In the appendix, we provide a full axiomatic
treatment on entropy, along with complete proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Grand Unification of Object Tracking <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Egocentric Scene Understanding via <span class="highlight-title">Multimodal</span> Spatial Rectifier <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tien Do, Khiem Vuong, Hyun Soo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a problem of egocentric scene understanding, i.e.,
predicting depths and surface normals from an egocentric image. Egocentric
scene understanding poses unprecedented challenges: (1) due to large head
movements, the images are taken from non-canonical viewpoints (i.e., tilted
images) where existing models of geometry prediction do not apply; (2) dynamic
foreground objects including hands constitute a large proportion of visual
scenes. These challenges limit the performance of the existing models learned
from large indoor datasets, such as ScanNet and NYUv2, which comprise
predominantly upright images of static scenes. We present a multimodal spatial
rectifier that stabilizes the egocentric images to a set of reference
directions, which allows learning a coherent visual representation. Unlike
unimodal spatial rectifier that often produces excessive perspective warp for
egocentric images, the multimodal spatial rectifier learns from multiple
directions that can minimize the impact of the perspective warp. To learn
visual representations of the dynamic foreground objects, we present a new
dataset called EDINA (Egocentric Depth on everyday INdoor Activities) that
comprises more than 500K synchronized RGBD frames and gravity directions.
Equipped with the multimodal spatial rectifier and the EDINA dataset, our
proposed method on single-view depth and surface normal estimation
significantly outperforms the baselines not only on our EDINA dataset, but also
on other popular egocentric datasets, such as First Person Hand Action (FPHA)
and EPIC-KITCHENS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in the Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Temporal Action Detection with Proposal-Free Masking <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing temporal action detection (TAD) methods rely on a large number of
training data with segment-level annotations. Collecting and annotating such a
training set is thus highly expensive and unscalable. Semi-supervised TAD
(SS-TAD) alleviates this problem by leveraging unlabeled videos freely
available at scale. However, SS-TAD is also a much more challenging problem
than supervised TAD, and consequently much under-studied. Prior SS-TAD methods
directly combine an existing proposal-based TAD method and a SSL method. Due to
their sequential localization (e.g, proposal generation) and classification
design, they are prone to proposal error propagation. To overcome this
limitation, in this work we propose a novel Semi-supervised Temporal action
detection model based on PropOsal-free Temporal mask (SPOT) with a parallel
localization (mask generation) and classification architecture. Such a novel
design effectively eliminates the dependence between localization and
classification by cutting off the route for error propagation in-between. We
further introduce an interaction mechanism between classification and
localization for prediction refinement, and a new pretext task for
self-supervised model pre-training. Extensive experiments on two standard
benchmarks show that our SPOT outperforms state-of-the-art alternatives, often
by a large margin. The PyTorch implementation of SPOT is available at
https://github.com/sauradip/SPOT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code available at https://github.com/sauradip/SPOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Bypasses Are Better Vision <span class="highlight-title">Transformer</span> Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibo Jie, Zhi-Hong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretrain-then-finetune paradigm has been widely adopted in computer
vision. But as the size of Vision Transformer (ViT) grows exponentially, the
full finetuning becomes prohibitive in view of the heavier storage overhead.
Motivated by parameter-efficient transfer learning (PETL) on language
transformers, recent studies attempt to insert lightweight adaptation modules
(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune
these modules while the pretrained weights are frozen. However, these modules
were originally proposed to finetune language models. Although ported well to
ViT, their design lacks prior knowledge for visual tasks. In this paper, we
propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation
modules, introducing only a small amount (less than 0.5% of model parameters)
of trainable parameters to adapt the large ViT. Different from other PETL
methods, Convpass benefits from the hard-coded inductive bias of convolutional
layers and thus is more suitable for visual tasks, especially in the low-data
regime. Experimental results on VTAB-1k benchmark and few-shot learning
datasets demonstrate that Convpass outperforms current language-oriented
adaptation modules, demonstrating the necessity to tailor vision-oriented
adaptation modules for vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Single <span class="highlight-title">Self-Supervised</span> Model for Many Speech Modalities Enables
  Zero-Shot Modality Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Ning Hsu, Bowen Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While audio-visual speech models can yield superior performance and
robustness compared to audio-only models, their development and adoption are
hindered by the lack of labeled and unlabeled audio-visual data and the cost to
deploy one model per modality. In this paper, we present u-HuBERT, a
self-supervised pre-training framework that can leverage both multimodal and
unimodal speech with a unified masked cluster prediction objective. By
utilizing modality dropout during pre-training, we demonstrate that a single
fine-tuned model can achieve performance on par or better than the
state-of-the-art modality-specific models. Moreover, our model fine-tuned only
on audio can perform well with audio-visual and visual speech input, achieving
zero-shot modality generalization for speech recognition and speaker
verification. In particular, our single model yields 1.2%/1.4%/27.2% speech
recognition word error rate on LRS3 with audio-visual/audio/visual input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks on Monocular Pose Estimation <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in deep learning have resulted in steady progress in computer vision
with improved accuracy on tasks such as object detection and semantic
segmentation. Nevertheless, deep neural networks are vulnerable to adversarial
attacks, thus presenting a challenge in reliable deployment. Two of the
prominent tasks in 3D scene-understanding for robotics and advanced drive
assistance systems are monocular depth and pose estimation, often learned
together in an unsupervised manner. While studies evaluating the impact of
adversarial attacks on monocular depth estimation exist, a systematic
demonstration and analysis of adversarial perturbations against pose estimation
are lacking. We show how additive imperceptible perturbations can not only
change predictions to increase the trajectory drift but also catastrophically
alter its geometry. We also study the relation between adversarial
perturbations targeting monocular depth and pose estimation networks, as well
as the transferability of perturbations to other networks with different
architectures and losses. Our experiments show how the generated perturbations
lead to notable errors in relative rotation and translation predictions and
elucidate vulnerabilities of the networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedFuse: <span class="highlight-title">Multi-modal</span> fusion with clinical time-series data and chest
  X-ray images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasir Hayat, Krzysztof J. Geras, Farah E. Shamout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal fusion approaches aim to integrate information from different
data sources. Unlike natural datasets, such as in audio-visual applications,
where samples consist of "paired" modalities, data in healthcare is often
collected asynchronously. Hence, requiring the presence of all modalities for a
given sample is not realistic for clinical tasks and significantly limits the
size of the dataset during training. In this paper, we propose MedFuse, a
conceptually simple yet promising LSTM-based fusion module that can accommodate
uni-modal as well as multi-modal input. We evaluate the fusion method and
introduce new benchmark results for in-hospital mortality prediction and
phenotype classification, using clinical time-series data in the MIMIC-IV
dataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more
complex multi-modal fusion strategies, MedFuse provides a performance
improvement by a large margin on the fully paired test set. It also remains
robust across the partially paired test set containing samples with missing
chest X-ray images. We release our code for reproducibility and to enable the
evaluation of competing models in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate Ground-Truth Depth Image Generation via Overfit Training of
  Point Cloud Registration using Local Frame Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwan Kim, Minchang Kim, Yeong-Gil Shin, Minyoung Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate three-dimensional perception is a fundamental task in several
computer vision applications. Recently, commercial RGB-depth (RGB-D) cameras
have been widely adopted as single-view depth-sensing devices owing to their
efficient depth-sensing abilities. However, the depth quality of most RGB-D
sensors remains insufficient owing to the inherent noise from a single-view
environment. Recently, several studies have focused on the single-view depth
enhancement of RGB-D cameras. Recent research has proposed deep-learning-based
approaches that typically train networks using high-quality supervised depth
datasets, which indicates that the quality of the ground-truth (GT) depth
dataset is a top-most important factor for accurate system; however, such
high-quality GT datasets are difficult to obtain. In this study, we developed a
novel method for high-quality GT depth generation based on an RGB-D stream
dataset. First, we defined consecutive depth frames in a local spatial region
as a local frame set. Then, the depth frames were aligned to a certain frame in
the local frame set using an unsupervised point cloud registration scheme. The
registration parameters were trained based on an overfit-training scheme, which
was primarily used to construct a single GT depth image for each frame set. The
final GT depth dataset was constructed using several local frame sets, and each
local frame set was trained independently. The primary advantage of this study
is that a high-quality GT depth dataset can be constructed under various
scanning environments using only the RGB-D stream dataset. Moreover, our
proposed method can be used as a new benchmark GT dataset for accurate
performance evaluations. We evaluated our GT dataset on previously benchmarked
GT depth datasets and demonstrated that our method is superior to
state-of-the-art depth enhancement frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Modelling with Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches, instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT,
further confirming the benefits of modelling language with pixels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree Structure-Aware Few-Shot Image Classification via Hierarchical
  Aggregation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhang, Siteng Huang, Wenbin Li, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we mainly focus on the problem of how to learn additional
feature representations for few-shot image classification through pretext tasks
(e.g., rotation or color permutation and so on). This additional knowledge
generated by pretext tasks can further improve the performance of few-shot
learning (FSL) as it differs from human-annotated supervision (i.e., class
labels of FSL tasks). To solve this problem, we present a plug-in Hierarchical
Tree Structure-aware (HTS) method, which not only learns the relationship of
FSL and pretext tasks, but more importantly, can adaptively select and
aggregate feature representations generated by pretext tasks to maximize the
performance of FSL tasks. A hierarchical tree constructing component and a
gated selection aggregating component is introduced to construct the tree
structure and find richer transferable knowledge that can rapidly adapt to
novel classes with a few labeled images. Extensive experiments show that our
HTS can significantly enhance multiple few-shot methods to achieve new
state-of-the-art performance on four benchmark datasets. The code is available
at: https://github.com/remiMZ/HTS-ECCV22.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures and 4 tables Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjectBox: From Centers to Boxes for Anchor-Free Object Detection <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Zand, Ali Etemad, Michael Greenspan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ObjectBox, a novel single-stage anchor-free and highly
generalizable object detection approach. As opposed to both existing
anchor-based and anchor-free detectors, which are more biased toward specific
object scales in their label assignments, we use only object center locations
as positive samples and treat all objects equally in different feature levels
regardless of the objects' sizes or shapes. Specifically, our label assignment
strategy considers the object center locations as shape- and size-agnostic
anchors in an anchor-free fashion, and allows learning to occur at all scales
for every object. To support this, we define new regression targets as the
distances from two corners of the center cell location to the four sides of the
bounding box. Moreover, to handle scale-variant objects, we propose a tailored
IoU loss to deal with boxes with different sizes. As a result, our proposed
object detector does not need any dataset-dependent hyperparameters to be tuned
across datasets. We evaluate our method on MS-COCO 2017 and PASCAL VOC 2012
datasets, and compare our results to state-of-the-art methods. We observe that
ObjectBox performs favorably in comparison to prior works. Furthermore, we
perform rigorous ablation experiments to evaluate different components of our
method. Our code is available at: https://github.com/MohsenZand/ObjectBox.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Discriminative Representation via Metric Learning for
  Imbalanced Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghua Zeng, Huijuan Lu, Kanghao Chen, Ruixuan Wang, Wei-Shi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data imbalance between common and rare diseases during model training often
causes intelligent diagnosis systems to have biased predictions towards common
diseases. The state-of-the-art approaches apply a two-stage learning framework
to alleviate the class-imbalance issue, where the first stage focuses on
training of a general feature extractor and the second stage focuses on
fine-tuning the classifier head for class rebalancing. However, existing
two-stage approaches do not consider the fine-grained property between
different diseases, often causing the first stage less effective for medical
image classification than for natural image classification tasks. In this
study, we propose embedding metric learning into the first stage of the
two-stage framework specially to help the feature extractor learn to extract
more discriminative feature representations. Extensive experiments mainly on
three medical image datasets show that the proposed approach consistently
outperforms existing onestage and two-stage approaches, suggesting that metric
learning can be used as an effective plug-in component in the two-stage
framework for fine-grained class-imbalanced image classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PR-DARTS: Pruning-Based Differentiable Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Mousavi, Mohammad Loni, Mina Alibeigi, Masoud Daneshtalab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of Convolutional Neural Networks (CNNs) on edge devices is
hindered by the substantial gap between performance requirements and available
processing power. While recent research has made large strides in developing
network pruning methods for reducing the computing overhead of CNNs, there
remains considerable accuracy loss, especially at high pruning ratios.
Questioning that the architectures designed for non-pruned networks might not
be effective for pruned networks, we propose to search architectures for
pruning methods by defining a new search space and a novel search objective. To
improve the generalization of the pruned networks, we propose two novel
PrunedConv and PrunedLinear operations. Specifically, these operations mitigate
the problem of unstable gradients by regularizing the objective function of the
pruned networks. The proposed search objective enables us to train architecture
parameters regarding the pruned weight elements. Quantitative analyses
demonstrate that our searched architectures outperform those used in the
state-of-the-art pruning networks on CIFAR-10 and ImageNet. In terms of
hardware effectiveness, PR-DARTS increases MobileNet-v2's accuracy from 73.44%
to 81.35% (+7.91% improvement) and runs 3.87$\times$ faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages with 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene Text Recognition with Permuted Autoregressive Sequence Models <span class="chip">ECCV
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darwin Bautista, Rowel Atienza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-aware STR methods typically use internal autoregressive (AR) language
models (LM). Inherent limitations of AR models motivated two-stage methods
which employ an external LM. The conditional independence of the external LM on
the input image may cause it to erroneously rectify correct predictions,
leading to significant inefficiencies. Our method, PARSeq, learns an ensemble
of internal AR LMs with shared weights using Permutation Language Modeling. It
unifies context-free non-AR and context-aware AR inference, and iterative
refinement using bidirectional context. Using synthetic training data, PARSeq
achieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and
more challenging datasets. It establishes new SOTA results (96.0% accuracy)
when trained on real data. PARSeq is optimal on accuracy vs parameter count,
FLOPS, and latency because of its simple, unified structure and parallel token
processing. Due to its extensive use of attention, it is robust on
arbitrarily-oriented text which is common in real-world images. Code,
pretrained weights, and data are available at: https://github.com/baudm/parseq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 17th European Conference on Computer Vision (ECCV
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoMerge: A Framework for Map Assembling and Smoothing in City-scale
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Yin, Haowen Lai, Shiqi Zhao, Ruijie Fu, Ivan Cisneros, Ruohai Ge, Ji Zhang, Howie Choset, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AutoMerge, a LiDAR data processing framework for assembling a
large number of map segments into a complete map. Traditional large-scale map
merging methods are fragile to incorrect data associations, and are primarily
limited to working only offline. AutoMerge utilizes multi-perspective fusion
and adaptive loop closure detection for accurate data associations, and it uses
incremental merging to assemble large maps from individual trajectory segments
given in random order and with no initial estimations. Furthermore, after
assembling the segments, AutoMerge performs fine matching and pose-graph
optimization to globally smooth the merged map. We demonstrate AutoMerge on
both city-scale merging (120km) and campus-scale repeated merging (4.5km x 8).
The experiments show that AutoMerge (i) surpasses the second- and third- best
methods by 14% and 24% recall in segment retrieval, (ii) achieves comparable 3D
mapping accuracy for 120 km large-scale map assembly, (iii) and it is robust to
temporally-spaced revisits. To the best of our knowledge, AutoMerge is the
first mapping approach that can merge hundreds of kilometers of individual
segments without the aid of GPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 18 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Implicit Templates for Point-Based Clothed Human Modeling <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyou Lin, Hongwen Zhang, Zerong Zheng, Ruizhi Shao, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FITE, a First-Implicit-Then-Explicit framework for modeling human
avatars in clothing. Our framework first learns implicit surface templates
representing the coarse clothing topology, and then employs the templates to
guide the generation of point sets which further capture pose-dependent
clothing deformations such as wrinkles. Our pipeline incorporates the merits of
both implicit and explicit representations, namely, the ability to handle
varying topology and the ability to efficiently capture fine details. We also
propose diffused skinning to facilitate template training especially for loose
clothing, and projection-based pose-encoding to extract pose information from
mesh templates without predefined UV map or connectivity. Our code is publicly
available at https://github.com/jsnln/fite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tackling Background Distraction in Video Object Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhwan Cho, Heansung Lee, Minhyeok Lee, Chaewon Park, Sungjun Jang, Minjung Kim, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised video object segmentation (VOS) aims to densely track certain
designated objects in videos. One of the main challenges in this task is the
existence of background distractors that appear similar to the target objects.
We propose three novel strategies to suppress such distractors: 1) a
spatio-temporally diversified template construction scheme to obtain
generalized properties of the target objects; 2) a learnable distance-scoring
function to exclude spatially-distant distractors by exploiting the temporal
consistency between two consecutive frames; 3) swap-and-attach augmentation to
force each object to have unique features by providing training samples
containing entangled objects. On all public benchmark datasets, our model
achieves a comparable performance to contemporary state-of-the-art approaches,
even with real-time performance. Qualitative results also demonstrate the
superiority of our approach over existing methods. We believe our approach will
be widely used for future VOS research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insurgency as Complex Network: Image Co-Appearance and Hierarchy in the
  PKK 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ollie Ballinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a growing recognition of the importance of insurgent group structure
on conflict outcomes, there is very little empirical research thereon. Though
this problem is rooted in the inaccessibility of data on militant group
structure, insurgents frequently publish large volumes of image data on the
internet. In this paper, I develop a new methodology that leverages this
abundant but underutilized source of data by automating the creation of a
social network graph based on co-appearance in photographs using deep learning.
Using a trove of 19,115 obituary images published online by the PKK, a Kurdish
militant group in Turkey, I demonstrate that an individual's centrality in the
resulting co-appearance network is closely correlated with their rank in the
insurgent group.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Streaming Video Denoising with Bidirectional Buffers <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Qi, Junming Chen, Xin Yang, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video streams are delivered continuously to save the cost of storage and
device memory. Real-time denoising algorithms are typically adopted on the user
device to remove the noise involved during the shooting and transmission of
video streams. However, sliding-window-based methods feed multiple input frames
for a single output and lack computation efficiency. Recent multi-output
inference works propagate the bidirectional temporal feature with a parallel or
recurrent framework, which either suffers from performance drops on the
temporal edges of clips or can not achieve online inference. In this paper, we
propose a Bidirectional Streaming Video Denoising (BSVD) framework, to achieve
high-fidelity real-time denoising for streaming videos with both past and
future temporal receptive fields. The bidirectional temporal fusion for online
inference is considered not applicable in the MoViNet. However, we introduce a
novel Bidirectional Buffer Block as the core module of our BSVD, which makes it
possible during our pipeline-style inference. In addition, our method is
concise and flexible to be utilized in both non-blind and blind video
denoising. We compare our model with various state-of-the-art video denoising
models qualitatively and quantitatively on synthetic and real noise. Our method
outperforms previous methods in terms of restoration fidelity and runtime. Our
source code is publicly available at https://github.com/ChenyangQiQi/BSVD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2022; Github link:
  https://github.com/ChenyangQiQi/BSVD ;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Factorized and Controllable Neural Re-Rendering of Outdoor Scene for
  Photo Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boming Zhao, Bangbang Yang, Zhenyang Li, Zuoyue Li, Guofeng Zhang, Jiashu Zhao, Dawei Yin, Zhao<span class="highlight-author">peng Cui</span>, Hujun Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expanding an existing tourist photo from a partially captured scene to a full
scene is one of the desired experiences for photography applications. Although
photo extrapolation has been well studied, it is much more challenging to
extrapolate a photo (i.e., selfie) from a narrow field of view to a wider one
while maintaining a similar visual style. In this paper, we propose a
factorized neural re-rendering model to produce photorealistic novel views from
cluttered outdoor Internet photo collections, which enables the applications
including controllable scene re-rendering, photo extrapolation and even
extrapolated 3D photo generation. Specifically, we first develop a novel
factorized re-rendering pipeline to handle the ambiguity in the decomposition
of geometry, appearance and illumination. We also propose a composited training
strategy to tackle the unexpected occlusion in Internet images. Moreover, to
enhance photo-realism when extrapolating tourist photographs, we propose a
novel realism augmentation process to complement appearance details, which
automatically propagates the texture details from a narrow captured photo to
the extrapolated neural rendered image. The experiments and photo editing
examples on outdoor scenes demonstrate the superior performance of our proposed
method in both photo-realism and downstream applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2022. Project Page:
  https://zju3dv.github.io/neural_outdoor_rerender/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E2FIF: Push the limit of Binarized Deep Imagery Super-resolution using
  End-to-end Full-precision Information Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Lang, Lei Zhang, Wei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary neural network (BNN) provides a promising solution to deploy
parameter-intensive deep single image super-resolution (SISR) models onto real
devices with limited storage and computational resources. To achieve comparable
performance with the full-precision counterpart, most existing BNNs for SISR
mainly focus on compensating the information loss incurred by binarizing
weights and activations in the network through better approximations to the
binarized convolution. In this study, we revisit the difference between BNNs
and their full-precision counterparts and argue that the key for good
generalization performance of BNNs lies on preserving a complete full-precision
information flow as well as an accurate gradient flow passing through each
binarized convolution layer. Inspired by this, we propose to introduce a
full-precision skip connection or its variant over each binarized convolution
layer across the entire network, which can increase the forward expressive
capability and the accuracy of back-propagated gradient, thus enhancing the
generalization performance. More importantly, such a scheme is applicable to
any existing BNN backbones for SISR without introducing any additional
computation cost. To testify its efficacy, we evaluate it using four different
backbones for SISR on four benchmark datasets and report obviously superior
performance over existing BNNs and even some 4-bit competitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Simple but Stronge baseline for binarized SR networks. Code is
  available at https://github.com/pppLang/E2FIF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen
  Neural Networks <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uddeshya Upadhyay, Shyamgopal Karthik, Yanbei Chen, Massimiliano Mancini, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality calibrated uncertainty estimates are crucial for numerous
real-world applications, especially for deep learning-based deployed ML
systems. While Bayesian deep learning techniques allow uncertainty estimation,
training them with large-scale datasets is an expensive process that does not
always yield models competitive with non-Bayesian counterparts. Moreover, many
of the high-performing deep learning models that are already trained and
deployed are non-Bayesian in nature and do not provide uncertainty estimates.
To address these issues, we propose BayesCap that learns a Bayesian identity
mapping for the frozen model, allowing uncertainty estimation. BayesCap is a
memory-efficient method that can be trained on a small fraction of the original
dataset, enhancing pretrained non-Bayesian computer vision models by providing
calibrated uncertainty estimates for the predictions without (i) hampering the
performance of the model and (ii) the need for expensive retraining the model
from scratch. The proposed method is agnostic to various architectures and
tasks. We show the efficacy of our method on a wide variety of tasks with a
diverse set of architectures, including image super-resolution, deblurring,
inpainting, and crucial application such as medical image translation.
Moreover, we apply the derived uncertainty estimates to detect
out-of-distribution samples in critical scenarios like depth estimation in
autonomous driving. Code is available at
https://github.com/ExplainableML/BayesCap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022. Code is available at
  https://github.com/ExplainableML/BayesCap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immunofluorescence Capillary Imaging Segmentation: Cases Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runpeng Hou, Ziyuan Ye, Chengyu Yang, Linhao Fu, Chao Liu, Quanying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonunion is one of the challenges faced by orthopedics clinics for the
technical difficulties and high costs in photographing interosseous
capillaries. Segmenting vessels and filling capillaries are critical in
understanding the obstacles encountered in capillary growth. However, existing
datasets for blood vessel segmentation mainly focus on the large blood vessels
of the body, and the lack of labeled capillary image datasets greatly limits
the methodological development and applications of vessel segmentation and
capillary filling. Here, we present a benchmark dataset, named IFCIS-155,
consisting of 155 2D capillary images with segmentation boundaries and vessel
fillings annotated by biomedical experts, and 19 large-scale, high-resolution
3D capillary images. To obtain better images of interosseous capillaries, we
leverage state-of-the-art immunofluorescence imaging techniques to highlight
the rich vascular morphology of interosseous capillaries. We conduct
comprehensive experiments to verify the effectiveness of the dataset and the
benchmarking deep learning models (\eg UNet/UNet++ and the modified
UNet/UNet++). Our work offers a benchmark dataset for training deep learning
models for capillary image segmentation and provides a potential tool for
future capillary research. The IFCIS-155 dataset and code are all publicly
available at \url{https://github.com/ncclabsustech/IFCIS-55}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIParsing: Anchor-free Instance-level Human Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanyi Zhang, Xiaochun Cao, Guo-Jun Qi, Zhanjie Song, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most state-of-the-art instance-level human parsing models adopt two-stage
anchor-based detectors and, therefore, cannot avoid the heuristic anchor box
design and the lack of analysis on a pixel level. To address these two issues,
we have designed an instance-level human parsing network which is anchor-free
and solvable on a pixel level. It consists of two simple sub-networks: an
anchor-free detection head for bounding box predictions and an edge-guided
parsing head for human segmentation. The anchor-free detector head inherits the
pixel-like merits and effectively avoids the sensitivity of hyper-parameters as
proved in object detection applications. By introducing the part-aware boundary
clue, the edge-guided parsing head is capable to distinguish adjacent human
parts from among each other up to 58 parts in a single human instance, even
overlapping instances. Meanwhile, a refinement head integrating box-level score
and part-level parsing quality is exploited to improve the quality of the
parsing results. Experiments on two multiple human parsing datasets (i.e., CIHP
and LV-MHP-v2.0) and one video instance-level human parsing dataset (i.e., VIP)
show that our method achieves the best global-level and instance-level
performance over state-of-the-art one-stage top-down alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Image Processing (TIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Dictionary Learning with An Intra-class Constraint <span class="chip">ICME2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xia Yuan, Jianping Gou, Baosheng Yu, Jiali Yu, Zhang Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep dictionary learning (DDL)has attracted a great amount
of attention due to its effectiveness for representation learning and visual
recognition.~However, most existing methods focus on unsupervised deep
dictionary learning, failing to further explore the category information.~To
make full use of the category information of different samples, we propose a
novel deep dictionary learning model with an intra-class constraint (DDLIC) for
visual classification. Specifically, we design the intra-class compactness
constraint on the intermediate representation at different levels to encourage
the intra-class representations to be closer to each other, and eventually the
learned representation becomes more discriminative.~Unlike the traditional DDL
methods, during the classification stage, our DDLIC performs a layer-wise
greedy optimization in a similar way to the training stage. Experimental
results on four image datasets show that our method is superior to the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 2 tables. It has been accepted in ICME2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enforcing connectivity of 3D linear structures using their 2D
  projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doruk Oner, Hussein Osman, Mateusz Kozinski, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many biological and medical tasks require the delineation of 3D curvilinear
structures such as blood vessels and neurites from image volumes. This is
typically done using neural networks trained by minimizing voxel-wise loss
functions that do not capture the topological properties of these structures.
As a result, the connectivity of the recovered structures is often wrong, which
lessens their usefulness. In this paper, we propose to improve the 3D
connectivity of our results by minimizing a sum of topology-aware losses on
their 2D projections. This suffices to increase the accuracy and to reduce the
annotation effort required to provide the required annotated training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iColoriT: Towards Propagating Local Hint to the Right Region in
  Interactive Colorization by Leveraging Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyeon Lee, Jooyeol Yun, Minho Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-interactive image colorization aims to colorize grayscale images when a
user provides the colors for specific locations. It is essential for
point-interactive colorization methods to appropriately propagate user-provided
colors (i.e., user hints) in the entire image to obtain a reasonably colorized
image with minimal user effort. However, existing approaches often produce
partially colorized results due to the inefficient design of stacking
convolutional layers to propagate hints to distant relevant regions. To address
this problem, we present iColoriT, a novel point-interactive colorization
Vision Transformer capable of propagating user hints to relevant regions,
leveraging the global receptive field of Transformers. The self-attention
mechanism of Transformers enables iColoriT to selectively colorize relevant
regions with only a few local hints. Our approach colorizes images in real-time
by utilizing pixel shuffling, an efficient upsampling technique that replaces
the decoder architecture. Also, in order to mitigate the artifacts caused by
pixel shuffling with large upsampling ratios, we present the local stabilizing
layer. Extensive quantitative and qualitative results demonstrate that our
approach highly outperforms existing methods for point-interactive
colorization, producing accurately colorized images with a user's minimal
effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-based Tremor Classification for Parkinson's Disease Diagnosis from
  Video <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozheng Zhang, Edmond S. L. Ho, Xiatian Zhang, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parkinson's disease (PD) is a progressive neurodegenerative disorder that
results in a variety of motor dysfunction symptoms, including tremors,
bradykinesia, rigidity and postural instability. The diagnosis of PD mainly
relies on clinical experience rather than a definite medical test, and the
diagnostic accuracy is only about 73-84% since it is challenged by the
subjective opinions or experiences of different medical experts. Therefore, an
efficient and interpretable automatic PD diagnosis system is valuable for
supporting clinicians with more robust diagnostic decision-making. To this end,
we propose to classify Parkinson's tremor since it is one of the most
predominant symptoms of PD with strong generalizability. Different from other
computer-aided time and resource-consuming Parkinson's Tremor (PT)
classification systems that rely on wearable sensors, we propose SPAPNet, which
only requires consumer-grade non-intrusive video recording of camera-facing
human movements as input to provide undiagnosed patients with low-cost PT
classification results as a PD warning sign. For the first time, we propose to
use a novel attention module with a lightweight pyramidal
channel-squeezing-fusion architecture to extract relevant PT information and
filter the noise efficiently. This design aids in improving both classification
performance and system interpretability. Experimental results show that our
system outperforms state-of-the-arts by achieving a balanced accuracy of 90.9%
and an F1-score of 90.6% in classifying PT with the non-PT class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-to-Box Network for Accurate Object Detection via Single Point
  Supervision <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Chen, Xuehui Yu, Xumeng Han, Najmul Hassan, Kai Wang, Jiachen Li, Jian Zhao, Humphrey Shi, Zhenjun Han, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection using single point supervision has received increasing
attention over the years. In this paper, we attribute such a large performance
gap to the failure of generating high-quality proposal bags which are crucial
for multiple instance learning (MIL). To address this problem, we introduce a
lightweight alternative to the off-the-shelf proposal (OTSP) method and thereby
create the Point-to-Box Network (P2BNet), which can construct an inter-objects
balanced proposal bag by generating proposals in an anchor-like way. By fully
investigating the accurate position information, P2BNet further constructs an
instance-level bag, avoiding the mixture of multiple objects. Finally, a
coarse-to-fine policy in a cascade fashion is utilized to improve the IoU
between proposals and ground-truth (GT). Benefiting from these strategies,
P2BNet is able to produce high-quality instance-level bags for object
detection. P2BNet improves the mean average precision (AP) by more than 50%
relative to the previous best PSOD method on the MS COCO dataset. It also
demonstrates the great potential to bridge the performance gap between point
supervised and bounding-box supervised detectors. The code will be released at
github.com/ucas-vg/P2BNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refign: Align and Refine for Adaptation of Semantic Segmentation to
  Adverse Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Bruggemann, Christos Sakaridis, Prune Truong, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the scarcity of dense pixel-level semantic annotations for images
recorded in adverse visual conditions, there has been a keen interest in
unsupervised domain adaptation (UDA) for the semantic segmentation of such
images. UDA adapts models trained on normal conditions to the target
adverse-condition domains. Meanwhile, multiple datasets with driving scenes
provide corresponding images of the same scenes across multiple conditions,
which can serve as a form of weak supervision for domain adaptation. We propose
Refign, a generic extension to self-training-based UDA methods which leverages
these cross-domain correspondences. Refign consists of two steps: (1) aligning
the normal-condition image to the corresponding adverse-condition image using
an uncertainty-aware dense matching network, and (2) refining the adverse
prediction with the normal prediction using an adaptive label correction
mechanism. We design custom modules to streamline both steps and set the new
state of the art for domain-adaptive semantic segmentation on several
adverse-condition benchmarks, including ACDC and Dark Zurich. The approach
introduces no extra training parameters, minimal computational overhead --
during training only -- and can be used as a drop-in extension to improve any
given self-training-based UDA method. Code is available at
https://github.com/brdav/refign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEXTER: An end-to-end system to extract table contents from electronic
  medical health documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandhinee PR, Harinath Krishnamoorthy, Anil Goyal, Sudarsun Santhiappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose DEXTER, an end to end system to extract information
from tables present in medical health documents, such as electronic health
records (EHR) and explanation of benefits (EOB). DEXTER consists of four
sub-system stages: i) table detection ii) table type classification iii) cell
detection; and iv) cell content extraction. We propose a two-stage transfer
learning-based approach using CDeC-Net architecture along with Non-Maximal
suppression for table detection. We design a conventional computer vision-based
approach for table type classification and cell detection using parameterized
kernels based on image size for detecting rows and columns. Finally, we extract
the text from the detected cells using pre-existing OCR engine Tessaract. To
evaluate our system, we manually annotated a sample of the real-world medical
dataset (referred to as Meddata) consisting of wide variations of documents (in
terms of appearance) covering different table structures, such as bordered,
partially bordered, borderless, or coloured tables. We experimentally show that
DEXTER outperforms the commercially available Amazon Textract and Microsoft
Azure Form Recognizer systems on the annotated real-world medical dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingping Dong, Ling Shao, Shengcai Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing few-shot learning (FSL) methods require a large amount of
labeled data in meta-training, which is a major limit. To reduce the
requirement of labels, a semi-supervised meta-training setting has been
proposed for FSL, which includes only a few labeled samples and numbers of
unlabeled samples in base classes. However, existing methods under this setting
require class-aware sample selection from the unlabeled set, which violates the
assumption of unlabeled set. In this paper, we propose a practical
semi-supervised meta-training setting with truly unlabeled data. Under the new
setting, the performance of existing methods drops notably. To better utilize
both the labeled and truly unlabeled data, we propose a simple and effective
meta-training framework, called pseudo-labeling based on meta-learning (PLML).
Firstly, we train a classifier via common semi-supervised learning (SSL) and
use it to obtain the pseudo-labels of unlabeled data. Then we build few-shot
tasks from labeled and pseudo-labeled data and run meta-learning over the
constructed tasks to learn the FSL model. Surprisingly, through extensive
experiments across two FSL datasets, we find that this simple meta-training
framework effectively prevents the performance degradation of FSL under limited
labeled data. Besides, benefiting from meta-training, the proposed method
improves the classifiers learned by two representative SSL algorithms as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Multi-Modal</span>ity Ovarian Tumor Ultrasound Image <span class="highlight-title">Dataset</span> for Unsupervised
  Cross-Domain Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhao, Shuchang Lyu, Wenpei Bai, Linghan Cai, Binghao Liu, Meijing Wu, Xiubo Sang, Min Yang, Lijiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ovarian cancer is one of the most harmful gynecological diseases. Detecting
ovarian tumors in early stage with computer-aided techniques can efficiently
decrease the mortality rate. With the improvement of medical treatment
standard, ultrasound images are widely applied in clinical treatment. However,
recent notable methods mainly focus on single-modality ultrasound ovarian tumor
segmentation or recognition, which means there still lacks of researches on
exploring the representation capability of multi-modality ultrasound ovarian
tumor images. To solve this problem, we propose a Multi-Modality Ovarian Tumor
Ultrasound (MMOTU) image dataset containing 1469 2d ultrasound images and 170
contrast enhanced ultrasonography (CEUS) images with pixel-wise and global-wise
annotations. Based on MMOTU, we mainly focus on unsupervised cross-domain
semantic segmentation task. To solve the domain shift problem, we propose a
feature alignment based architecture named Dual-Scheme Domain-Selected Network
(DS$^2$Net). Specifically, we first design source-encoder and target-encoder to
extract two-style features of source and target images. Then, we propose
Domain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module
(DUSM) to extract the distinct and universal features in two styles
(source-style or target-style). Finally, we fuse these two kinds of features
and feed them into the source-decoder and target-decoder to generate final
predictions. Extensive comparison experiments and analysis on MMOTU image
dataset show that DS$^2$Net can boost the segmentation performance for
bidirectional cross-domain adaptation of 2d ultrasound images and CEUS images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code: https://github.com/cv516Buaa/MMOTU_DS2Net; paper:10 pages, 8
  figures, 9 tables, 15 formulas</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural apparent BRDF fields for multiview photometric stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meghna Asthana, William A. P. Smith, Patrik Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Inertial Hallucinations -- When Wearable Inertial Devices Start Seeing
  Things 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Masullo, Toby Perrett, Tilo Burghardt, Ian Craddock, <span class="highlight-author">Dima Damen</span>, Majid Mirmehdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to multimodal sensor fusion for Ambient Assisted
Living (AAL) which takes advantage of learning using privileged information
(LUPI). We address two major shortcomings of standard multimodal approaches,
limited area coverage and reduced reliability. Our new framework fuses the
concept of modality hallucination with triplet learning to train a model with
different modalities to handle missing sensors at inference time. We evaluate
the proposed model on inertial data from a wearable accelerometer device, using
RGB videos and skeletons as privileged modalities, and show an improvement of
accuracy of an average 6.6% on the UTD-MHAD dataset and an average 5.5% on the
Berkeley MHAD dataset, reaching a new state-of-the-art for inertial-only
classification accuracy on these datasets. We validate our framework through
several ablation studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Evaluation of Four Off-the-Shelf Proprietary
  Visual-Inertial Odometry Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungha Kim, Minkyeong Song, Yeoeun Lee, Moonkyeong Jung, Pyojin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commercial visual-inertial odometry (VIO) systems have been gaining attention
as cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion
tracking methods for estimating accurate and consistent camera pose data, in
addition to their ability to operate without external localization from motion
capture or global positioning systems. It is unclear from existing results,
however, which commercial VIO platforms are the most stable, consistent, and
accurate in terms of state estimation for indoor and outdoor robotic
applications. We assess four popular proprietary VIO systems (Apple ARKit,
Google ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of
both indoor and outdoor experiments where we show their positioning stability,
consistency, and accuracy. We present our complete results as a benchmark
comparison for the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted, under review paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoSegNet: Point Cloud Semantic Segmentation via Geometric
  Encoder-Decoder Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Yisen Wang, Honghua Chen, Xuefeng Yan, Dayong Ren, Yanwen Guo, Haoran Xie, Fu Lee Wang, Mingqiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of point clouds, aiming to assign each point a semantic
category, is critical to 3D scene understanding.Despite of significant advances
in recent years, most of existing methods still suffer from either the
object-level misclassification or the boundary-level ambiguity. In this paper,
we present a robust semantic segmentation network by deeply exploring the
geometry of point clouds, dubbed GeoSegNet. Our GeoSegNet consists of a
multi-geometry based encoder and a boundary-guided decoder. In the encoder, we
develop a new residual geometry module from multi-geometry perspectives to
extract object-level features. In the decoder, we introduce a contrastive
boundary learning module to enhance the geometric representation of boundary
points. Benefiting from the geometric encoder-decoder modeling, our GeoSegNet
can infer the segmentation of objects effectively while making the
intersections (boundaries) of two or more objects clear. Experiments show
obvious improvements of our method over its competitors in terms of the overall
segmentation accuracy and object boundary clearness. Code is available at
https://github.com/Chen-yuiyui/GeoSegNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighbor Correspondence Matching for Flow-based Video Frame Synthesis <span class="chip">ACM MM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Jia, Yan Lu, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video frame synthesis, which consists of interpolation and extrapolation, is
an essential video processing technique that can be applied to various
scenarios. However, most existing methods cannot handle small objects or large
motion well, especially in high-resolution videos such as 4K videos. To
eliminate such limitations, we introduce a neighbor correspondence matching
(NCM) algorithm for flow-based frame synthesis. Since the current frame is not
available in video frame synthesis, NCM is performed in a
current-frame-agnostic fashion to establish multi-scale correspondences in the
spatial-temporal neighborhoods of each pixel. Based on the powerful motion
representation capability of NCM, we further propose to estimate intermediate
flows for frame synthesis in a heterogeneous coarse-to-fine scheme.
Specifically, the coarse-scale module is designed to leverage neighbor
correspondences to capture large motion, while the fine-scale module is more
computationally efficient to speed up the estimation process. Both modules are
trained progressively to eliminate the resolution gap between training dataset
and real-world videos. Experimental results show that NCM achieves
state-of-the-art performance on several benchmarks. In addition, NCM can be
applied to various practical scenarios such as video compression to achieve
better performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E2-AEN: End-to-End Incremental Learning with Adaptively Expandable
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimei Cao, Zhanzhan Cheng, Yunlu Xu, Duo Li, Shiliang Pu, Yi Niu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expandable networks have demonstrated their advantages in dealing with
catastrophic forgetting problem in incremental learning. Considering that
different tasks may need different structures, recent methods design dynamic
structures adapted to different tasks via sophisticated skills. Their routine
is to search expandable structures first and then train on the new tasks,
which, however, breaks tasks into multiple training stages, leading to
suboptimal or overmuch computational cost. In this paper, we propose an
end-to-end trainable adaptively expandable network named E2-AEN, which
dynamically generates lightweight structures for new tasks without any accuracy
drop in previous tasks. Specifically, the network contains a serial of powerful
feature adapters for augmenting the previously learned representations to new
tasks, and avoiding task interference. These adapters are controlled via an
adaptive gate-based pruning strategy which decides whether the expanded
structures can be pruned, making the network structure dynamically changeable
according to the complexity of the new tasks. Moreover, we introduce a novel
sparsity-activation regularization to encourage the model to learn
discriminative features with limited parameters. E2-AEN reduces cost and can be
built upon any feed-forward architectures in an end-to-end manner. Extensive
experiments on both classification (i.e., CIFAR and VDD) and detection (i.e.,
COCO, VOC and ICCV2021 SSLAD challenge) benchmarks demonstrate the
effectiveness of the proposed method, which achieves the new remarkable
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Pixel Image Reconstruction Based on Block Compressive Sensing and
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen L. H. Lau, Edwin K. P. Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-pixel imaging (SPI) is a novel imaging technique whose working
principle is based on the compressive sensing (CS) theory. In SPI, data is
obtained through a series of compressive measurements and the corresponding
image is reconstructed. Typically, the reconstruction algorithm such as basis
pursuit relies on the sparsity assumption in images. However, recent advances
in deep learning have found its uses in reconstructing CS images. Despite
showing a promising result in simulations, it is often unclear how such an
algorithm can be implemented in an actual SPI setup. In this paper, we
demonstrate the use of deep learning on the reconstruction of SPI images in
conjunction with block compressive sensing (BCS). We also proposed a novel
reconstruction model based on convolutional neural networks that outperforms
other competitive CS reconstruction algorithms. Besides, by incorporating BCS
in our deep learning model, we were able to reconstruct images of any size
above a certain smallest image size. In addition, we show that our model is
capable of reconstructing images obtained from an SPI setup while being priorly
trained on natural images, which can be vastly different from the SPI images.
This opens up opportunity for the feasibility of pretrained deep learning
models for CS reconstructions of images from various domain areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIE++: Towards End-to-End Information Extraction from Visually Rich
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanzhan Cheng, Peng Zhang, Can Li, Qiao Liang, Yunlu Xu, Pengfei Li, Shiliang Pu, Yi Niu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, automatically extracting information from visually rich documents
(e.g., tickets and resumes) has become a hot and vital research topic due to
its widespread commercial value. Most existing methods divide this task into
two subparts: the text reading part for obtaining the plain text from the
original document images and the information extraction part for extracting key
contents. These methods mainly focus on improving the second, while neglecting
that the two parts are highly correlated. This paper proposes a unified
end-to-end information extraction framework from visually rich documents, where
text reading and information extraction can reinforce each other via a
well-designed multi-modal context block. Specifically, the text reading part
provides multi-modal features like visual, textual and layout features. The
multi-modal context block is developed to fuse the generated multi-modal
features and even the prior knowledge from the pre-trained language model for
better semantic representation. The information extraction part is responsible
for generating key contents with the fused context features. The framework can
be trained in an end-to-end trainable manner, achieving global optimization.
What is more, we define and group visually rich documents into four categories
across two dimensions, the layout and text type. For each document category, we
provide or recommend the corresponding benchmarks, experimental settings and
strong baselines for remedying the problem that this research area lacks the
uniform evaluation standard. Extensive experiments on four kinds of benchmarks
(from fixed layout to variable layout, from full-structured text to
semi-unstructured text) are reported, demonstrating the proposed method's
effectiveness. Data, source code and models are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Vector-Quantization in Visual SLAM using HGCN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Zarringhalam, Saeed Shiry Ghidary, Ali Mohades Khorasani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, two semi-supervised appearance based loop closure detection
technique, HGCN-FABMAP and HGCN-BoW are introduced. Furthermore an extension to
the current state of the art localization SLAM algorithm, ORB-SLAM, is
presented. The proposed HGCN-FABMAP method is implemented in an off-line manner
incorporating Bayesian probabilistic schema for loop detection decision making.
Specifically, we let a Hyperbolic Graph Convolutional Neural Network (HGCN) to
operate over the SURF features graph space, and perform vector quantization
part of the SLAM procedure. This part previously was performed in an
unsupervised manner using algorithms like HKmeans, kmeans++,..etc. The main
Advantage of using HGCN, is that it scales linearly in number of graph edges.
Experimental results shows that HGCN-FABMAP algorithm needs far more cluster
centroids than HGCN-ORB, otherwise it fails to detect loop closures. Therefore
we consider HGCN-ORB to be more efficient in terms of memory consumption, also
we conclude the superiority of HGCN-BoW and HGCN-FABMAP with respect to other
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConCL: Concept <span class="highlight-title">Contrastive Learning</span> for Dense Prediction <span class="highlight-title">Pre-train</span>ing in
  Pathology Images <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Yang, Hanbo Chen, Yuan Liang, Junzhou Huang, Lei He, Jianhua Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detectingandsegmentingobjectswithinwholeslideimagesis essential in
computational pathology workflow. Self-supervised learning (SSL) is appealing
to such annotation-heavy tasks. Despite the extensive benchmarks in natural
images for dense tasks, such studies are, unfortunately, absent in current
works for pathology. Our paper intends to narrow this gap. We first benchmark
representative SSL methods for dense prediction tasks in pathology images.
Then, we propose concept contrastive learning (ConCL), an SSL framework for
dense pre-training. We explore how ConCL performs with concepts provided by
different sources and end up with proposing a simple dependency-free concept
generating method that does not rely on external segmentation algorithms or
saliency detection models. Extensive experiments demonstrate the superiority of
ConCL over previous state-of-the-art SSL methods across different settings.
Along our exploration, we distll several important and intriguing components
contributing to the success of dense pre-training for pathology images. We hope
this work could provide useful data points and encourage the community to
conduct ConCL pre-training for problems of interest. Code is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an ECCV 2022 paper. Code is available at
  https://github.com/Jiawei-Yang/ConCL or
  https://github.com/TencentAILabHealthcare/ConCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octuplet Loss: Make Face Recognition Robust to Image Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Knoche, Mohamed Elkadeem, Stefan Hörmann, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image resolution, or in general, image quality, plays an essential role in
the performance of today's face recognition systems. To address this problem,
we propose a novel combination of the popular triplet loss to improve
robustness against image resolution via fine-tuning of existing face
recognition models. With octuplet loss, we leverage the relationship between
high-resolution images and their synthetically down-sampled variants jointly
with their identity labels. Fine-tuning several state-of-the-art approaches
with our method proves that we can significantly boost performance for
cross-resolution (high-to-low resolution) face verification on various datasets
without meaningfully exacerbating the performance on high-to-high resolution
images. Our method applied on the FaceTransformer network achieves 95.12% face
verification accuracy on the challenging XQLFW dataset while reaching 99.73% on
the LFW database. Moreover, the low-to-low face verification accuracy benefits
from our method. We release our code to allow seamless integration of the
octuplet loss into existing frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHREC 2022 Track on Online Detection of Heterogeneous Gestures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Caputo, Marco Emporio, Andrea Giachetti, Marco Cristani, Guido Borghi, Andrea D'Eusanio, Minh-Quan Le, Hai-Dang Nguyen, Minh-Triet Tran, F. Ambellan, M. Hanik, E. Nava-Yazdani, C. von Tycowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on Computer & Graphics journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DavarOCR: A Toolbox for OCR and <span class="highlight-title">Multi-Modal</span> Document Understanding <span class="chip">ACM MM2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Qiao, Hui Jiang, Ying Chen, Can Li, Pengfei Li, Zaisheng Li, Baorui Zou, Dashan Guo, Yingda Xu, Yunlu Xu, Zhanzhan Cheng, Yi Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents DavarOCR, an open-source toolbox for OCR and document
understanding tasks. DavarOCR currently implements 19 advanced algorithms,
covering 9 different task forms. DavarOCR provides detailed usage instructions
and the trained models for each algorithm. Compared with the previous
opensource OCR toolbox, DavarOCR has relatively more complete support for the
sub-tasks of the cutting-edge technology of document understanding. In order to
promote the development and application of OCR technology in academia and
industry, we pay more attention to the use of modules that different
sub-domains of technology can share. DavarOCR is publicly released at
https://github.com/hikopensource/Davar-Lab-OCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper, Accept by ACM MM2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Low-Resolution <span class="highlight-title">Distillation</span> for Cost-Efficient End-to-End Text
  Spotting <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Chen, Liang Qiao1, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end text spotting has attached great attention recently due to its
benefits on global optimization and high maintainability for real applications.
However, the input scale has always been a tough trade-off since recognizing a
small text instance usually requires enlarging the whole image, which brings
high computational costs. In this paper, to address this problem, we propose a
novel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting
framework, which aims to infer images in different small but recognizable
resolutions and achieve a better balance between accuracy and efficiency.
Concretely, we adopt a resolution selector to dynamically decide the input
resolutions for different images, which is constraint by both inference
accuracy and computational cost. Another sequential knowledge distillation
strategy is conducted on the text recognition branch, making the low-res input
obtains comparable performance to a high-res image. The proposed method can be
optimized end-to-end and adopted in any current text spotting framework to
improve the practicability. Extensive experiments on several text spotting
benchmarks show that the proposed method vastly improves the usability of
low-res models. The code is available at
https://github.com/hikopensource/DAVAR-Lab-OCR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Frequency Distribution Estimation using Graph Neural Networks <span class="chip">KDD 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongren Chen, Xinyue Xu, Shengyi Jiang, Hao Wang, Lu Mi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small subgraphs (graphlets) are important features to describe fundamental
units of a large network. The calculation of the subgraph frequency
distributions has a wide application in multiple domains including biology and
engineering. Unfortunately due to the inherent complexity of this task, most of
the existing methods are computationally intensive and inefficient. In this
work, we propose GNNS, a novel representational learning framework that
utilizes graph neural networks to sample subgraphs efficiently for estimating
their frequency distribution. Our framework includes an inference model and a
generative model that learns hierarchical embeddings of nodes, subgraphs, and
graph types. With the learned model and embeddings, subgraphs are sampled in a
highly scalable and parallel way and the frequency distribution estimation is
then performed based on these sampled subgraphs. Eventually, our methods
achieve comparable accuracy and a significant speedup by three orders of
magnitude compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by KDD 2022 Workshop on Deep Learning on Graphs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Volunteer Cotton Plants in a Corn Field with Deep Learning on
  UAV Remote-Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pappu Kumar Yadav, J. Alex Thomasson, Robert Hardin, Stephen W. Searcy, Ulisses Braga-Neto, Sorin C. Popescu, Daniel E. Martin, Roberto Rodriguez, Karem Meza, Juan Enciso, Jorge Solorzano Diaz, Tianyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cotton boll weevil, Anthonomus grandis Boheman is a serious pest to the
U.S. cotton industry that has cost more than 16 billion USD in damages since it
entered the United States from Mexico in the late 1800s. This pest has been
nearly eradicated; however, southern part of Texas still faces this issue and
is always prone to the pest reinfestation each year due to its sub-tropical
climate where cotton plants can grow year-round. Volunteer cotton (VC) plants
growing in the fields of inter-seasonal crops, like corn, can serve as hosts to
these pests once they reach pin-head square stage (5-6 leaf stage) and
therefore need to be detected, located, and destroyed or sprayed . In this
paper, we present a study to detect VC plants in a corn field using YOLOv3 on
three band aerial images collected by unmanned aircraft system (UAS). The
two-fold objectives of this paper were : (i) to determine whether YOLOv3 can be
used for VC detection in a corn field using RGB (red, green, and blue) aerial
images collected by UAS and (ii) to investigate the behavior of YOLOv3 on
images at three different scales (320 x 320, S1; 416 x 416, S2; and 512 x 512,
S3 pixels) based on average precision (AP), mean average precision (mAP) and
F1-score at 95% confidence level. No significant differences existed for mAP
among the three scales, while a significant difference was found for AP between
S1 and S3 (p = 0.04) and S2 and S3 (p = 0.02). A significant difference was
also found for F1-score between S2 and S3 (p = 0.02). The lack of significant
differences of mAP at all the three scales indicated that the trained YOLOv3
model can be used on a computer vision-based remotely piloted aerial
application system (RPAAS) for VC detection and spray application in near
real-time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Point-to-Plane Registration by Efficient Backpropagation for Error
  Minimizing Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuya Yatagawa, Yutaka Ohtake, Hiromasa Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional algorithms of point set registration minimizing point-to-plane
distances often achieve a better estimation of rigid transformation than those
minimizing point-to-point distances. Nevertheless, recent deep-learning-based
methods minimize the point-to-point distances. In contrast to these methods,
this paper proposes the first deep-learning-based approach to point-to-plane
registration. A challenging part of this problem is that a typical solution for
point-to-plane registration requires an iterative process of accumulating small
transformations obtained by minimizing a linearized energy function. The
iteration significantly increases the size of the computation graph needed for
backpropagation and can slow down both forward and backward network
evaluations. To solve this problem, we consider the estimated rigid
transformation as a function of input point clouds and derive its analytic
gradients using the implicit function theorem. The analytic gradient that we
introduce is independent of how the error minimizing function (i.e., the rigid
transformation) is obtained, thus allowing us to calculate both the rigid
transformation and its gradient efficiently. We implement the proposed
point-to-plane registration module over several previous methods that minimize
point-to-point distances and demonstrate that the extensions outperform the
base methods even with point clouds with noise and low-quality point normals
estimated with local point distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forcing the Whole Video as Background: An Adversarial Learning Strategy
  for Weakly Temporal Action Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Li, Yongxin Ge, Jiaruo Yu, Zhongming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With video-level labels, weakly supervised temporal action localization
(WTAL) applies a localization-by-classification paradigm to detect and classify
the action in untrimmed videos. Due to the characteristic of classification,
class-specific background snippets are inevitably mis-activated to improve the
discriminability of the classifier in WTAL. To alleviate the disturbance of
background, existing methods try to enlarge the discrepancy between action and
background through modeling background snippets with pseudo-snippet-level
annotations, which largely rely on artificial hypotheticals. Distinct from the
previous works, we present an adversarial learning strategy to break the
limitation of mining pseudo background snippets. Concretely, the background
classification loss forces the whole video to be regarded as the background by
a background gradient reinforcement strategy, confusing the recognition model.
Reversely, the foreground(action) loss guides the model to focus on action
snippets under such conditions. As a result, competition between the two
classification losses drives the model to boost its ability for action
modeling. Simultaneously, a novel temporal enhancement network is designed to
facilitate the model to construct temporal relation of affinity snippets based
on the proposed strategy, for further improving the performance of action
localization. Finally, extensive experiments conducted on THUMOS14 and
ActivityNet1.2 demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Adaptive Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaogang Xu, Hengshuang Zhao, Philip Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing automatic data augmentation (DA) methods either ignore updating DA's
parameters according to the target model's state during training or adopt
update strategies that are not effective enough. In this work, we design a
novel data augmentation strategy called "Universal Adaptive Data Augmentation"
(UADA). Different from existing methods, UADA would adaptively update DA's
parameters according to the target model's gradient information during
training: given a pre-defined set of DA operations, we randomly decide types
and magnitudes of DA operations for every data batch during training, and
adaptively update DA's parameters along the gradient direction of the loss
concerning DA's parameters. In this way, UADA can increase the training loss of
the target networks, and the target networks would learn features from harder
samples to improve the generalization. Moreover, UADA is very general and can
be utilized in numerous tasks, e.g., image classification, semantic
segmentation and object detection. Extensive experiments with various models
are conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and
VOC07+12 to prove the significant performance improvements brought by our
proposed adaptive augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploration of an End-to-End Automatic Number-plate Recognition neural
  network for Indian <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Sirisha Nadiminti, Pranav Kant Gaur, Abhilash Bhardwaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indian vehicle number plates have wide variety in terms of size, font, script
and shape. Development of Automatic Number Plate Recognition (ANPR) solutions
is therefore challenging, necessitating a diverse dataset to serve as a
collection of examples. However, a comprehensive dataset of Indian scenario is
missing, thereby, hampering the progress towards publicly available and
reproducible ANPR solutions. Many countries have invested efforts to develop
comprehensive ANPR datasets like Chinese City Parking Dataset (CCPD) for China
and Application-oriented License Plate (AOLP) dataset for US. In this work, we
release an expanding dataset presently consisting of 1.5k images and a scalable
and reproducible procedure of enhancing this dataset towards development of
ANPR solution for Indian conditions. We have leveraged this dataset to explore
an End-to-End (E2E) ANPR architecture for Indian scenario which was originally
proposed for Chinese Vehicle number-plate recognition based on the CCPD
dataset. As we customized the architecture for our dataset, we came across
insights, which we have discussed in this paper. We report the hindrances in
direct reusability of the model provided by the authors of CCPD because of the
extreme diversity in Indian number plates and differences in distribution with
respect to the CCPD dataset. An improvement of 42.86% was observed in LP
detection after aligning the characteristics of Indian dataset with Chinese
dataset. In this work, we have also compared the performance of the E2E
number-plate detection model with YOLOv5 model, pre-trained on COCO dataset and
fine-tuned on Indian vehicle images. Given that the number Indian vehicle
images used for fine-tuning the detection module and yolov5 were same, we
concluded that it is more sample efficient to develop an ANPR solution for
Indian conditions based on COCO dataset rather than CCPD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Contrast Adaptation for Domain Adaptive Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) aims to adapt the model trained on the
labeled source domain to an unlabeled target domain. In this paper, we present
Prototypical Contrast Adaptation (ProCA), a simple and efficient contrastive
learning method for unsupervised domain adaptive semantic segmentation.
Previous domain adaptation methods merely consider the alignment of the
intra-class representational distributions across various domains, while the
inter-class structural relationship is insufficiently explored, resulting in
the aligned representations on the target domain might not be as easily
discriminated as done on the source domain anymore. Instead, ProCA incorporates
inter-class information into class-wise prototypes, and adopts the
class-centered distribution alignment for adaptation. By considering the same
class prototypes as positives and other class prototypes as negatives to
achieve class-centered distribution alignment, ProCA achieves state-of-the-art
performance on classical domain adaptation tasks, {\em i.e., GTA5 $\to$
Cityscapes \text{and} SYNTHIA $\to$ Cityscapes}. Code is available at
\href{https://github.com/jiangzhengkai/ProCA}{ProCA}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Model Uncertainty Estimation via Stochastic Data Centering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayaraman J. Thiagarajan, Rushil Anirudh, Vivek Narayanaswamy, Peer-Timo Bremer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are interested in estimating the uncertainties of deep neural networks,
which play an important role in many scientific and engineering problems. In
this paper, we present a striking new finding that an ensemble of neural
networks with the same weight initialization, trained on datasets that are
shifted by a constant bias gives rise to slightly inconsistent trained models,
where the differences in predictions are a strong indicator of epistemic
uncertainties. Using the neural tangent kernel (NTK), we demonstrate that this
phenomena occurs in part because the NTK is not shift-invariant. Since this is
achieved via a trivial input transformation, we show that it can therefore be
approximated using just a single neural network -- using a technique that we
call $\Delta-$UQ -- that estimates uncertainty around prediction by
marginalizing out the effect of the biases. We show that $\Delta-$UQ's
uncertainty estimates are superior to many of the current methods on a variety
of benchmarks -- outlier rejection, calibration under distribution shift, and
sequential design optimization of black box functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lipschitz Bound Analysis of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarosij Bose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lipschitz Bound Estimation is an effective method of regularizing deep neural
networks to make them robust against adversarial attacks. This is useful in a
variety of applications ranging from reinforcement learning to autonomous
systems. In this paper, we highlight the significant gap in obtaining a
non-trivial Lipschitz bound certificate for Convolutional Neural Networks
(CNNs) and empirically support it with extensive graphical analysis. We also
show that unrolling Convolutional layers or Toeplitz matrices can be employed
to convert Convolutional Neural Networks (CNNs) to a Fully Connected Network.
Further, we propose a simple algorithm to show the existing 20x-50x gap in a
particular data distribution between the actual lipschitz constant and the
obtained tight bound. We also ran sets of thorough experiments on various
network architectures and benchmark them on datasets like MNIST and CIFAR-10.
All these proposals are supported by extensive testing, graphs, histograms and
comparative analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Current Trends in Deep Learning for Earth Observation: An Open-source
  Benchmark Arena for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivica Dimitrovski, Ivan Kitanovski, Dragi Kocev, Nikola Simidjievski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present 'AiTLAS: Benchmark Arena' -- an open-source benchmark framework
for evaluating state-of-the-art deep learning approaches for image
classification in Earth Observation (EO). To this end, we present a
comprehensive comparative analysis of more than 400 models derived from nine
different state-of-the-art architectures, and compare them to a variety of
multi-class and multi-label classification tasks from 22 datasets with
different sizes and properties. In addition to models trained entirely on these
datasets, we also benchmark models trained in the context of transfer learning,
leveraging pre-trained model variants, as it is typically performed in
practice. All presented approaches are general and can be easily extended to
many other remote sensing image classification tasks not considered in this
study. To ensure reproducibility and facilitate better usability and further
developments, all of the experimental resources including the trained models,
model configurations and processing details of the datasets (with their
corresponding splits used for training and evaluating the models) are publicly
available on the repository: https://github.com/biasvariancelabs/aitlas-arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Clustering with <span class="highlight-title">Contrastive Learning</span> and Multi-scale Graph
  Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanku Xu, Dong Huang, Chang-Dong Wang, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep clustering has recently attracted significant attention. Despite the
remarkable progress, most of the previous deep clustering works still suffer
from two limitations. First, many of them focus on some distribution-based
clustering loss, lacking the ability to exploit sample-wise (or
augmentation-wise) relationships via contrastive learning. Second, they often
neglect the indirect sample-wise structure information, overlooking the rich
possibilities of multi-scale neighborhood structure learning. In view of this,
this paper presents a new deep clustering approach termed Image clustering with
contrastive learning and multi-scale Graph Convolutional Networks (IcicleGCN),
which bridges the gap between convolutional neural network (CNN) and graph
convolutional network (GCN) as well as the gap between contrastive learning and
multi-scale neighborhood structure learning for the image clustering task. The
proposed IcicleGCN framework consists of four main modules, namely, the
CNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster
Structure Learning and Instance reconstruction Module (JC-SLIM), and the
Multi-scale GCN module (M-GCN). Specifically, with two random augmentations
performed on each image, the backbone network with two weight-sharing views is
utilized to learn the representations for the augmented samples, which are then
fed to ISM and JC-SLIM for instance-level and cluster-level contrastive
learning, respectively. Further, to enforce multi-scale neighborhood structure
learning, two streams of GCNs and an auto-encoder are simultaneously trained
via (i) the layer-wise interaction with representation fusion and (ii) the
joint self-adaptive learning that ensures their last-layer output distributions
to be consistent. Experiments on multiple image datasets demonstrate the
superior clustering performance of IcicleGCN over the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Organic Priors in Non-Rigid Structure from Motion <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suryansh Kumar, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper advocates the use of organic priors in classical non-rigid
structure from motion (NRSfM). By organic priors, we mean invaluable
intermediate prior information intrinsic to the NRSfM matrix factorization
theory. It is shown that such priors reside in the factorized matrices, and
quite surprisingly, existing methods generally disregard them. The paper's main
contribution is to put forward a simple, methodical, and practical method that
can effectively exploit such organic priors to solve NRSfM. The proposed method
does not make assumptions other than the popular one on the low-rank shape and
offers a reliable solution to NRSfM under orthographic projection. Our work
reveals that the accessibility of organic priors is independent of the camera
motion and shape deformation type. Besides that, the paper provides insights
into the NRSfM factorization -- both in terms of shape and motion -- and is the
first approach to show the benefit of single rotation averaging for NRSfM.
Furthermore, we outline how to effectively recover motion and non-rigid 3D
shape using the proposed organic prior based approach and demonstrate results
that outperform prior-free NRSfM performance by a significant margin. Finally,
we present the benefits of our method via extensive experiments and evaluations
on several benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV 2022 Conference (Oral Presentation). Draft info: 18
  Pages, 4 Figures, and 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surface Normal Estimation of Tilted Images via Spatial Rectifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.09264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.09264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tien Do, Khiem Vuong, Stergios I. Roumeliotis, Hyun Soo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a spatial rectifier to estimate surface normals of
tilted images. Tilted images are of particular interest as more visual data are
captured by arbitrarily oriented sensors such as body-/robot-mounted cameras.
Existing approaches exhibit bounded performance on predicting surface normals
because they were trained using gravity-aligned images. Our two main hypotheses
are: (1) visual scene layout is indicative of the gravity direction; and (2)
not all surfaces are equally represented by a learned estimator due to the
structured distribution of the training data, thus, there exists a
transformation for each tilted image that is more responsive to the learned
estimator than others. We design a spatial rectifier that is learned to
transform the surface normal distribution of a tilted image to the rectified
one that matches the gravity-aligned training data distribution. Along with the
spatial rectifier, we propose a novel truncated angular loss that offers a
stronger gradient at smaller angular errors and robustness to outliers. The
resulting estimator outperforms the state-of-the-art methods including data
augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset
called Tilt-RGBD that includes considerable roll and pitch camera motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in the European Conference on Computer Vision 2020. This
  version fixes a typo on the L2 loss function</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAR: Class-aware Regularizations for Semantic Segmentation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Huang, Di Kang, Liang Chen, Xuefei Zhe, Wenjing Jia, Xiangjian He, Linchao Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent segmentation methods, such as OCR and CPNet, utilizing "class level"
information in addition to pixel features, have achieved notable success for
boosting the accuracy of existing network modules. However, the extracted
class-level information was simply concatenated to pixel features, without
explicitly being exploited for better pixel representation learning. Moreover,
these approaches learn soft class centers based on coarse mask prediction,
which is prone to error accumulation. In this paper, aiming to use class level
information more effectively, we propose a universal Class-Aware Regularization
(CAR) approach to optimize the intra-class variance and inter-class distance
during feature learning, motivated by the fact that humans can recognize an
object by itself no matter which other objects it appears with. Three novel
loss functions are proposed. The first loss function encourages more compact
class representations within each class, the second directly maximizes the
distance between different class centers, and the third further pushes the
distance between inter-class centers and pixels. Furthermore, the class center
in our approach is directly generated from ground truth instead of from the
error-prone coarse prediction. Our method can be easily applied to most
existing segmentation models during training, including OCR and CPNet, and can
largely improve their accuracy at no additional inference overhead. Extensive
experiments and ablation studies conducted on multiple benchmark datasets
demonstrate that the proposed CAR can boost the accuracy of all baseline models
by up to 2.23% mIOU with superior generalization ability. The complete code is
available at https://github.com/edwardyehuang/CAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022 camera ready. Codes and models are available at
  https://github.com/edwardyehuang/CAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive</span> <span class="highlight-title">Pretrain</span>ing for Echocardiography Segmentation with Limited
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Saeed, Rand Muhtaseb, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has proven useful in many applications where access to
labelled data is limited. The lack of annotated data is particularly
problematic in medical image segmentation as it is difficult to have clinical
experts manually annotate large volumes of data such as cardiac structures in
ultrasound images of the heart. In this paper, We propose a self supervised
contrastive learning method to segment the left ventricle from echocardiography
when limited annotated images exist. Furthermore, we study the effect of
contrastive pretraining on two well-known segmentation networks, UNet and
DeepLabV3. Our results show that contrastive pretraining helps improve the
performance on left ventricle segmentation, particularly when annotated data is
scarce. We show how to achieve comparable results to state-of-the-art fully
supervised algorithms when we train our models in a self-supervised fashion
followed by fine-tuning on just 5\% of the data. We show that our solution
outperforms what is currently published on a large public dataset
(EchoNet-Dynamic) achieving a Dice score of 0.9252. We also compare the
performance of our solution on another smaller dataset (CAMUS) to demonstrate
the generalizability of our proposed solution. The code is available at
(https://github.com/BioMedIA-MBZUAI/contrastive-echo).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOCUS: Familiar Objects in Common and Uncommon Settings <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyatham Kattakinda, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard training datasets for deep learning often contain objects in common
settings (e.g., "a horse on grass" or "a ship in water") since they are usually
collected by randomly scraping the web. Uncommon and rare settings (e.g., "a
plane on water", "a car in snowy weather") are thus severely under-represented
in the training data. This can lead to an undesirable bias in model predictions
towards common settings and create a false sense of accuracy. In this paper, we
introduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset
for stress-testing the generalization power of deep image classifiers. By
leveraging the power of modern search engines, we deliberately gather data
containing objects in common and uncommon settings in a wide range of
locations, weather conditions, and time of day. We present a detailed analysis
of the performance of various popular image classifiers on our dataset and
demonstrate a clear drop in performance when classifying images in uncommon
settings. By analyzing deep features of these models, we show that such errors
can be due to the use of spurious features in model predictions. We believe
that our dataset will aid researchers in understanding the inability of deep
models to generalize well to uncommon settings and drive future work on
improving their distributional robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 14 figures, 4 tables. Accepted to ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature robustness and sex differences in medical imaging: a case study
  in MRI-based Alzheimer's disease detection <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eike Petersen, Aasa Feragen, Maria Luise da Costa Zemsch, Anders Henriksen, Oskar Eiler Wiese Christensen, Melanie Ganz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks have enabled significant improvements in
medical image-based diagnosis. It is, however, increasingly clear that these
models are susceptible to performance degradation when facing spurious
correlations and dataset shift, leading, e.g., to underperformance on
underrepresented patient groups. In this paper, we compare two classification
schemes on the ADNI MRI dataset: a simple logistic regression model using
manually selected volumetric features, and a convolutional neural network
trained on 3D MRI data. We assess the robustness of the trained models in the
face of varying dataset splits, training set sex composition, and stage of
disease. In contrast to earlier work in other imaging modalities, we do not
observe a clear pattern of improved model performance for the majority group in
the training dataset. Instead, while logistic regression is fully robust to
dataset composition, we find that CNN performance is generally improved for
both male and female subjects when including more female subjects in the
training dataset. We hypothesize that this might be due to inherent differences
in the pathology of the two sexes. Moreover, in our analysis, the logistic
regression model outperforms the 3D CNN, emphasizing the utility of manual
feature specification based on prior knowledge, and the need for more robust
automatic feature selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose2Room: Understanding 3D Scenes from Human Activities <span class="chip">ECCV'2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinyu Nie, Angela Dai, Xiaoguang Han, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With wearable IMU sensors, one can estimate human poses from wearable devices
without requiring visual input~\cite{von2017sparse}. In this work, we pose the
question: Can we reason about object structure in real-world environments
solely from human trajectory information? Crucially, we observe that human
motion and interactions tend to give strong information about the objects in a
scene -- for instance a person sitting indicates the likely presence of a chair
or sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of
the objects in a scene characterized by their class categories and oriented 3D
bounding boxes, based on an input observed human trajectory in the environment.
P2R-Net models the probability distribution of object class as well as a deep
Gaussian mixture model for object boxes, enabling sampling of multiple,
diverse, likely modes of object configurations from an observed human
trajectory. In our experiments we show that P2R-Net can effectively learn
multi-modal distributions of likely objects for human motions, and produce a
variety of plausible object structures of the environment, even without any
visual information. The results demonstrate that P2R-Net consistently
outperforms the baselines on the PROX dataset and the VirtualHome platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV'2022; Project page:
  https://yinyunie.github.io/pose2room-page/ Video:
  https://www.youtube.com/watch?v=MFfKTcvbM5o</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Animatable Implicit Neural Representations for Creating Realistic
  Avatars from Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce a pose-driven deformation field based on the linear blend skinning
algorithm, which combines the blend weight field and the 3D human skeleton to
produce observation-to-canonical correspondences. Since 3D human skeletons are
more observable, they can regularize the learning of the deformation field.
Moreover, the pose-driven deformation field can be controlled by input skeletal
motions to generate new deformation fields to animate the canonical human
model. Experiments show that our approach significantly outperforms recent
human modeling methods. The code is available at
https://zju3dv.github.io/animatable_nerf/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/animatable_nerf/ and
  https://zju3dv.github.io/animatable_sdf/. arXiv admin note: substantial text
  overlap with arXiv:2105.02872</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Susceptibility of Continual Learning Against Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikmat Khan, Pir Masoom Shah, Syed Farhan Alam Zaidi, Saif ul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider all three scenarios (i.e, task-incremental leaning, domain-incremental
learning and class-incremental learning) of continual learning and explore
three regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations, we identify potential
limitations in continual learning approaches against adversarial attacks. Our
empirical study recommends that the research community consider the robustness
of the proposed continual learning approaches and invest extensive efforts in
mitigating catastrophic forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy,
  Uncertainty, and Robustness <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.12639v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.12639v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namuk Park, Songkuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network ensembles, such as Bayesian neural networks (BNNs), have shown
success in the areas of uncertainty estimation and robustness. However, a
crucial challenge prohibits their use in practice. BNNs require a large number
of predictions to produce reliable results, leading to a significant increase
in computational cost. To alleviate this issue, we propose spatial smoothing, a
method that spatially ensembles neighboring feature map points of convolutional
neural networks. By simply adding a few blur layers to the models, we
empirically show that spatial smoothing improves accuracy, uncertainty
estimation, and robustness of BNNs across a whole range of ensemble sizes. In
particular, BNNs incorporating spatial smoothing achieve high predictive
performance merely with a handful of ensembles. Moreover, this method also can
be applied to canonical deterministic neural networks to improve the
performances. A number of evidences suggest that the improvements can be
attributed to the stabilized feature maps and the smoothing of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing them
as special cases of spatial smoothing. These not only enhance accuracy, but
also improve uncertainty estimation and robustness by making the loss landscape
smoother in the same manner as spatial smoothing. The code is available at
https://github.com/xxxnell/spatial-smoothing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Fairness of Visual Attribute Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Hänel, Nishant Kumar, Dmitrij Schlesinger, Mengze Li, Erdem Ünal, Abouzar Eslami, Stefan Gumhold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of deep neural networks for image recognition tasks such as
predicting a smiling face is known to degrade with under-represented classes of
sensitive attributes. We address this problem by introducing fairness-aware
regularization losses based on batch estimates of Demographic Parity, Equalized
Odds, and a novel Intersection-over-Union measure. The experiments performed on
facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma
classification challenge show the effectiveness of our proposed fairness losses
for bias mitigation as they improve model fairness while maintaining high
classification performance. To the best of our knowledge, our work is the first
attempt to incorporate these types of losses in an end-to-end training scheme
for mitigating biases of visual attribute predictors. Our code is available at
https://github.com/nish03/FVAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Makes for Automatic Reconstruction of Pulmonary Segments <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiming Kuang, Li Zhang, Jingyu Li, Hongwei Li, Jiajun Chen, Bo Du, Jiancheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of pulmonary segments plays an important role in surgical
treatment planning of lung cancer, which facilitates preservation of pulmonary
function and helps ensure low recurrence rates. However, automatic
reconstruction of pulmonary segments remains unexplored in the era of deep
learning. In this paper, we investigate what makes for automatic reconstruction
of pulmonary segments. First and foremost, we formulate, clinically and
geometrically, the anatomical definitions of pulmonary segments, and propose
evaluation metrics adhering to these definitions. Second, we propose ImPulSe
(Implicit Pulmonary Segment), a deep implicit surface model designed for
pulmonary segment reconstruction. The automatic reconstruction of pulmonary
segments by ImPulSe is accurate in metrics and visually appealing. Compared
with canonical segmentation methods, ImPulSe outputs continuous predictions of
arbitrary resolutions with higher training efficiency and fewer parameters.
Lastly, we experiment with different network inputs to analyze what matters in
the task of pulmonary segment reconstruction. Our code is available at
https://github.com/M3DV/ImPulSe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Cross-Modal</span> <span class="highlight-title">Transformer</span> GAN: A Brain Structure-Function Deep Fusing
  Framework for Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junren Pan, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal fusion of different types of neuroimaging data has shown great
promise for predicting the progression of Alzheimer's Disease(AD). However,
most existing methods applied in neuroimaging can not efficiently fuse the
functional and structural information from multi-modal neuroimages. In this
work, a novel cross-modal transformer generative adversarial network(CT-GAN) is
proposed to fuse functional information contained in resting-state functional
magnetic resonance imaging (rs-fMRI) and structural information contained in
Diffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match
functional information to structural information efficiently and maximize the
capability of extracting complementary information from rs-fMRI and DTI. By
capturing the deep complementary information between structural features and
functional features, the proposed CT-GAN can detect the AD-related brain
connectivity, which could be used as a bio-marker of AD. Experimental results
show that the proposed model can not only improve classification performance
but also detect the AD-related brain connectivity effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entry-Flipped <span class="highlight-title">Transformer</span> for Inference and Prediction of Participant
  Behavior <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Hu, Tat-Jen Cham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some group activities, such as team sports and choreographed dances, involve
closely coupled interaction between participants. Here we investigate the tasks
of inferring and predicting participant behavior, in terms of motion paths and
actions, under such conditions. We narrow the problem to that of estimating how
a set target participants react to the behavior of other observed participants.
Our key idea is to model the spatio-temporal relations among participants in a
manner that is robust to error accumulation during frame-wise inference and
prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),
which models the relations of participants by attention mechanisms on both
spatial and temporal domains. Unlike typical transformers, we tackle the
problem of error accumulation by flipping the order of query, key, and value
entries, to increase the importance and fidelity of observed features in the
current frame. Comparative experiments show that our EF-Transformer achieves
the best performance on a newly-collected tennis doubles dataset, a Ceilidh
dance dataset, and two pedestrian datasets. Furthermore, it is also
demonstrated that our EF-Transformer is better at limiting accumulated errors
and recovering from wrong estimations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes <span class="chip">CVPR 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1904.03848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1904.03848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Zhong, Pan Ji, Jianyuan Wang, Yuchao Dai, Hongdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised deep learning for optical flow computation has achieved
promising results. Most existing deep-net based methods rely on image
brightness consistency and local smoothness constraint to train the networks.
Their performance degrades at regions where repetitive textures or occlusions
occur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical
flow method which incorporates global geometric constraints into network
learning. In particular, we investigate multiple ways of enforcing the epipolar
constraint in flow estimation. To alleviate a "chicken-and-egg" type of problem
encountered in dynamic scenes where multiple motions may be present, we propose
a low-rank constraint as well as a union-of-subspaces constraint for training.
Experimental results on various benchmarking datasets show that our method
achieves competitive performance compared with supervised methods and
outperforms state-of-the-art unsupervised deep-learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative training of robust k-space interpolation networks for improved
  image reconstruction with limited scan specific training samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Dawood, Felix Breuer, Paul R. Burd, István Homolya, Johannes Oberberger, Peter M. Jakob, Martin Blaimer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To evaluate an iterative learning approach for enhanced performance
of Robust Artificial-neural-networks for K-space Interpolation (RAKI), when
only a limited amount of training data (auto-calibration signals, ACS) are
available for accelerated standard 2D imaging. Methods: In a first step, the
RAKI model was optimized for the case of strongly limited training data amount.
In the iterative learning approach (termed iterative RAKI), the optimized RAKI
model is initially trained using original and augmented ACS obtained from a
linear parallel imaging reconstruction. Subsequently, the RAKI convolution
filters are refined iteratively using original and augmented ACS extracted from
the previous RAKI reconstruction. Evaluation was carried out on 200
retrospectively undersampled in-vivo datasets from the fastMRI neuro database
with different contrast settings. Results: For limited training data (18 and 22
ACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard
RAKI by reducing residual artefacts and yields strong noise suppression when
compared to standard parallel imaging, underlined by quantitative
reconstruction quality metrics. In combination with a phase constraint, further
reconstruction improvements can be achieved. Additionally, iterative RAKI shows
better performance than both GRAPPA and RAKI in case of pre-scan calibration
with varying contrast between training- and undersampled data. Conclusion: The
iterative learning approach with RAKI benefits from standard RAKIs well known
noise suppression feature but requires less original training data for the
accurate reconstruction of standard 2D images thereby improving net
acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Magnetic Resonance in Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGB-D Salient Object Detection: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.00230v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.00230v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing Shen, Ling Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient object detection (SOD), which simulates the human visual perception
system to locate the most attractive object(s) in a scene, has been widely
applied to various computer vision tasks. Now, with the advent of depth
sensors, depth maps with affluent spatial information that can be beneficial in
boosting the performance of SOD, can easily be captured. Although various RGB-D
based SOD models with promising performance have been proposed over the past
several years, an in-depth understanding of these models and challenges in this
topic remains lacking. In this paper, we provide a comprehensive survey of
RGB-D based SOD models from various perspectives, and review related benchmark
datasets in detail. Further, considering that the light field can also provide
depth maps, we review SOD models and popular benchmark datasets from this
domain as well. Moreover, to investigate the SOD ability of existing models, we
carry out a comprehensive evaluation, as well as attribute-based evaluation of
several representative RGB-D based SOD models. Finally, we discuss several
challenges and open directions of RGB-D based SOD for future research. All
collected models, benchmark datasets, source code links, datasets constructed
for attribute-based evaluation, and codes for evaluation will be made publicly
available at https://github.com/taozh2017/RGBDSODsurvey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strongly Augmented <span class="highlight-title">Contrastive</span> Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhi Deng, Dong Huang, Ding-Hua Chen, Chang-Dong Wang, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep clustering has attracted increasing attention in recent years due to its
capability of joint representation learning and clustering via deep neural
networks. In its latest developments, the contrastive learning has emerged as
an effective technique to substantially enhance the deep clustering
performance. However, the existing contrastive learning based deep clustering
algorithms mostly focus on some carefully-designed augmentations (often with
limited transformations to preserve the structure), referred to as weak
augmentations, but cannot go beyond the weak augmentations to explore the more
opportunities in stronger augmentations (with more aggressive transformations
or even severe distortions). In this paper, we present an end-to-end deep
clustering approach termed Strongly Augmented Contrastive Clustering (SACC),
which extends the conventional two-augmentation-view paradigm to multiple views
and jointly leverages strong and weak augmentations for strengthened deep
clustering. Particularly, we utilize a backbone network with triply-shared
weights, where a strongly augmented view and two weakly augmented views are
incorporated. Based on the representations produced by the backbone, the
weak-weak view pair and the strong-weak view pairs are simultaneously exploited
for the instance-level contrastive learning (via an instance projector) and the
cluster-level contrastive learning (via a cluster projector), which, together
with the backbone, can be jointly optimized in a purely unsupervised manner.
Experimental results on five challenging image datasets have shown the
superiority of our SACC approach over the state-of-the-art. The code is
available at https://github.com/dengxiaozhi/SACC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing variational generation through self-decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Asperti, Laura Bugo, Daniele Filippini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we introduce the notion of Split Variational Autoencoder
(SVAE), whose output $\hat{x}$ is obtained as a weighted sum $\sigma \odot
\hat{x_1} + (1-\sigma) \odot \hat{x_2}$ of two generated images
$\hat{x_1},\hat{x_2}$, and $\sigma$ is a {\em learned} compositional map. The
composing images $\hat{x_1},\hat{x_2}$, as well as the $\sigma$-map are
automatically synthesized by the model. The network is trained as a usual
Variational Autoencoder with a negative loglikelihood loss between training and
reconstructed images. No additional loss is required for $\hat{x_1},\hat{x_2}$
or $\sigma$, neither any form of human tuning. The decomposition is
nondeterministic, but follows two main schemes, that we may roughly categorize
as either \say{syntactic} or \say{semantic}. In the first case, the map tends
to exploit the strong correlation between adjacent pixels, splitting the image
in two complementary high frequency sub-images. In the second case, the map
typically focuses on the contours of objects, splitting the image in
interesting variations of its content, with more marked and distinctive
features. In this case, according to empirical observations, the Fr\'echet
Inception Distance (FID) of $\hat{x_1}$ and $\hat{x_2}$ is usually lower (hence
better) than that of $\hat{x}$, that clearly suffers from being the average of
the former. In a sense, a SVAE forces the Variational Autoencoder to make
choices, in contrast with its intrinsic tendency to {\em average} between
alternatives with the aim to minimize the reconstruction loss towards a
specific sample. According to the FID metric, our technique, tested on typical
datasets such as Mnist, Cifar10 and CelebA, allows us to outperform all
previous purely variational architectures (not relying on normalization flows).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Behaviour of Vision <span class="highlight-title">Transformer</span>s with Token-consistent
  Stochastic Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15111v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15111v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Popovic, Danda Pani Paudel, Thomas Probst, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce token-consistent stochastic layers in vision transformers,
without causing any severe drop in performance. The added stochasticity
improves network calibration, robustness and strengthens privacy. We use linear
layers with token-consistent stochastic parameters inside the multilayer
perceptron blocks, without altering the architecture of the transformer. The
stochastic parameters are sampled from the uniform distribution, both during
training and inference. The applied linear operations preserve the topological
structure, formed by the set of tokens passing through the shared multilayer
perceptron. This operation encourages the learning of the recognition task to
rely on the topological structures of the tokens, instead of their values,
which in turn offers the desired robustness and privacy of the visual features.
The effectiveness of the token-consistent stochasticity is demonstrated on
three different applications, namely, network calibration, adversarial
robustness, and feature privacy, by boosting the performance of the respective
established baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is under consideration at the Computer Vision and Image
  Understanding journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatially Multi-conditional Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritika Chakraborty, Nikola Popovic, Danda Pani Paudel, Thomas Probst, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In most scenarios, conditional image generation can be thought of as an
inversion of the image understanding process. Since generic image understanding
involves solving multiple tasks, it is natural to aim at generating images via
multi-conditioning. However, multi-conditional image generation is a very
challenging problem due to the heterogeneity and the sparsity of the (in
practice) available conditioning labels. In this work, we propose a novel
neural architecture to address the problem of heterogeneity and sparsity of the
spatially multi-conditional labels. Our choice of spatial conditioning, such as
by semantics and depth, is driven by the promise it holds for better control of
the image generation process. The proposed method uses a transformer-like
architecture operating pixel-wise, which receives the available labels as input
tokens to merge them in a learned homogeneous space of labels. The merged
labels are then used for image generation via conditional generative
adversarial training. In this process, the sparsity of the labels is handled by
simply dropping the input tokens corresponding to the missing labels at the
desired locations, thanks to the proposed pixel-wise operating architecture.
Our experiments on three benchmark datasets demonstrate the clear superiority
of our method over the state-of-the-art and compared baselines. The source code
will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Machines Help Us Answering Question 16 in Datasheets, and In Turn
  Reflecting on Inappropriate Content? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Schramowski, Christopher Tauchmann, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large datasets underlying much of current machine learning raise serious
issues concerning inappropriate content such as offensive, insulting,
threatening, or might otherwise cause anxiety. This calls for increased dataset
documentation, e.g., using datasheets. They, among other topics, encourage to
reflect on the composition of the datasets. So far, this documentation,
however, is done manually and therefore can be tedious and error-prone,
especially for large image datasets. Here we ask the arguably "circular"
question of whether a machine can help us reflect on inappropriate content,
answering Question 16 in Datasheets. To this end, we propose to use the
information stored in pre-trained transformer models to assist us in the
documentation process. Specifically, prompt-tuning based on a dataset of
socio-moral values steers CLIP to identify potentially inappropriate content,
therefore reducing human labor. We then document the inappropriate images found
using word clouds, based on captions generated using a vision-language model.
The documentations of two popular, large-scale computer vision datasets --
ImageNet and OpenImages -- produced this way suggest that machines can indeed
help dataset creators to answer Question 16 on inappropriate image content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2110.04222</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Preserving Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10120v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10120v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Taiello, Melek Önen, Olivier Humbert, Marco Lorenzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image registration is a key task in medical imaging applications, allowing to
represent medical images in a common spatial reference frame. Current
literature on image registration is generally based on the assumption that
images are usually accessible to the researcher, from which the spatial
transformation is subsequently estimated. This common assumption may not be met
in current practical applications, since the sensitive nature of medical images
may ultimately require their analysis under privacy constraints, preventing to
share the image content in clear form. In this work, we formulate the problem
of image registration under a privacy preserving regime, where images are
assumed to be confidential and cannot be disclosed in clear. We derive our
privacy preserving image registration framework by extending classical
registration paradigms to account for advanced cryptographic tools, such as
secure multi-party computation and homomorphic encryption, that enable the
execution of operations without leaking the underlying data. To overcome the
problem of performance and scalability of cryptographic tools in high
dimensions, we first propose to optimize the underlying image registration
operations using gradient approximations. We further revisit the use of
homomorphic encryption and use a packing method to allow the encryption and
multiplication of large matrices more efficiently. We demonstrate our privacy
preserving framework in linear and non-linear registration problems, evaluating
its accuracy and scalability with respect to standard image registration. Our
results show that privacy preserving image registration is feasible and can be
adopted in sensitive medical imaging applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radiomics-Guided Global-Local <span class="highlight-title">Transformer</span> for Weakly Supervised
  Pathology Localization in Chest X-Rays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Han, Gregory Holste, Ying Ding, Ahmed Tewfik, Yifan Peng, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Before the recent success of deep learning methods for automated medical
image analysis, practitioners used handcrafted radiomic features to
quantitatively describe local patches of medical images. However, extracting
discriminative radiomic features relies on accurate pathology localization,
which is difficult to acquire in real-world settings. Despite advances in
disease classification and localization from chest X-rays, many approaches fail
to incorporate clinically-informed domain knowledge. For these reasons, we
propose a Radiomics-Guided Transformer (RGT) that fuses \textit{global} image
information with \textit{local} knowledge-guided radiomics information to
provide accurate cardiopulmonary pathology localization and classification
\textit{without any bounding box annotations}. RGT consists of an image
Transformer branch, a radiomics Transformer branch, and fusion layers that
aggregate image and radiomic information. Using the learned self-attention of
its image branch, RGT extracts a bounding box for which to compute radiomic
features, which are further processed by the radiomics branch; learned image
and radiomic features are then fused and mutually interact via cross-attention
layers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap
accurate pathology localization only using image-level disease labels.
Experiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior
works in weakly supervised disease localization (by an average margin of 3.6\%
over various intersection-over-union thresholds) and classification (by 1.1\%
in average area under the receiver operating characteristic curve). We publicly
release our codes and pre-trained models at
\url{https://github.com/VITA-Group/chext}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video
  Retrieval <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Aozhu Chen, Ziyue Wang, Fang<span class="highlight-author">ming Zhou</span>, Jianfeng Dong, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we revisit \emph{feature fusion}, an old-fashioned topic, in
the new context of text-to-video retrieval. Different from previous research
that considers feature fusion only at one end, let it be video or text, we aim
for feature fusion for both ends within a unified framework. We hypothesize
that optimizing the convex combination of the features is preferred to modeling
their correlations by computationally heavy multi-head self attention. We
propose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature
fusion at both early and late stages and at both video and text ends, making it
a powerful method for exploiting diverse (off-the-shelf) features. The
interpretability of LAFF can be used for feature selection. Extensive
experiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and
TRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.07518v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.07518v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai, Chia-Wen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image motion blur usually results from moving objects or camera shakes. Such
blur is generally directional and non-uniform. Previous research efforts
attempt to solve non-uniform blur by using self-recurrent multi-scale or
multi-patch architectures accompanying with self-attention. However, using
self-recurrent frameworks typically leads to a longer inference time, while
inter-pixel or inter-channel self-attention may cause excessive memory usage.
This paper proposes blur-aware attention networks (BANet) that accomplish
accurate and efficient deblurring via a single forward pass. Our BANet utilizes
region-based self-attention with multi-kernel strip pooling to disentangle blur
patterns of different degrees and with cascaded parallel dilated convolution to
aggregate multi-scale content features. Extensive experimental results on the
GoPro and HIDE benchmarks demonstrate that the proposed BANet performs
favorably against the state-of-the-art in blurred image restoration and can
provide deblurred results in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superclass Adversarial Attack <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks have only focused on changing the predictions of the
classifier, but their danger greatly depends on how the class is mistaken. For
example, when an automatic driving system mistakes a Persian cat for a Siamese
cat, it is hardly a problem. However, if it mistakes a cat for a 120km/h
minimum speed sign, serious problems can arise. As a stepping stone to more
threatening adversarial attacks, we consider the superclass adversarial attack,
which causes misclassification of not only fine classes, but also superclasses.
We conducted the first comprehensive analysis of superclass adversarial attacks
(an existing and 19 new methods) in terms of accuracy, speed, and stability,
and identified several strategies to achieve better performance. Although this
study is aimed at superclass misclassification, the findings can be applied to
other problem settings involving multiple classes, such as top-k and
multi-label classification attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML Workshop 2022 on Adversarial Machine Learning Frontiers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of ADHD based on Eye Movements during Natural Viewing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01377v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01377v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuwen Deng, Paul Prasse, David R. Reich, Sabine Dziemian, Maja Stegenwallner-Schütz, Daniel Krakowczyk, Silvia Makowski, Nicolas Langer, Tobias Scheffer, Lena A. Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental
disorder that is highly prevalent and requires clinical specialists to
diagnose. It is known that an individual's viewing behavior, reflected in their
eye movements, is directly related to attentional mechanisms and higher-order
cognitive processes. We therefore explore whether ADHD can be detected based on
recorded eye movements together with information about the video stimulus in a
free-viewing task. To this end, we develop an end-to-end deep learning-based
sequence model which we pre-train on a related task for which more data are
available. We find that the method is in fact able to detect ADHD and
outperforms relevant baselines. We investigate the relevance of the input
features in an ablation study. Interestingly, we find that the model's
performance is closely related to the content of the video, which provides
insights for future experimental designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print for Proceedings of the European Conference on Machine
  Learning, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Quantization Embeddings for Intra-Subject Prostate MR
  Image Registration <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Shen, Qianye Yang, Yuming Shen, Francesco Giganti, Vasilis Stavrinides, Richard Fan, Caroline Moore, Mirabela Rusu, Geoffrey Sonn, Philip Torr, Dean Barratt, Yipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image registration is useful for quantifying morphological changes in
longitudinal MR images from prostate cancer patients. This paper describes a
development in improving the learning-based registration algorithms, for this
challenging clinical application often with highly variable yet limited
training data. First, we report that the latent space can be clustered into a
much lower dimensional space than that commonly found as bottleneck features at
the deep layer of a trained registration network. Based on this observation, we
propose a hierarchical quantization method, discretizing the learned feature
vectors using a jointly-trained dictionary with a constrained size, in order to
improve the generalisation of the registration networks. Furthermore, a novel
collaborative dictionary is independently optimised to incorporate additional
prior information, such as the segmentation of the gland or other regions of
interest, in the latent quantized space. Based on 216 real clinical images from
86 prostate cancer patients, we show the efficacy of both the designed
components. Improved registration accuracy was obtained with statistical
significance, in terms of both Dice on gland and target registration error on
corresponding landmarks, the latter of which achieved 5.46 mm, an improvement
of 28.7\% from the baseline without quantization. Experimental results also
show that the difference in performance was indeed minimised between training
and testing data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint version, accepted for MICCAI 2022 (25th International
  Conference on Medical Image Computing and Computer Assisted Intervention)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning for brain metastasis detection and segmentation in
  longitudinal MRI data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11833v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11833v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Huang, Christoph Bert, Philipp Sommer, Benjamin Frey, Udo Gaipl, Luitpold V. Distel, Thomas Weissmann, Michael Uder, Manuel A. Schmidt, Arnd Dörfler, Andreas Maier, Rainer Fietkau, Florian Putz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain metastases occur frequently in patients with metastatic cancer. Early
and accurate detection of brain metastases is very essential for treatment
planning and prognosis in radiation therapy. To improve brain metastasis
detection performance with deep learning, a custom detection loss called
volume-level sensitivity-specificity (VSS) is proposed, which rates individual
metastasis detection sensitivity and specificity in (sub-)volume levels. As
sensitivity and precision are always a trade-off in a metastasis level, either
a high sensitivity or a high precision can be achieved by adjusting the weights
in the VSS loss without decline in dice score coefficient for segmented
metastases. To reduce metastasis-like structures being detected as false
positive metastases, a temporal prior volume is proposed as an additional input
of DeepMedic. The modified network is called DeepMedic+ for distinction. Our
proposed VSS loss improves the sensitivity of brain metastasis detection for
DeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it
improves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic
with the same VSS loss, 44.4% of the false positive metastases are reduced in
the high sensitivity model and the precision reaches 99.6% for the high
specificity model. The mean dice coefficient for all metastases is about 0.81.
With the ensemble of the high sensitivity and high specificity models, on
average only 1.5 false positive metastases per patient needs further check,
while the majority of true positive metastases are confirmed. The ensemble
learning is able to distinguish high confidence true positive metastases from
metastases candidates that require special expert review or further follow-up,
being particularly well-fit to the requirements of expert support in real
clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wide Neural Networks Forget Less Catastrophically <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11526v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11526v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A primary focus area in continual learning research is alleviating the
"catastrophic forgetting" problem in neural networks by designing new
algorithms that are more robust to the distribution shifts. While the recent
progress in continual learning literature is encouraging, our understanding of
what properties of neural networks contribute to catastrophic forgetting is
still limited. To address this, instead of focusing on continual learning
algorithms, in this work, we focus on the model itself and study the impact of
"width" of the neural network architecture on catastrophic forgetting, and show
that width has a surprisingly significant effect on forgetting. To explain this
effect, we study the learning dynamics of the network from various perspectives
such as gradient orthogonality, sparsity, and lazy training regime. We provide
potential explanations that are consistent with the empirical results across
different architectures and continual learning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Video Text Spotting with <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Wu, Yuanqiang Cai, Chunhua Shen, Debing Zhang, Ying Fu, Hong Zhou, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video text spotting methods usually require the three-staged pipeline,
i.e., detecting text in individual images, recognizing localized text, tracking
text streams with post-processing to generate final results. These methods
typically follow the tracking-by-match paradigm and develop sophisticated
pipelines. In this paper, rooted in Transformer sequence modeling, we propose a
simple, but effective end-to-end video text DEtection, Tracking, and
Recognition framework (TransDETR). TransDETR mainly includes two advantages: 1)
Different from the explicit match paradigm in the adjacent frame, TransDETR
tracks and recognizes each text implicitly by the different query termed text
query over long-range temporal sequence (more than 7 frames). 2) TransDETR is
the first end-to-end trainable video text spotting framework, which
simultaneously addresses the three sub-tasks (e.g., text detection, tracking,
recognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013
Video, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to
demonstrate that TransDETR achieves state-of-the-art performance with up to
around 8.0% improvements on video text spotting tasks. The code of TransDETR
can be found at https://github.com/weijiawu/TransDETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptive Hand Keypoint and Pixel Localization in the Wild <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08344v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08344v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takehiko Ohkawa, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarially-Aware Robust Object Detector <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Dong, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2022 oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Modularity: Towards Understanding the Cross-Layer Transition of
  Feature Representations in Deep Neural Networks <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Lu, Wen Yang, Yunzhe Zhang, Zuohui Chen, Jinyin Chen, Qi Xuan, Zhen Wang, Xiaoniu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are good arguments to support the claim that deep neural networks
(DNNs) capture better feature representations than the previous hand-crafted
feature engineering, which leads to a significant performance improvement. In
this paper, we move a tiny step towards understanding the dynamics of feature
representations over layers. Specifically, we model the process of class
separation of intermediate representations in pre-trained DNNs as the evolution
of communities in dynamic graphs. Then, we introduce modularity, a generic
metric in graph theory, to quantify the evolution of communities. In the
preliminary experiment, we find that modularity roughly tends to increase as
the layer goes deeper and the degradation and plateau arise when the model
complexity is great relative to the dataset. Through an asymptotic analysis, we
prove that modularity can be broadly used for different applications. For
example, modularity provides new insights to quantify the difference between
feature representations. More crucially, we demonstrate that the degradation
and plateau in modularity curves represent redundant layers in DNNs and can be
pruned with minimal impact on performance, which provides theoretical guidance
for layer pruning. Our code is available at
https://github.com/yaolu-zjut/Dynamic-Graphs-Construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Feature Interpolation for Low-Shot Image Generation <span class="chip">ECCV'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyu Dai, Haibin Hang, Xiaoyang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training of generative models especially Generative Adversarial Networks can
easily diverge in low-data setting. To mitigate this issue, we propose a novel
implicit data augmentation approach which facilitates stable training and
synthesize high-quality samples without need of label information.
Specifically, we view the discriminator as a metric embedding of the real data
manifold, which offers proper distances between real data points. We then
utilize information in the feature space to develop a fully unsupervised and
data-driven augmentation method. Experiments on few-shot generation tasks show
the proposed method significantly improve results from strong baselines with
hundreds of training samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'22. Code available at
  https://github.com/dzld00/Adaptive-Feature-Interpolation-for-Low-Shot-Image-Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Data-Free Neural Architecture Search via Recursive Label Calibration <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Zhiqiang Shen, Yun Long, <span class="highlight-author">Eric Xing</span>, Kwang-<span class="highlight-author">Ting Chen</span>g, Chas Leichner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to explore the feasibility of neural architecture search
(NAS) given only a pre-trained model without using any original training data.
This is an important circumstance for privacy protection, bias avoidance, etc.,
in real-world scenarios. To achieve this, we start by synthesizing usable data
through recovering the knowledge from a pre-trained deep neural network. Then
we use the synthesized data and their predicted soft-labels to guide neural
architecture search. We identify that the NAS task requires the synthesized
data (we target at image domain here) with enough semantics, diversity, and a
minimal domain gap from the natural images. For semantics, we propose recursive
label calibration to produce more informative outputs. For diversity, we
propose a regional update strategy to generate more diverse and
semantically-enriched synthetic data. For minimal domain gap, we use input and
feature-level regularization to mimic the original data distribution in latent
space. We instantiate our proposed framework with three popular NAS algorithms:
DARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the
architectures discovered by searching with our synthetic data achieve accuracy
that is comparable to, or even higher than, architectures discovered by
searching from the original ones, for the first time, deriving the conclusion
that NAS can be done effectively with no need of access to the original or
called natural data if the synthesis method is well designed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust <span class="highlight-title">Self-Supervised</span> Audio-Visual Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Shi, Wei-Ning Hsu, Abdelrahman Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-based automatic speech recognition (ASR) degrades significantly in
noisy environments and is particularly vulnerable to interfering speech, as the
model cannot determine which speaker to transcribe. Audio-visual speech
recognition (AVSR) systems improve robustness by complementing the audio stream
with the visual information that is invariant to noise and helps the model
focus on the desired speaker. However, previous AVSR work focused solely on the
supervised learning setup; hence the progress was hindered by the amount of
labeled data available. In this work, we present a self-supervised AVSR
framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art
audio-visual speech representation learning model. On the largest available
AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by
~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in
the presence of babble noise, while reducing the WER of an audio-based model by
over 75% (25.8% vs. 5.8%) on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Lip-Based Audio-Visual Speaker Embeddings with AV-Hu<span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Shi, Abdelrahman Mohamed, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates self-supervised pre-training for audio-visual speaker
representation learning where a visual stream showing the speaker's mouth area
is used alongside speech as inputs. Our study focuses on the Audio-Visual
Hidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose
audio-visual speech pre-training framework. We conducted extensive experiments
probing the effectiveness of pre-training and visual modality. Experimental
results suggest that AV-HuBERT generalizes decently to speaker related
downstream tasks, improving label efficiency by roughly ten fold for both
audio-only and audio-visual speaker verification. We also show that
incorporating visual information, even just the lip area, greatly improves the
performance and noise robustness, reducing EER by 38% in the clean condition
and 75% in noisy conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Human Vision Inspired Action Recognition using Adaptive
  Spatiotemporal Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoi-Nguyen C. Mac, Minh N. Do, Minh P. Vo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive sampling that exploits the spatiotemporal redundancy in videos is
critical for always-on action recognition on wearable devices with limited
computing and battery resources. The commonly used fixed sampling strategy is
not context-aware and may under-sample the visual content, and thus adversely
impacts both computation efficiency and accuracy. Inspired by the concepts of
foveal vision and pre-attentive processing from the human visual perception
mechanism, we introduce a novel adaptive spatiotemporal sampling scheme for
efficient action recognition. Our system pre-scans the global scene context at
low-resolution and decides to skip or request high-resolution features at
salient regions for further processing. We validate the system on EPIC-KITCHENS
and UCF-101 datasets for action recognition, and show that our proposed
approach can greatly speed up inference with a tolerable loss of accuracy
compared with those from state-of-the-art baselines. Source code is available
in https://github.com/knmac/adaptive_spatiotemporal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Diverse Feature Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saachi Jain, Dimitris Tsipras, Aleksander Madry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve model generalization, model designers often restrict the features
that their models use, either implicitly or explicitly. In this work, we
explore the design space of leveraging such feature priors by viewing them as
distinct perspectives on the data. Specifically, we find that models trained
with diverse sets of feature priors have less overlapping failure modes, and
can thus be combined more effectively. Moreover, we demonstrate that jointly
training such models on additional (unlabeled) data allows them to correct each
other's mistakes, which, in turn, leads to better generalization and resilience
to spurious correlations. Code available at
https://github.com/MadryLab/copriors
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Robust Deep Learning using Hardness Weighted Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2001.02658v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2001.02658v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Fidon, Michael Aertsen, Thomas Deprest, Doaa Emam, Frédéric Guffens, Nada Mufti, Esther Van Elslander, Ernst Schwartz, Michael Ebner, Daniela Prayer, Gregor Kasprian, Anna L. David, Andrew Melbourne, Sébastien Ourselin, Jan Deprest, Georg Langs, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limiting failures of machine learning systems is of paramount importance for
safety-critical applications. In order to improve the robustness of machine
learning systems, Distributionally Robust Optimization (DRO) has been proposed
as a generalization of Empirical Risk Minimization (ERM). However, its use in
deep learning has been severely restricted due to the relative inefficiency of
the optimizers available for DRO in comparison to the wide-spread variants of
Stochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with
hardness weighted sampling, a principled and efficient optimization method for
DRO in machine learning that is particularly suited in the context of deep
learning. Similar to a hard example mining strategy in practice, the proposed
algorithm is straightforward to implement and computationally as efficient as
SGD-based optimizers used for deep learning, requiring minimal overhead
computation. In contrast to typical ad hoc hard mining approaches, we prove the
convergence of our DRO algorithm for over-parameterized deep learning networks
with ReLU activation and a finite number of layers and parameters. Our
experiments on fetal brain 3D MRI segmentation and brain tumor segmentation in
MRI demonstrate the feasibility and the usefulness of our approach. Using our
hardness weighted sampling for training a state-of-the-art deep learning
pipeline leads to improved robustness to anatomical variabilities in automatic
fetal brain 3D MRI segmentation using deep learning and to improved robustness
to the image protocol variations in brain tumor segmentation. Our code is
available at https://github.com/LucasFidon/HardnessWeightedSampler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:019.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Batch Norm Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jim Davis, Logan Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch normalization (BN) is comprised of a normalization component followed
by an affine transformation and has become essential for training deep neural
networks. Standard initialization of each BN in a network sets the affine
transformation scale and shift to 1 and 0, respectively. However, after
training we have observed that these parameters do not alter much from their
initialization. Furthermore, we have noticed that the normalization process can
still yield overly large values, which is undesirable for training. We revisit
the BN formulation and present a new initialization method and update approach
for BN to address the aforementioned issues. Experiments are designed to
emphasize and demonstrate the positive influence of proper BN scale
initialization on performance, and use rigorous statistical significance tests
for evaluation. The approach can be used with existing implementations at no
additional computational cost. Source code is available at
https://github.com/osu-cvl/revisiting-bn-init.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision, October 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making the Most of Text Semantics to Improve Biomedical Vision--Language
  Processing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, Ozan Oktay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:
  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ blob loss: instance imbalance aware loss functions for semantic
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kofler, Suprosanna Shit, Ivan Ezhov, Lucas Fidon, Izabela Horvath, Rami Al-Maskari, Hongwei Li, Harsharan Bhatia, Timo Loehr, Marie Piraud, Ali Erturk, Jan Kirschke, Jan Peeken, Tom Vercauteren, Claus Zimmer, Benedikt Wiestler, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional neural networks have proven to be remarkably effective in
semantic segmentation tasks. Most popular loss functions were introduced
targeting improved volumetric scores, such as the Sorensen Dice coefficient. By
design, DSC can tackle class imbalance; however, it does not recognize instance
imbalance within a class. As a result, a large foreground instance can dominate
minor instances and still produce a satisfactory Sorensen Dice coefficient.
Nevertheless, missing out on instances will lead to poor detection performance.
This represents a critical issue in applications such as disease progression
monitoring. For example, it is imperative to locate and surveil small-scale
lesions in the follow-up of multiple sclerosis patients. We propose a novel
family of loss functions, nicknamed blob loss, primarily aimed at maximizing
instance-level detection metrics, such as F1 score and sensitivity. Blob loss
is designed for semantic segmentation problems in which the instances are the
connected components within a class. We extensively evaluate a DSC-based blob
loss in five complex 3D semantic segmentation tasks featuring pronounced
instance heterogeneity in terms of texture and morphology. Compared to soft
Dice loss, we achieve 5 percent improvement for MS lesions, 3 percent
improvement for liver tumor, and an average 2 percent improvement for
Microscopy segmentation tasks considering F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures // corrected one mistake where it said beta
  instead of alpha in the text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied Scene-aware Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Luo, Shun Iwase, Ye Yuan, Kris Kitani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose embodied scene-aware human pose estimation where we estimate 3D
poses based on a simulated agent's proprioception and scene awareness, along
with external third-person observations. Unlike prior methods that often resort
to multistage optimization, non-causal inference, and complex contact modeling
to estimate human pose and human scene interactions, our method is one stage,
causal, and recovers global 3D human poses in a simulated environment. Since 2D
third-person observations are coupled with the camera pose, we propose to
disentangle the camera pose and use a multi-step projection gradient defined in
the global coordinate frame as the movement cue for our embodied agent.
Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we
simulate our agent in everyday environments (libraries, offices, bedrooms,
etc.) and equip our agent with environmental sensors to intelligently navigate
and interact with scene geometries. Our method also relies only on 2D keypoints
and can be trained on synthetic datasets derived from popular human motion
databases. To evaluate, we use the popular H36M and PROX datasets and, for the
first time, achieve a success rate of 96.7% on the challenging PROX dataset
without ever using PROX motion sequences for training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://embodiedscene.github.io/embodiedpose/
  Zhengyi Luo and Shun Iwase contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFormer: The Relational <span class="highlight-title">Transformer</span> for Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.14178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.14178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Yang, Yingru Liu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning is shown to be able to achieve a better performance by using
scene graphs to represent the relations of objects in the image. The current
captioning encoders generally use a Graph Convolutional Net (GCN) to represent
the relation information and merge it with the object region features via
concatenation or convolution to get the final input for sentence decoding.
However, the GCN-based encoders in the existing methods are less effective for
captioning due to two reasons. First, using the image captioning as the
objective (i.e., Maximum Likelihood Estimation) rather than a relation-centric
loss cannot fully explore the potential of the encoder. Second, using a
pre-trained model instead of the encoder itself to extract the relationships is
not flexible and cannot contribute to the explainability of the model. To
improve the quality of image captioning, we propose a novel architecture
ReFormer -- a RElational transFORMER to generate features with relation
information embedded and to explicitly express the pair-wise relationships
between objects in the image. ReFormer incorporates the objective of scene
graph generation with that of image captioning using one modified Transformer
model. This design allows ReFormer to generate not only better image captions
with the bene-fit of extracting strong relational image features, but also
scene graphs to explicitly describe the pair-wise relation-ships. Experiments
on publicly available datasets show that our model significantly outperforms
state-of-the-art methods on image captioning and scene graph generation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DFNet: Enhance Absolute Pose Regression with Direct Feature Matching <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Chen, Xinghui Li, Zirui Wang, Victor Adrian Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Bootstrapped Masked Autoencoders for Vision <span class="highlight-title">BERT</span> <span class="highlight-title">Pretrain</span>ing <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, <span class="highlight-author">Lu Yuan</span>, Dong Chen, Fang Wen, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose bootstrapped masked autoencoders (BootMAE), a new approach for
vision BERT pretraining. BootMAE improves the original masked autoencoders
(MAE) with two core designs: 1) momentum encoder that provides online feature
as extra BERT prediction targets; 2) target-aware decoder that tries to reduce
the pressure on the encoder to memorize target-specific information in BERT
pretraining. The first design is motivated by the observation that using a
pretrained MAE to extract the features as the BERT prediction target for masked
tokens can achieve better pretraining performance. Therefore, we add a momentum
encoder in parallel with the original MAE encoder, which bootstraps the
pretraining performance by using its own representation as the BERT prediction
target. In the second design, we introduce target-specific information (e.g.,
pixel values of unmasked patches) from the encoder directly to the decoder to
reduce the pressure on the encoder of memorizing the target-specific
information. Thus, the encoder focuses on semantic modeling, which is the goal
of BERT pretraining, and does not need to waste its capacity in memorizing the
information of unmasked tokens related to the prediction target. Through
extensive experiments, our BootMAE achieves $84.2\%$ Top-1 accuracy on
ImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\%$ under the same
pre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic
segmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object
detection and segmentation on COCO dataset. Code is released at
https://github.com/LightDXY/BootMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022, code is available at https://github.com/LightDXY/BootMAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Continuous-time Analysis for Variational Inequalities: An <span class="highlight-title">Overview</span> and
  Desiderata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatjana Chavdarova, Ya-Ping Hsieh, <span class="highlight-author">Michael I. Jordan</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms that solve zero-sum games, multi-objective agent objectives, or,
more generally, variational inequality (VI) problems are notoriously unstable
on general problems. Owing to the increasing need for solving such problems in
machine learning, this instability has been highlighted in recent years as a
significant research challenge. In this paper, we provide an overview of recent
progress in the use of continuous-time perspectives in the analysis and design
of methods targeting the broad VI problem class. Our presentation draws
parallels between single-objective problems and multi-objective problems,
highlighting the challenges of the latter. We also formulate various desiderata
for algorithms that apply to general VIs and we argue that achieving these
desiderata may profit from an understanding of the associated continuous-time
dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse
  Representation Based Domain Adaption to Energy Efficient Abnormal Beat
  Detection for Practical ECG Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Yamaç, Mert Duman, İlke Adalıoğlu, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a low-cost and highly accurate ECG-monitoring system
intended for personalized early arrhythmia detection for wearable mobile
sensors. Earlier supervised approaches for personalized ECG monitoring require
both abnormal and normal heartbeats for the training of the dedicated
classifier. However, in a real-world scenario where the personalized algorithm
is embedded in a wearable device, such training data is not available for
healthy people with no cardiac disorder history. In this study, (i) we propose
a null space analysis on the healthy signal space obtained via sparse
dictionary learning, and investigate how a simple null space projection or
alternatively regularized least squares-based classification methods can reduce
the computational complexity, without sacrificing the detection accuracy, when
compared to sparse representation-based classification. (ii) Then we introduce
a sparse representation-based domain adaptation technique in order to project
other existing users' abnormal and normal signals onto the new user's signal
space, enabling us to train the dedicated classifier without having any
abnormal heartbeat of the new user. Therefore, zero-shot learning can be
achieved without the need for synthetic abnormal heartbeat generation. An
extensive set of experiments performed on the benchmark MIT-BIH ECG dataset
shows that when this domain adaptation-based training data generator is used
with a simple 1-D CNN classifier, the method outperforms the prior work by a
significant margin. (iii) Then, by combining (i) and (ii), we propose an
ensemble classifier that further improves the performance. This approach for
zero-shot arrhythmia detection achieves an average accuracy level of 98.2% and
an F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring
scheme is proposed using the above-mentioned innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Software implementation: https://github.com/MertDuman/Zero-Shot-ECG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient <span class="highlight-title">Prompt</span> Tuning Makes Generalized and Calibrated
  Neural Text Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning attempts to update few task-specific parameters in pre-trained
models. It has achieved comparable performance to fine-tuning of the full
parameter set on both language understanding and generation tasks. In this
work, we study the problem of prompt tuning for neural text retrievers. We
introduce parameter-efficient prompt tuning for text retrieval across
in-domain, cross-domain, and cross-topic settings. Through an extensive
analysis, we show that the strategy can mitigate the two issues --
parameter-inefficiency and weak generalizability -- faced by fine-tuning based
retrieval methods. Notably, it can significantly improve the out-of-domain
zero-shot generalization of the retrieval models. By updating only 0.1% of the
model parameters, the prompt tuning strategy can help retrieval models achieve
better generalization performance than traditional methods in which all
parameters are updated. Finally, to facilitate research on retrievers'
cross-topic generalizability, we curate and release an academic retrieval
dataset with 18K query-results pairs in 87 topics, making it the largest
topic-specific one to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Asymmetric <span class="highlight-title">Contrastive</span> Loss for Handling Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentino Vito, Lim Yohanes Stefanus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning is a representation learning method performed by
contrasting a sample to other similar samples so that they are brought closely
together, forming clusters in the feature space. The learning process is
typically conducted using a two-stage training architecture, and it utilizes
the contrastive loss (CL) for its feature learning. Contrastive learning has
been shown to be quite successful in handling imbalanced datasets, in which
some classes are overrepresented while some others are underrepresented.
However, previous studies have not specifically modified CL for imbalanced
datasets. In this work, we introduce an asymmetric version of CL, referred to
as ACL, in order to directly address the problem of class imbalance. In
addition, we propose the asymmetric focal contrastive loss (AFCL) as a further
generalization of both ACL and focal contrastive loss (FCL). Results on the
FMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of
outperforming CL and FCL in terms of both weighted and unweighted
classification accuracies. In the appendix, we provide a full axiomatic
treatment on entropy, along with complete proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Query-Optimal Algorithm for Finding Counterfactuals <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Blanc, Caleb Koch, Jane Lange, Li-Yang Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design an algorithm for finding counterfactuals with strong theoretical
guarantees on its performance. For any monotone model $f : X^d \to \{0,1\}$ and
instance $x^\star$, our algorithm makes \[ {S(f)^{O(\Delta_f(x^\star))}\cdot
\log d}\] queries to $f$ and returns {an {\sl optimal}} counterfactual for
$x^\star$: a nearest instance $x'$ to $x^\star$ for which $f(x')\ne
f(x^\star)$. Here $S(f)$ is the sensitivity of $f$, a discrete analogue of the
Lipschitz constant, and $\Delta_f(x^\star)$ is the distance from $x^\star$ to
its nearest counterfactuals. The previous best known query complexity was
$d^{\,O(\Delta_f(x^\star))}$, achievable by brute-force local search. We
further prove a lower bound of $S(f)^{\Omega(\Delta_f(x^\star))} + \Omega(\log
d)$ on the query complexity of any algorithm, thereby showing that the
guarantees of our algorithm are essentially optimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bia Mitigation for Machine Learning Classifiers: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Hort, Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive survey of bias mitigation methods for
achieving fairness in Machine Learning (ML) models. We collect a total of 234
publications concerning bias mitigation for ML classifiers. These methods can
be distinguished based on their intervention procedure (i.e., pre-processing,
in-processing, post-processing) and the technology they apply. We investigate
how existing bias mitigation methods are evaluated in the literature. In
particular, we consider datasets, metrics and benchmarking. Based on the
gathered insights (e.g., what is the most popular fairness metric? How many
datasets are used for evaluating bias mitigation methods?). We hope to support
practitioners in making informed choices when developing and evaluating new
bias mitigation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Strong Correlation Between Model Invariance and Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Deng, Stephen Gould, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization and invariance are two essential properties of any machine
learning model. Generalization captures a model's ability to classify unseen
data while invariance measures consistency of model predictions on
transformations of the data. Existing research suggests a positive
relationship: a model generalizing well should be invariant to certain visual
factors. Building on this qualitative implication we make two contributions.
First, we introduce effective invariance (EI), a simple and reasonable measure
of model invariance which does not rely on image labels. Given predictions on a
test image and its transformed version, EI measures how well the predictions
agree and with what level of confidence. Second, using invariance scores
computed by EI, we perform large-scale quantitative correlation studies between
generalization and invariance, focusing on rotation and grayscale
transformations. From a model-centric view, we observe generalization and
invariance of different models exhibit a strong linear relationship, on both
in-distribution and out-of-distribution datasets. From a dataset-centric view,
we find a certain model's accuracy and invariance linearly correlated on
different test sets. Apart from these major findings, other minor but
interesting insights are also discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures; this version is not fully edited and will be
  updated soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confident Adaptive Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, Donald Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Temporal Action Detection with Proposal-Free Masking <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing temporal action detection (TAD) methods rely on a large number of
training data with segment-level annotations. Collecting and annotating such a
training set is thus highly expensive and unscalable. Semi-supervised TAD
(SS-TAD) alleviates this problem by leveraging unlabeled videos freely
available at scale. However, SS-TAD is also a much more challenging problem
than supervised TAD, and consequently much under-studied. Prior SS-TAD methods
directly combine an existing proposal-based TAD method and a SSL method. Due to
their sequential localization (e.g, proposal generation) and classification
design, they are prone to proposal error propagation. To overcome this
limitation, in this work we propose a novel Semi-supervised Temporal action
detection model based on PropOsal-free Temporal mask (SPOT) with a parallel
localization (mask generation) and classification architecture. Such a novel
design effectively eliminates the dependence between localization and
classification by cutting off the route for error propagation in-between. We
further introduce an interaction mechanism between classification and
localization for prediction refinement, and a new pretext task for
self-supervised model pre-training. Extensive experiments on two standard
benchmarks show that our SPOT outperforms state-of-the-art alternatives, often
by a large margin. The PyTorch implementation of SPOT is available at
https://github.com/sauradip/SPOT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code available at https://github.com/sauradip/SPOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language models show human-like content effects on reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract reasoning is a key ability for an intelligent system. Large language
models achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect, and depends on our knowledge and beliefs about the content of the
reasoning problem. For example, humans reason much more reliably about logical
rules that are grounded in everyday situations than arbitrary rules about
abstract attributes. The training experiences of language models similarly
endow them with prior expectations that reflect human knowledge and beliefs. We
therefore hypothesized that language models would show human-like content
effects on abstract reasoning problems. We explored this hypothesis across
three logical reasoning tasks: natural language inference, judging the logical
validity of syllogisms, and the Wason selection task (Wason, 1968). We find
that state of the art large language models (with 7 or 70 billion parameters;
Hoffman et al., 2022) reflect many of the same patterns observed in humans
across these tasks -- like humans, models reason more effectively about
believable situations than unrealistic or abstract ones. Our findings have
implications for understanding both these cognitive effects, and the factors
that contribute to language model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How do tuna schools associate to dFADs? A study using echo-sounder buoys
  to identify global patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Navarro-García, Daniel Precioso, Kathryn Gavira-O'Neill, Alberto Torres-Barrán, David Gordo, Víctor Gallego, David Gómez-Ullate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on the data gathered by echo-sounder buoys attached to drifting Fish
Aggregating Devices (dFADs) across tropical oceans, the current study applies a
Machine Learning protocol to examine the temporal trends of tuna schools'
association to drifting objects. Using a binary output, metrics typically used
in the literature were adapted to account for the fact that the entire tuna
aggregation under the dFAD was considered. The median time it took tuna to
colonize the dFADs for the first time varied between 25 and 43 days, depending
on the ocean, and the longest soak and colonization times were registered in
the Pacific Ocean. The tuna schools' Continuous Residence Times were generally
shorter than Continuous Absence Times (median values between 5 and 7 days, and
9 and 11 days, respectively), in line with the results found by previous
studies. Using a regression output, two novel metrics, namely aggregation time
and disaggregation time, were estimated to obtain further insight into the
symmetry of the aggregation process. Across all oceans, the time it took for
the tuna aggregation to depart from the dFADs was not significantly longer than
the time it took for the aggregation to form. The value of these results in the
context of the "ecological trap" hypothesis is discussed, and further analyses
to enrich and make use of this data source are proposed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leakage and the Reproducibility Crisis in ML-based Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayash Kapoor, Arvind Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of machine learning (ML) methods for prediction and forecasting has
become widespread across the quantitative sciences. However, there are many
known methodological pitfalls, including data leakage, in ML-based science. In
this paper, we systematically investigate reproducibility issues in ML-based
science. We show that data leakage is indeed a widespread problem and has led
to severe reproducibility failures. Specifically, through a survey of
literature in research communities that adopted ML methods, we find 17 fields
where errors have been found, collectively affecting 329 papers and in some
cases leading to wildly overoptimistic conclusions. Based on our survey, we
present a fine-grained taxonomy of 8 types of leakage that range from textbook
errors to open research problems.
  We argue for fundamental methodological changes to ML-based science so that
cases of leakage can be caught before publication. To that end, we propose
model info sheets for reporting scientific claims based on ML models that would
address all types of leakage identified in our survey. To investigate the
impact of reproducibility errors and the efficacy of model info sheets, we
undertake a reproducibility study in a field where complex ML models are
believed to vastly outperform older statistical models such as Logistic
Regression (LR): civil war prediction. We find that all papers claiming the
superior performance of complex ML models compared to LR models fail to
reproduce due to data leakage, and complex ML models don't perform
substantively better than decades-old LR models. While none of these errors
could have been caught by reading the papers, model info sheets would enable
the detection of leakage in each case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Shapley back to Pearson: Hypothesis Testing via the Shapley Value 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo Teneggi, Beepul Bharti, Yaniv Romano, Jeremias Sulam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models, in particular artificial neural networks, are
increasingly used to inform decision making in high-stakes scenarios across a
variety of fields--from financial services, to public safety, and healthcare.
While neural networks have achieved remarkable performance in many settings,
their complex nature raises concerns on their reliability, trustworthiness, and
fairness in real-world scenarios. As a result, several a-posteriori explanation
methods have been proposed to highlight the features that influence a model's
prediction. Notably, the Shapley value--a game theoretic quantity that
satisfies several desirable properties--has gained popularity in the machine
learning explainability literature. More traditionally, however, feature
importance in statistical learning has been formalized by conditional
independence, and a standard way to test for it is via Conditional
Randomization Tests (CRTs). So far, these two perspectives on interpretability
and feature importance have been considered distinct and separate. In this
work, we show that Shapley-based explanation methods and conditional
independence testing for feature importance are closely related. More
precisely, we prove that evaluating a Shapley coefficient amounts to performing
a specific set of conditional independence tests, as implemented by a procedure
similar to the CRT but for a different null hypothesis. Furthermore, the
obtained game-theoretic values upper bound the $p$-values of such tests. As a
result, we grant large Shapley coefficients with a precise statistical sense of
importance with controlled type I error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Detection of Ovarian Cancer by Wavelet Analysis of Protein Mass
  Spectra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dixon Vimalajeewa, Scott Alan Bruce, Brani Vidakovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient detection of ovarian cancer at early stages is
critical to ensure proper treatments for patients. Among the first-line
modalities investigated in studies of early diagnosis are features distilled
from protein mass spectra. This method, however, considers only a specific
subset of spectral responses and ignores the interplay among protein expression
levels, which can also contain diagnostic information. We propose a new
modality that automatically searches protein mass spectra for discriminatory
features by considering the self-similar nature of the spectra. Self-similarity
is assessed by taking a wavelet decomposition of protein mass spectra and
estimating the rate of level-wise decay in the energies of the resulting
wavelet coefficients. Level-wise energies are estimated in a robust manner
using distance variance, and rates are estimated locally via a rolling window
approach. This results in a collection of rates that can be used to
characterize the interplay among proteins, which can be indicative of cancer
presence. Discriminatory descriptors are then selected from these evolutionary
rates and used as classifying features. The proposed wavelet-based features are
used in conjunction with features proposed in the existing literature for early
stage diagnosis of ovarian cancer using two datasets published by the American
National Cancer Institute. Including the wavelet-based features from the new
modality results in improvements in diagnostic performance for early-stage
ovarian cancer detection. This demonstrates the ability of the proposed
modality to characterize new ovarian cancer diagnostic information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pages 18, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedFuse: <span class="highlight-title">Multi-modal</span> fusion with clinical time-series data and chest
  X-ray images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasir Hayat, Krzysztof J. Geras, Farah E. Shamout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal fusion approaches aim to integrate information from different
data sources. Unlike natural datasets, such as in audio-visual applications,
where samples consist of "paired" modalities, data in healthcare is often
collected asynchronously. Hence, requiring the presence of all modalities for a
given sample is not realistic for clinical tasks and significantly limits the
size of the dataset during training. In this paper, we propose MedFuse, a
conceptually simple yet promising LSTM-based fusion module that can accommodate
uni-modal as well as multi-modal input. We evaluate the fusion method and
introduce new benchmark results for in-hospital mortality prediction and
phenotype classification, using clinical time-series data in the MIMIC-IV
dataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more
complex multi-modal fusion strategies, MedFuse provides a performance
improvement by a large margin on the fully paired test set. It also remains
robust across the partially paired test set containing samples with missing
chest X-ray images. We release our code for reproducibility and to enable the
evaluation of competing models in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Modelling with Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches, instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT,
further confirming the benefits of modelling language with pixels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitrack Music <span class="highlight-title">Transformer</span>: Learning Long-Term Dependencies in Music
  with Diverse Instruments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for generating multitrack music with transformer models
have been limited to either a small set of instruments or short music segments.
This is partly due to the memory requirements of the lengthy input sequences
necessitated by existing representations for multitrack music. In this work, we
propose a compact representation that allows a diverse set of instruments while
keeping a short sequence length. Using our proposed representation, we present
the Multitrack Music Transformer (MTMT) for learning long-term dependencies in
multitrack music. In a subjective listening test, our proposed model achieves
competitive quality on unconditioned generation against two baseline models. We
also show that our proposed model can generate samples that are twice as long
as those produced by the baseline models, and, further, can do so in half the
inference time. Moreover, we propose a new measure for analyzing musical
self-attentions and show that the trained model learns to pay less attention to
notes that form a dissonant interval with the current note, yet attending more
to notes that are 4N beats away from current. Finally, our findings provide a
novel foundation for future work exploring longer-form multitrack music
generation and improving self-attentions for music. All source code and audio
samples can be found at https://salu133445.github.io/mtmt/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forming Trees with Treeformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilay Patel, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular models such as Transformers and LSTMs use tokens as its unit of
information. That is, each token is encoded into a vector representation, and
those vectors are used directly in a computation. However, humans frequently
consider spans of tokens (i.e., phrases) instead of their constituent tokens.
In this paper we introduce Treeformer, an architecture inspired by the CKY
algorithm and Transformer which learns a composition operator and pooling
function in order to construct hierarchical encodings for phrases and
sentences. Our extensive experiments demonstrate the benefits of incorporating
a hierarchical structure into the Transformer, and show significant
improvements compared to a baseline Transformer in machine translation,
abstractive summarization, and various natural language understanding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Propagation Learning for Network-Wide Flight Delay
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Wu, Hongyu Yang, Yi Lin, Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demystifying the delay propagation mechanisms among multiple airports is
fundamental to precise and interpretable delay prediction, which is crucial
during decision-making for all aviation industry stakeholders. The principal
challenge lies in effectively leveraging the spatiotemporal dependencies and
exogenous factors related to the delay propagation. However, previous works
only consider limited spatiotemporal patterns with few factors. To promote more
comprehensive propagation modeling for delay prediction, we propose
SpatioTemporal Propagation Network (STPN), a space-time separable graph
convolutional network, which is novel in spatiotemporal dependency capturing.
From the aspect of spatial relation modeling, we propose a multi-graph
convolution model considering both geographic proximity and airline schedule.
From the aspect of temporal dependency capturing, we propose a multi-head
self-attentional mechanism that can be learned end-to-end and explicitly reason
multiple kinds of temporal dependency of delay time series. We show that the
joint spatial and temporal learning models yield a sum of the Kronecker
product, which factors the spatiotemporal dependence into the sum of several
spatial and temporal adjacency matrices. By this means, STPN allows cross-talk
of spatial and temporal factors for modeling delay propagation. Furthermore, a
squeeze and excitation module is added to each layer of STPN to boost
meaningful spatiotemporal features. To this end, we apply STPN to multi-step
ahead arrival and departure delay prediction in large-scale airport networks.
To validate the effectiveness of our model, we experiment with two real-world
delay datasets, including U.S and China flight delays; and we show that STPN
outperforms state-of-the-art methods. In addition, counterfactuals produced by
STPN show that it learns explainable delay propagation patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proceedings of the ICML 2022 Expressive Vocalizations Workshop and
  Competition: Recognizing, Generating, and Personalizing Vocal Bursts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Baird, Panagiotis Tzirakis, Gauthier Gidel, Marco Jiralerspong, Eilif B. Muller, Kory Mathewson, Björn Schuller, Erik Cambria, Dacher Keltner, Alan Cowen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is the Proceedings of the ICML Expressive Vocalization (ExVo)
Competition. The ExVo competition focuses on understanding and generating vocal
bursts: laughs, gasps, cries, and other non-verbal vocalizations that are
central to emotional expression and communication. ExVo 2022, included three
competition tracks using a large-scale dataset of 59,201 vocalizations from
1,702 speakers. The first, ExVo-MultiTask, requires participants to train a
multi-task model to recognize expressed emotions and demographic traits from
vocal bursts. The second, ExVo-Generate, requires participants to train a
generative model that produces vocal bursts conveying ten different emotions.
The third, ExVo-FewShot, requires participants to leverage few-shot learning
incorporating speaker identity to train a model for the recognition of 10
emotions conveyed by vocal bursts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Hu, Jie Chen, Vijayan N. Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-order functional ANOVA (fANOVA) models have been rediscovered in the
machine learning (ML) community under the guise of inherently interpretable
machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and
GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting
functional main effects and second-order interactions. We propose a new
algorithm, called GAMI-Tree, that is similar to EBM, but has a number of
features that lead to better performance. It uses model-based trees as base
learners and incorporates a new interaction filtering method that is better at
capturing the underlying interactions. In addition, our iterative training
method converges to a model with better predictive performance, and the
embedded purification ensures that interactions are hierarchically orthogonal
to main effects. The algorithm does not need extensive tuning, and our
implementation is fast and efficient. We use simulated and real datasets to
compare the performance and interpretability of GAMI-Tree with EBM and
GAMI-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages plus appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeking the Truth Beyond the Data. An Unsupervised Machine Learning
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Saligkaras, Vasileios E. Papageorgiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering is an unsupervised machine learning methodology where unlabeled
elements/objects are grouped together aiming to the construction of
well-established clusters that their elements are classified according to their
similarity. The goal of this process is to provide a useful aid to the
researcher that will help her/him to identify patterns among the data. Dealing
with large databases, such patterns may not be easily detectable without the
contribution of a clustering algorithm. This article provides a deep
description of the most widely used clustering methodologies accompanied by
useful presentations concerning suitable parameter selection and
initializations. Simultaneously, this article not only represents a review
highlighting the major elements of examined clustering techniques but
emphasizes the comparison of these algorithms' clustering efficiency based on 3
datasets, revealing their existing weaknesses and capabilities through accuracy
and complexity, during the confrontation of discrete and continuous
observations. The produced results help us extract valuable conclusions about
the appropriateness of the examined clustering techniques in accordance with
the dataset's size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the proceedings of
  the 3rd International Scientific Forum on Computer and Energy Sciences (WFCES
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insurgency as Complex Network: Image Co-Appearance and Hierarchy in the
  PKK 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ollie Ballinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a growing recognition of the importance of insurgent group structure
on conflict outcomes, there is very little empirical research thereon. Though
this problem is rooted in the inaccessibility of data on militant group
structure, insurgents frequently publish large volumes of image data on the
internet. In this paper, I develop a new methodology that leverages this
abundant but underutilized source of data by automating the creation of a
social network graph based on co-appearance in photographs using deep learning.
Using a trove of 19,115 obituary images published online by the PKK, a Kurdish
militant group in Turkey, I demonstrate that an individual's centrality in the
resulting co-appearance network is closely correlated with their rank in the
insurgent group.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Graph Learning via Sensitivity-Bounded
  Personalized PageRank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Epasto, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Peilin Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of
graph representations such as node ranking, labeling, and graph embedding.
However, while data privacy is one of the most important recent concerns,
existing PPR algorithms are not designed to protect user privacy. PPR is highly
sensitive to the input graph edges: the difference of only one edge may cause a
big change in the PPR vector, potentially leaking private user data.
  In this work, we propose an algorithm which outputs an approximate PPR and
has provably bounded sensitivity to input edges. In addition, we prove that our
algorithm achieves similar accuracy to non-private algorithms when the input
graph has large degrees. Our sensitivity-bounded PPR directly implies private
algorithms for several tools of graph learning, such as, differentially private
(DP) PPR ranking, DP node classification, and DP node embedding. To complement
our theoretical analysis, we also empirically verify the practical performances
of our algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PASHA: Efficient HPO with Progressive Resource Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondrej Bohdal, Lukas Balles, Beyza Ermis, Cédric Archambeau, Giovanni Zappella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperparameter optimization (HPO) and neural architecture search (NAS) are
methods of choice to obtain the best-in-class machine learning models, but in
practice they can be costly to run. When models are trained on large datasets,
tuning them with HPO or NAS rapidly becomes prohibitively expensive for
practitioners, even when efficient multi-fidelity methods are employed. We
propose an approach to tackle the challenge of tuning machine learning models
trained on large datasets with limited computational resources. Our approach,
named PASHA, is able to dynamically allocate maximum resources for the tuning
procedure depending on the need. The experimental comparison shows that PASHA
identifies well-performing hyperparameter configurations and architectures
while consuming significantly fewer computational resources than solutions like
ASHA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shorter version accepted at AutoML Conference 2022 Workshop Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Branched Regularization for Federated Learning <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkyu Kim, Geeho Kim, Bohyung Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical challenge of federated learning is data heterogeneity and
imbalance across clients, which leads to inconsistency between local networks
and unstable convergence of global models. To alleviate the limitations, we
propose a novel architectural regularization technique that constructs multiple
auxiliary branches in each local model by grafting local and global subnetworks
at several different levels and that learns the representations of the main
pathway in the local model congruent to the auxiliary hybrid pathways via
online knowledge distillation. The proposed technique is effective to robustify
the global model even in the non-iid setting and is applicable to various
federated learning frameworks conveniently without incurring extra
communication costs. We perform comprehensive empirical studies and demonstrate
remarkable performance gains in terms of accuracy and efficiency compared to
existing methods. The source code is available at our project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance Learner: Incorporating Manifold Prior to Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Chetan, Nipun Kwatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manifold hypothesis (real world data concentrates near low-dimensional
manifolds) is suggested as the principle behind the effectiveness of machine
learning algorithms in very high dimensional problems that are common in
domains such as vision and speech. Multiple methods have been proposed to
explicitly incorporate the manifold hypothesis as a prior in modern Deep Neural
Networks (DNNs), with varying success. In this paper, we propose a new method,
Distance Learner, to incorporate this prior for DNN-based classifiers. Distance
Learner is trained to predict the distance of a point from the underlying
manifold of each class, rather than the class label. For classification,
Distance Learner then chooses the class corresponding to the closest predicted
class manifold. Distance Learner can also identify points as being out of
distribution (belonging to neither class), if the distance to the closest
manifold is higher than a threshold. We evaluate our method on multiple
synthetic datasets and show that Distance Learner learns much more meaningful
classification boundaries compared to a standard classifier. We also evaluate
our method on the task of adversarial robustness, and find that it not only
outperforms standard classifier by a large margin, but also performs at par
with classifiers trained via state-of-the-art adversarial training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically
  Ambiguous Settings for Low Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Pandey, Swayatta Daw, Narendra Babu Unnam, Vikram Pudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We leverage pre-trained language models to solve the task of complex NER for
two low-resource languages: Chinese and Spanish. We use the technique of Whole
Word Masking(WWM) to boost the performance of masked language modeling
objective on large and unsupervised corpora. We experiment with multiple neural
network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on
top of a fine-tuned BERT layer. All our models outperform the baseline by a
significant margin and our best performing model obtains a competitive position
on the evaluation leaderboard for the blind test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent Memory <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models show their effectiveness across multiple domains and
tasks. The self-attention allows to combine information from all sequence
elements into context-aware representations. However, global and local
information has to be stored mostly in the same element-wise representations.
Moreover, the length of an input sequence is limited by quadratic computational
complexity of self-attention.
  In this work, we propose and study a memory-augmented segment-level recurrent
Transformer (Recurrent Memory Transformer). Memory allows to store and process
local and global information as well as to pass information between segments of
the long sequence with the help of recurrence. We implement a memory mechanism
with no changes to Transformer model by adding special memory tokens to the
input or output sequence. Then Transformer is trained to control both memory
operations and sequence representations processing.
  Results of experiments show that our model performs on par with the
Transformer-XL on language modeling for smaller memory sizes and outperforms it
for tasks that require longer sequence processing. We show that adding memory
tokens to Tr-XL is able to improve it performance. This makes Recurrent Memory
Transformer a promising architecture for applications that require learning of
long-term dependencies and general purpose in memory processing, such as
algorithmic tasks and reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RSD-GAN: Regularized Sobolev Defense GAN Against Speech-to-Text
  Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Esmaeilpour, Nourhene Chaalia, Patrick Cardinal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new synthesis-based defense algorithm for
counteracting with a varieties of adversarial attacks developed for challenging
the performance of the cutting-edge speech-to-text transcription systems. Our
algorithm implements a Sobolev-based GAN and proposes a novel regularizer for
effectively controlling over the functionality of the entire generative model,
particularly the discriminator network during training. Our achieved results
upon carrying out numerous experiments on the victim DeepSpeech, Kaldi, and
Lingvo speech transcription systems corroborate the remarkable performance of
our defense approach against a comprehensive range of targeted and non-targeted
adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted to IEEE Signal Processing Letters Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Precision Arithmetic for Fast Gaussian Processes <span class="chip">UAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wesley J. Maddox, Andres Potapczynski, Andrew Gordon Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-precision arithmetic has had a transformative effect on the training of
neural networks, reducing computation, memory and energy requirements. However,
despite its promise, low-precision arithmetic has received little attention for
Gaussian processes (GPs), largely because GPs require sophisticated linear
algebra routines that are unstable in low-precision. We study the different
failure modes that can occur when training GPs in half precision. To circumvent
these failure modes, we propose a multi-faceted approach involving conjugate
gradients with re-orthogonalization, mixed precision, and preconditioning. Our
approach significantly improves the numerical stability and practical
performance of conjugate gradients in low-precision over a wide range of
settings, enabling GPs to train on $1.8$ million data points in $10$ hours on a
single GPU, without any sparse approximations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UAI 2022. Code available at https://github.com/AndPotap/halfpres_gps</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Dictionary Learning with An Intra-class Constraint <span class="chip">ICME2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xia Yuan, Jianping Gou, Baosheng Yu, Jiali Yu, Zhang Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep dictionary learning (DDL)has attracted a great amount
of attention due to its effectiveness for representation learning and visual
recognition.~However, most existing methods focus on unsupervised deep
dictionary learning, failing to further explore the category information.~To
make full use of the category information of different samples, we propose a
novel deep dictionary learning model with an intra-class constraint (DDLIC) for
visual classification. Specifically, we design the intra-class compactness
constraint on the intermediate representation at different levels to encourage
the intra-class representations to be closer to each other, and eventually the
learned representation becomes more discriminative.~Unlike the traditional DDL
methods, during the classification stage, our DDLIC performs a layer-wise
greedy optimization in a similar way to the training stage. Experimental
results on four image datasets show that our method is superior to the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 2 tables. It has been accepted in ICME2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance Selection Mechanisms for Human-in-the-Loop Systems in Few-Shot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Jakubik, Benedikt Blumenstiel, Michael Vössing, Patrick Hemmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Business analytics and machine learning have become essential success factors
for various industries - with the downside of cost-intensive gathering and
labeling of data. Few-shot learning addresses this challenge and reduces data
gathering and labeling costs by learning novel classes with very few labeled
data. In this paper, we design a human-in-the-loop (HITL) system for few-shot
learning and analyze an extensive range of mechanisms that can be used to
acquire human expert knowledge for instances that have an uncertain prediction
outcome. We show that the acquisition of human expert knowledge significantly
accelerates the few-shot model performance given a negligible labeling effort.
We validate our findings in various experiments on a benchmark dataset in
computer vision and real-world datasets. We further demonstrate the
cost-effectiveness of HITL systems for few-shot learning. Overall, our work
aims at supporting researchers and practitioners in effectively adapting
machine learning models to novel classes at reduced costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Wirtschaftsinformatik, 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-based Tremor Classification for Parkinson's Disease Diagnosis from
  Video <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozheng Zhang, Edmond S. L. Ho, Xiatian Zhang, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parkinson's disease (PD) is a progressive neurodegenerative disorder that
results in a variety of motor dysfunction symptoms, including tremors,
bradykinesia, rigidity and postural instability. The diagnosis of PD mainly
relies on clinical experience rather than a definite medical test, and the
diagnostic accuracy is only about 73-84% since it is challenged by the
subjective opinions or experiences of different medical experts. Therefore, an
efficient and interpretable automatic PD diagnosis system is valuable for
supporting clinicians with more robust diagnostic decision-making. To this end,
we propose to classify Parkinson's tremor since it is one of the most
predominant symptoms of PD with strong generalizability. Different from other
computer-aided time and resource-consuming Parkinson's Tremor (PT)
classification systems that rely on wearable sensors, we propose SPAPNet, which
only requires consumer-grade non-intrusive video recording of camera-facing
human movements as input to provide undiagnosed patients with low-cost PT
classification results as a PD warning sign. For the first time, we propose to
use a novel attention module with a lightweight pyramidal
channel-squeezing-fusion architecture to extract relevant PT information and
filter the noise efficiently. This design aids in improving both classification
performance and system interpretability. Experimental results show that our
system outperforms state-of-the-arts by achieving a balanced accuracy of 90.9%
and an F1-score of 90.6% in classifying PT with the non-PT class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomal-E: A <span class="highlight-title">Self-Supervised</span> Network Intrusion Detection System based on
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Caville, Wai Weng Lo, Siamak Layeghy, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Graph Neural Networks (GNNs) application for
self-supervised network intrusion and anomaly detection. GNNs are a deep
learning approach for graph-based data that incorporate graph structures into
learning to generalise graph representations and output embeddings. As network
flows are naturally graph-based, GNNs are a suitable fit for analysing and
learning network behaviour. The majority of current implementations of
GNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled
network traffic which can not only restrict the amount and structure of input
traffic, but also the NIDSs potential to adapt to unseen attacks. To overcome
these restrictions, we present Anomal-E, a GNN approach to intrusion and
anomaly detection that leverages edge features and graph topological structure
in a self-supervised process. This approach is, to the best our knowledge, the
first successful and practical approach to network intrusion detection that
utilises network flows in a self-supervised, edge leveraging GNN. Experimental
results on two modern benchmark NIDS datasets not only clearly display the
improvement of using Anomal-E embeddings rather than raw features, but also the
potential Anomal-E has for detection on wild network traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing the latent space of generative models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Asperti, Valerio Tonelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different encodings of datapoints in the latent space of latent-vector
generative models may result in more or less effective and disentangled
characterizations of the different explanatory factors of variation behind the
data. Many works have been recently devoted to the explorationof the latent
space of specific models, mostly focused on the study of how features are
disentangled and of how trajectories producing desired alterations of data in
the visible space can be found. In this work we address the more general
problem of comparing the latent spaces of different models, looking for
transformations between them. We confined the investigation to the familiar and
largely investigated case of generative models for the data manifold of human
faces. The surprising, preliminary result reported in this article is that
(provided models have not been taught or explicitly conceived to act
differently) a simple linear mapping is enough to pass from a latent space to
another while preserving most of the information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-memory Realization of In-situ Few-shot Continual Learning with a
  Dynamically Evolving Explicit Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geethan Karunaratne, Michael Hersche, Jovin Langenegger, Giovanni Cherubini, Manuel Le Gallo-Bourdeau, Urs Egger, Kevin Brew, Sam Choi, INJO OK, Mary Claire Silvestre, Ning Li, Nicole Saulnier, Victor Chan, Ishtiaq Ahsan, Vijay Narayanan, Luca Benini, Abu Sebastian, Abbas Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continually learning new classes from a few training examples without
forgetting previous old classes demands a flexible architecture with an
inevitably growing portion of storage, in which new examples and classes can be
incrementally stored and efficiently retrieved. One viable architectural
solution is to tightly couple a stationary deep neural network to a dynamically
evolving explicit memory (EM). As the centerpiece of this architecture, we
propose an EM unit that leverages energy-efficient in-memory compute (IMC)
cores during the course of continual learning operations. We demonstrate for
the first time how the EM unit can physically superpose multiple training
examples, expand to accommodate unseen classes, and perform similarity search
during inference, using operations on an IMC core based on phase-change memory
(PCM). Specifically, the physical superposition of a few encoded training
examples is realized via in-situ progressive crystallization of PCM devices.
The classification accuracy achieved on the IMC core remains within a range of
1.28%--2.5% compared to that of the state-of-the-art full-precision baseline
software model on both the CIFAR-100 and miniImageNet datasets when continually
learning 40 novel classes (from only five examples per class) on top of 60 old
classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the European Solid-state Devices and Circuits Conference
  (ESSDERC), September 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GrabQC: Graph based Query Contextualization for automated ICD coding <span class="chip">PAKDD 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeshuren Chelladurai, Sudarsun Santhiappan, Balaraman Ravindran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated medical coding is a process of codifying clinical notes to
appropriate diagnosis and procedure codes automatically from the standard
taxonomies such as ICD (International Classification of Diseases) and CPT
(Current Procedure Terminology). The manual coding process involves the
identification of entities from the clinical notes followed by querying a
commercial or non-commercial medical codes Information Retrieval (IR) system
that follows the Centre for Medicare and Medicaid Services (CMS) guidelines. We
propose to automate this manual process by automatically constructing a query
for the IR system using the entities auto-extracted from the clinical notes. We
propose \textbf{GrabQC}, a \textbf{Gra}ph \textbf{b}ased \textbf{Q}uery
\textbf{C}ontextualization method that automatically extracts queries from the
clinical text, contextualizes the queries using a Graph Neural Network (GNN)
model and obtains the ICD Codes using an external IR system. We also propose a
method for labelling the dataset for training the model. We perform experiments
on two datasets of clinical text in three different setups to assert the
effectiveness of our approach. The experimental results show that our proposed
method is better than the compared baselines in all three settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25th Pacific-Asia Conference on Knowledge Discovery and Data Mining
  (PAKDD 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strain-Minimizing Hyperbolic Network Embeddings with Landmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Keller-Ressel, Stephanie Nargang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce L-hydra (landmarked hyperbolic distance recovery and
approximation), a method for embedding network- or distance-based data into
hyperbolic space, which requires only the distance measurements to a few
'landmark nodes'. This landmark heuristic makes L-hydra applicable to
large-scale graphs and improves upon previously introduced methods. As a
mathematical justification, we show that a point configuration in d-dimensional
hyperbolic space can be perfectly recovered (up to isometry) from distance
measurements to just d+1 landmarks. We also show that L-hydra solves a
two-stage strain-minimization problem, similar to our previous (unlandmarked)
method 'hydra'. Testing on real network data, we show that L-hydra is an order
of magnitude faster than existing hyperbolic embedding methods and scales
linearly in the number of nodes. While the embedding error of L-hydra is higher
than the error of existing methods, we introduce an extension, L-hydra+, which
outperforms existing methods in both runtime and embedding quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised cross-lingual speech emotion recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirko Agarla, Simone Bianco, Luigi Celona, Paolo Napoletano, Alexey Petrovsky, Flavio Piccoli, Raimondo Schettini, Ivan Shanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) on a single language has achieved remarkable
results through deep learning approaches over the last decade. However,
cross-lingual SER remains a challenge in real-world applications due to (i) a
large difference between the source and target domain distributions, (ii) the
availability of few labeled and many unlabeled utterances for the new language.
Taking into account previous aspects, we propose a Semi-Supervised Learning
(SSL) method for cross-lingual emotion recognition when a few labels from the
new language are available. Based on a Convolutional Neural Network (CNN), our
method adapts to a new language by exploiting a pseudo-labeling strategy for
the unlabeled utterances. In particular, the use of a hard and soft
pseudo-labels approach is investigated. We thoroughly evaluate the performance
of the method in a speaker-independent setup on both the source and the new
language and show its robustness across five languages belonging to different
linguistic strains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Work In Progress: Safety and Robustness Verification of
  Autoencoder-Based Regression Models using the NNV Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neelanjana Pal, Taylor T Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work in progress paper introduces robustness verification for
autoencoder-based regression neural network (NN) models, following
state-of-the-art approaches for robustness verification of image classification
NNs. Despite the ongoing progress in developing verification methods for safety
and robustness in various deep neural networks (DNNs), robustness checking of
autoencoder models has not yet been considered. We explore this open space of
research and check ways to bridge the gap between existing DNN verification
methods by extending existing robustness analysis methods for such autoencoder
networks. While classification models using autoencoders work more or less
similar to image classification NNs, the functionality of regression models is
distinctly different. We introduce two definitions of robustness evaluation
metrics for autoencoder-based regression models, specifically the percentage
robustness and un-robustness grade. We also modified the existing Imagestar
approach, adjusting the variables to take care of the specific input types for
regression networks. The approach is implemented as an extension of NNV, then
applied and evaluated on a dataset, with a case study experiment shown using
the same dataset. As per the authors' understanding, this work in progress
paper is the first to show possible reachability analysis of autoencoder-based
NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings SNR 2021, arXiv:2207.04391</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verification of Sigmoidal Artificial Neural Networks using iSAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Grundt, Sorin Liviu Jurj, Willem Hagemann, Paul Kröger, Martin Fränzle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an approach for verifying the behaviour of nonlinear
Artificial Neural Networks (ANNs) found in cyber-physical safety-critical
systems. We implement a dedicated interval constraint propagator for the
sigmoid function into the SMT solver iSAT and compare this approach with a
compositional approach encoding the sigmoid function by basic arithmetic
features available in iSAT and an approximating approach. Our experimental
results show that the dedicated and the compositional approach clearly
outperform the approximating approach. Throughout all our benchmarks, the
dedicated approach showed an equal or better performance compared to the
compositional approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings SNR 2021, arXiv:2207.04391</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Logics for Neural Network Training and Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalia Slusarz, Ekaterina Komendantskaya, Matthew L. Daggitt, Robert Stewart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising popularity of neural networks (NNs) in recent years and their
increasing prevalence in real-world applications have drawn attention to the
importance of their verification. While verification is known to be
computationally difficult theoretically, many techniques have been proposed for
solving it in practice. It has been observed in the literature that by default
neural networks rarely satisfy logical constraints that we want to verify. A
good course of action is to train the given NN to satisfy said constraint prior
to verifying them. This idea is sometimes referred to as continuous
verification, referring to the loop between training and verification. Usually
training with constraints is implemented by specifying a translation for a
given formal logic language into loss functions. These loss functions are then
used to train neural networks. Because for training purposes these functions
need to be differentiable, these translations are called differentiable logics
(DL). This raises several research questions. What kind of differentiable
logics are possible? What difference does a specific choice of DL make in the
context of continuous verification? What are the desirable criteria for a DL
viewed from the point of view of the resulting loss function? In this extended
abstract we will discuss and answer these questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FOMLAS'22 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ problexity -- an open-source Python library for binary classification
  problem complexity assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Komorniczak, Pawel Ksieniewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification problem's complexity assessment is an essential element of
many topics in the supervised learning domain. It plays a significant role in
meta-learning -- becoming the basis for determining meta-attributes or
multi-criteria optimization -- allowing the evaluation of the training set
resampling without needing to rebuild the recognition model. The tools
currently available for the academic community, which would enable the
calculation of problem complexity measures, are available only as libraries of
the C++ and R languages. This paper describes the software module that allows
for the estimation of 22 complexity measures for the Python language --
compatible with the scikit-learn programming interface -- allowing for the
implementation of research using them in the most popular programming
environment of the machine learning community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MDEAW: A <span class="highlight-title">Multimodal</span> <span class="highlight-title">Dataset</span> for Emotion Analysis through EDA and PPG
  signals from wireless wearable low-cost off-the-shelf Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arijit Nandi, Fatos Xhafa, Laia Subirats, Santi Fort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MDEAW, a multimodal database consisting of Electrodermal Activity
(EDA) and Photoplethysmography (PPG) signals recorded during the exams for the
course taught by the teacher at Eurecat Academy, Sabadell, Barcelona in order
to elicit the emotional reactions to the students in a classroom scenario.
Signals from 10 students were recorded along with the students' self-assessment
of their affective state after each stimulus, in terms of 6 basic emotion
states. All the signals were captured using portable, wearable, wireless,
low-cost, and off-the-shelf equipment that has the potential to allow the use
of affective computing methods in everyday applications. A baseline for
student-wise affect recognition using EDA and PPG-based features, as well as
their fusion, was established through ReMECS, Fed-ReMECS, and Fed-ReMECS-U.
These results indicate the prospects of using low-cost devices for affective
state recognition applications. The proposed database will be made publicly
available in order to allow researchers to achieve a more thorough evaluation
of the suitability of these capturing devices for emotion state recognition
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Improved OOD Generalization via Conditional Invariant Regularizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Yi, Ruoyu Wang, Jia<span class="highlight-author">chen Sun</span>, Zhenguo Li, Zhi-Ming Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, generalization on out-of-distribution (OOD) data with correlation
shift has attracted great attention. The correlation shift is caused by the
spurious attributes that correlate to the class label, as the correlation
between them may vary in training and test data. For such a problem, we show
that given the class label, the conditionally independent models of spurious
attributes are OOD generalizable. Based on this, a metric Conditional Spurious
Variation (CSV) which controls OOD generalization error, is proposed to measure
such conditional independence. To improve the OOD generalization, we regularize
the training process with the proposed CSV. Under mild assumptions, our
training objective can be formulated as a nonconvex-concave mini-max problem.
An algorithm with provable convergence rate is proposed to solve the problem.
Extensive empirical results verify our algorithm's efficacy in improving OOD
generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Frequency Distribution Estimation using Graph Neural Networks <span class="chip">KDD 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongren Chen, Xinyue Xu, Shengyi Jiang, Hao Wang, Lu Mi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small subgraphs (graphlets) are important features to describe fundamental
units of a large network. The calculation of the subgraph frequency
distributions has a wide application in multiple domains including biology and
engineering. Unfortunately due to the inherent complexity of this task, most of
the existing methods are computationally intensive and inefficient. In this
work, we propose GNNS, a novel representational learning framework that
utilizes graph neural networks to sample subgraphs efficiently for estimating
their frequency distribution. Our framework includes an inference model and a
generative model that learns hierarchical embeddings of nodes, subgraphs, and
graph types. With the learned model and embeddings, subgraphs are sampled in a
highly scalable and parallel way and the frequency distribution estimation is
then performed based on these sampled subgraphs. Eventually, our methods
achieve comparable accuracy and a significant speedup by three orders of
magnitude compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by KDD 2022 Workshop on Deep Learning on Graphs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Hypergraph Diffusion Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraph neural networks (HNNs) using neural networks to encode hypergraphs
provide a promising way to model higher-order relations in data and further
solve relevant prediction tasks built upon such higher-order relations.
However, higher-order relations in practice contain complex patterns and are
often highly irregular. So, it is often challenging to design an HNN that
suffices to express those relations while keeping computational efficiency.
Inspired by hypergraph diffusion algorithms, this work proposes a new HNN
architecture named ED-HNN, which provably represents any continuous equivariant
hypergraph diffusion operators that can model a wide range of higher-order
relations. ED-HNN can be implemented efficiently by combining star expansions
of hypergraphs with standard message passing neural networks. ED-HNN further
shows great superiority in processing heterophilic hypergraphs and constructing
deep models. We evaluate ED-HNN for node classification on nine real-world
hypergraph datasets. ED-HNN uniformly outperforms the best baselines over these
nine datasets and achieves more than 2\%$\uparrow$ in prediction accuracy over
four datasets therein.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Graph-COM/ED-HNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Prove Trigonometric Identities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhou Liu, Yujun Li, Zhengying Liu, Lin Li, Zhenguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic theorem proving with deep learning methods has attracted attentions
recently. In this paper, we construct an automatic proof system for
trigonometric identities. We define the normalized form of trigonometric
identities, design a set of rules for the proof and put forward a method which
can generate theoretically infinite trigonometric identities. Our goal is not
only to complete the proof, but to complete the proof in as few steps as
possible. For this reason, we design a model to learn proof data generated by
random BFS (rBFS), and it is proved theoretically and experimentally that the
model can outperform rBFS after a simple imitation learning. After further
improvement through reinforcement learning, we get AutoTrig, which can give
proof steps for identities in almost as short steps as BFS (theoretically
shortest method), with a time cost of only one-thousandth. In addition,
AutoTrig also beats Sympy, Matlab and human in the synthetic dataset, and
performs well in many generalization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Methods for Protein Family Classification on PDB
  Sequencing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed of amino acid chains that influence how they fold and thus dictating
their function and features, proteins are a class of macromolecules that play a
central role in major biological processes and are required for the structure,
function, and regulation of the body's tissues. Understanding protein functions
is vital to the development of therapeutics and precision medicine, and hence
the ability to classify proteins and their functions based on measurable
features is crucial; indeed, the automatic inference of a protein's properties
from its sequence of amino acids, known as its primary structure, remains an
important open problem within the field of bioinformatics, especially given the
recent advancements in sequencing technologies and the extensive number of
known but uncategorized proteins with unknown properties. In this work, we
demonstrate and compare the performance of several deep learning frameworks,
including novel bi-directional LSTM and convolutional models, on widely
available sequencing data from the Protein Data Bank (PDB) of the Research
Collaboratory for Structural Bioinformatics (RCSB), as well as benchmark this
performance against classical machine learning approaches, including k-nearest
neighbors and multinomial regression classifiers, trained on experimental data.
Our results show that our deep learning models deliver superior performance to
classical machine learning methods, with the convolutional architecture
providing the most impressive inference performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Meta-learning Formulation of the Autoencoder Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey A. Popov, Arash Sarshar, Austin Chennault, Adrian Sandu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A rapidly growing area of research is the use of machine learning approaches
such as autoencoders for dimensionality reduction of data and models in
scientific applications. We show that the canonical formulation of autoencoders
suffers from several deficiencies that can hinder their performance. Using a
meta-learning approach, we reformulate the autoencoder problem as a bi-level
optimization procedure that explicitly solves the dimensionality reduction
task. We prove that the new formulation corrects the identified deficiencies
with canonical autoencoders, provide a practical way to solve it, and showcase
the strength of this formulation with a simple numerical illustration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale Knowledge <span class="highlight-title">Distillation</span> with Elastic Heterogeneous Computing
  Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Liu, Daxiang Dong, Xi Wang, An Qin, Xingjian Li, Patrick Valduriez, Dejing Dou, Dianhai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although more layers and more parameters generally improve the accuracy of
the models, such big models generally have high computational complexity and
require big memory, which exceed the capacity of small devices for inference
and incurs long training time. In addition, it is difficult to afford long
training time and inference time of big models even in high performance
servers, as well. As an efficient approach to compress a large deep model (a
teacher model) to a compact model (a student model), knowledge distillation
emerges as a promising approach to deal with the big models. Existing knowledge
distillation methods cannot exploit the elastic available computing resources
and correspond to low efficiency. In this paper, we propose an Elastic Deep
Learning framework for knowledge Distillation, i.e., EDL-Dist. The advantages
of EDL-Dist are three-fold. First, the inference and the training process is
separated. Second, elastic available computing resources can be utilized to
improve the efficiency. Third, fault-tolerance of the training and inference
processes is supported. We take extensive experimentation to show that the
throughput of EDL-Dist is up to 3.125 times faster than the baseline method
(online knowledge distillation) while the accuracy is similar or higher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Concurrency and Computation: Practice and Experience, 16
  pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Every Preference Changes Differently: Neural Multi-Interest Preference
  Model with Temporal Dynamics for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shi, Yupeng Gu, Yitong Zhou, Bo Zhao, Sicun Gao, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings (vectorized representations of a user) are essential in
recommendation systems. Numerous approaches have been proposed to construct a
representation for the user in order to find similar items for retrieval tasks,
and they have been proven effective in industrial recommendation systems as
well. Recently people have discovered the power of using multiple embeddings to
represent a user, with the hope that each embedding represents the user's
interest in a certain topic. With multi-interest representation, it's important
to model the user's preference over the different topics and how the preference
change with time. However, existing approaches either fail to estimate the
user's affinity to each interest or unreasonably assume every interest of every
user fades with an equal rate with time, thus hurting the recall of candidate
retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,
an approach that not only produces multi-interest for users by using the user's
sequential engagement more effectively but also automatically learns a set of
weights to represent the preference over each embedding so that the candidates
can be retrieved from each interest proportionally. Extensive experiments have
been done on various industrial-scale datasets to demonstrate the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have we been Naive to Select Machine Learning Models? Noisy Data are
  here to Stay! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Costa Farias, Teresa Bernarda Ludermir, Carmelo José Albanez Bastos-Filho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The model selection procedure is usually a single-criterion decision making
in which we select the model that maximizes a specific metric in a specific
set, such as the Validation set performance. We claim this is very naive and
can perform poor selections of over-fitted models due to the over-searching
phenomenon, which over-estimates the performance on that specific set.
Futhermore, real world data contains noise that should not be ignored by the
model selection procedure and must be taken into account when performing model
selection. Also, we have defined four theoretical optimality conditions that we
can pursue to better select the models and analyze them by using a
multi-criteria decision-making algorithm (TOPSIS) that considers proxies to the
optimality conditions to select reasonable models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Recognition in <span class="highlight-title">Conversation</span> using Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eriq Augustine, Pegah Jandaghi, Alon Albalak, Connor Pryor, Charles Dickens, William Wang, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating agents that can both appropriately respond to conversations and
understand complex human linguistic tendencies and social cues has been a long
standing challenge in the NLP community. A recent pillar of research revolves
around emotion recognition in conversation (ERC); a sub-field of emotion
recognition that focuses on conversations or dialogues that contain two or more
utterances. In this work, we explore an approach to ERC that exploits the use
of neural embeddings along with complex structures in dialogues. We implement
our approach in a framework called Probabilistic Soft Logic (PSL), a
declarative templating language that uses first-order like logical rules, that
when combined with data, define a particular class of graphical model.
Additionally, PSL provides functionality for the incorporation of results from
neural models into PSL models. This allows our model to take advantage of
advanced neural methods, such as sentence embeddings, and logical reasoning
over the structure of a dialogue. We compare our method with state-of-the-art
purely neural ERC systems, and see almost a 20% improvement. With these
results, we provide an extensive qualitative and quantitative analysis over the
DailyDialog conversation dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Model Uncertainty Estimation via Stochastic Data Centering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayaraman J. Thiagarajan, Rushil Anirudh, Vivek Narayanaswamy, Peer-Timo Bremer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are interested in estimating the uncertainties of deep neural networks,
which play an important role in many scientific and engineering problems. In
this paper, we present a striking new finding that an ensemble of neural
networks with the same weight initialization, trained on datasets that are
shifted by a constant bias gives rise to slightly inconsistent trained models,
where the differences in predictions are a strong indicator of epistemic
uncertainties. Using the neural tangent kernel (NTK), we demonstrate that this
phenomena occurs in part because the NTK is not shift-invariant. Since this is
achieved via a trivial input transformation, we show that it can therefore be
approximated using just a single neural network -- using a technique that we
call $\Delta-$UQ -- that estimates uncertainty around prediction by
marginalizing out the effect of the biases. We show that $\Delta-$UQ's
uncertainty estimates are superior to many of the current methods on a variety
of benchmarks -- outlier rejection, calibration under distribution shift, and
sequential design optimization of black box functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lipschitz Bound Analysis of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarosij Bose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lipschitz Bound Estimation is an effective method of regularizing deep neural
networks to make them robust against adversarial attacks. This is useful in a
variety of applications ranging from reinforcement learning to autonomous
systems. In this paper, we highlight the significant gap in obtaining a
non-trivial Lipschitz bound certificate for Convolutional Neural Networks
(CNNs) and empirically support it with extensive graphical analysis. We also
show that unrolling Convolutional layers or Toeplitz matrices can be employed
to convert Convolutional Neural Networks (CNNs) to a Fully Connected Network.
Further, we propose a simple algorithm to show the existing 20x-50x gap in a
particular data distribution between the actual lipschitz constant and the
obtained tight bound. We also ran sets of thorough experiments on various
network architectures and benchmark them on datasets like MNIST and CIFAR-10.
All these proposals are supported by extensive testing, graphs, histograms and
comparative analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Federated Learning with Decoupled Adaptive Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayin Jin, Jiaxiang Ren, Yang Zhou, Lingjuan Lyu, Ji Liu, Dejing Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The federated learning (FL) framework enables edge clients to collaboratively
learn a shared inference model while keeping privacy of training data on
clients. Recently, many heuristics efforts have been made to generalize
centralized adaptive optimization methods, such as SGDM, Adam, AdaGrad, etc.,
to federated settings for improving convergence and accuracy. However, there is
still a paucity of theoretical principles on where to and how to design and
utilize adaptive optimization methods in federated settings. This work aims to
develop novel adaptive optimization methods for FL from the perspective of
dynamics of ordinary differential equations (ODEs). First, an analytic
framework is established to build a connection between federated optimization
methods and decompositions of ODEs of corresponding centralized optimizers.
Second, based on this analytic framework, a momentum decoupling adaptive
optimization method, FedDA, is developed to fully utilize the global momentum
on each local iteration and accelerate the training convergence. Last but not
least, full batch gradients are utilized to mimic centralized optimization in
the end of the training process to ensure the convergence and overcome the
possible inconsistency caused by adaptive optimization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assortment Optimization with Customer Choice Modeling in a Crowdfunding
  Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Nosrat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowdfunding, which is the act of raising funds from a large number of
people's contributions, is among the most popular research topics in economic
theory. Due to the fact that crowdfunding platforms (CFPs) have facilitated the
process of raising funds by offering several features, we should take their
existence and survival in the marketplace into account. In this study, we
investigated the significant role of platform features in a customer behavioral
choice model. In particular, we proposed a multinomial logit model to describe
the customers' (backers') behavior in a crowdfunding setting. We proceed by
discussing the revenue-sharing model in these platforms. For this purpose, we
conclude that an assortment optimization problem could be of major importance
in order to maximize the platforms' revenue. We were able to derive a
reasonable amount of data in some cases and implement two well-known machine
learning methods such as multivariate regression and classification problems to
predict the best assortments the platform could offer to every arriving
customer. We compared the results of these two methods and investigated how
well they perform in all cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Bilevel Optimization: Regret Analysis of Online Alternating
  Gradient Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davoud Ataee Tarzanagh, Laura Balzano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online optimization is a well-established optimization paradigm that aims to
make a sequence of correct decisions given knowledge of the correct answer to
previous decision tasks. Bilevel programming involves a hierarchical
optimization problem where the feasible region of the so-called outer problem
is restricted by the graph of the solution set mapping of the inner problem.
This paper brings these two ideas together and studies an online bilevel
optimization setting in which a sequence of time-varying bilevel problems are
revealed one after the other. We extend the known regret bounds for
single-level online algorithms to the bilevel setting. Specifically, we
introduce new notions of bilevel regret, develop an online alternating
time-averaged gradient method that is capable of leveraging smoothness, and
provide regret bounds in terms of the path-length of the inner and outer
minimizer sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs
  Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11291v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11291v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrong Chen, Guan-Horng Liu, Evangelos A. Theodorou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schr\"odinger Bridge (SB) is an entropy-regularized optimal transport problem
that has received increasing attention in deep generative modeling for its
mathematical flexibility compared to the Scored-based Generative Model (SGM).
However, it remains unclear whether the optimization principle of SB relates to
the modern training of deep generative models, which often rely on constructing
log-likelihood objectives.This raises questions on the suitability of SB models
as a principled alternative for generative applications. In this work, we
present a novel computational framework for likelihood training of SB models
grounded on Forward-Backward Stochastic Differential Equations Theory - a
mathematical methodology appeared in stochastic optimal control that transforms
the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be
used to construct the likelihood objectives for SB that, surprisingly,
generalizes the ones for SGM as special cases. This leads to a new optimization
principle that inherits the same SB optimality yet without losing applications
of modern generative training techniques, and we show that the resulting
training algorithm achieves comparable results on generating realistic images
on MNIST, CelebA, and CIFAR10. Our code is available at
https://github.com/ghliu/SB-FBSDE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Loop: A Framework for Trustworthy Machine Learning in Power
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jochen Stiasny, Samuel Chevalier, Rahul Nellikkath, Brynjar Sævarsson, Spyros Chatzivasileiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep decarbonization of the energy sector will require massive penetration of
stochastic renewable energy resources and an enormous amount of grid asset
coordination; this represents a challenging paradigm for the power system
operators who are tasked with maintaining grid stability and security in the
face of such changes. With its ability to learn from complex datasets and
provide predictive solutions on fast timescales, machine learning (ML) is
well-posed to help overcome these challenges as power systems transform in the
coming decades. In this work, we outline five key challenges (dataset
generation, data pre-processing, model training, model assessment, and model
embedding) associated with building trustworthy ML models which learn from
physics-based simulation data. We then demonstrate how linking together
individual modules, each of which overcomes a respective challenge, at
sequential stages in the machine learning pipeline can help enhance the overall
performance of the training process. In particular, we implement methods that
connect different elements of the learning pipeline through feedback, thus
"closing the loop" between model training, performance assessments, and
re-training. We demonstrate the effectiveness of this framework, its
constituent modules, and its feedback connections by learning the N-1
small-signal stability margin associated with a detailed model of a proposed
North Sea Wind Power Hub system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the 11th Bulk Power Systems Dynamics and Control
  Symposium (IREP 2022), July 25-30, 2022, Banff, Canada. 21 pages, 12 figures,
  5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Contrastive</span> <span class="highlight-title">Pretrain</span>ing for Echocardiography Segmentation with Limited
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Saeed, Rand Muhtaseb, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has proven useful in many applications where access to
labelled data is limited. The lack of annotated data is particularly
problematic in medical image segmentation as it is difficult to have clinical
experts manually annotate large volumes of data such as cardiac structures in
ultrasound images of the heart. In this paper, We propose a self supervised
contrastive learning method to segment the left ventricle from echocardiography
when limited annotated images exist. Furthermore, we study the effect of
contrastive pretraining on two well-known segmentation networks, UNet and
DeepLabV3. Our results show that contrastive pretraining helps improve the
performance on left ventricle segmentation, particularly when annotated data is
scarce. We show how to achieve comparable results to state-of-the-art fully
supervised algorithms when we train our models in a self-supervised fashion
followed by fine-tuning on just 5\% of the data. We show that our solution
outperforms what is currently published on a large public dataset
(EchoNet-Dynamic) achieving a Dice score of 0.9252. We also compare the
performance of our solution on another smaller dataset (CAMUS) to demonstrate
the generalizability of our proposed solution. The code is available at
(https://github.com/BioMedIA-MBZUAI/contrastive-echo).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOCUS: Familiar Objects in Common and Uncommon Settings <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyatham Kattakinda, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard training datasets for deep learning often contain objects in common
settings (e.g., "a horse on grass" or "a ship in water") since they are usually
collected by randomly scraping the web. Uncommon and rare settings (e.g., "a
plane on water", "a car in snowy weather") are thus severely under-represented
in the training data. This can lead to an undesirable bias in model predictions
towards common settings and create a false sense of accuracy. In this paper, we
introduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset
for stress-testing the generalization power of deep image classifiers. By
leveraging the power of modern search engines, we deliberately gather data
containing objects in common and uncommon settings in a wide range of
locations, weather conditions, and time of day. We present a detailed analysis
of the performance of various popular image classifiers on our dataset and
demonstrate a clear drop in performance when classifying images in uncommon
settings. By analyzing deep features of these models, we show that such errors
can be due to the use of spurious features in model predictions. We believe
that our dataset will aid researchers in understanding the inability of deep
models to generalize well to uncommon settings and drive future work on
improving their distributional robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 14 figures, 4 tables. Accepted to ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature robustness and sex differences in medical imaging: a case study
  in MRI-based Alzheimer's disease detection <span class="chip">MICCAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eike Petersen, Aasa Feragen, Maria Luise da Costa Zemsch, Anders Henriksen, Oskar Eiler Wiese Christensen, Melanie Ganz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks have enabled significant improvements in
medical image-based diagnosis. It is, however, increasingly clear that these
models are susceptible to performance degradation when facing spurious
correlations and dataset shift, leading, e.g., to underperformance on
underrepresented patient groups. In this paper, we compare two classification
schemes on the ADNI MRI dataset: a simple logistic regression model using
manually selected volumetric features, and a convolutional neural network
trained on 3D MRI data. We assess the robustness of the trained models in the
face of varying dataset splits, training set sex composition, and stage of
disease. In contrast to earlier work in other imaging modalities, we do not
observe a clear pattern of improved model performance for the majority group in
the training dataset. Instead, while logistic regression is fully robust to
dataset composition, we find that CNN performance is generally improved for
both male and female subjects when including more female subjects in the
training dataset. We hypothesize that this might be due to inherent differences
in the pathology of the two sexes. Moreover, in our analysis, the logistic
regression model outperforms the 3D CNN, emphasizing the utility of manual
feature specification based on prior knowledge, and the need for more robust
automatic feature selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at MICCAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovery of New Multi-Level Features for Domain Generalization via
  Knowledge Corruption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Frikha, Denis Krompaß, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models that can generalize to unseen domains are essential
when applied in real-world scenarios involving strong domain shifts. We address
the challenging domain generalization (DG) problem, where a model trained on a
set of source domains is expected to generalize well in unseen domains without
any exposure to their data. The main challenge of DG is that the features
learned from the source domains are not necessarily present in the unseen
target domains, leading to performance deterioration. We assume that learning a
richer set of features is crucial to improve the transfer to a wider set of
unknown domains. For this reason, we propose COLUMBUS, a method that enforces
new feature discovery via a targeted corruption of the most relevant input and
multi-level representations of the data. We conduct an extensive empirical
evaluation to demonstrate the effectiveness of the proposed approach which
achieves new state-of-the-art results by outperforming 18 DG algorithms on
multiple DG benchmark datasets in the DomainBed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepTIMe: Deep Time-Index Meta-Learning for Non-Stationary Time-Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, Steven Hoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been actively applied to time-series forecasting, leading
to a deluge of new autoregressive model architectures. Yet, despite the
attractive properties of time-index based models, such as being a continuous
signal function over time leading to smooth representations, little attention
has been given to them. Indeed, while naive deep time-index based models are
far more expressive than the manually predefined function representations of
classical time-index based models, they are inadequate for forecasting due to
the lack of inductive biases, and the non-stationarity of time-series. In this
paper, we propose DeepTIMe, a deep time-index based model trained via a
meta-learning formulation which overcomes these limitations, yielding an
efficient and accurate forecasting model. Extensive experiments on real world
datasets demonstrate that our approach achieves competitive results with
state-of-the-art methods, and is highly efficient. Code is available at
https://github.com/salesforce/DeepTIMe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Inverse Optimization: Offline and Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.14015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.14015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Besbes, Yuri Fonseca, Ilan Lobel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problems of offline and online contextual optimization with
feedback information, where instead of observing the loss, we observe,
after-the-fact, the optimal action an oracle with full knowledge of the
objective function would have taken. We aim to minimize regret, which is
defined as the difference between our losses and the ones incurred by an
all-knowing oracle. In the offline setting, the decision-maker has information
available from past periods and needs to make one decision, while in the online
setting, the decision-maker optimizes decisions dynamically over time based a
new set of feasible actions and contextual functions in each period. For the
offline setting, we characterize the optimal minimax policy, establishing the
performance that can be achieved as a function of the underlying geometry of
the information induced by the data. In the online setting, we leverage this
geometric characterization to optimize the cumulative regret. We develop an
algorithm that yields the first regret bound for this problem that is
logarithmic in the time horizon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise-Stable Rigid Graphs for Euclidean Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1907.06441v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1907.06441v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zishuo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed a new criterion \textit{noise-stability}, which revised the
classical rigidity theory, for evaluation of MDS algorithms which can
truthfully represent the fidelity of global structure reconstruction; then we
proved the noise-stability of the cMDS algorithm in generic conditions, which
provides a rigorous theoretical guarantee for the precision and theoretical
bounds for Euclidean embedding and its application in fields including wireless
sensor network localization and satellite positioning.
  Furthermore, we looked into previous work about minimum-cost globally rigid
spanning subgraph, and proposed an algorithm to construct a minimum-cost
noise-stable spanning graph in the Euclidean space, which enabled reliable
localization on sparse graphs of noisy distance constraints with linear numbers
of edges and sublinear costs in total edge lengths. Additionally, this
algorithm also suggests a scheme to reconstruct point clouds from pairwise
distances at a minimum of $O(n)$ time complexity, down from $O(n^3)$ for cMDS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>low quality as is my undergraduate work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Musical Instrument Classification via Low-Dimensional Feature Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.08444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.08444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zishuo Zhao, Haoyun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music is a mysterious language that conveys feeling and thoughts via
different tones and timbre. For better understanding of timbre in music, we
chose music data of 6 representative instruments, analysed their timbre
features and classified them. Instead of the current trend of Neural Network
for black-box classification, our project is based on a combination of MFCC and
LPC, and augmented with a 6-dimensional feature vector designed by ourselves
from observation and attempts. In our white-box model, we observed significant
patterns of sound that distinguish different timbres, and discovered some
connection between objective data and subjective senses. With a totally
32-dimensional feature vector and a naive all-pairs SVM, we achieved improved
classification accuracy compared to a single tool. We also attempted to analyze
music pieces downloaded from the Internet, found out different performance on
different instruments, explored the reasons and suggested possible ways to
improve the performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>low quality as is my undergraduate work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Binary Forward Exploration: Learning Rate Scheduling Method for
  Stochastic Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new gradient-based optimization approach by automatically scheduling the
learning rate has been proposed recently, which is called Binary Forward
Exploration (BFE). The Adaptive version of BFE has also been discussed
thereafter. In this paper, the improved algorithms based on them will be
investigated, in order to optimize the efficiency and robustness of the new
methodology. This improved approach provides a new perspective to scheduling
the update of learning rate and will be compared with the stochastic gradient
descent, aka SGD algorithm with momentum or Nesterov momentum and the most
successful adaptive learning rate algorithm e.g. Adam. The goal of this method
does not aim to beat others but provide a different viewpoint to optimize the
gradient descent process. This approach combines the advantages of the
first-order and second-order optimizations in the aspects of speed and
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robot Program Parameter Inference via Differentiable Shadow Program
  Inversion <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.14452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.14452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Alt, Darko Katic, Rainer Jäkel, Asil Kaan Bozcuoglu, Michael Beetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Challenging manipulation tasks can be solved effectively by combining
individual robot skills, which must be parameterized for the concrete physical
environment and task at hand. This is time-consuming and difficult for human
programmers, particularly for force-controlled skills. To this end, we present
Shadow Program Inversion (SPI), a novel approach to infer optimal skill
parameters directly from data. SPI leverages unsupervised learning to train an
auxiliary differentiable program representation ("shadow program") and realizes
parameter inference via gradient-based model inversion. Our method enables the
use of efficient first-order optimizers to infer optimal parameters for
originally non-differentiable skills, including many skill variants currently
used in production. SPI zero-shot generalizes across task objectives, meaning
that shadow programs do not need to be retrained to infer parameters for
different task variants. We evaluate our methods on three different robots and
skill frameworks in industrial and household scenarios. Code and examples are
available at https://innolab.artiminds.com/icra2021.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, presented at IEEE International Conference on
  Robotics and Automation (ICRA), Xi'an, China, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Susceptibility of Continual Learning Against Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikmat Khan, Pir Masoom Shah, Syed Farhan Alam Zaidi, Saif ul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider all three scenarios (i.e, task-incremental leaning, domain-incremental
learning and class-incremental learning) of continual learning and explore
three regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations, we identify potential
limitations in continual learning approaches against adversarial attacks. Our
empirical study recommends that the research community consider the robustness
of the proposed continual learning approaches and invest extensive efforts in
mitigating catastrophic forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy,
  Uncertainty, and Robustness <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.12639v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.12639v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namuk Park, Songkuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network ensembles, such as Bayesian neural networks (BNNs), have shown
success in the areas of uncertainty estimation and robustness. However, a
crucial challenge prohibits their use in practice. BNNs require a large number
of predictions to produce reliable results, leading to a significant increase
in computational cost. To alleviate this issue, we propose spatial smoothing, a
method that spatially ensembles neighboring feature map points of convolutional
neural networks. By simply adding a few blur layers to the models, we
empirically show that spatial smoothing improves accuracy, uncertainty
estimation, and robustness of BNNs across a whole range of ensemble sizes. In
particular, BNNs incorporating spatial smoothing achieve high predictive
performance merely with a handful of ensembles. Moreover, this method also can
be applied to canonical deterministic neural networks to improve the
performances. A number of evidences suggest that the improvements can be
attributed to the stabilized feature maps and the smoothing of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing them
as special cases of spatial smoothing. These not only enhance accuracy, but
also improve uncertainty estimation and robustness by making the loss landscape
smoother in the same manner as spatial smoothing. The code is available at
https://github.com/xxxnell/spatial-smoothing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Meta-learning for Low-resource Text Classification and
  Generation via Memory Imitation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng, Dongkyu Lee, Yiping Song, Jian Sun, Nevin L. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building models of natural language processing (NLP) is challenging in
low-resource scenarios where only limited data are available.
Optimization-based meta-learning algorithms achieve promising results in
low-resource scenarios by adapting a well-generalized model initialization to
handle new tasks. Nonetheless, these approaches suffer from the memorization
overfitting issue, where the model tends to memorize the meta-training tasks
while ignoring support sets when adapting to new tasks. To address this issue,
we propose a memory imitation meta-learning (MemIML) method that enhances the
model's reliance on support sets for task adaptation. Specifically, we
introduce a task-specific memory module to store support set information and
construct an imitation module to force query sets to imitate the behaviors of
some representative support-set samples stored in the memory. A theoretical
analysis is provided to prove the effectiveness of our method, and empirical
results also demonstrate that our method outperforms competitive baselines on
both text classification and generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Camera Ready; modified emails</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards the Practical Utility of Federated Learning in the Medical
  Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongjun Yang, Hyeonji Hwang, Daeyoung Kim, Radhika Dua, Jong-Yeup Kim, Eunho Yang, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is an active area of research. One of the most
suitable areas for adopting FL is the medical domain, where patient privacy
must be respected. Previous research, however, does not fully consider who will
most likely use FL in the medical domain. It is not the hospitals who are eager
to adopt FL, but the service providers such as IT companies who want to develop
machine learning models with real patient records. Moreover, service providers
would prefer to focus on maximizing the performance of the models at the lowest
cost possible. In this work, we propose empirical benchmarks of FL methods
considering both performance and monetary cost with three real-world datasets:
electronic health records, skin cancer images, and electrocardiogram datasets.
We also propose Federated learning with Proximal regularization eXcept local
Normalization (FedPxN), which, using a simple combination of FedProx and FedBN,
outperforms all other FL algorithms while consuming only slightly more power
than the most power efficient method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Certified Defenses against Data Poisoning with (Deterministic)
  Finite Aggregation <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02628v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02628v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Alexander Levine, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data poisoning attacks aim at manipulating model behaviors through distorting
training data. Previously, an aggregation-based certified defense, Deep
Partition Aggregation (DPA), was proposed to mitigate this threat. DPA predicts
through an aggregation of base classifiers trained on disjoint subsets of data,
thus restricting its sensitivity to dataset distortions. In this work, we
propose an improved certified defense against general poisoning attacks, namely
Finite Aggregation. In contrast to DPA, which directly splits the training set
into disjoint subsets, our method first splits the training set into smaller
disjoint subsets and then combines duplicates of them to build larger (but not
disjoint) subsets for training base classifiers. This reduces the worst-case
impacts of poison samples and thus improves certified robustness bounds. In
addition, we offer an alternative view of our method, bridging the designs of
deterministic and stochastic aggregation-based certified defenses. Empirically,
our proposed Finite Aggregation consistently improves certificates on MNIST,
CIFAR-10, and GTSRB, boosting certified fractions by up to 3.05%, 3.87% and
4.77%, respectively, while keeping the same clean accuracies as DPA's,
effectively establishing a new state of the art in (pointwise) certified
robustness against data poisoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Optimal Treatment Strategies for Sepsis Using Offline
  Reinforcement Learning in Continuous Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wang, Huiying Zhao, Peng Ren, Yuxi Zhou, Ming Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sepsis is a leading cause of death in the ICU. It is a disease requiring
complex interventions in a short period of time, but its optimal treatment
strategy remains uncertain. Evidence suggests that the practices of currently
used treatment strategies are problematic and may cause harm to patients. To
address this decision problem, we propose a new medical decision model based on
historical data to help clinicians recommend the best reference option for
real-time treatment. Our model combines offline reinforcement learning and deep
reinforcement learning to solve the problem of traditional reinforcement
learning in the medical field due to the inability to interact with the
environment, while enabling our model to make decisions in a continuous
state-action space. We demonstrate that, on average, the treatments recommended
by the model are more valuable and reliable than those recommended by
clinicians. In a large validation dataset, we find out that the patients whose
actual doses from clinicians matched the decisions made by AI has the lowest
mortality rates. Our model provides personalized and clinically interpretable
treatment decisions for sepsis to improve patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Networks for Encoding Dynamic Security-Constrained Optimal Power
  Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.07939v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.07939v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilgiz Murzakhanov, Andreas Venzke, George S. Misyris, Spyros Chatzivasileiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a framework to capture previously intractable
optimization constraints and transform them to a mixed-integer linear program,
through the use of neural networks. We encode the feasible space of
optimization problems characterized by both tractable and intractable
constraints, e.g. differential equations, to a neural network. Leveraging an
exact mixed-integer reformulation of neural networks, we solve mixed-integer
linear programs that accurately approximate solutions to the originally
intractable non-linear optimization problem. We apply our methods to the AC
optimal power flow problem (AC-OPF), where directly including dynamic security
constraints renders the AC-OPF intractable. Our proposed approach has the
potential to be significantly more scalable than traditional approaches. We
demonstrate our approach for power system operation considering N-1 security
and small-signal stability, showing how it can efficiently obtain cost-optimal
solutions which at the same time satisfy both static and dynamic security
constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the 11th Bulk Power Systems Dynamics and Control
  Symposium (IREP 2022), July 25-30, 2022, Banff, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Cross-Modal</span> <span class="highlight-title">Transformer</span> GAN: A Brain Structure-Function Deep Fusing
  Framework for Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junren Pan, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal fusion of different types of neuroimaging data has shown great
promise for predicting the progression of Alzheimer's Disease(AD). However,
most existing methods applied in neuroimaging can not efficiently fuse the
functional and structural information from multi-modal neuroimages. In this
work, a novel cross-modal transformer generative adversarial network(CT-GAN) is
proposed to fuse functional information contained in resting-state functional
magnetic resonance imaging (rs-fMRI) and structural information contained in
Diffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match
functional information to structural information efficiently and maximize the
capability of extracting complementary information from rs-fMRI and DTI. By
capturing the deep complementary information between structural features and
functional features, the proposed CT-GAN can detect the AD-related brain
connectivity, which could be used as a bio-marker of AD. Experimental results
show that the proposed model can not only improve classification performance
but also detect the AD-related brain connectivity effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comparison of latent semantic analysis and correspondence analysis of
  document-term matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Qi, David J. Hessen, Tejaswini Deoskar, Peter G. M. van der Heijden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent semantic analysis (LSA) and correspondence analysis (CA) are two
techniques that use a singular value decomposition (SVD) for dimensionality
reduction. LSA has been extensively used to obtain low-dimensional
representations that capture relationships among documents and terms. In this
article, we present a theoretical analysis and comparison of the two techniques
in the context of document-term matrices. We show that CA has some attractive
properties as compared to LSA, for instance that effects of margins arising
from differing document-lengths and term-frequencies are effectively
eliminated, so that the CA solution is optimally suited to focus on
relationships among documents and terms. A unifying framework is proposed that
includes both CA and LSA as special cases. We empirically compare CA to various
LSA based methods on text categorization in English and authorship attribution
on historical Dutch texts, and find that CA performs significantly better. We
also apply CA to a long-standing question regarding the authorship of the Dutch
national anthem Wilhelmus and provide further support that it can be attributed
to the author Datheen, amongst several contenders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Multimodal</span> Interactive Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Timothy Lillicrap, Alistair Muldal, Blake Richards, Adam Santoro, Tamara von Glehn, Greg Wayne, Nathaniel Wong, Chen Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating agents that can interact naturally with humans is a common goal in
artificial intelligence (AI) research. However, evaluating these interactions
is challenging: collecting online human-agent interactions is slow and
expensive, yet faster proxy metrics often do not correlate well with
interactive evaluation. In this paper, we assess the merits of these existing
evaluation metrics and present a novel approach to evaluation called the
Standardised Test Suite (STS). The STS uses behavioural scenarios mined from
real human interaction data. Agents see replayed scenario context, receive an
instruction, and are then given control to complete the interaction offline.
These agent continuations are recorded and sent to human annotators to mark as
success or failure, and agents are ranked according to the proportion of
continuations in which they succeed. The resulting STS is fast, controlled,
interpretable, and representative of naturalistic interactions. Altogether, the
STS consolidates much of what is desirable across many of our standard
evaluation metrics, allowing us to accelerate research progress towards
producing agents that can interact naturally with humans. A video may be found
at https://youtu.be/YR1TngGORGQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Parallelize in a Shared-Memory Environment with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.12835v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.12835v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Re'em Harel, Yuval Pinter, Gal Oren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In past years, the world has switched to many-core and multi-core shared
memory architectures. As a result, there is a growing need to utilize these
architectures by introducing shared memory parallelization schemes to software
applications. OpenMP is the most comprehensive API that implements such
schemes, characterized by a readable interface. Nevertheless, introducing
OpenMP into code is challenging due to pervasive pitfalls in management of
parallel shared memory. To facilitate the performance of this task, many
source-to-source (S2S) compilers have been created over the years, tasked with
inserting OpenMP directives into code automatically. In addition to having
limited robustness to their input format, these compilers still do not achieve
satisfactory coverage and precision in locating parallelizable code and
generating appropriate directives. In this work, we propose leveraging recent
advances in ML techniques, specifically in natural language processing (NLP),
to replace S2S compilers altogether. We create a database (corpus), Open-OMP,
specifically for this goal. Open-OMP contains over 28,000 code snippets, half
of which contain OpenMP directives while the other half do not need
parallelization at all with high probability. We use the corpus to train
systems to automatically classify code segments in need of parallelization, as
well as suggest individual OpenMP clauses. We train several transformer models,
named PragFormer, for these tasks, and show that they outperform
statistically-trained baselines and automatic S2S parallelization compilers in
both classifying the overall need for an OpenMP directive and the introduction
of private and reduction clauses.
  Our source code and database are available at:
https://github.com/Scientific-Computing-Lab-NRCN/PragFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypergraphon Mean Field Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.16223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.16223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Cui, Wasiur R. KhudaBukhsh, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to modelling large-scale multi-agent dynamical systems
allowing interactions among more than just pairs of agents using the theory of
mean-field games and the notion of hypergraphons, which are obtained as limits
of large hypergraphs. To the best of our knowledge, ours is the first work on
mean field games on hypergraphs. Together with an extension to a multi-layer
setup, we obtain limiting descriptions for large systems of non-linear,
weakly-interacting dynamical agents. On the theoretical side, we prove the
well-foundedness of the resulting hypergraphon mean field game, showing both
existence and approximate Nash properties. On the applied side, we extend
numerical and learning algorithms to compute the hypergraphon mean field
equilibria. To verify our approach empirically, we consider an epidemic control
problem and a social rumor spreading model, where we give agents intrinsic
motivation to spread rumors to unaware agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The following article has been submitted to Chaos. v2: Full-length
  references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative training of robust k-space interpolation networks for improved
  image reconstruction with limited scan specific training samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Dawood, Felix Breuer, Paul R. Burd, István Homolya, Johannes Oberberger, Peter M. Jakob, Martin Blaimer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To evaluate an iterative learning approach for enhanced performance
of Robust Artificial-neural-networks for K-space Interpolation (RAKI), when
only a limited amount of training data (auto-calibration signals, ACS) are
available for accelerated standard 2D imaging. Methods: In a first step, the
RAKI model was optimized for the case of strongly limited training data amount.
In the iterative learning approach (termed iterative RAKI), the optimized RAKI
model is initially trained using original and augmented ACS obtained from a
linear parallel imaging reconstruction. Subsequently, the RAKI convolution
filters are refined iteratively using original and augmented ACS extracted from
the previous RAKI reconstruction. Evaluation was carried out on 200
retrospectively undersampled in-vivo datasets from the fastMRI neuro database
with different contrast settings. Results: For limited training data (18 and 22
ACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard
RAKI by reducing residual artefacts and yields strong noise suppression when
compared to standard parallel imaging, underlined by quantitative
reconstruction quality metrics. In combination with a phase constraint, further
reconstruction improvements can be achieved. Additionally, iterative RAKI shows
better performance than both GRAPPA and RAKI in case of pre-scan calibration
with varying contrast between training- and undersampled data. Conclusion: The
iterative learning approach with RAKI benefits from standard RAKIs well known
noise suppression feature but requires less original training data for the
accurate reconstruction of standard 2D images thereby improving net
acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Magnetic Resonance in Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strongly Augmented <span class="highlight-title">Contrastive</span> Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhi Deng, Dong Huang, Ding-Hua Chen, Chang-Dong Wang, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep clustering has attracted increasing attention in recent years due to its
capability of joint representation learning and clustering via deep neural
networks. In its latest developments, the contrastive learning has emerged as
an effective technique to substantially enhance the deep clustering
performance. However, the existing contrastive learning based deep clustering
algorithms mostly focus on some carefully-designed augmentations (often with
limited transformations to preserve the structure), referred to as weak
augmentations, but cannot go beyond the weak augmentations to explore the more
opportunities in stronger augmentations (with more aggressive transformations
or even severe distortions). In this paper, we present an end-to-end deep
clustering approach termed Strongly Augmented Contrastive Clustering (SACC),
which extends the conventional two-augmentation-view paradigm to multiple views
and jointly leverages strong and weak augmentations for strengthened deep
clustering. Particularly, we utilize a backbone network with triply-shared
weights, where a strongly augmented view and two weakly augmented views are
incorporated. Based on the representations produced by the backbone, the
weak-weak view pair and the strong-weak view pairs are simultaneously exploited
for the instance-level contrastive learning (via an instance projector) and the
cluster-level contrastive learning (via a cluster projector), which, together
with the backbone, can be jointly optimized in a purely unsupervised manner.
Experimental results on five challenging image datasets have shown the
superiority of our SACC approach over the state-of-the-art. The code is
available at https://github.com/dengxiaozhi/SACC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting
  Epidemics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhurima Panja, Tanujit Chakraborty, Uttam Kumar, Nan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infectious diseases remain among the top contributors to human illness and
death worldwide, among which many diseases produce epidemic waves of infection.
The unavailability of specific drugs and ready-to-use vaccines to prevent most
of these epidemics makes the situation worse. These force public health
officials and policymakers to rely on early warning systems generated by
reliable and accurate forecasts of epidemics. Accurate forecasts of epidemics
can assist stakeholders in tailoring countermeasures, such as vaccination
campaigns, staff scheduling, and resource allocation, to the situation at hand,
which could translate to reductions in the impact of a disease. Unfortunately,
most of these past epidemics exhibit nonlinear and non-stationary
characteristics due to their spreading fluctuations based on seasonal-dependent
variability and the nature of these epidemics. We analyse a wide variety of
epidemic time series datasets using a maximal overlap discrete wavelet
transform (MODWT) based autoregressive neural network and call it EWNet model.
MODWT techniques effectively characterize non-stationary behavior and seasonal
dependencies in the epidemic time series and improve the nonlinear forecasting
scheme of the autoregressive neural network in the proposed ensemble wavelet
network framework. From a nonlinear time series viewpoint, we explore the
asymptotic stationarity of the proposed EWNet model to show the asymptotic
behavior of the associated Markov Chain. We also theoretically investigate the
effect of learning stability and the choice of hidden neurons in the proposal.
From a practical perspective, we compare our proposed EWNet framework with
several statistical, machine learning, and deep learning models. Experimental
results show that the proposed EWNet is highly competitive compared to the
state-of-the-art epidemic forecasting methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AGIC: Approximate Gradient Inversion Attack on Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13784v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13784v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Xu, Chi Hong, Jiyue Huang, Lydia Y. Chen, Jérémie Decouchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a private-by-design distributed learning paradigm where
clients train local models on their own data before a central server aggregates
their local updates to compute a global model. Depending on the aggregation
method used, the local updates are either the gradients or the weights of local
learning models. Recent reconstruction attacks apply a gradient inversion
optimization on the gradient update of a single minibatch to reconstruct the
private data used by clients during training. As the state-of-the-art
reconstruction attacks solely focus on single update, realistic adversarial
scenarios are overlooked, such as observation across multiple updates and
updates trained from multiple mini-batches. A few studies consider a more
challenging adversarial scenario where only model updates based on multiple
mini-batches are observable, and resort to computationally expensive simulation
to untangle the underlying samples for each local step. In this paper, we
propose AGIC, a novel Approximate Gradient Inversion Attack that efficiently
and effectively reconstructs images from both model or gradient updates, and
across multiple epochs. In a nutshell, AGIC (i) approximates gradient updates
of used training samples from model updates to avoid costly simulation
procedures, (ii) leverages gradient/model updates collected from multiple
epochs, and (iii) assigns increasing weights to layers with respect to the
neural network structure for reconstruction quality. We extensively evaluate
AGIC on three datasets, CIFAR-10, CIFAR-100 and ImageNet. Our results show that
AGIC increases the peak signal-to-noise ratio (PSNR) by up to 50% compared to
two representative state-of-the-art gradient inversion attacks. Furthermore,
AGIC is faster than the state-of-the-art simulation based attack, e.g., it is
5x faster when attacking FedAvg with 8 local steps in between model updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the 41st International Symposium on
  Reliable Distributed Systems (SRDS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regotron: Regularizing the Tacotron2 architecture via monotonic
  alignment loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efthymios Georgiou, Kosmas Kritsis, Georgios Paraskevopoulos, Athanasios Katsamanis, Vassilis Katsouros, Alexandros Potamianos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep learning Text-to-Speech (TTS) systems have achieved impressive
performance by generating speech close to human parity. However, they suffer
from training stability issues as well as incorrect alignment of the
intermediate acoustic representation with the input text sequence. In this
work, we introduce Regotron, a regularized version of Tacotron2 which aims to
alleviate the training issues and at the same time produce monotonic
alignments. Our method augments the vanilla Tacotron2 objective function with
an additional term, which penalizes non-monotonic alignments in the
location-sensitive attention mechanism. By properly adjusting this
regularization term we show that the loss curves become smoother, and at the
same time Regotron consistently produces monotonic alignments in unseen
examples even at an early stage (13\% of the total number of epochs) of its
training process, whereas the fully converged Tacotron2 fails to do so.
Moreover, our proposed regularization method has no additional computational
overhead, while reducing common TTS mistakes and achieving slighlty improved
speech naturalness according to subjective mean opinion scores (MOS) collected
from 50 evaluators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Lasso based Sparse Learning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1908.07220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1908.07220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingvild M. Helgøy, Yushu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bayesian Lasso is constructed in the linear regression framework and
applies the Gibbs sampling to estimate the regression parameters. This paper
develops a new sparse learning model, named the Bayesian Lasso Sparse (BLS)
model, that takes the hierarchical model formulation of the Bayesian Lasso. The
main difference from the original Bayesian Lasso lies in the estimation
procedure; the BLS method uses a learning algorithm based on the type-II
maximum likelihood procedure. Opposed to the Bayesian Lasso, the BLS provides
sparse estimates of the regression parameters. The BLS method is also derived
for nonlinear supervised learning problems by introducing kernel functions. We
compare the BLS model to the well known Relevance Vector Machine, the Fast
Laplace method, the Byesian Lasso, and the Lasso, on both simulated and real
data. The numerical results show that the BLS is sparse and precise, especially
when dealing with noisy and irregular dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Granular-ball Learning Model of Pawlak Rough Set and
  Neighborhood Rough Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03349v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03349v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyin Xia, Cheng Wang, Guoyin Wang, Weiping Ding, Xinbo Gao, Jianhang Yu, Yujia Zhai, Zizhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pawlak rough set and neighborhood rough set are the two most common rough set
theoretical models. Pawlak can use equivalence classes to represent knowledge,
but it cannot process continuous data; neighborhood rough sets can process
continuous data, but it loses the ability of using equivalence classes to
represent knowledge. To this end, this paper presents a granular-ball rough set
based on the granular-ball computing. The granular-ball rough set can
simultaneously represent Pawlak rough sets, and the neighborhood rough set, so
as to realize the unified representation of the two. This makes the
granular-ball rough set not only can deal with continuous data, but also can
use equivalence classes for knowledge representation. In addition, we propose
an implementation algorithms of granular-ball rough sets. The experimental
results on benchmark datasets demonstrate that, due to the combination of the
robustness and adaptability of the granular-ball computing, the learning
accuracy of the granular-ball rough set has been greatly improved compared with
the Pawlak rough set and the traditional neighborhood rough set. The
granular-ball rough set also outperforms nine popular or the state-of-the-art
feature selection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combating Distribution Shift for Accurate Time Series Forecasting via
  Hypernetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenying Duan, Xiaoxi He, Lu Zhou, Lothar Thiele, Hong Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting has widespread applications in urban life ranging
from air quality monitoring to traffic analysis. However, accurate time series
forecasting is challenging because real-world time series suffer from the
distribution shift problem, where their statistical properties change over
time. Despite extensive solutions to distribution shifts in domain adaptation
or generalization, they fail to function effectively in unknown,
constantly-changing distribution shifts, which are common in time series. In
this paper, we propose Hyper Time- Series Forecasting (HTSF), a
hypernetwork-based framework for accurate time series forecasting under
distribution shift. HTSF jointly learns the time-varying distributions and the
corresponding forecasting models in an end-to-end fashion. Specifically, HTSF
exploits the hyper layers to learn the best characterization of the
distribution shifts, generating the model parameters for the main layers to
make accurate predictions. We implement HTSF as an extensible framework that
can incorporate diverse time series forecasting models such as RNNs and
Transformers. Extensive experiments on 9 benchmarks demonstrate that HTSF
achieves state-of-the-art performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Decision Trees Through MaxSAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josep Alos, Carlos Ansotegui, Eduard Torres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach to improve the accuracy-interpretability trade-off of
Machine Learning (ML) Decision Trees (DTs). In particular, we apply Maximum
Satisfiability technology to compute Minimum Pure DTs (MPDTs). We improve the
runtime of previous approaches and, show that these MPDTs can outperform the
accuracy of DTs generated with the ML framework sklearn.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wide Neural Networks Forget Less Catastrophically <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11526v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11526v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A primary focus area in continual learning research is alleviating the
"catastrophic forgetting" problem in neural networks by designing new
algorithms that are more robust to the distribution shifts. While the recent
progress in continual learning literature is encouraging, our understanding of
what properties of neural networks contribute to catastrophic forgetting is
still limited. To address this, instead of focusing on continual learning
algorithms, in this work, we focus on the model itself and study the impact of
"width" of the neural network architecture on catastrophic forgetting, and show
that width has a surprisingly significant effect on forgetting. To explain this
effect, we study the learning dynamics of the network from various perspectives
such as gradient orthogonality, sparsity, and lazy training regime. We provide
potential explanations that are consistent with the empirical results across
different architectures and continual learning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Spatio-Temporal Neural Network Forecasting Approach for Emulation of
  Firefront Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Bolt, Carolyn Huston, Petra Kuhnert, Joel Janek Dabrowski, James Hilton, Conrad Sanderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational simulations of wildfire spread typically employ empirical
rate-of-spread calculations under various conditions (such as terrain, fuel
type, weather). Small perturbations in conditions can often lead to significant
changes in fire spread (such as speed and direction), necessitating a
computationally expensive large set of simulations to quantify uncertainty.
Model emulation seeks alternative representations of physical models using
machine learning, aiming to provide more efficient and/or simplified surrogate
models. We propose a dedicated spatio-temporal neural network based framework
for model emulation, able to capture the complex behaviour of fire spread
models. The proposed approach can approximate forecasts at fine spatial and
temporal resolutions that are often challenging for neural network based
approaches. Furthermore, the proposed approach is robust even with small
training sets, due to novel data augmentation methods. Empirical experiments
show good agreement between simulated and emulated firefronts, with an average
Jaccard score of 0.76.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptive Hand Keypoint and Pixel Localization in the Wild <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08344v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08344v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takehiko Ohkawa, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Mohammed Saifuddin, Briana Bumgardner, Farhan Tanvir, Esra Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in
the worst scenario, they may lead to adverse drug reactions (ADRs). Predicting
all DDIs is a challenging and critical problem. Most existing computational
models integrate drug-centric information from different sources and leverage
them as features in machine learning classifiers to predict DDIs. However,
these models have a high chance of failure, especially for the new drugs when
all the information is not available. This paper proposes a novel Hypergraph
Neural Network (HyGNN) model based on only the SMILES string of drugs,
available for any drug, for the DDI prediction problem. To capture the drug
similarities, we create a hypergraph from drugs' chemical substructures
extracted from the SMILES strings. Then, we develop HyGNN consisting of a novel
attention-based hypergraph edge encoder to get the representation of drugs as
hyperedges and a decoder to predict the interactions between drug pairs.
Furthermore, we conduct extensive experiments to evaluate our model and compare
it with several state-of-the-art methods. Experimental results demonstrate that
our proposed HyGNN model effectively predicts DDIs and impressively outperforms
the baselines with a maximum ROC-AUC and PR-AUC of 97.9% and 98.1%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Defense of Core-set: A Density-aware Core-set Selection for Active
  Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeachan Kim, Bonggun Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning enables the efficient construction of a labeled dataset by
labeling informative samples from an unlabeled dataset. In a real-world active
learning scenario, considering the diversity of the selected samples is crucial
because many redundant or highly similar samples exist. Core-set approach is
the promising diversity-based method selecting diverse samples based on the
distance between samples. However, the approach poorly performs compared to the
uncertainty-based approaches that select the most difficult samples where
neural models reveal low confidence. In this work, we analyze the feature space
through the lens of the density and, interestingly, observe that locally sparse
regions tend to have more informative samples than dense regions. Motivated by
our analysis, we empower the core-set approach with the density-awareness and
propose a density-aware core-set (DACS). The strategy is to estimate the
density of the unlabeled samples and select diverse samples mainly from sparse
regions. To reduce the computational bottlenecks in estimating the density, we
also introduce a new density approximation based on locality-sensitive hashing.
Experimental results clearly demonstrate the efficacy of DACS in both
classification and regression tasks and specifically show that DACS can produce
state-of-the-art performance in a practical scenario. Since DACS is weakly
dependent on neural architectures, we present a simple yet effective
combination method to show that the existing methods can be beneficially
combined with DACS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the ACM SIGKDD Conference on Knowledge Discovery &
  Data Mining, 2022 (KDD'22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Several Approximation Algorithms for Sparse Best Rank-1 Approximation to
  Higher-Order Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.03092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.03092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianpeng Mao, Yuning Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse tensor best rank-1 approximation (BR1Approx), which is a sparsity
generalization of the dense tensor BR1Approx, and is a higher-order extension
of the sparse matrix BR1Approx, is one of the most important problems in sparse
tensor decomposition and related problems arising from statistics and machine
learning. By exploiting the multilinearity as well as the sparsity structure of
the problem, four approximation algorithms are proposed, which are easily
implemented, of low computational complexity, and can serve as initial
procedures for iterative algorithms. In addition, theoretically guaranteed
worst-case approximation lower bounds are proved for all the algorithms. We
provide numerical experiments on synthetic and real data to illustrate the
effectiveness of the proposed algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Multidimensional Discriminator Output for Generative
  Adversarial Networks <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyu Dai, Haibin Hang, Anuj Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of multidimensional discriminator (critic) output for Generative
Adversarial Networks has been underexplored in the literature. In this paper,
we generalize the Wasserstein GAN framework to take advantage of
multidimensional critic output and explore its properties. We also introduce a
square-root velocity transformation (SRVT) block which favors training in the
multidimensional setting. Proofs of properties are based on our proposed
maximal p-centrality discrepancy, which is bounded above by p-Wasserstein
distance and fits the Wasserstein GAN framework with multidimensional critic
output n. Especially when n = 1 and p = 1, the proposed discrepancy equals
1-Wasserstein distance. Theoretical analysis and empirical evidence show that
high-dimensional critic output has its advantage on distinguishing real and
fake distributions, and benefits faster convergence and diversity of results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Frontiers in Adversarial Machine Learning ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Modularity: Towards Understanding the Cross-Layer Transition of
  Feature Representations in Deep Neural Networks <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Lu, Wen Yang, Yunzhe Zhang, Zuohui Chen, Jinyin Chen, Qi Xuan, Zhen Wang, Xiaoniu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are good arguments to support the claim that deep neural networks
(DNNs) capture better feature representations than the previous hand-crafted
feature engineering, which leads to a significant performance improvement. In
this paper, we move a tiny step towards understanding the dynamics of feature
representations over layers. Specifically, we model the process of class
separation of intermediate representations in pre-trained DNNs as the evolution
of communities in dynamic graphs. Then, we introduce modularity, a generic
metric in graph theory, to quantify the evolution of communities. In the
preliminary experiment, we find that modularity roughly tends to increase as
the layer goes deeper and the degradation and plateau arise when the model
complexity is great relative to the dataset. Through an asymptotic analysis, we
prove that modularity can be broadly used for different applications. For
example, modularity provides new insights to quantify the difference between
feature representations. More crucially, we demonstrate that the degradation
and plateau in modularity curves represent redundant layers in DNNs and can be
pruned with minimal impact on performance, which provides theoretical guidance
for layer pruning. Our code is available at
https://github.com/yaolu-zjut/Dynamic-Graphs-Construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Estimating Instance-dependent Bayes-label Transition Matrix using a Deep
  Neural Network <span class="chip">ICML 22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.13001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.13001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Erkun Yang, Bo Han, <span class="highlight-author">Yang Liu</span>, Min Xu, Gang Niu, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In label-noise learning, estimating the transition matrix is a hot topic as
the matrix plays an important role in building statistically consistent
classifiers. Traditionally, the transition from clean labels to noisy labels
(i.e., clean-label transition matrix (CLTM)) has been widely exploited to learn
a clean label classifier by employing the noisy data. Motivated by that
classifiers mostly output Bayes optimal labels for prediction, in this paper,
we study to directly model the transition from Bayes optimal labels to noisy
labels (i.e., Bayes-label transition matrix (BLTM)) and learn a classifier to
predict Bayes optimal labels. Note that given only noisy data, it is ill-posed
to estimate either the CLTM or the BLTM. But favorably, Bayes optimal labels
have less uncertainty compared with the clean labels, i.e., the class
posteriors of Bayes optimal labels are one-hot vectors while those of clean
labels are not. This enables two advantages to estimate the BLTM, i.e., (a) a
set of examples with theoretically guaranteed Bayes optimal labels can be
collected out of noisy data; (b) the feasible solution space is much smaller.
By exploiting the advantages, we estimate the BLTM parametrically by employing
a deep neural network, leading to better generalization and superior
classification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 22 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Ranking and Tuning <span class="highlight-title">Pre-train</span>ed Models: A New Paradigm for Exploiting
  Model Hubs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10545v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10545v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang, <span class="highlight-author">Michael I. Jordan</span>, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model hubs with many pre-trained models (PTMs) have become a cornerstone of
deep learning. Although built at a high cost, they remain
\emph{under-exploited} -- practitioners usually pick one PTM from the provided
model hub by popularity and then fine-tune the PTM to solve the target task.
This na\"ive but common practice poses two obstacles to full exploitation of
pre-trained model hubs: first, the PTM selection by popularity has no
optimality guarantee, and second, only one PTM is used while the remaining PTMs
are ignored. An alternative might be to consider all possible combinations of
PTMs and extensively fine-tune each combination, but this would not only be
prohibitive computationally but may also lead to statistical over-fitting. In
this paper, we propose a new paradigm for exploiting model hubs that is
intermediate between these extremes. The paradigm is characterized by two
aspects: (1) We use an evidence maximization procedure to estimate the maximum
value of label evidence given features extracted by pre-trained models. This
procedure can rank all the PTMs in a model hub for various types of PTMs and
tasks \emph{before fine-tuning}. (2) The best ranked PTM can either be
fine-tuned and deployed if we have no preference for the model's architecture
or the target PTM can be tuned by the top $K$ ranked PTMs via a Bayesian
procedure that we propose. This procedure, which we refer to as
\emph{B-Tuning}, not only improves upon specialized methods designed for tuning
homogeneous PTMs, but also applies to the challenging problem of tuning
heterogeneous PTMs where it yields a new level of benchmark performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, camera-ready version for JMLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Data-Free Neural Architecture Search via Recursive Label Calibration <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Zhiqiang Shen, Yun Long, <span class="highlight-author">Eric Xing</span>, Kwang-<span class="highlight-author">Ting Chen</span>g, Chas Leichner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to explore the feasibility of neural architecture search
(NAS) given only a pre-trained model without using any original training data.
This is an important circumstance for privacy protection, bias avoidance, etc.,
in real-world scenarios. To achieve this, we start by synthesizing usable data
through recovering the knowledge from a pre-trained deep neural network. Then
we use the synthesized data and their predicted soft-labels to guide neural
architecture search. We identify that the NAS task requires the synthesized
data (we target at image domain here) with enough semantics, diversity, and a
minimal domain gap from the natural images. For semantics, we propose recursive
label calibration to produce more informative outputs. For diversity, we
propose a regional update strategy to generate more diverse and
semantically-enriched synthetic data. For minimal domain gap, we use input and
feature-level regularization to mimic the original data distribution in latent
space. We instantiate our proposed framework with three popular NAS algorithms:
DARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the
architectures discovered by searching with our synthetic data achieve accuracy
that is comparable to, or even higher than, architectures discovered by
searching from the original ones, for the first time, deriving the conclusion
that NAS can be done effectively with no need of access to the original or
called natural data if the synthesis method is well designed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust <span class="highlight-title">Self-Supervised</span> Audio-Visual Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Shi, Wei-Ning Hsu, Abdelrahman Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-based automatic speech recognition (ASR) degrades significantly in
noisy environments and is particularly vulnerable to interfering speech, as the
model cannot determine which speaker to transcribe. Audio-visual speech
recognition (AVSR) systems improve robustness by complementing the audio stream
with the visual information that is invariant to noise and helps the model
focus on the desired speaker. However, previous AVSR work focused solely on the
supervised learning setup; hence the progress was hindered by the amount of
labeled data available. In this work, we present a self-supervised AVSR
framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art
audio-visual speech representation learning model. On the largest available
AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by
~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in
the presence of babble noise, while reducing the WER of an audio-based model by
over 75% (25.8% vs. 5.8%) on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedFly: Towards Migration in Edge-based Distributed Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.01516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.01516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rehmat Ullah, Di Wu, Paul Harvey, Peter Kilpatrick, Ivor Spence, Blesson Varghese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a privacy-preserving distributed machine learning
technique that trains models while keeping all the original data generated on
devices locally. Since devices may be resource constrained, offloading can be
used to improve FL performance by transferring computational workload from
devices to edge servers. However, due to mobility, devices participating in FL
may leave the network during training and need to connect to a different edge
server. This is challenging because the offloaded computations from edge server
need to be migrated. In line with this assertion, we present FedFly, which is,
to the best of our knowledge, the first work to migrate a deep neural network
(DNN) when devices move between edge servers during FL training. Our empirical
results on the CIFAR10 dataset, with both balanced and imbalanced data
distribution, support our claims that FedFly can reduce training time by up to
33% when a device moves after 50% of the training is completed, and by up to
45% when 90% of the training is completed when compared to state-of-the-art
offloading approach in FL. FedFly has negligible overhead of up to two seconds
and does not compromise accuracy. Finally, we highlight a number of open
research issues for further investigation. FedFly can be downloaded from
https://github.com/qub-blesson/FedFly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Diverse Feature Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saachi Jain, Dimitris Tsipras, Aleksander Madry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve model generalization, model designers often restrict the features
that their models use, either implicitly or explicitly. In this work, we
explore the design space of leveraging such feature priors by viewing them as
distinct perspectives on the data. Specifically, we find that models trained
with diverse sets of feature priors have less overlapping failure modes, and
can thus be combined more effectively. Moreover, we demonstrate that jointly
training such models on additional (unlabeled) data allows them to correct each
other's mistakes, which, in turn, leads to better generalization and resilience
to spurious correlations. Code available at
https://github.com/MadryLab/copriors
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Fine-tuning of Language Models <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give simpler, sparser, and faster algorithms for differentially private
fine-tuning of large-scale pre-trained language models, which achieve the
state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.
We propose a meta-framework for this problem, inspired by the recent success of
highly parameter-efficient methods for fine-tuning. Our experiments show that
differentially private adaptations of these approaches outperform previous
private algorithms in three important dimensions: utility, privacy, and the
computational and memory cost of private training. On many commonly studied
datasets, the utility of private models approaches that of non-private models.
For example, on the MNLI dataset we achieve an accuracy of $87.8\%$ using
RoBERTa-Large and $83.5\%$ using RoBERTa-Base with a privacy budget of
$\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large
achieves an accuracy of $90.2\%$. Our findings are similar for natural language
generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,
GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8
respectively (privacy budget of $\epsilon = 6.8,\delta=$ 1e-5) whereas the
non-private baseline is $48.1$. All our experiments suggest that larger models
are better suited for private fine-tuning: while they are well known to achieve
superior accuracy non-privately, we find that they also better maintain their
accuracy when privacy is introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022. Code available at
  https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient <span class="highlight-title">Prompt</span> Tuning Makes Generalized and Calibrated
  Neural Text Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning attempts to update few task-specific parameters in pre-trained
models. It has achieved comparable performance to fine-tuning of the full
parameter set on both language understanding and generation tasks. In this
work, we study the problem of prompt tuning for neural text retrievers. We
introduce parameter-efficient prompt tuning for text retrieval across
in-domain, cross-domain, and cross-topic settings. Through an extensive
analysis, we show that the strategy can mitigate the two issues --
parameter-inefficiency and weak generalizability -- faced by fine-tuning based
retrieval methods. Notably, it can significantly improve the out-of-domain
zero-shot generalization of the retrieval models. By updating only 0.1% of the
model parameters, the prompt tuning strategy can help retrieval models achieve
better generalization performance than traditional methods in which all
parameters are updated. Finally, to facilitate research on retrievers'
cross-topic generalizability, we curate and release an academic retrieval
dataset with 18K query-results pairs in 87 topics, making it the largest
topic-specific one to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced Path Reasoning for Counterfactual Explainable Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangmeng Wang, Qian Li, Dianer Yu, Guandong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations interpret the recommendation mechanism via
exploring how minimal alterations on items or users affect the recommendation
decisions. Existing counterfactual explainable approaches face huge search
space and their explanations are either action-based (e.g., user click) or
aspect-based (i.e., item description). We believe item attribute-based
explanations are more intuitive and persuadable for users since they explain by
fine-grained item demographic features (e.g., brand). Moreover, counterfactual
explanation could enhance recommendations by filtering out negative items.
  In this work, we propose a novel Counterfactual Explainable Recommendation
(CERec) to generate item attribute-based counterfactual explanations meanwhile
to boost recommendation performance. Our CERec optimizes an explanation policy
upon uniformly searching candidate counterfactuals within a reinforcement
learning environment. We reduce the huge search space with an adaptive path
sampler by using rich context information of a given knowledge graph. We also
deploy the explanation policy to a recommendation model to enhance the
recommendation. Extensive explainability and recommendation evaluations
demonstrate CERec's ability to provide explanations consistent with user
preferences and maintain improved recommendations. We release our code at
https://github.com/Chrystalii/CERec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Every Preference Changes Differently: Neural Multi-Interest Preference
  Model with Temporal Dynamics for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shi, Yupeng Gu, Yitong Zhou, Bo Zhao, Sicun Gao, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings (vectorized representations of a user) are essential in
recommendation systems. Numerous approaches have been proposed to construct a
representation for the user in order to find similar items for retrieval tasks,
and they have been proven effective in industrial recommendation systems as
well. Recently people have discovered the power of using multiple embeddings to
represent a user, with the hope that each embedding represents the user's
interest in a certain topic. With multi-interest representation, it's important
to model the user's preference over the different topics and how the preference
change with time. However, existing approaches either fail to estimate the
user's affinity to each interest or unreasonably assume every interest of every
user fades with an equal rate with time, thus hurting the recall of candidate
retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,
an approach that not only produces multi-interest for users by using the user's
sequential engagement more effectively but also automatically learns a set of
weights to represent the preference over each embedding so that the candidates
can be retrieved from each interest proportionally. Extensive experiments have
been done on various industrial-scale datasets to demonstrate the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NASRec: Weight Sharing Neural Architecture Search for Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, Wei Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of deep neural networks provides an important driver in optimizing
recommender systems. However, the success of recommender systems lies in
delicate architecture fabrication, and thus calls for Neural Architecture
Search (NAS) to further improve its modeling. We propose NASRec, a paradigm
that trains a single supernet and efficiently produces abundant
models/sub-architectures by weight sharing. To overcome the data multi-modality
and architecture heterogeneity challenges in recommendation domain, NASRec
establishes a large supernet (i.e., search space) to search the full
architectures, with the supernet incorporating versatile operator choices and
dense connectivity minimizing human prior for flexibility. The scale and
heterogeneity in NASRec impose challenges in search, such as training
inefficiency, operator-imbalance, and degraded rank correlation. We tackle
these challenges by proposing single-operator any-connection sampling,
operator-balancing interaction modules, and post-training fine-tuning. Our
results on three Click-Through Rates (CTR) prediction benchmarks show that
NASRec can outperform both manually designed models and existing NAS methods,
achieving state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bug Fix Time Optimization Using Matrix Factorization and Iterative
  Gale-Shaply Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madonna Mayez, Khaled Nagaty, Abeer Hamdy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bug triage is an essential task in software maintenance phase. It assigns
developers (fixers) to bug reports to fix them. This process is performed
manually by a triager, who analyzes developers profiles and submitted bug
reports to make suitable assignments. Bug triaging process is time consuming
thus automating this process is essential to improve the quality of software.
Previous work addressed triaging problem either as an information retrieval or
classification problem. This paper tackles this problem as a resource
allocation problem, that aims at the best assignments of developers to bug
reports, that reduces the total fixing time of the newly submitted bug reports,
in addition to the even distribution of bug reports over developers. In this
paper, a combination of matrix factorization and Gale Shapely algorithm,
supported by the differential evolution is firstly introduced to optimize the
total fix time and normalize developers work load. Matrix factorization is used
to establish a recommendation system for Gale-Shapley to make assignment
decisions. Differential evolution provides the best set of weights to build
developers score profiles. The proposed approach is assessed over three
repositories, Linux, Apache and Eclipse. Experimental results show that the
proposed approach reduces the bug fixing time, in comparison to the manual
triage, by 80.67%, 23.61% and 60.22% over Linux, Eclipse and Apache
respectively. Moreover, the workload for the developers is uniform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 page, 7 figures, 8 tables, 10 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Multi-Interest Network with Stable Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaocheng Liu, Yingtao Luo, Di Zeng, Qiang Liu, Daqing Chang, Dongying Kong, Zhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling users' dynamic preferences from historical behaviors lies at the
core of modern recommender systems. Due to the diverse nature of user
interests, recent advances propose the multi-interest networks to encode
historical behaviors into multiple interest vectors. In real scenarios, the
corresponding items of captured interests are usually retrieved together to get
exposure and collected into training data, which produces dependencies among
interests. Unfortunately, multi-interest networks may incorrectly concentrate
on subtle dependencies among captured interests. Misled by these dependencies,
the spurious correlations between irrelevant interests and targets are
captured, resulting in the instability of prediction results when training and
test distributions do not match. In this paper, we introduce the widely used
Hilbert-Schmidt Independence Criterion (HSIC) to measure the degree of
independence among captured interests and empirically show that the continuous
increase of HSIC may harm model performance. Based on this, we propose a novel
multi-interest network, named DEep Stable Multi-Interest Learning (DESMIL),
which tries to eliminate the influence of subtle dependencies among captured
interests via learning weights for training samples and make model concentrate
more on underlying true causation. We conduct extensive experiments on public
recommendation datasets, a large-scale industrial dataset and the synthetic
datasets which simulate the out-of-distribution data. Experimental results
demonstrate that our proposed DESMIL outperforms state-of-the-art models by a
significant margin. Besides, we also conduct comprehensive model analysis to
reveal the reason why DESMIL works to a certain extent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comparison of latent semantic analysis and correspondence analysis of
  document-term matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Qi, David J. Hessen, Tejaswini Deoskar, Peter G. M. van der Heijden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent semantic analysis (LSA) and correspondence analysis (CA) are two
techniques that use a singular value decomposition (SVD) for dimensionality
reduction. LSA has been extensively used to obtain low-dimensional
representations that capture relationships among documents and terms. In this
article, we present a theoretical analysis and comparison of the two techniques
in the context of document-term matrices. We show that CA has some attractive
properties as compared to LSA, for instance that effects of margins arising
from differing document-lengths and term-frequencies are effectively
eliminated, so that the CA solution is optimally suited to focus on
relationships among documents and terms. A unifying framework is proposed that
includes both CA and LSA as special cases. We empirically compare CA to various
LSA based methods on text categorization in English and authorship attribution
on historical Dutch texts, and find that CA performs significantly better. We
also apply CA to a long-standing question regarding the authorship of the Dutch
national anthem Wilhelmus and provide further support that it can be attributed
to the author Datheen, amongst several contenders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating and Penalizing Induced Preference Shifts in Recommender
  Systems <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micah Carroll, Anca Dragan, Stuart Russell, Dylan Hadfield-Menell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The content that a recommender system (RS) shows to users influences them.
Therefore, when choosing a recommender to deploy, one is implicitly also
choosing to induce specific internal states in users. Even more, systems
trained via long-horizon optimization will have direct incentives to manipulate
users: in this work, we focus on the incentive to shift user preferences so
they are easier to satisfy. We argue that - before deployment - system
designers should: estimate the shifts a recommender would induce; evaluate
whether such shifts would be undesirable; and perhaps even actively optimize to
avoid problematic shifts. These steps involve two challenging ingredients:
estimation requires anticipating how hypothetical algorithms would influence
user preferences if deployed - we do this by using historical user interaction
data to train a predictive user model which implicitly contains their
preference dynamics; evaluation and optimization additionally require metrics
to assess whether such influences are manipulative or otherwise unwanted - we
use the notion of "safe shifts", that define a trust region within which
behavior is safe: for instance, the natural way in which users would shift
without interference from the system could be deemed "safe". In simulated
experiments, we show that our learned preference dynamics model is effective in
estimating user preferences and how they would respond to new recommenders.
Additionally, we show that recommenders that optimize for staying in the trust
region can avoid manipulative behaviors while still generating engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2022 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Image Enhancement Black-Box Methods through a Path Planning
  Based Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Cotogni, Claudio Cusano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, image-to-image translation methods, are the state of the art for
the enhancement of natural images. Even if they usually show high performance
in terms of accuracy, they often suffer from several limitations such as the
generation of artifacts and the scalability to high resolutions. Moreover,
their main drawback is the completely black-box approach that does not allow to
provide the final user with any insight about the enhancement processes
applied. In this paper we present a path planning algorithm which provides a
step-by-step explanation of the output produced by state of the art enhancement
methods, overcoming black-box limitation. This algorithm, called eXIE, uses a
variant of the A* algorithm to emulate the enhancement process of another
method through the application of an equivalent sequence of enhancing
operators. We applied eXIE to explain the output of several state-of-the-art
models trained on the Five-K dataset, obtaining sequences of enhancing
operators able to produce very similar results in terms of performance and
overcoming the huge limitation of poor interpretability of the best performing
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Temporal Action Detection with Proposal-Free Masking <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing temporal action detection (TAD) methods rely on a large number of
training data with segment-level annotations. Collecting and annotating such a
training set is thus highly expensive and unscalable. Semi-supervised TAD
(SS-TAD) alleviates this problem by leveraging unlabeled videos freely
available at scale. However, SS-TAD is also a much more challenging problem
than supervised TAD, and consequently much under-studied. Prior SS-TAD methods
directly combine an existing proposal-based TAD method and a SSL method. Due to
their sequential localization (e.g, proposal generation) and classification
design, they are prone to proposal error propagation. To overcome this
limitation, in this work we propose a novel Semi-supervised Temporal action
detection model based on PropOsal-free Temporal mask (SPOT) with a parallel
localization (mask generation) and classification architecture. Such a novel
design effectively eliminates the dependence between localization and
classification by cutting off the route for error propagation in-between. We
further introduce an interaction mechanism between classification and
localization for prediction refinement, and a new pretext task for
self-supervised model pre-training. Extensive experiments on two standard
benchmarks show that our SPOT outperforms state-of-the-art alternatives, often
by a large margin. The PyTorch implementation of SPOT is available at
https://github.com/sauradip/SPOT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code available at https://github.com/sauradip/SPOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitrack Music <span class="highlight-title">Transformer</span>: Learning Long-Term Dependencies in Music
  with Diverse Instruments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for generating multitrack music with transformer models
have been limited to either a small set of instruments or short music segments.
This is partly due to the memory requirements of the lengthy input sequences
necessitated by existing representations for multitrack music. In this work, we
propose a compact representation that allows a diverse set of instruments while
keeping a short sequence length. Using our proposed representation, we present
the Multitrack Music Transformer (MTMT) for learning long-term dependencies in
multitrack music. In a subjective listening test, our proposed model achieves
competitive quality on unconditioned generation against two baseline models. We
also show that our proposed model can generate samples that are twice as long
as those produced by the baseline models, and, further, can do so in half the
inference time. Moreover, we propose a new measure for analyzing musical
self-attentions and show that the trained model learns to pay less attention to
notes that form a dissonant interval with the current note, yet attending more
to notes that are 4N beats away from current. Finally, our findings provide a
novel foundation for future work exploring longer-form multitrack music
generation and improving self-attentions for music. All source code and audio
samples can be found at https://salu133445.github.io/mtmt/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layout-Aware Information Extraction for Document-Grounded <span class="highlight-title">Dialogue</span>:
  <span class="highlight-title">Dataset</span>, Method and Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Zhang, Bowen Yu, Haiyang Yu, Tingwen Liu, Cheng Fu, Jingyang Li, Chengguang Tang, Jian Sun, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building document-grounded dialogue systems have received growing interest as
documents convey a wealth of human knowledge and commonly exist in enterprises.
Wherein, how to comprehend and retrieve information from documents is a
challenging research problem. Previous work ignores the visual property of
documents and treats them as plain text, resulting in incomplete modality. In
this paper, we propose a Layout-aware document-level Information Extraction
dataset, LIE, to facilitate the study of extracting both structural and
semantic knowledge from visually rich documents (VRDs), so as to generate
accurate responses in dialogue systems. LIE contains 62k annotations of three
extraction tasks from 4,061 pages in product and official documents, becoming
the largest VRD-based information extraction dataset to the best of our
knowledge. We also develop benchmark methods that extend the token-based
language model to consider layout features like humans. Empirical results show
that layout is critical for VRD-based extraction, and system demonstration also
verifies that the extracted knowledge can help locate the answers that users
care about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia (MM) Industry Track 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Action Detection with Global Segmentation Mask Learning <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing temporal action detection (TAD) methods rely on generating an
overwhelmingly large number of proposals per video. This leads to complex model
designs due to proposal generation and/or per-proposal action instance
evaluation and the resultant high computational cost. In this work, for the
first time, we propose a proposal-free Temporal Action detection model with
Global Segmentation mask (TAGS). Our core idea is to learn a global
segmentation mask of each action instance jointly at the full video length. The
TAGS model differs significantly from the conventional proposal-based methods
by focusing on global temporal representation learning to directly detect local
start and end points of action instances without proposals. Further, by
modeling TAD holistically rather than locally at the individual proposal level,
TAGS needs a much simpler model architecture with lower computational cost.
Extensive experiments show that despite its simpler design, TAGS outperforms
existing TAD methods, achieving new state-of-the-art performance on two
benchmarks. Importantly, it is ~ 20x faster to train and ~1.6x more efficient
for inference. Our PyTorch implementation of TAGS is available at
https://github.com/sauradip/TAGS .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2022; Code available at https://github.com/sauradip/TAGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Emotion Contagion on Social Media via Localized Diffusion in
  Dynamic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trisha Mittal, Puneet Mathur, Rohan Chandra, Apurva Bhatt, Vikram Gupta, Debdoot Mukherjee, Aniket Bera, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a computational approach for estimating emotion contagion on
social media networks. Built on a foundation of psychology literature, our
approach estimates the degree to which the perceivers' emotional states
(positive or negative) start to match those of the expressors, based on the
latter's content. We use a combination of deep learning and social network
analysis to model emotion contagion as a diffusion process in dynamic social
network graphs, taking into consideration key aspects like causality,
homophily, and interference. We evaluate our approach on user behavior data
obtained from a popular social media platform for sharing short videos. We
analyze the behavior of 48 users over a span of 8 weeks (over 200k audio-visual
short posts analyzed) and estimate how contagious the users with whom they
engage with are on social media. As per the theory of diffusion, we account for
the videos a user watches during this time (inflow) and the daily engagements;
liking, sharing, downloading or creating new videos (outflow) to estimate
contagion. To validate our approach and analysis, we obtain human feedback on
these 48 social media platform users with an online study by collecting
responses of about 150 participants. We report users who interact with more
number of creators on the platform are 12% less prone to contagion, and those
who consume more content of `negative' sentiment are 23% more prone to
contagion. We will publicly release our code upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-guided Album Cover Art Generation with Genetic Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Marien, Sam Leroux, Bart Dhoedt, Cedric De Boom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over 60,000 songs are released on Spotify every day, and the competition for
the listener's attention is immense. In that regard, the importance of
captivating and inviting cover art cannot be underestimated, because it is
deeply entangled with a song's character and the artist's identity, and remains
one of the most important gateways to lead people to discover music. However,
designing cover art is a highly creative, lengthy and sometimes expensive
process that can be daunting, especially for non-professional artists. For this
reason, we propose a novel deep-learning framework to generate cover art guided
by audio features. Inspired by VQGAN-CLIP, our approach is highly flexible
because individual components can easily be replaced without the need for any
retraining. This paper outlines the architectural details of our models and
discusses the optimization challenges that emerge from them. More specifically,
we will exploit genetic algorithms to overcome bad local minima and adversarial
examples. We find that our framework can generate suitable cover art for most
genres, and that the visual features adapt themselves to audio feature changes.
Given these results, we believe that our framework paves the road for
extensions and more advanced applications in audio-guided visual generation
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video
  Retrieval <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Aozhu Chen, Ziyue Wang, Fang<span class="highlight-author">ming Zhou</span>, Jianfeng Dong, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we revisit \emph{feature fusion}, an old-fashioned topic, in
the new context of text-to-video retrieval. Different from previous research
that considers feature fusion only at one end, let it be video or text, we aim
for feature fusion for both ends within a unified framework. We hypothesize
that optimizing the convex combination of the features is preferred to modeling
their correlations by computationally heavy multi-head self attention. We
propose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature
fusion at both early and late stages and at both video and text ends, making it
a powerful method for exploiting diverse (off-the-shelf) features. The
interpretability of LAFF can be used for feature selection. Extensive
experiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and
TRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech-enhanced and Noise-aware Networks for Robust Speech Recognition <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung-Shin Lee, Pin-Yuan Chen, Yao-Fei Cheng, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compensation for channel mismatch and noise interference is essential for
robust automatic speech recognition. Enhanced speech has been introduced into
the multi-condition training of acoustic models to improve their generalization
ability. In this paper, a noise-aware training framework based on two cascaded
neural structures is proposed to jointly optimize speech enhancement and speech
recognition. The feature enhancement module is composed of a multi-task
autoencoder, where noisy speech is decomposed into clean speech and noise. By
concatenating its enhanced, noise-aware, and noisy features for each frame, the
acoustic-modeling module maps each feature-augmented frame into a triphone
state by optimizing the lattice-free maximum mutual information and cross
entropy between the predicted and actual state sequences. On top of the
factorized time delay neural network (TDNN-F) and its convolutional variant
(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error
rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared
with the best existing systems that use bigram and trigram language models for
decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction
of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based
system also outperforms the baseline CNN-TDNNF system on the AMI task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ISCSLP 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>
<br>
<footer>
    <div>
        <time id="build-timestamp" datetime="2022-07-22T05:36:54.339735775Z">
            <a href="https://github.com/LooperXX/ArxivDaily/actions">
                <img id="build-timestamp-badge"
                     src="https://img.shields.io/github/workflow/status/looperxx/arxivdaily/ArxivDaily?label=2022-07-22 05:36:54 UTC&style=for-the-badge"
                alt="2022-07-22 05:36:54 UTC">
            </a>
        </time>
    </div>
</footer>
<br>
<script src="index.js"></script>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=386&t=tt&d=sDvlbgmeTw_E_GoVDGdggVOFT21w54hFtP9VETatnEM&cmo=ff4242&cmn=3dd13d"></script>
</html>
